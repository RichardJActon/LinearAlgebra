# Applications of Least Squares {#lsapp}

```{r, echo=F}
thmcounter=0
excounter=0
cid='lsapp'
```

## Simple Linear Regression
### Cars Data

The `cars' dataset is included in the datasets package. This dataset contains observations of speed and stopping distance for 50 cars. We can take a look at the summary statistics by using the **summary** function.

```{r}
summary(cars)
```

We can plot these two variables as follows:

```{r fig=T, fig.width=5, fig.align = 'center'}
plot(cars)
```

### Setting up the Normal Equations

Let's set up a system of equations
$$\mathbf{X}\boldsymbol\beta=\mathbf{y}$$
to create the model
$$stopping\_distance=\beta_0+\beta_1speed.$$

To do this, we need a design matrix $\mathbf{X}$ containing a column of ones for the intercept term and a column containing the speed variable. We also need a vector $\mathbf{y}$ containing the corresponding stopping distances. The ``` model.matrix()``` function will be of use to us here. ``` model.matrix()``` takes a formula and data matrix as input and exports the matrix that we represent as $\mathbf{X}$ in the normal equations. For datasets with categorical (factor) inputs, this function would also create dummy variables for each level, leaving out a reference level by default. You can override the default to leave out a reference level (when you override this default, you **one-hot-encode** your categorical variable) by including the following option as a third input to the function: ``` contrasts.arg = lapply(starwars[,sapply(starwars,is.factor) ], contrasts, contrasts=FALSE ```

```{r}
# Create matrix X and label the columns
X=model.matrix(dist~speed, data=cars)

# Create vector y and label the column
y=cars$dist
```

Let's print the first 10 rows of each to see what we did:

```{r}
# Show first 10 rows, all columns. To show only observations 2,4, and 7, for
# example, the code would be X[c(2,4,7), ]
X[1:10, ]
y[1:10]
```

### Solving for Parameter Estimates and Statistics

Now lets find our parameter estimates by solving the normal equations,
$$\mathbf{X}^T\mathbf{X}\boldsymbol\beta = \mathbf{X}^T\mathbf{y}$$
using the built in **solve** function. To solve the system $\mathbf{A}\mathbf{x}=\mathbf{b}$ we'd use ``` solve(A,b)```.

```{r}
(beta=solve(t(X) %*% X ,t(X)%*%y))
```

At the same time we can compute the residuals,
$$\mathbf{r}=\mathbf{y}-\mathbf{\hat{y}}$$
the total sum of squares (SST),
$$\sum_{i=1}^n (y-\bar{y})^2=(\mathbf{y}-\mathbf{\bar{y}})^T(\mathbf{y}-\mathbf{\bar{y}})=\|\mathbf{y}-\mathbf{\bar{y}}\|^2$$
the regression sum of squares (SSR or SSM)
$$\sum_{i=1}^n (\hat{y}-\bar{y})^2=(\mathbf{\hat{y}}-\mathbf{\bar{y}})^T(\mathbf{\hat{y}}-\mathbf{\bar{y}})=\|\mathbf{\hat{y}}-\mathbf{\bar{y}}\|^2$$
the residual sum of squares (SSE)
$$\sum_{i=1}^n r_i =\mathbf{r}^T\mathbf{r}=\|\mathbf{r}\|^2$$
and the unbiased estimator of the variance of the residuals, using the model degrees of freedom which is $n-2=48$:
$$\widehat{\sigma_{\epsilon}}^2 =\frac{SSE}{d.f.} = \frac{\|\mathbf{r}\|^2}{48}$$

Then $R^2$:
$$R^2 = \frac{SSR}{SST}$$

We can also compute the standard error of $\widehat{\boldsymbol\beta}$ since

\begin{eqnarray*}
\widehat{\boldsymbol\beta} &=& (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\\
var(\widehat{\boldsymbol\beta})&=&var((\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y})\\
 &=&(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T var(\mathbf{y}) \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1} \\
 &=&(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T (\widehat{\sigma_{\epsilon}}^2) \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1} \\
  &=& \widehat{\sigma_{\epsilon}}^2 (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1} \\
  &=& \widehat{\sigma_{\epsilon}}^2(\mathbf{X}^T\mathbf{X})^{-1}
\end{eqnarray*}

Thus, the variances of each $\widehat\beta$ are given by the diagonal elements of the covariance matrix, and the standard errors of each $\widehat\beta$ are given by the square roots of these diagonal elements:
$$s.e.(\widehat{\beta_i})=\sqrt{\widehat{\sigma_{\epsilon}}[(\mathbf{X}^T\mathbf{X})^{-1}]_{ii}}$$

```{r results='hide'}
meany=mean(y)
XXinv=solve(t(X)%*%X)
yhat=X%*%XXinv%*%t(X)%*%y
resid=y-yhat
SStotal=norm(y-meany,type="2")^2
### OR  SStotal=t(y-meany)%*%(y-meany)
SSreg=norm(yhat-meany,type="2")^2
### OR  SSreg=t(yhat-meany)%*%(yhat-meany)
SSresid=norm(resid,type="2")^2
### OR SSresid=t(resid)%*%resid
Rsquared=SSreg/SStotal
StdErrorResiduals=norm(resid/sqrt(48), type="2") #=sqrt(SSresid/48)
CovBeta=SSresid*XXinv/48
StdErrorIntercept = sqrt(CovBeta[1,1])
StdErrorSlope = sqrt(CovBeta[2,2])
```

```{r echo=F}
sprintf("Rsquared: %s",Rsquared)
sprintf("SSresid: %s",SSresid)
sprintf("SSmodel: %s",SSreg)
sprintf("StdErrorResiduals: %s",StdErrorResiduals)
sprintf("StdErrorIntercept: %s",StdErrorIntercept)
sprintf("StdErrorIntercept: %s",StdErrorSlope)
```

Let's plot our regression line over the original data:

```{r fig=T,fig.width=7, fig.height=6}
plot(cars)
abline(beta[1],beta[2],col='blue')
```


### OLS in R via ```lm()```

Finally, let's compare our results to the built in linear model solver, ``` lm()```:

```{r}
fit = lm(dist ~ speed, data=cars)
summary(fit)
anova(fit)
```

## Multiple Linear Regression

### Bike Sharing Dataset
