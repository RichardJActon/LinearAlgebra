<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Matrix Arithmetic | Linear Algebra for Data Science</title>
  <meta name="description" content="A traditional textbook fused with a collection of data science case studies that was engineered to weave practicality and applied problem solving into a linear algebra curriculum" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Matrix Arithmetic | Linear Algebra for Data Science" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A traditional textbook fused with a collection of data science case studies that was engineered to weave practicality and applied problem solving into a linear algebra curriculum" />
  <meta name="github-repo" content="shainarace/linearalgebra" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Matrix Arithmetic | Linear Algebra for Data Science" />
  
  <meta name="twitter:description" content="A traditional textbook fused with a collection of data science case studies that was engineered to weave practicality and applied problem solving into a linear algebra curriculum" />
  

<meta name="author" content="Shaina Race Bennett, PhD" />


<meta name="date" content="2021-08-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="intro.html"/>
<link rel="next" href="multapp.html"/>
<script src="libs/header-attrs-2.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.4.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.57.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.57.1/plotly-latest.min.js"></script>
<script src="libs/d3-4.5.0/d3.min.js"></script>
<script src="libs/forceNetwork-binding-0.4/forceNetwork.js"></script>
<!DOCTYPE html>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  loader: {load: ['[tex]/cancel', '[tex]/systeme','[tex]/require']},
  TeX: {
    packages: {'[+]': ['cancel','systeme','boldsymbol','require']}
  }
});
</script>


<script type="text/javascript" id="MathJax-script"
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

<span class="math" style="display:none">
\(\usepackage{amsfonts}
\usepackage{cancel}
\usepackage{amsmath}
\usepackage{systeme}
\usepackage{amsthm}
\usepackage{xcolor}
\usepackage{boldsymbol}
\newenvironment{am}[1]{%
  \left(\begin{array}{@{}*{#1}{c}|c@{}}
}{%
  \end{array}\right)
}
\newcommand{\bordermatrix}[3]{\begin{matrix} ~ & \begin{matrix} #1 \end{matrix} \\ \begin{matrix} #2 \end{matrix}\hspace{-1em} & #3 \end{matrix}}
\newcommand{\eref}[1]{Example~\ref{#1}}
\newcommand{\fref}[1]{Figure~\ref{#1}}
\newcommand{\tref}[1]{Table~\ref{#1}}
\newcommand{\sref}[1]{Section~\ref{#1}}
\newcommand{\cref}[1]{Chapter~\ref{#1}}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{fact}{Fact}
\newtheorem{thm}{Theorem}
\newtheorem{example}{Example}[section]
\newcommand{\To}{\Rightarrow}
\newcommand{\del}{\nabla}
\renewcommand{\Re}{\mathbb{R}}
\renewcommand{\O}{\mathcal{O}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\eps}{\epsilon}
\newcommand{\cont}{\Rightarrow \Leftarrow}
\newcommand{\back}{\backslash}
\newcommand{\norm}[1]{\|{#1}\|}
\newcommand{\abs}[1]{|{#1}|}
\newcommand{\ip}[1]{\langle{#1}\rangle}
\newcommand{\bo}{\mathbf}
\newcommand{\mean}{\boldsymbol\mu}
\newcommand{\cov}{\boldsymbol\Sigma}
\newcommand{\wt}{\widetilde}
\newcommand{\p}{\textbf{p}}
\newcommand{\ff}{\textbf{f}}
\newcommand{\aj}{\textbf{a}_j}
\newcommand{\ajhat}{\widehat{\textbf{a}_j}}
\newcommand{\I}{\textbf{I}}
\newcommand{\A}{\textbf{A}}
\newcommand{\B}{\textbf{B}}
\newcommand{\bL}{\textbf{L}}
\newcommand{\bP}{\textbf{P}}
\newcommand{\bD}{\textbf{D}}
\newcommand{\bS}{\textbf{S}}
\newcommand{\bW}{\textbf{W}}
\newcommand{\id}{\textbf{I}}
\newcommand{\M}{\textbf{M}}
\renewcommand{\B}{\textbf{B}}
\newcommand{\V}{\textbf{V}}
\newcommand{\U}{\textbf{U}}
\newcommand{\y}{\textbf{y}}
\newcommand{\bv}{\textbf{v}}
\renewcommand{\v}{\textbf{v}}
\newcommand{\cC}{\mathscr{C}}
\newcommand{\e}{\textbf{e}}
\newcommand{\w}{\textbf{w}}
\newcommand{\h}{\textbf{h}}
\renewcommand{\b}{\textbf{b}}
\renewcommand{\a}{\textbf{a}}
\renewcommand{\u}{\textbf{u}}
\newcommand{\C}{\textbf{C}}
\newcommand{\D}{\textbf{D}}
\newcommand{\cc}{\textbf{c}}
\newcommand{\Q}{\textbf{Q}}
\renewcommand{\S}{\textbf{S}}
\newcommand{\X}{\textbf{X}}
\newcommand{\Z}{\textbf{Z}}
\newcommand{\z}{\textbf{z}}
\newcommand{\Y}{\textbf{Y}}
\newcommand{\plane}{\textit{P}}
\newcommand{\mxn}{$m\mbox{x}n$}
\newcommand{\kmeans}{\textit{k}-means\,}
\newcommand{\bbeta}{\boldsymbol\beta}
\newcommand{\ssigma}{\boldsymbol\Sigma}
\newcommand{\xrow}[1]{\mathbf{X}_{{#1}\star}}
\newcommand{\xcol}[1]{\mathbf{X}_{\star{#1}}}
\newcommand{\yrow}[1]{\mathbf{Y}_{{#1}\star}}
\newcommand{\ycol}[1]{\mathbf{Y}_{\star{#1}}}
\newcommand{\crow}[1]{\mathbf{C}_{{#1}\star}}
\newcommand{\ccol}[1]{\mathbf{C}_{\star{#1}}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\arow}[1]{\mathbf{A}_{{#1}\star}}
\newcommand{\acol}[1]{\mathbf{A}_{\star{#1}}}
\newcommand{\brow}[1]{\mathbf{B}_{{#1}\star}}
\newcommand{\bcol}[1]{\mathbf{B}_{\star{#1}}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\renewcommand{\t}{ \indent}
\newcommand{\nt}{ \indent}
\newcommand{\x}{\mathbf{x}}
\renewcommand{\Y}{\mathbf{Y}}
\newcommand{\ep}{\mathbf{\epsilon}}
\renewcommand{\pm}{\left(\begin{matrix}}
\renewcommand{\mp}{\end{matrix}\right)}
\newcommand{\bm}{\bordermatrix}
\usepackage{pdfpages,cancel}
\newenvironment{code}{\Verbatim [formatcom=\color{blue}]}{\endVerbatim}
\)
</span>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><center><img src="figs/matrixlogo.jpg" width="50"></center></li>
<li><center><strong> Linear Algebra for Data Science </strong></center></li>
<li><center><strong> with examples in R </strong></center></li>

<li class="divider"></li>
<li><a href="index.html#section"></a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#structure-of-the-book"><i class="fa fa-check"></i>Structure of the book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i>About the author</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#what-is-linear-algebra"><i class="fa fa-check"></i><b>1.1</b> What is Linear Algebra?</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#why-linear-algebra"><i class="fa fa-check"></i><b>1.2</b> Why Linear Algebra</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#describing-matrices-and-vectors"><i class="fa fa-check"></i><b>1.3</b> Describing Matrices and Vectors</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="intro.html"><a href="intro.html#dimensionsize-of-a-matrix"><i class="fa fa-check"></i><b>1.3.1</b> Dimension/Size of a Matrix</a></li>
<li class="chapter" data-level="1.3.2" data-path="intro.html"><a href="intro.html#ij-notation"><i class="fa fa-check"></i><b>1.3.2</b> <span class="math inline">\((i,j)\)</span> Notation</a></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#example-defining-social-networks"><i class="fa fa-check"></i>Example: Defining social networks</a></li>
<li class="chapter" data-level="1.3.3" data-path="intro.html"><a href="intro.html#introcorr"><i class="fa fa-check"></i><b>1.3.3</b> Example: Correlation matrices</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#vectors"><i class="fa fa-check"></i><b>1.4</b> Vectors</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="intro.html"><a href="intro.html#vector-geometry-n-space"><i class="fa fa-check"></i><b>1.4.1</b> Vector Geometry: <span class="math inline">\(n\)</span>-space</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#matrix-operations"><i class="fa fa-check"></i><b>1.5</b> Matrix Operations</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="intro.html"><a href="intro.html#transpose"><i class="fa fa-check"></i><b>1.5.1</b> Transpose</a></li>
<li class="chapter" data-level="1.5.2" data-path="intro.html"><a href="intro.html#trace-of-a-matrix"><i class="fa fa-check"></i><b>1.5.2</b> Trace of a Matrix</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#special"><i class="fa fa-check"></i><b>1.6</b> Special Matrices and Vectors</a></li>
<li class="chapter" data-level="1.7" data-path="intro.html"><a href="intro.html#summary-of-conventional-notation"><i class="fa fa-check"></i><b>1.7</b> Summary of Conventional Notation</a></li>
<li class="chapter" data-level="1.8" data-path="intro.html"><a href="intro.html#exercises"><i class="fa fa-check"></i><b>1.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="mult.html"><a href="mult.html"><i class="fa fa-check"></i><b>2</b> Matrix Arithmetic</a>
<ul>
<li class="chapter" data-level="2.1" data-path="mult.html"><a href="mult.html#matrix-addition-subtraction-and-scalar-multiplication"><i class="fa fa-check"></i><b>2.1</b> Matrix Addition, Subtraction, and Scalar Multiplication</a></li>
<li class="chapter" data-level="2.2" data-path="mult.html"><a href="mult.html#sec:vectoradd"><i class="fa fa-check"></i><b>2.2</b> Geometry of Vector Addition and Scalar Multiplication</a></li>
<li class="chapter" data-level="2.3" data-path="mult.html"><a href="mult.html#linear-combinations"><i class="fa fa-check"></i><b>2.3</b> Linear Combinations</a></li>
<li class="chapter" data-level="2.4" data-path="mult.html"><a href="mult.html#matrix-multiplication"><i class="fa fa-check"></i><b>2.4</b> Matrix Multiplication</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="mult.html"><a href="mult.html#the-inner-product"><i class="fa fa-check"></i><b>2.4.1</b> The Inner Product</a></li>
<li class="chapter" data-level="2.4.2" data-path="mult.html"><a href="mult.html#matrix-product"><i class="fa fa-check"></i><b>2.4.2</b> Matrix Product</a></li>
<li class="chapter" data-level="2.4.3" data-path="mult.html"><a href="mult.html#matrix-vector-product"><i class="fa fa-check"></i><b>2.4.3</b> Matrix-Vector Product</a></li>
<li class="chapter" data-level="" data-path="mult.html"><a href="mult.html#linear-combination-view-of-matrix-products"><i class="fa fa-check"></i>Linear Combination view of Matrix Products</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="mult.html"><a href="mult.html#vector-outer-products"><i class="fa fa-check"></i><b>2.5</b> Vector Outer Products</a></li>
<li class="chapter" data-level="2.6" data-path="mult.html"><a href="mult.html#the-identity-and-the-matrix-inverse"><i class="fa fa-check"></i><b>2.6</b> The Identity and the Matrix Inverse</a></li>
<li class="chapter" data-level="2.7" data-path="mult.html"><a href="mult.html#exercises-1"><i class="fa fa-check"></i><b>2.7</b> Exercises</a></li>
<li class="chapter" data-level="" data-path="mult.html"><a href="mult.html#list-of-key-terms"><i class="fa fa-check"></i>List of Key Terms</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="multapp.html"><a href="multapp.html"><i class="fa fa-check"></i><b>3</b> Applications of Matrix Multiplication</a>
<ul>
<li class="chapter" data-level="3.1" data-path="multapp.html"><a href="multapp.html#systems-of-equations"><i class="fa fa-check"></i><b>3.1</b> Systems of Equations</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="multapp.html"><a href="multapp.html#big-systems-of-equations"><i class="fa fa-check"></i><b>3.1.1</b> <em>Big</em> Systems of Equations</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="multapp.html"><a href="multapp.html#regression-analysis"><i class="fa fa-check"></i><b>3.2</b> Regression Analysis</a></li>
<li class="chapter" data-level="3.3" data-path="multapp.html"><a href="multapp.html#linear-combinations-1"><i class="fa fa-check"></i><b>3.3</b> Linear Combinations</a></li>
<li class="chapter" data-level="3.4" data-path="multapp.html"><a href="multapp.html#multapp-ex"><i class="fa fa-check"></i><b>3.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="r-programming-basics.html"><a href="r-programming-basics.html"><i class="fa fa-check"></i><b>4</b> R Programming Basics</a></li>
<li class="chapter" data-level="5" data-path="solvesys.html"><a href="solvesys.html"><i class="fa fa-check"></i><b>5</b> Solving Systems of Equations</a>
<ul>
<li class="chapter" data-level="5.1" data-path="solvesys.html"><a href="solvesys.html#gaussian-elimination"><i class="fa fa-check"></i><b>5.1</b> Gaussian Elimination</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="solvesys.html"><a href="solvesys.html#row-operations"><i class="fa fa-check"></i><b>5.1.1</b> Row Operations</a></li>
<li class="chapter" data-level="5.1.2" data-path="solvesys.html"><a href="solvesys.html#the-augmented-matrix"><i class="fa fa-check"></i><b>5.1.2</b> The Augmented Matrix</a></li>
<li class="chapter" data-level="5.1.3" data-path="solvesys.html"><a href="solvesys.html#gaussian-elimination-summary"><i class="fa fa-check"></i><b>5.1.3</b> Gaussian Elimination Summary</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="solvesys.html"><a href="solvesys.html#gauss-jordan-elimination"><i class="fa fa-check"></i><b>5.2</b> Gauss-Jordan Elimination</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="solvesys.html"><a href="solvesys.html#gauss-jordan-elimination-summary"><i class="fa fa-check"></i><b>5.2.1</b> Gauss-Jordan Elimination Summary</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="solvesys.html"><a href="solvesys.html#three-types-of-systems"><i class="fa fa-check"></i><b>5.3</b> Three Types of Systems</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="solvesys.html"><a href="solvesys.html#uniquesol"><i class="fa fa-check"></i><b>5.3.1</b> The Unique Solution Case</a></li>
<li class="chapter" data-level="5.3.2" data-path="solvesys.html"><a href="solvesys.html#inconsistent"><i class="fa fa-check"></i><b>5.3.2</b> The Inconsistent Case</a></li>
<li class="chapter" data-level="5.3.3" data-path="solvesys.html"><a href="solvesys.html#infinitesol"><i class="fa fa-check"></i><b>5.3.3</b> The Infinite Solutions Case</a></li>
<li class="chapter" data-level="5.3.4" data-path="solvesys.html"><a href="solvesys.html#matrix-rank"><i class="fa fa-check"></i><b>5.3.4</b> Matrix Rank</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="solvesys.html"><a href="solvesys.html#solving-matrix-equations"><i class="fa fa-check"></i><b>5.4</b> Solving Matrix Equations</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="solvesys.html"><a href="solvesys.html#solving-for-the-inverse-of-a-matrix"><i class="fa fa-check"></i><b>5.4.1</b> Solving for the Inverse of a Matrix</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="solvesys.html"><a href="solvesys.html#exercises-2"><i class="fa fa-check"></i><b>5.5</b> Exercises</a></li>
<li class="chapter" data-level="5.6" data-path="solvesys.html"><a href="solvesys.html#list-of-key-terms-1"><i class="fa fa-check"></i><b>5.6</b> List of Key Terms</a></li>
<li class="chapter" data-level="5.7" data-path="solvesys.html"><a href="solvesys.html#gauss-jordan-elimination-in-r"><i class="fa fa-check"></i><b>5.7</b> Gauss-Jordan Elimination in R</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="norms.html"><a href="norms.html"><i class="fa fa-check"></i><b>6</b> Norms, Similarity, and Distance</a>
<ul>
<li class="chapter" data-level="6.1" data-path="norms.html"><a href="norms.html#sec-norms"><i class="fa fa-check"></i><b>6.1</b> Norms and Distances</a></li>
<li class="chapter" data-level="6.2" data-path="norms.html"><a href="norms.html#other-useful-norms-and-distances"><i class="fa fa-check"></i><b>6.2</b> Other useful norms and distances</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="norms.html"><a href="norms.html#norm-star_1."><i class="fa fa-check"></i><b>6.2.1</b> 1-norm, <span class="math inline">\(\|\star\|_1\)</span>.</a></li>
<li class="chapter" data-level="6.2.2" data-path="norms.html"><a href="norms.html#infty-norm-star_infty."><i class="fa fa-check"></i><b>6.2.2</b> <span class="math inline">\(\infty\)</span>-norm, <span class="math inline">\(\|\star\|_{\infty}\)</span>.</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="norms.html"><a href="norms.html#inner-products"><i class="fa fa-check"></i><b>6.3</b> Inner Products</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="norms.html"><a href="norms.html#covariance"><i class="fa fa-check"></i><b>6.3.1</b> Covariance</a></li>
<li class="chapter" data-level="6.3.2" data-path="norms.html"><a href="norms.html#mahalanobis-distance"><i class="fa fa-check"></i><b>6.3.2</b> Mahalanobis Distance</a></li>
<li class="chapter" data-level="6.3.3" data-path="norms.html"><a href="norms.html#angular-distance"><i class="fa fa-check"></i><b>6.3.3</b> Angular Distance</a></li>
<li class="chapter" data-level="6.3.4" data-path="norms.html"><a href="norms.html#correlation"><i class="fa fa-check"></i><b>6.3.4</b> Correlation</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="norms.html"><a href="norms.html#exercises-3"><i class="fa fa-check"></i><b>6.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="linind.html"><a href="linind.html"><i class="fa fa-check"></i><b>7</b> Linear Independence</a>
<ul>
<li class="chapter" data-level="7.1" data-path="linind.html"><a href="linind.html#linear-independence"><i class="fa fa-check"></i><b>7.1</b> Linear Independence</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="linind.html"><a href="linind.html#determining-linear-independence"><i class="fa fa-check"></i><b>7.1.1</b> Determining Linear Independence</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="linind.html"><a href="linind.html#span"><i class="fa fa-check"></i><b>7.2</b> Span of Vectors</a></li>
<li class="chapter" data-level="7.3" data-path="linind.html"><a href="linind.html#exercises-4"><i class="fa fa-check"></i><b>7.3</b> Exercises</a></li>
<li class="chapter" data-level="" data-path="linind.html"><a href="linind.html#list-of-key-terms-2"><i class="fa fa-check"></i>List of Key Terms</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="basis.html"><a href="basis.html"><i class="fa fa-check"></i><b>8</b> Basis and Change of Basis</a>
<ul>
<li class="chapter" data-level="8.1" data-path="basis.html"><a href="basis.html#exercises-5"><i class="fa fa-check"></i><b>8.1</b> Exercises</a></li>
<li class="chapter" data-level="" data-path="basis.html"><a href="basis.html#list-of-key-terms-3"><i class="fa fa-check"></i>List of Key Terms</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="orthog.html"><a href="orthog.html"><i class="fa fa-check"></i><b>9</b> Orthogonality</a>
<ul>
<li class="chapter" data-level="9.1" data-path="orthog.html"><a href="orthog.html#orthonormal-basis"><i class="fa fa-check"></i><b>9.1</b> Orthonormal Basis</a></li>
<li class="chapter" data-level="9.2" data-path="orthog.html"><a href="orthog.html#orthogonal-projection"><i class="fa fa-check"></i><b>9.2</b> Orthogonal Projection</a></li>
<li class="chapter" data-level="9.3" data-path="orthog.html"><a href="orthog.html#why"><i class="fa fa-check"></i><b>9.3</b> Why??</a></li>
<li class="chapter" data-level="9.4" data-path="orthog.html"><a href="orthog.html#exercises-6"><i class="fa fa-check"></i><b>9.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="leastsquares.html"><a href="leastsquares.html"><i class="fa fa-check"></i><b>10</b> Least Squares</a>
<ul>
<li class="chapter" data-level="10.1" data-path="leastsquares.html"><a href="leastsquares.html#introducing-error"><i class="fa fa-check"></i><b>10.1</b> Introducing Error</a></li>
<li class="chapter" data-level="10.2" data-path="leastsquares.html"><a href="leastsquares.html#why-the-normal-equations"><i class="fa fa-check"></i><b>10.2</b> Why the normal equations?</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="leastsquares.html"><a href="leastsquares.html#geometrical-interpretation"><i class="fa fa-check"></i><b>10.2.1</b> Geometrical Interpretation</a></li>
<li class="chapter" data-level="10.2.2" data-path="leastsquares.html"><a href="leastsquares.html#calculus-derivation"><i class="fa fa-check"></i><b>10.2.2</b> Calculus Derivation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="lsapp.html"><a href="lsapp.html"><i class="fa fa-check"></i><b>11</b> Applications of Least Squares</a>
<ul>
<li class="chapter" data-level="11.1" data-path="lsapp.html"><a href="lsapp.html#simple-linear-regression"><i class="fa fa-check"></i><b>11.1</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="lsapp.html"><a href="lsapp.html#cars-data"><i class="fa fa-check"></i><b>11.1.1</b> Cars Data</a></li>
<li class="chapter" data-level="11.1.2" data-path="lsapp.html"><a href="lsapp.html#setting-up-the-normal-equations"><i class="fa fa-check"></i><b>11.1.2</b> Setting up the Normal Equations</a></li>
<li class="chapter" data-level="11.1.3" data-path="lsapp.html"><a href="lsapp.html#solving-for-parameter-estimates-and-statistics"><i class="fa fa-check"></i><b>11.1.3</b> Solving for Parameter Estimates and Statistics</a></li>
<li class="chapter" data-level="11.1.4" data-path="lsapp.html"><a href="lsapp.html#ols-in-r-via-lm"><i class="fa fa-check"></i><b>11.1.4</b> OLS in R via <code>lm()</code></a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="lsapp.html"><a href="lsapp.html#multiple-linear-regression"><i class="fa fa-check"></i><b>11.2</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="lsapp.html"><a href="lsapp.html#bike-sharing-dataset"><i class="fa fa-check"></i><b>11.2.1</b> Bike Sharing Dataset</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="eigen.html"><a href="eigen.html"><i class="fa fa-check"></i><b>12</b> Eigenvalues and Eigenvectors</a>
<ul>
<li class="chapter" data-level="12.1" data-path="eigen.html"><a href="eigen.html#diagonalization"><i class="fa fa-check"></i><b>12.1</b> Diagonalization</a></li>
<li class="chapter" data-level="12.2" data-path="eigen.html"><a href="eigen.html#geometric-interpretation-of-eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>12.2</b> Geometric Interpretation of Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="12.3" data-path="eigen.html"><a href="eigen.html#exercises-7"><i class="fa fa-check"></i><b>12.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>13</b> Principal Components Analysis</a>
<ul>
<li class="chapter" data-level="13.1" data-path="pca.html"><a href="pca.html#gods-flashlight"><i class="fa fa-check"></i><b>13.1</b> God’s Flashlight</a></li>
<li class="chapter" data-level="13.2" data-path="pca.html"><a href="pca.html#pca-details"><i class="fa fa-check"></i><b>13.2</b> PCA Details</a></li>
<li class="chapter" data-level="13.3" data-path="pca.html"><a href="pca.html#geometrical-comparison-with-least-squares"><i class="fa fa-check"></i><b>13.3</b> Geometrical comparison with Least Squares</a></li>
<li class="chapter" data-level="13.4" data-path="pca.html"><a href="pca.html#covariance-or-correlation-matrix"><i class="fa fa-check"></i><b>13.4</b> Covariance or Correlation Matrix?</a></li>
<li class="chapter" data-level="13.5" data-path="pca.html"><a href="pca.html#pca-in-r"><i class="fa fa-check"></i><b>13.5</b> PCA in R</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="pca.html"><a href="pca.html#covariance-pca"><i class="fa fa-check"></i><b>13.5.1</b> Covariance PCA</a></li>
<li class="chapter" data-level="13.5.2" data-path="pca.html"><a href="pca.html#principal-components-loadings-and-variance-explained"><i class="fa fa-check"></i><b>13.5.2</b> Principal Components, Loadings, and Variance Explained</a></li>
<li class="chapter" data-level="13.5.3" data-path="pca.html"><a href="pca.html#scores-and-pca-projection"><i class="fa fa-check"></i><b>13.5.3</b> Scores and PCA Projection</a></li>
<li class="chapter" data-level="13.5.4" data-path="pca.html"><a href="pca.html#pca-functions-in-r"><i class="fa fa-check"></i><b>13.5.4</b> PCA functions in R</a></li>
<li class="chapter" data-level="13.5.5" data-path="pca.html"><a href="pca.html#the-biplot"><i class="fa fa-check"></i><b>13.5.5</b> The Biplot</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="pca.html"><a href="pca.html#variable-clustering-with-pca"><i class="fa fa-check"></i><b>13.6</b> Variable Clustering with PCA</a>
<ul>
<li class="chapter" data-level="13.6.1" data-path="pca.html"><a href="pca.html#correlation-pca"><i class="fa fa-check"></i><b>13.6.1</b> Correlation PCA</a></li>
<li class="chapter" data-level="13.6.2" data-path="pca.html"><a href="pca.html#which-projection-is-better"><i class="fa fa-check"></i><b>13.6.2</b> Which Projection is Better?</a></li>
<li class="chapter" data-level="13.6.3" data-path="pca.html"><a href="pca.html#beware-of-biplots"><i class="fa fa-check"></i><b>13.6.3</b> Beware of biplots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="pcaapp.html"><a href="pcaapp.html"><i class="fa fa-check"></i><b>14</b> Applications of Principal Components</a>
<ul>
<li class="chapter" data-level="14.1" data-path="pcaapp.html"><a href="pcaapp.html#dimension-reduction"><i class="fa fa-check"></i><b>14.1</b> Dimension reduction</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="pcaapp.html"><a href="pcaapp.html#feature-selection"><i class="fa fa-check"></i><b>14.1.1</b> Feature Selection</a></li>
<li class="chapter" data-level="14.1.2" data-path="pcaapp.html"><a href="pcaapp.html#feature-extraction"><i class="fa fa-check"></i><b>14.1.2</b> Feature Extraction</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="pcaapp.html"><a href="pcaapp.html#exploratory-analysis"><i class="fa fa-check"></i><b>14.2</b> Exploratory Analysis</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="pcaapp.html"><a href="pcaapp.html#uk-food-consumption"><i class="fa fa-check"></i><b>14.2.1</b> UK Food Consumption</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="pcaapp.html"><a href="pcaapp.html#fifa-soccer-players"><i class="fa fa-check"></i><b>14.3</b> FIFA Soccer Players</a></li>
<li class="chapter" data-level="14.4" data-path="pcaapp.html"><a href="pcaapp.html#cancer-genetics"><i class="fa fa-check"></i><b>14.4</b> Cancer Genetics</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="pcaapp.html"><a href="pcaapp.html#computing-the-pca"><i class="fa fa-check"></i><b>14.4.1</b> Computing the PCA</a></li>
<li class="chapter" data-level="14.4.2" data-path="pcaapp.html"><a href="pcaapp.html#d-plot-with-package"><i class="fa fa-check"></i><b>14.4.2</b> 3D plot with  package</a></li>
<li class="chapter" data-level="14.4.3" data-path="pcaapp.html"><a href="pcaapp.html#d-plot-with-package-1"><i class="fa fa-check"></i><b>14.4.3</b> 3D plot with  package</a></li>
<li class="chapter" data-level="14.4.4" data-path="pcaapp.html"><a href="pcaapp.html#variance-explained"><i class="fa fa-check"></i><b>14.4.4</b> Variance explained</a></li>
<li class="chapter" data-level="14.4.5" data-path="pcaapp.html"><a href="pcaapp.html#using-correlation-pca"><i class="fa fa-check"></i><b>14.4.5</b> Using Correlation PCA</a></li>
<li class="chapter" data-level="14.4.6" data-path="pcaapp.html"><a href="pcaapp.html#range-standardization-as-an-alternative-to-covariance-pca"><i class="fa fa-check"></i><b>14.4.6</b> Range standardization as an alternative to covariance PCA</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="svd.html"><a href="svd.html"><i class="fa fa-check"></i><b>15</b> The Singular Value Decomposition (SVD)</a>
<ul>
<li class="chapter" data-level="15.1" data-path="svd.html"><a href="svd.html#resolving-a-matrix-into-components"><i class="fa fa-check"></i><b>15.1</b> Resolving a Matrix into Components</a></li>
<li class="chapter" data-level="15.2" data-path="svd.html"><a href="svd.html#data-compression"><i class="fa fa-check"></i><b>15.2</b> Data Compression</a></li>
<li class="chapter" data-level="15.3" data-path="svd.html"><a href="svd.html#noise-reduction"><i class="fa fa-check"></i><b>15.3</b> Noise Reduction</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="svdapp.html"><a href="svdapp.html"><i class="fa fa-check"></i><b>16</b> Applications of SVD</a>
<ul>
<li class="chapter" data-level="16.1" data-path="svdapp.html"><a href="svdapp.html#tm"><i class="fa fa-check"></i><b>16.1</b> Text Mining</a>
<ul>
<li class="chapter" data-level="16.1.1" data-path="svdapp.html"><a href="svdapp.html#note-about-rows-vs.-columns"><i class="fa fa-check"></i><b>16.1.1</b> Note About Rows vs. Columns</a></li>
<li class="chapter" data-level="16.1.2" data-path="svdapp.html"><a href="svdapp.html#term-weighting"><i class="fa fa-check"></i><b>16.1.2</b> Term Weighting</a></li>
<li class="chapter" data-level="16.1.3" data-path="svdapp.html"><a href="svdapp.html#other-considerations"><i class="fa fa-check"></i><b>16.1.3</b> Other Considerations</a></li>
<li class="chapter" data-level="16.1.4" data-path="svdapp.html"><a href="svdapp.html#latent-semantic-indexing"><i class="fa fa-check"></i><b>16.1.4</b> Latent Semantic Indexing</a></li>
<li class="chapter" data-level="16.1.5" data-path="svdapp.html"><a href="svdapp.html#example"><i class="fa fa-check"></i><b>16.1.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="svdapp.html"><a href="svdapp.html#rappasvd"><i class="fa fa-check"></i><b>16.2</b> Image Compression</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="svdapp.html"><a href="svdapp.html#image-data-in-r"><i class="fa fa-check"></i><b>16.2.1</b> Image data in R</a></li>
<li class="chapter" data-level="16.2.2" data-path="svdapp.html"><a href="svdapp.html#computing-the-svd-of-dr.-rappa"><i class="fa fa-check"></i><b>16.2.2</b> Computing the SVD of Dr. Rappa</a></li>
<li class="chapter" data-level="16.2.3" data-path="svdapp.html"><a href="svdapp.html#the-noise"><i class="fa fa-check"></i><b>16.2.3</b> The Noise</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="fa.html"><a href="fa.html"><i class="fa fa-check"></i><b>17</b> Factor Analysis</a>
<ul>
<li class="chapter" data-level="17.1" data-path="fa.html"><a href="fa.html#assumptions-of-factor-analysis"><i class="fa fa-check"></i><b>17.1</b> Assumptions of Factor Analysis</a></li>
<li class="chapter" data-level="17.2" data-path="fa.html"><a href="fa.html#determining-factorability"><i class="fa fa-check"></i><b>17.2</b> Determining Factorability</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="fa.html"><a href="fa.html#visual-examination-of-correlation-matrix"><i class="fa fa-check"></i><b>17.2.1</b> Visual Examination of Correlation Matrix</a></li>
<li class="chapter" data-level="17.2.2" data-path="fa.html"><a href="fa.html#barletts-sphericity-test"><i class="fa fa-check"></i><b>17.2.2</b> Barlett’s Sphericity Test</a></li>
<li class="chapter" data-level="17.2.3" data-path="fa.html"><a href="fa.html#kaiser-meyer-olkin-kmo-measure-of-sampling-adequacy"><i class="fa fa-check"></i><b>17.2.3</b> Kaiser-Meyer-Olkin (KMO) Measure of Sampling Adequacy</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="fa.html"><a href="fa.html#communalities"><i class="fa fa-check"></i><b>17.3</b> Communalities</a></li>
<li class="chapter" data-level="17.4" data-path="fa.html"><a href="fa.html#number-of-factors"><i class="fa fa-check"></i><b>17.4</b> Number of Factors</a></li>
<li class="chapter" data-level="17.5" data-path="fa.html"><a href="fa.html#rotation-of-factors"><i class="fa fa-check"></i><b>17.5</b> Rotation of Factors</a></li>
<li class="chapter" data-level="17.6" data-path="fa.html"><a href="fa.html#fa-apps"><i class="fa fa-check"></i><b>17.6</b> Methods of Factor Analysis</a>
<ul>
<li class="chapter" data-level="17.6.1" data-path="fa.html"><a href="fa.html#pca-rotations"><i class="fa fa-check"></i><b>17.6.1</b> PCA Rotations</a></li>
</ul></li>
<li class="chapter" data-level="17.7" data-path="fa.html"><a href="fa.html#case-study-personality-tests"><i class="fa fa-check"></i><b>17.7</b> Case Study: Personality Tests</a>
<ul>
<li class="chapter" data-level="17.7.1" data-path="fa.html"><a href="fa.html#raw-pca-factors"><i class="fa fa-check"></i><b>17.7.1</b> Raw PCA Factors</a></li>
<li class="chapter" data-level="17.7.2" data-path="fa.html"><a href="fa.html#rotated-principal-components"><i class="fa fa-check"></i><b>17.7.2</b> Rotated Principal Components</a></li>
<li class="chapter" data-level="17.7.3" data-path="fa.html"><a href="fa.html#visualizing-rotation-via-biplots"><i class="fa fa-check"></i><b>17.7.3</b> Visualizing Rotation via BiPlots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="otherdimred.html"><a href="otherdimred.html"><i class="fa fa-check"></i><b>18</b> Dimension Reduction for Visualization</a>
<ul>
<li class="chapter" data-level="18.1" data-path="otherdimred.html"><a href="otherdimred.html#multidimensional-scaling"><i class="fa fa-check"></i><b>18.1</b> Multidimensional Scaling</a>
<ul>
<li class="chapter" data-level="18.1.1" data-path="otherdimred.html"><a href="otherdimred.html#mds-of-iris-data"><i class="fa fa-check"></i><b>18.1.1</b> MDS of Iris Data</a></li>
<li class="chapter" data-level="18.1.2" data-path="otherdimred.html"><a href="otherdimred.html#mds-of-leukemia-dataset"><i class="fa fa-check"></i><b>18.1.2</b> MDS of Leukemia dataset</a></li>
<li class="chapter" data-level="" data-path="otherdimred.html"><a href="otherdimred.html#a-note-on-standardization"><i class="fa fa-check"></i>A note on standardization</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="sna.html"><a href="sna.html"><i class="fa fa-check"></i><b>19</b> Social Network Analysis</a>
<ul>
<li class="chapter" data-level="19.1" data-path="sna.html"><a href="sna.html#working-with-network-data"><i class="fa fa-check"></i><b>19.1</b> Working with Network Data</a></li>
<li class="chapter" data-level="19.2" data-path="sna.html"><a href="sna.html#network-visualization---igraph-package"><i class="fa fa-check"></i><b>19.2</b> Network Visualization - <code>igraph</code> package</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="sna.html"><a href="sna.html#layout-algorithms-for-igraph-package"><i class="fa fa-check"></i><b>19.2.1</b> Layout algorithms for <code>igraph</code> package</a></li>
<li class="chapter" data-level="19.2.2" data-path="sna.html"><a href="sna.html#adding-attribute-information-to-your-visualization"><i class="fa fa-check"></i><b>19.2.2</b> Adding attribute information to your visualization</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="sna.html"><a href="sna.html#package-networkd3"><i class="fa fa-check"></i><b>19.3</b> Package <code>networkD3</code></a>
<ul>
<li class="chapter" data-level="19.3.1" data-path="sna.html"><a href="sna.html#preparing-the-data-for-networkd3"><i class="fa fa-check"></i><b>19.3.1</b> Preparing the data for <code>networkD3</code></a></li>
<li class="chapter" data-level="19.3.2" data-path="sna.html"><a href="sna.html#creating-an-interactive-visualization-with-networkd3"><i class="fa fa-check"></i><b>19.3.2</b> Creating an Interactive Visualization with <code>networkD3</code></a></li>
<li class="chapter" data-level="19.3.3" data-path="sna.html"><a href="sna.html#saving-your-interactive-visualization-to-.html"><i class="fa fa-check"></i><b>19.3.3</b> Saving your Interactive Visualization to .html</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Clustering</b></span></li>
<li class="chapter" data-level="20" data-path="clusintro.html"><a href="clusintro.html"><i class="fa fa-check"></i><b>20</b> Introduction</a>
<ul>
<li class="chapter" data-level="20.1" data-path="clusintro.html"><a href="clusintro.html#mathematical-setup"><i class="fa fa-check"></i><b>20.1</b> Mathematical Setup</a>
<ul>
<li class="chapter" data-level="20.1.1" data-path="clusintro.html"><a href="clusintro.html#data"><i class="fa fa-check"></i><b>20.1.1</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="clusintro.html"><a href="clusintro.html#the-number-of-clusters-k"><i class="fa fa-check"></i><b>20.2</b> The Number of Clusters, <span class="math inline">\(k\)</span></a></li>
<li class="chapter" data-level="20.3" data-path="clusintro.html"><a href="clusintro.html#partitioning-of-graphs-and-networks"><i class="fa fa-check"></i><b>20.3</b> Partitioning of Graphs and Networks</a></li>
<li class="chapter" data-level="20.4" data-path="clusintro.html"><a href="clusintro.html#history-of-data-clustering"><i class="fa fa-check"></i><b>20.4</b> History of Data Clustering</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="clusteralgos.html"><a href="clusteralgos.html"><i class="fa fa-check"></i><b>21</b> Algorithms for Data Clustering</a>
<ul>
<li class="chapter" data-level="21.1" data-path="clusteralgos.html"><a href="clusteralgos.html#hc"><i class="fa fa-check"></i><b>21.1</b> Hierarchical Algorithms</a>
<ul>
<li class="chapter" data-level="21.1.1" data-path="clusteralgos.html"><a href="clusteralgos.html#agglomerative-hierarchical-clustering"><i class="fa fa-check"></i><b>21.1.1</b> Agglomerative Hierarchical Clustering</a></li>
<li class="chapter" data-level="21.1.2" data-path="clusteralgos.html"><a href="clusteralgos.html#principal-direction-divisive-partitioning-pddp"><i class="fa fa-check"></i><b>21.1.2</b> Principal Direction Divisive Partitioning (PDDP)</a></li>
</ul></li>
<li class="chapter" data-level="21.2" data-path="clusteralgos.html"><a href="clusteralgos.html#kmeanshistory"><i class="fa fa-check"></i><b>21.2</b> Iterative Partitional Algorithms</a>
<ul>
<li class="chapter" data-level="21.2.1" data-path="clusteralgos.html"><a href="clusteralgos.html#early-partitional-algorithms"><i class="fa fa-check"></i><b>21.2.1</b> Early Partitional Algorithms</a></li>
<li class="chapter" data-level="21.2.2" data-path="clusteralgos.html"><a href="clusteralgos.html#kmeans"><i class="fa fa-check"></i><b>21.2.2</b> <span class="math inline">\(k\)</span>-means</a></li>
<li class="chapter" data-level="21.2.3" data-path="clusteralgos.html"><a href="clusteralgos.html#the-expectation-maximization-em-clustering-algorithm"><i class="fa fa-check"></i><b>21.2.3</b> The Expectation-Maximization (EM) Clustering Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="21.3" data-path="clusteralgos.html"><a href="clusteralgos.html#density-search-algorithms"><i class="fa fa-check"></i><b>21.3</b> Density Search Algorithms</a>
<ul>
<li class="chapter" data-level="21.3.1" data-path="clusteralgos.html"><a href="clusteralgos.html#density-based-spacial-clustering-of-applications-with-noise-dbscan"><i class="fa fa-check"></i><b>21.3.1</b> Density Based Spacial Clustering of Applications with Noise (DBSCAN)</a></li>
</ul></li>
<li class="chapter" data-level="21.4" data-path="clusteralgos.html"><a href="clusteralgos.html#conclusion"><i class="fa fa-check"></i><b>21.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="chap1-5.html"><a href="chap1-5.html"><i class="fa fa-check"></i><b>22</b> Algorithms for Graph Partitioning</a>
<ul>
<li class="chapter" data-level="22.1" data-path="chap1-5.html"><a href="chap1-5.html#spectral"><i class="fa fa-check"></i><b>22.1</b> Spectral Clustering</a></li>
<li class="chapter" data-level="22.2" data-path="chap1-5.html"><a href="chap1-5.html#fiedler-partitioning"><i class="fa fa-check"></i><b>22.2</b> Fiedler Partitioning</a>
<ul>
<li class="chapter" data-level="22.2.1" data-path="chap1-5.html"><a href="chap1-5.html#linear-algebraic-motivation-for-the-fiedler-vector"><i class="fa fa-check"></i><b>22.2.1</b> Linear Algebraic Motivation for the Fiedler vector</a></li>
<li class="chapter" data-level="22.2.2" data-path="chap1-5.html"><a href="chap1-5.html#graph-cuts"><i class="fa fa-check"></i><b>22.2.2</b> Graph Cuts</a></li>
<li class="chapter" data-level="22.2.3" data-path="chap1-5.html"><a href="chap1-5.html#pic"><i class="fa fa-check"></i><b>22.2.3</b> Power Iteration Clustering</a></li>
<li class="chapter" data-level="22.2.4" data-path="chap1-5.html"><a href="chap1-5.html#modularity"><i class="fa fa-check"></i><b>22.2.4</b> Clustering via Modularity Maximization</a></li>
</ul></li>
<li class="chapter" data-level="22.3" data-path="chap1-5.html"><a href="chap1-5.html#stochastic-clustering"><i class="fa fa-check"></i><b>22.3</b> Stochastic Clustering</a>
<ul>
<li class="chapter" data-level="22.3.1" data-path="chap1-5.html"><a href="chap1-5.html#stochastic-clustering-algorithm-sca"><i class="fa fa-check"></i><b>22.3.1</b> Stochastic Clustering Algorithm (SCA)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="validation.html"><a href="validation.html"><i class="fa fa-check"></i><b>23</b> Cluster Validation</a>
<ul>
<li class="chapter" data-level="23.1" data-path="validation.html"><a href="validation.html#internal-validity-metrics"><i class="fa fa-check"></i><b>23.1</b> Internal Validity Metrics</a>
<ul>
<li class="chapter" data-level="23.1.1" data-path="validation.html"><a href="validation.html#common-measures-of-cohesion-and-separation"><i class="fa fa-check"></i><b>23.1.1</b> Common Measures of Cohesion and Separation</a></li>
</ul></li>
<li class="chapter" data-level="23.2" data-path="validation.html"><a href="validation.html#external"><i class="fa fa-check"></i><b>23.2</b> External Validity Metrics</a>
<ul>
<li class="chapter" data-level="23.2.1" data-path="validation.html"><a href="validation.html#accuracy"><i class="fa fa-check"></i><b>23.2.1</b> Accuracy</a></li>
<li class="chapter" data-level="23.2.2" data-path="validation.html"><a href="validation.html#entropy"><i class="fa fa-check"></i><b>23.2.2</b> Entropy</a></li>
<li class="chapter" data-level="23.2.3" data-path="validation.html"><a href="validation.html#purity"><i class="fa fa-check"></i><b>23.2.3</b> Purity</a></li>
<li class="chapter" data-level="23.2.4" data-path="validation.html"><a href="validation.html#mutual-information-mi-and-normalized-mutual-information-nmi"><i class="fa fa-check"></i><b>23.2.4</b> Mutual Information (MI) and <br> Normalized Mutual Information (NMI)</a></li>
<li class="chapter" data-level="23.2.5" data-path="validation.html"><a href="validation.html#other-external-measures-of-validity"><i class="fa fa-check"></i><b>23.2.5</b> Other External Measures of Validity</a></li>
<li class="chapter" data-level="23.2.6" data-path="validation.html"><a href="validation.html#summary-table"><i class="fa fa-check"></i><b>23.2.6</b> Summary Table</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="24" data-path="findk.html"><a href="findk.html"><i class="fa fa-check"></i><b>24</b> Determining the Number of Clusters <span class="math inline">\(k\)</span></a>
<ul>
<li class="chapter" data-level="24.1" data-path="findk.html"><a href="findk.html#methods-based-on-cluster-validity-stopping-rules"><i class="fa fa-check"></i><b>24.1</b> Methods based on Cluster Validity (Stopping Rules)</a></li>
<li class="chapter" data-level="24.2" data-path="findk.html"><a href="findk.html#sum-squared-error-sse-cohesion-plots"><i class="fa fa-check"></i><b>24.2</b> Sum Squared Error (SSE) Cohesion Plots</a>
<ul>
<li class="chapter" data-level="24.2.1" data-path="findk.html"><a href="findk.html#cosine-cohesion-plots-for-text-data"><i class="fa fa-check"></i><b>24.2.1</b> Cosine-Cohesion Plots for Text Data</a></li>
<li class="chapter" data-level="24.2.2" data-path="findk.html"><a href="findk.html#ray-and-turis-method"><i class="fa fa-check"></i><b>24.2.2</b> Ray and Turi’s Method</a></li>
<li class="chapter" data-level="24.2.3" data-path="findk.html"><a href="findk.html#the-gap-statistic"><i class="fa fa-check"></i><b>24.2.3</b> The Gap Statistic</a></li>
</ul></li>
<li class="chapter" data-level="24.3" data-path="findk.html"><a href="findk.html#perroncluster"><i class="fa fa-check"></i><b>24.3</b> Graph Methods Based on Eigenvalues <br> (Perron Cluster Analysis)</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>25</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Linear Algebra for Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mult" class="section level1" number="2">
<h1><span class="header-section-number">Chapter 2</span> Matrix Arithmetic</h1>
<div id="matrix-addition-subtraction-and-scalar-multiplication" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Matrix Addition, Subtraction, and Scalar Multiplication</h2>
<p>Addition, subtraction, and scalar multiplication are the only operations which act <em>element-wise</em> on matrices - they are performed in a way you might expect given your previous studies.</p>
<div class="definition">
<p><span id="def:addsubdef" class="definition"><strong>Definition 2.1  (Addition, Subtraction, and Scalar Multiplication) </strong></span>Two matrices can be added or subtracted only when they have the same dimensions. If <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{B}\)</span> are both <span class="math inline">\(m\times n\)</span> matrices then the (i,j) element of the sum (or difference), written <span class="math inline">\((\mathbf{A}_-^+ \mathbf{B})_{ij}\)</span> is:</p>
<p><span class="math display">\[\begin{equation*} (\mathbf{A}+\mathbf{B})_{ij}=A_{ij}+B_{ij} \end{equation*}\]</span>
similarly,
<span class="math display">\[\begin{equation*}(\mathbf{A}-\mathbf{B})_{ij}=A_{ij}-B_{ij} \end{equation*}\]</span>
Multiplying a scalar by a matrix or vector also works element-wise:
<span class="math display">\[\begin{equation*}(\alpha\mathbf{A})_{ij}=\alpha A_{ij} \end{equation*}\]</span></p>
</div>
<p>Two matrices can be added or subtracted only when they have the same dimensions. If <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{B}\)</span> are both <span class="math inline">\(m\times n\)</span> matrices then the (i,j) element of the sum (or difference), written <span class="math inline">\((\mathbf{A}_-^+ \mathbf{B})_{ij}\)</span> is:</p>
<div class="example">
<p><span id="exm:addsub" class="example"><strong>Example 2.1  (Addition, Subtraction, and Scalar Multiplication) </strong></span><br>
a. Compute <span class="math inline">\(\mathbf{A}+\mathbf{B}\)</span>, if possible:
<span class="math display">\[\mathbf{A}=\left(\begin{matrix} 2 &amp; 3 &amp; -1\\1&amp;-1&amp;1\\2&amp;2&amp;1 \end{matrix}\right) \quad \mathbf{B}=\left(\begin{matrix} 4 &amp; 5 &amp; 6\\-1&amp;0&amp;4\\3&amp;4&amp;3 \end{matrix}\right)\]</span>
<em>We can add the matrices because they have the same size.</em>
<span class="math display">\[\mathbf{A}+\mathbf{B} = \left(\begin{matrix} 6 &amp; 8 &amp; 5\\0&amp;-1&amp;5\\5&amp;6&amp;4\end{matrix}\right)\]</span>
b. Compute <span class="math inline">\(\mathbf{A}-\bo{H}\)</span>, if possible:
<span class="math display">\[\mathbf{A}=\left(\begin{matrix} 1 &amp; 2\\3&amp;5 \end{matrix}\right) \qquad \bo{H}= \left(\begin{matrix} 6 &amp; 5&amp; 10\\0.1 &amp; 0.5 &amp; 0.9 \end{matrix}\right)\]</span>
<em>We cannot subtract these matrices because they don’t have the same size.</em><br />
</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Compute <span class="math inline">\(2\mathbf{A}\)</span>:
<span class="math display">\[\mathbf{A}=\left(\begin{matrix} 2 &amp; 3 &amp; -1\\1&amp;-1&amp;1\\2&amp;2&amp;1 \end{matrix}\right)\]</span>
<em>We simply multiply every element in <span class="math inline">\(\mathbf{A}\)</span> by 2,</em>
<span class="math display">\[2\mathbf{A}=\left(\begin{matrix} 4 &amp; 6 &amp; -2\\2&amp;-2&amp;2\\4&amp;4&amp;2 \end{matrix}\right)\]</span></li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:addsubexer" class="exercise"><strong>Exercise 2.1  (Addition, Subtraction, and Scalar Multiplication) </strong></span><br>
a. Compute <span class="math inline">\(\v-\y\)</span>, if possible: <span class="math display">\[\v=\left(\begin{matrix} 2\\-3\\4 \end{matrix}\right) \quad \y=\left(\begin{matrix} 1\\4\\1 \end{matrix}\right)\]</span>
b. Compute <span class="math inline">\(\v+\bo{h}\)</span>, if possible:
<span class="math display">\[\v=\left(\begin{matrix} 4\\-5\\3 \end{matrix}\right) \quad \bo{h}=\left(\begin{matrix} -1\\-4\\1\\2 \end{matrix}\right)\]</span>
c. Compute <span class="math inline">\(\frac{1}{\sqrt{2}}\v\)</span>:
<span class="math display">\[\v=\left(\begin{matrix} 4\\-5\\3 \end{matrix}\right)\]</span></p>
</div>
</div>
<div id="sec:vectoradd" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Geometry of Vector Addition and Scalar Multiplication</h2>
<p>You’ve already learned how vector addition works algebraically: it occurs element-wise between two vectors of the same length:
<span class="math display">\[
 \a+\b =\left(\begin{matrix} a_1\\ a_2\\ a_3\\ \vdots \\ a_n \end{matrix}\right) +\left(\begin{matrix} b_1\\ b_2\\ b_3\\ \vdots \\ b_n \end{matrix}\right) = \left(\begin{matrix} a_1+b_1\\a_2+b_2\\a_3+b_3\\ \vdots \\a_n+b_n \end{matrix}\right)
\]</span></p>
<p>Geometrically, vector addition is witnessed by placing the two vectors, <span class="math inline">\(\a\)</span> and <span class="math inline">\(\b\)</span>, <em>tail-to-head</em>. The result, <span class="math inline">\(\a+\b\)</span>, is the vector from the open tail to the open head. This is called the parallelogram law and is demonstrated in Figure <a href="mult.html#fig:vectoradd">2.1</a>.</p>
<!-- ![](figs/vectoradd.pdf)     | ![](figs/vectorsub.pdf) -->
<!-- :-------------------------:|:-------------------------: -->
<!-- Addition of vectors  |  Subtraction of Vectors -->
<div class="figure" style="text-align: center"><span id="fig:vectoradd"></span>
<img src="figs/vectoradd.png" alt="Geometry of Vector Addition" width="50%" />
<p class="caption">
Figure 2.1: Geometry of Vector Addition
</p>
</div>
<p>When subtracting vectors as <span class="math inline">\(\a-\b\)</span> we simply add <span class="math inline">\(-\b\)</span> to <span class="math inline">\(\a\)</span>. The vector <span class="math inline">\(-\b\)</span> has the same length as <span class="math inline">\(\b\)</span> but points in the opposite direction. This vector has the same length as the one which connects the two heads of <span class="math inline">\(\a\)</span> and <span class="math inline">\(\b\)</span> as shown in Figure <a href="mult.html#fig:vectorsub">2.2</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:vectorsub"></span>
<img src="figs/vectorsub.jpg" alt="Geometry of Vector Subtraction" width="50%" />
<p class="caption">
Figure 2.2: Geometry of Vector Subtraction
</p>
</div>
<div class="example">
<p><span id="exm:centering" class="example"><strong>Example 2.2  (Centering Data) </strong></span>One thing we will do frequently in this course is deal with centered and/or standardized data. To center a group of data points, we merely subtract the mean of each variable from each measurement on that variable. Geometrically, this amounts to a <em>translation</em> (shift) of the data so that its center (or mean) is at the origin. Figure <a href="mult.html#fig:centerall">2.3</a> illustrates this process using 4 data points.</p>
<div class="figure" style="text-align: center"><span id="fig:centerall"></span>
<img src="figs/centerall.jpg" alt="Centering a Data Cloud as a Geometric Translation" width="80%" />
<p class="caption">
Figure 2.3: Centering a Data Cloud as a Geometric Translation
</p>
</div>
</div>
<p><strong>Scalar multiplication</strong> is another operation which acts element-wise:
<span class="math display">\[\alpha \a = \alpha \left(\begin{matrix} a_1\\a_2\\a_3\\ \vdots \\a_n \end{matrix}\right) = \left(\begin{matrix} \alpha a_1 \\ \alpha a_2\\ \alpha a_3 \\ \vdots \\ \alpha a_n\end{matrix}\right) \]</span></p>
<p>Scalar multiplication changes the length of a vector but not the overall direction (although a negative scalar will scale the vector in the opposite direction through the origin). We can see this geometric interpretation of scalar multiplication in Figure <a href="mult.html#fig:vectormult">2.4</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:vectormult"></span>
<img src="figs/vectormult.jpg" alt="Geometric Effect of Scalar Multiplication" width="50%" />
<p class="caption">
Figure 2.4: Geometric Effect of Scalar Multiplication
</p>
</div>
<!-- :::{.example name='Vector Scaling: Standardizing Data' #scaling} -->
<!-- Once data has been centered, it is also common to then scale the variables according to their standard deviation (or some other normalization factor). Geometrically this amounts to a proportional shrinking of the data. The following graphic illustrates this process using the same 4 data points from Example \@ref(exm:centering).  -->
<!-- ```{r, fig=T, label='scaleall', fig.show="hold", out.width="80%", echo=F,fig.align='center', fig.cap = 'Standardizing a Data Cloud.'} -->
<!-- knitr::include_graphics("figs/scaleall.jpg") -->
<!-- ``` -->
<!-- Note that it's the _variable_ vectors undergo the scalar multiplication whereas what's depicted in Figure \@ref(fig:scaleall) is the coordinates of _observations_. This is the first time we might contemplate the fact that any data matrix can have two equivalent geometric views: For an $m \times n$ matrix, the rows create vectors (points) that live in $\Re^n$ and the columns create vectors (points) that live in $\Re^m$. Depending on our task, either vantage point can provide analytical insights. -->
<!-- ::: -->
</div>
<div id="linear-combinations" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Linear Combinations</h2>
<div class="definition">
<p><span id="def:lincombdef" class="definition"><strong>Definition 2.2  (Linear Combination) </strong></span>A <strong>linear combination</strong> is constructed from a set of terms <span class="math inline">\(\v_1, \v_2, \dots, \v_n\)</span> by multiplying each term by a scalar constant and adding the result:
<span class="math display">\[\bo{c}=\alpha_1\v_1+\alpha_2 \v_2+ \dots+ \alpha_n\v_n = \sum_{i=1}^n \alpha_i \v_n\]</span>
The coefficients <span class="math inline">\(\alpha_i\)</span> are scalar constants and the terms, <span class="math inline">\(\{\v_i\}\)</span> can be scalars, vectors, or matrices. Most often, we will consider linear combinations where the terms <span class="math inline">\(\{\v_i\}\)</span> are vectors.</p>
</div>
<p>Linear combinations are quite simple to understand. Once the equation is written, we can consider the expression as a breakdown into parts.</p>
<div class="example">
<p><span id="exm:lincomb" class="example"><strong>Example 2.3  (Linear Combination) </strong></span>The simplest linear combination might involve columns of the identity matrix:
<span class="math display">\[\left(\begin{matrix} 3 \\ -2\\4 \end{matrix}\right) = 3\left(\begin{matrix} 1\\0\\0 \end{matrix}\right) -2 \left(\begin{matrix} 0\\1\\0 \end{matrix}\right) +4 \left(\begin{matrix} 0\\0\\1 \end{matrix}\right)\]</span>
We can easily picture this linear combination as a "breakdown into parts where the parts give directions along the 3 coordinate axis with which we are all familiar.</p>
</div>
<p>We don’t necessarily have to use vectors as the terms for a linear combination. Example <a href="mult.html#exm:matrixlincomb">2.4</a> shows how we can write any <span class="math inline">\(m\times n\)</span> matrix as a linear combination of <span class="math inline">\(nm\)</span> elementary matrices.</p>
<div class="example">
<p><span id="exm:matrixlincomb" class="example"><strong>Example 2.4  (Linear Combination of Matrices) </strong></span>Write the matrix <span class="math inline">\(\mathbf{A}=\left(\begin{matrix} 1 &amp; 3\\4&amp;2 \end{matrix}\right)\)</span> as a linear combination of the following matrices:
<span class="math display">\[\left\lbrace \left(\begin{matrix} 1 &amp; 0\\0&amp;0 \end{matrix}\right),\left(\begin{matrix} 0 &amp; 1\\0&amp;0 \end{matrix}\right),\left(\begin{matrix} 0 &amp; 0\\1&amp;0 \end{matrix}\right),\left(\begin{matrix} 0 &amp; 0\\0&amp;1 \end{matrix}\right) \right\rbrace\]</span>
Solution:
<span class="math display">\[\mathbf{A}=\left(\begin{matrix} 1 &amp; 3\\4&amp;2 \end{matrix}\right) = 1\left(\begin{matrix} 1 &amp; 0\\0&amp;0 \end{matrix}\right)+3\left(\begin{matrix} 0 &amp; 1\\0&amp;0 \end{matrix}\right)+4\left(\begin{matrix} 0 &amp; 0\\1&amp;0 \end{matrix}\right)+2\left(\begin{matrix} 0 &amp; 0\\0&amp;1 \end{matrix}\right)\]</span></p>
</div>
</div>
<div id="matrix-multiplication" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Matrix Multiplication</h2>
<p>When we multiply matrices, we do not perform the operation element-wise as we did with addition and scalar multiplication. Matrix multiplication is, in itself, a very powerful tool for summarizing information. In fact, many of the analytical tools we will focus on in this course, like Markov Chains, Principal Components Analysis, Factor Analysis, and the Singular Value Decomposition, can all be understood more clearly with a firm grasp on matrix multiplication. Because this operation is so important, we will spend a considerable amount of energy breaking it down in many ways.</p>
<div id="the-inner-product" class="section level3" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> The Inner Product</h3>
<p>We’ll begin by defining the multiplication of a row vector times a column vector, known as an inner product (sometimes called the <em>dot product</em> in applied sciences). For the remainder of this course, unless otherwise specified, we will consider vectors to be columns rather than rows. This makes the notation more simple because if <span class="math inline">\(\x\)</span> is a column vector,
<span class="math display">\[\x=\left(\begin{matrix} x_1\\x_2\\\vdots\\ x_n\end{matrix}\right)\]</span>
then we can automatically assume that <span class="math inline">\(\x^T\)</span> is a row vector:
<span class="math display">\[\x^T = \left(\begin{matrix} x_1&amp;x_2&amp;\dots&amp;x_n\end{matrix}\right).\]</span></p>
<div class="definition">
<p><span id="def:innerproddef" class="definition"><strong>Definition 2.3  (Inner Product) </strong></span>The <strong>inner product</strong> of two vectors, <span class="math inline">\(\x\)</span> and <span class="math inline">\(\y\)</span>, written <span class="math inline">\(\x^T\y\)</span>, is defined as the sum of the product of corresponding elements in <span class="math inline">\(\x\)</span> and <span class="math inline">\(\y\)</span>:</p>
<p><span class="math display">\[\x^T\y = \sum_{i=1}^n x_i y_i.\]</span>
If we write this out for two vectors with 4 elements each, we’d have:</p>
<p><span class="math display">\[\x^T\y=\left(\begin{matrix} x_1 &amp; x_2 &amp; x_3 &amp; x_4 \end{matrix}\right) \left(\begin{matrix} y_1\\y_2\\y_3\\y_4 \end{matrix}\right) = x_1y_1+x_2y_2+x_3y_3+x_4y_4\]</span></p>
<p><em>Note: The inner product between vectors is only possible when the two vectors have the same number of elements!</em><br />
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:animinnerproduct"></span>
<img src="figs/animinnerprod.gif" alt="Animation of Inner Product between two vectors" width="50%" />
<p class="caption">
Figure 2.5: Animation of Inner Product between two vectors
</p>
</div>
<div class="example">
<p><span id="exm:innerprod" class="example"><strong>Example 2.5  (Vector Inner Product) </strong></span>Let <span class="math display">\[\x=\left(\begin{matrix} -1 \\2\\4\\0 \end{matrix}\right) \quad \y=\left(\begin{matrix} 3 \\5\\1\\7 \end{matrix}\right) \quad \v=\left(\begin{matrix} -3 \\-2\\5\\3\\-2 \end{matrix}\right) \quad \u= \left(\begin{matrix} 2\\-1\\3\\-3\\-2 \end{matrix}\right)\]</span></p>
<p>If possible, compute the following inner products:</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(\x^T\y\)</span>
<span class="math display">\[\begin{eqnarray}
\x^T\y &amp;=&amp;\left(\begin{matrix} -1 &amp;2&amp;4&amp;0 \end{matrix}\right) \left(\begin{matrix} 3 \\5\\1\\7 \end{matrix}\right) \cr &amp;=&amp; (-1)(3)+(2)(5)+(4)(1)+(0)(7) \cr &amp;=&amp; -3+10+4=\framebox{11}
\end{eqnarray}\]</span></li>
<li><span class="math inline">\(\x^T\v\)</span>
This is not possible because <span class="math inline">\(\x\)</span> and <span class="math inline">\(\v\)</span> do not have the same number of elements</li>
<li><span class="math inline">\(\v^T\u\)</span>
<span class="math display">\[\begin{eqnarray}
\v^T\u &amp;=&amp; \left(\begin{matrix} -3 &amp;-2&amp;5&amp;3&amp;-2 \end{matrix}\right) \left(\begin{matrix} 2\\-1\\3\\-3\\-2 \end{matrix}\right)  \cr &amp;=&amp; (-3)(2)+(-2)(-1)+(5)(3)+(3)(-3)+(-2)(-2) \cr &amp;=&amp; -6+2+15-9+4 = \framebox{6}
\end{eqnarray}\]</span></li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:innerprodexer" class="exercise"><strong>Exercise 2.2  (Vector Inner Product) </strong></span>Let <span class="math display">\[\bo{v}=\left(\begin{matrix} 1 \\2\\3\\4\\5 \end{matrix}\right) \quad \e=\left(\begin{matrix} 1 \\1\\1\\1\\1 \end{matrix}\right) \quad \bo{p}=\left(\begin{matrix} 0.5 \\0.1\\0.2\\0\\0.2 \end{matrix}\right) \quad \u= \left(\begin{matrix} 10\\4\\3\\2\\1 \end{matrix}\right) \quad \bo{s} = \left(\begin{matrix} 2\\2\\-3 \end{matrix}\right)\]</span></p>
<p>If possible, compute the following inner products:</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(\bo{v}^T\e\)</span></li>
<li><span class="math inline">\(\bo{e}^T\bo{v}\)</span></li>
<li><span class="math inline">\(\bo{v}^T\bo{s}\)</span></li>
<li><span class="math inline">\(\bo{p}^T\u\)</span></li>
<li><span class="math inline">\(\bo{v}^T\bo{v}\)</span></li>
</ol>
</div>
<p>It should be clear from the definition and from the previous exercise, that for all vectors <span class="math inline">\(\x\)</span> and <span class="math inline">\(\y\)</span>,
<span class="math display">\[\x^T\y = \y^T\x.\]</span>
Also, if we take the inner product of a vector with itself, the result is the sum of squared elements in that vector:
<span class="math display">\[\x^T\x = \sum_{i=1}^n x_i^2 = x_1^2 + x_2^2+ \dots + x_n^2.\]</span></p>
<p>Now that we are comfortable multiplying a row vector (<span class="math inline">\(\x^T\)</span> in the definition) and a column vector (<span class="math inline">\(\y\)</span> in the definition), we can define multiplication for matrices in general.</p>
</div>
<div id="matrix-product" class="section level3" number="2.4.2">
<h3><span class="header-section-number">2.4.2</span> Matrix Product</h3>
<p>Matrix multiplication is nothing more than a collection of inner products done simultaneously in one operation. We must be careful when multiplying matrices because, as with vectors, the operation is not always possible. Unlike the vector inner product, the order in which you multiply matrices makes a big difference!</p>
<div class="definition">
<p><span id="def:matmultdef" class="definition"><strong>Definition 2.4  (Matrix Multiplication) </strong></span>Let <span class="math inline">\(\mathbf{A}_{m\times n}\)</span> and <span class="math inline">\(\mathbf{B}_{k\times p}\)</span> be matrices. The matrix product <span class="math inline">\(\mathbf{A}\mathbf{B}\)</span> is possible if and only if <span class="math inline">\(n=k\)</span>; that is, when the number of columns in <span class="math inline">\(\mathbf{A}\)</span> is the same as the number of rows in <span class="math inline">\(\mathbf{B}\)</span>. If this condition holds, then the dimension of the product, <span class="math inline">\(\mathbf{A}\mathbf{B}\)</span> is <span class="math inline">\(m\times p\)</span> and the (ij)-entry of the product <span class="math inline">\(\mathbf{A}\mathbf{B}\)</span> is the inner product of the <span class="math inline">\(i^{th}\)</span> row of <span class="math inline">\(\mathbf{A}\)</span> and the <span class="math inline">\(j^{th}\)</span> column of <span class="math inline">\(\mathbf{B}\)</span>:</p>
<p><span class="math display">\[(\mathbf{A}\mathbf{B})_{ij} = \mathbf{A}_{i\star}\mathbf{B}_{\star j}\]</span></p>
</div>
<p>This definition may be easier to dissect using an example:</p>
<div class="example">
<p><span id="exm:matmult" class="example"><strong>Example 2.6  (Steps to Compute a Matrix Product) </strong></span>Let <span class="math display">\[\mathbf{A}=\left(\begin{matrix} 2 &amp; 3 \\ -1 &amp; 4 \\ 5 &amp; 1 \end{matrix}\right) \quad \mbox{and} \quad \mathbf{B}=\left(\begin{matrix}  0 &amp; -2 \\ 2 &amp; -3 \end{matrix}\right)\]</span></p>
<p>When we first get started with matrix multiplication, we often follow a few simple steps:</p>
<ol style="list-style-type: decimal">
<li>Write down the matrices and their dimensions. Make sure the “inside” dimensions match - those corresponding to the columns of the first matrix and the rows of the second matrix:
<span class="math display">\[\underset{(3\times \red{2})}{\mathbf{A}} \underset{(\red{2} \times 2)}{\mathbf{B}}\]</span>
If these dimensions match, then we can multiply the matrices. If they don’t, we stop right there - multiplication is not possible.</li>
<li>Now, look at the “outer” dimensions - this will tell you the size of the resulting matrix.
<span class="math display">\[\underset{(\blue{3}\times 2)}{\mathbf{A}} \underset{(2\times \blue{2})}{\mathbf{B}}\]</span>
So the product <span class="math inline">\(\mathbf{A}\mathbf{B}\)</span> is a <span class="math inline">\(3\times 2\)</span> matrix.</li>
<li>Finally, we compute the product of the matrices by multiplying each row of <span class="math inline">\(\mathbf{A}\)</span> by each column of <span class="math inline">\(\mathbf{B}\)</span> using inner products. The element in the first row and first column of the product (written <span class="math inline">\((\mathbf{A}\mathbf{B})_{11}\)</span>) will be the inner product of the first row of <span class="math inline">\(\mathbf{A}\)</span> and the first column of <span class="math inline">\(\mathbf{B}\)</span>. Then, <span class="math inline">\((\mathbf{A}\mathbf{B})_{12}\)</span> will be the inner product of the first row of <span class="math inline">\(\mathbf{A}\)</span> and the second column of <span class="math inline">\(\mathbf{B}\)</span>, etc.</li>
</ol>
<p><span class="math display">\[\begin{eqnarray}
\mathbf{A}\mathbf{B} &amp;=&amp;\left(\begin{matrix} (2)(0)+(3)(2) &amp; (2)(-2)+(3)(-3)\\
             (-1)(0)+(4)(2) &amp; (-1)(-2)+(4)(-3)\\
              (5)(0)+(1)(2) &amp; (5)(-2)+(1)(-3) \end{matrix}\right) \cr
    &amp;=&amp; \left(\begin{matrix} 6&amp;-13\\8 &amp; -10\\2&amp;-13\end{matrix}\right)
\end{eqnarray}\]</span></p>
</div>
<p>Matrix multiplication is incredibly important for data analysis. You may not see why all these multiplications and additions are so useful at this point, but we will visit some basic applications shortly. For now, let’s practice so that we are prepared for the applications!</p>
<div class="exercise">
<p><span id="exr:matmultexer" class="exercise"><strong>Exercise 2.3  (Matrix Multiplication) </strong></span>Suppose we have <span class="math display">\[\mathbf{A}_{4\times 6} \quad \mathbf{B}_{5\times 5} \quad \M_{5\times 4} \quad \bP_{6\times 5}\]</span>
Circle the matrix products that are possible to compute and write the dimension of the result.
<span class="math display">\[\mathbf{A}\M \qquad \M\mathbf{A} \qquad \mathbf{B}\M  \qquad \M\mathbf{B} \qquad \bP\mathbf{A} \qquad \bP\M \qquad \mathbf{A}\bP \qquad \mathbf{A}^T\bP \qquad \M^T\mathbf{B}\]</span>
Let
<span class="math display">\[\begin{equation}
\mathbf{A}=\left(\begin{matrix} 1&amp;1&amp;0&amp;1\\0&amp;1&amp;1&amp;1\\1&amp;0&amp;1&amp;0\end{matrix}\right) \quad \M = \left(\begin{matrix} -2&amp;1&amp;-1&amp;2&amp;-2\\1&amp;-2&amp;0&amp;-1&amp;2\\2&amp;1&amp;-3&amp;-2&amp;3 \\ 1&amp;3&amp;2&amp;-1&amp;2\end{matrix}\right)  \end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\C=\left(\begin{matrix} -1&amp;0&amp;1&amp;0\\1&amp;-1&amp;0&amp;0\\0&amp;0&amp;1&amp;-1 \end{matrix}\right) \end{equation}\]</span></p>
<p>Determine the following matrix products, if possible:</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(\mathbf{A}\C\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{A}\M\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{A}^T\C\)</span></p></li>
</ol>
</div>
<p>One very important thing to keep in mind is this:</p>
<p style="text-align:center">
<strong> matrix multiplication is NOT commutative! </strong>
</p>
<p>As we see from the previous exercises, it’s quite common to be able to compute a product <span class="math inline">\(\mathbf{A}\mathbf{B}\)</span> where the reverse product, <span class="math inline">\(\mathbf{B}\mathbf{A}\)</span> is not even possible to compute. Even if both products are possible it is almost <em>never</em> the case that <span class="math inline">\(\mathbf{A}\mathbf{B}\)</span> equals <span class="math inline">\(\mathbf{B}\mathbf{A}\)</span>.</p>
</div>
<div id="matrix-vector-product" class="section level3" number="2.4.3">
<h3><span class="header-section-number">2.4.3</span> Matrix-Vector Product</h3>
<p>A matrix-vector product works exactly the same way as matrix multiplication; after all, a vector <span class="math inline">\(\x\)</span> is nothing but an <span class="math inline">\(n\times 1\)</span> matrix. In order to multiply a matrix by a vector, again we must match the dimensions to make sure they line up correctly. For example, if we have an <span class="math inline">\(m\times n\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span>, we can multiply by a <span class="math inline">\(1\times m\)</span> row vector <span class="math inline">\(\v^T\)</span> on the left:
<span class="math display">\[\v^T\mathbf{A} \quad \mbox{works because } \underset{ (1\times \red{m})}{\v^T} \underset{(\red{m}\times n)}{\mathbf{A}}\]</span>
<span class="math display">\[\Longrightarrow \mbox{The result will be a   } 1 \times n \mbox{ row vector.}\]</span>
or we can multiply by an <span class="math inline">\(n\times 1\)</span> column vector <span class="math inline">\(\x\)</span> on the right:</p>
<p><span class="math display">\[\mathbf{A}\x \quad \mbox{works because } \underset{(m\times \red{n})}{\mathbf{A}}\underset{(\red{n}\times 1)}{\x} \]</span>
<span class="math display">\[\Longrightarrow \mbox{The result will be a   } m\times 1 \mbox{ column vector.}\]</span></p>
<p>Matrix-vector multiplication works the same way as matrix multiplication: we simply multiply rows by columns until we’ve completed the answer. In the case of <span class="math inline">\(\v^T\mathbf{A}\)</span>, we’d multiply the row <span class="math inline">\(\v\)</span> by each of the <span class="math inline">\(n\)</span> columns of <span class="math inline">\(\mathbf{A}\)</span>, carving out our solution, one entry at a time :</p>
<p><span class="math display">\[\v^T\mathbf{A} = \left(\begin{matrix} \v^T\mathbf{A}_{*1} &amp; \v^T\acol{2} &amp; \dots &amp; \v^T\acol{n} \end{matrix}\right).\]</span></p>
<p>In the case of <span class="math inline">\(\mathbf{A}\x\)</span>, we’d multiply each of the <span class="math inline">\(m\)</span> rows of <span class="math inline">\(\mathbf{A}\)</span> by the column <span class="math inline">\(\x\)</span>:</p>
<p><span class="math display">\[\mathbf{A}\x = \left(\begin{matrix} \arow{1}\x \\ \arow{2}\x \\ \vdots \\ \arow{m}\x \end{matrix}\right).\]</span></p>
<p>Let’s see an example of this:</p>
<div class="example">
<p><span id="exm:matvecprod" class="example"><strong>Example 2.7  (Matrix-Vector Products) </strong></span>Let <span class="math display">\[\mathbf{A}=\left(\begin{matrix} 2 &amp; 3 \\ -1 &amp; 4 \\ 5 &amp; 1 \end{matrix}\right)  \quad \v=\left(\begin{matrix} 3\\2 \end{matrix}\right) \quad \bo{q}=\left(\begin{matrix} 2\\-1\\3\end{matrix}\right)\]</span></p>
<p>Determine whether the following matrix-vector products are possible. When possible, compute the product.</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(\mathbf{A}\bo{q}\)</span>
<span class="math display">\[\mbox{Not Possible: Inner dimensions do not match} \quad \underset{(3\times \red{2})}{\mathbf{A}}\underset{(\red{3}\times 1)}{\bo{q}}\]</span></p></li>
<li><p><span class="math inline">\(\mathbf{A}\v\)</span>
<span class="math display">\[
\left(\begin{matrix} 2 &amp; 3 \\ -1 &amp; 4 \\ 5 &amp; 1 \end{matrix}\right) \left(\begin{matrix} 3\\2 \end{matrix}\right) = \left(\begin{matrix} 2(3)+3(2) \\  -1(3)+4(2)\\5(3)+1(2) \end{matrix}\right) = \left(\begin{matrix} 12\\5\\17\end{matrix}\right)
\]</span></p></li>
<li><p><span class="math inline">\(\bo{q}^T\mathbf{A}\)</span></p>
<center>
<p>Rather than write out the entire calculation, the blue text highlights one of the two inner products required:</p>
</center>
<p><span class="math display">\[
\left(\begin{matrix} \blue{2} &amp; \blue{-1} &amp; \blue{3}\end{matrix}\right) \left(\begin{matrix} \blue{2} &amp; 3 \\ \blue{-1} &amp; 4 \\ \blue{5} &amp; 1 \end{matrix}\right)  =  \left(\begin{matrix} \blue{20} &amp; 5  \end{matrix}\right)
\]</span></p></li>
<li><p><span class="math inline">\(\v^T\mathbf{A}\)</span>
<span class="math display">\[\mbox{Not Possible: Inner dimensions do not match} \quad \underset{(1\times \red{2})}{\v^T}\underset{(\red{3}\times 2)}{\mathbf{A}}\]</span></p></li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:matvecprodexer" class="exercise"><strong>Exercise 2.4  (Matrix-Vector Products) </strong></span>Let
<span class="math display">\[ \mathbf{A}=\left(\begin{matrix} 1&amp;1&amp;0&amp;1\\0&amp;1&amp;1&amp;1\\1&amp;0&amp;1&amp;0\end{matrix}\right) \quad \mathbf{B}=\left(\begin{matrix}  0 &amp; -2 \\ 1 &amp; -3 \end{matrix}\right) \]</span> <span class="math display">\[ \x=\left(\begin{matrix} 2\\1\\3 \end{matrix}\right) \quad \y = \left(\begin{matrix} 1\\1 \end{matrix}\right) \quad \z = \left(\begin{matrix} 3\\1\\2\\3 \end{matrix}\right)\]</span>
Determine whether the following matrix-vector products are possible. When possible, compute the product.</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(\mathbf{A}\z\)</span></p></li>
<li><p><span class="math inline">\(\z^T\mathbf{A}\)</span></p></li>
<li><p><span class="math inline">\(\y^T\mathbf{B}\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{B}\y\)</span></p></li>
<li><p><span class="math inline">\(\x^T\mathbf{A}\)</span></p></li>
</ol>
</div>
</div>
<div id="linear-combination-view-of-matrix-products" class="section level3 unnumbered">
<h3>Linear Combination view of Matrix Products</h3>
<p>All matrix products can be viewed as linear combinations or a collection of linear combinations. This vantage point is <em>extremely</em> crucial to our understanding of data science techniques that are based on matrix-factorization. Let’s start with matrix-vector product and see how we can depict it as a linear combination of the columns of the matrix.</p>
<div class="definition">
<p><span id="def:matvecprodlincomb" class="definition"><strong>Definition 2.5  (Matrix-Vector Product as a Linear Combination) </strong></span>Let <span class="math inline">\(\mathbf{A}\)</span> be an <span class="math inline">\(m\times n\)</span> matrix partitioned into columns,
<span class="math display">\[\mathbf{A} = [\mathbf{A}_1 | \mathbf{A}_2 | \dots | \mathbf{A}_n]\]</span>
and let <span class="math inline">\(\x\)</span> be a vector in <span class="math inline">\(\Re^n\)</span>. Then,
<span class="math display">\[\mathbf{A}\x = x_1\mathbf{A}_1 + x_2\mathbf{A}_2 + \dots + x_n\mathbf{A}_n\]</span><br />
</p>
</div>
<p>We use the animation in Figure <a href="mult.html#fig:matvecprodlincombanim">2.6</a> to illustrate Definition <a href="mult.html#def:matvecprodlincomb">2.5</a>.</p>

<div class="figure" style="text-align: center"><span id="fig:matvecprodlincombanim"></span>
<img src="figs/animmatveclincomb.gif" alt="Illustration of Definition 2.5" width="50%" />
<p class="caption">
Figure 2.6: Illustration of Definition <a href="mult.html#def:matvecprodlincomb">2.5</a>
</p>
</div>
<p>Definition <a href="mult.html#def:matvecprodlincomb">2.5</a> extends to <em>any</em> matrix product. If <span class="math inline">\(\mathbf{A}\mathbf{B}=\mathbf{C}\)</span> then the columns of <span class="math inline">\(\mathbf{C}\)</span> can be viewed as linear combinations of the columns of <span class="math inline">\(\mathbf{A}\)</span> and, likewise, the rows of <span class="math inline">\(\C\)</span> can be viewed as linear combinations of the rows of <span class="math inline">\(\mathbf{B}\)</span>. We leave the latter fact for the reader to explore independently (see end-of-chapter exercise 5), and animate the former in Figure <a href="mult.html#fig:multlincombanim">2.7</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:multlincombanim"></span>
<img src="figs/multlincombanim.gif" alt="Illustration of Definition 2.5" width="50%" />
<p class="caption">
Figure 2.7: Illustration of Definition <a href="mult.html#def:matvecprodlincomb">2.5</a>
</p>
</div>
<div id="multiplication-by-a-diagonal-matrix" class="section level4" number="2.4.3.1">
<h4><span class="header-section-number">2.4.3.1</span> Multiplication by a Diagonal Matrix</h4>
<p>As we will see in the next example, multiplication by a diagonal matrix causes a very specific effect on a matrix.</p>
<div class="example">
<p><span id="exm:diagmult" class="example"><strong>Example 2.8  (Multiplication by a Diagonal Matrix) </strong></span>Compute the following matrix product and comment on what you find in the results:
<span class="math display">\[\D=\left(\begin{matrix} 2&amp;0&amp;0\\0&amp;3&amp;0\\0&amp;0&amp;-2 \end{matrix}\right) \mathbf{A}= \left(\begin{matrix} 1&amp;2&amp;3\\1&amp;1&amp;2\\2&amp;1&amp;3 \end{matrix}\right)\]</span>
<span class="math display">\[\D\mathbf{A}=\left(\begin{matrix} 2&amp;4&amp;6\\3&amp;3&amp;6\\-4&amp;-2&amp;-6 \end{matrix}\right)\]</span>
In doing this multiplication, we see that the effect of multiplying the matrix <span class="math inline">\(\mathbf{A}\)</span> by a diagonal matrix on the left is that the rows of the matrix <span class="math inline">\(\mathbf{A}\)</span> are simply scaled by the entries in the diagonal matrix. You should work this computation out by hand to convince yourself that this effect will happen every time. Diagonal scaling can be important, and from now on when you see a matrix product like <span class="math inline">\(\D\mathbf{A}\)</span> where <span class="math inline">\(\D\)</span> is diagonal, you should automatically put together that the result is just a row-scaled version of <span class="math inline">\(\mathbf{A}\)</span>.</p>
</div>
<div class="exercise">
<p><span id="exr:diagmultexer" class="exercise"><strong>Exercise 2.5  (Multiplication by a Diagonal Matrix) </strong></span>What happens if we were to compute the product from Example <a href="mult.html#exm:diagmult">2.8</a> in the reversed order, with the diagonal matrix on the right: <span class="math inline">\(\mathbf{A}\D?\)</span>
<br>
Would we expect the same result? Is multiplication by a diagonal matrix commutative? Work out the calculation and comment on what you’ve found.</p>
</div>
</div>
</div>
</div>
<div id="vector-outer-products" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> Vector Outer Products</h2>
<p>Whereas inner products were the product of a row vector with a column vector (think <span class="math inline">\(\x^T\y\)</span>), <strong>outer products</strong> are the product of a <em>column</em> vector with a <em>row</em> vector (think <span class="math inline">\(\x\y^T\)</span>).
Let’s first consider the dimensions of the outcome:</p>
<p><span class="math display">\[\underset{(m\times \red{1})}{\x} \underset{(\red{1} \times n)}{\y^T} = \bo{M}_{m\times n}\]</span></p>
<p>So the result is a matrix! We’ll want to treat this product in the same way we treat any matrix product, by multiplying row <span class="math inline">\(\times\)</span> column until we’ve run out of rows and columns. Let’s take a look at an example:</p>
<div class="example">
<p><span id="exm:outerprod" class="example"><strong>Example 2.9  (Vector Outer Product) </strong></span>Let <span class="math inline">\(\x = \left(\begin{matrix} 3\\4\\-2 \end{matrix}\right)\)</span> and <span class="math inline">\(\y=\left(\begin{matrix} 1\\5\\3 \end{matrix}\right)\)</span>. Then,
<span class="math display">\[\x\y^T = \left(\begin{matrix} \red{3}\\4\\-2 \end{matrix}\right) \left(\begin{matrix} \red{1}&amp;5&amp;3 \end{matrix}\right) = \left(\begin{matrix} \red{3}&amp;15&amp;9\\4&amp;20&amp;12\\-2&amp;-10&amp;-3\end{matrix}\right)\]</span>
As you can see by performing this calculation, a vector outer product will <em>always</em> produce a matrix whose rows are multiples of each other!</p>
</div>
</div>
<div id="the-identity-and-the-matrix-inverse" class="section level2" number="2.6">
<h2><span class="header-section-number">2.6</span> The Identity and the Matrix Inverse</h2>
<p>The <em>identity matrix</em>, introduced in Section <a href="intro.html#special">1.6</a>, is to matrices as the number `1’ is to scalars. It is the <strong>multiplicative identity</strong>. For any matrix (or vector) <span class="math inline">\(\mathbf{A}\)</span>, multiplying <span class="math inline">\(\mathbf{A}\)</span> by the identity matrix on either side does not change <span class="math inline">\(\mathbf{A}\)</span>:
<span class="math display">\[\begin{align*}
\mathbf{A}\I&amp;=\mathbf{A} \\
\I\mathbf{A} &amp;= \mathbf{A} 
\end{align*}\]</span></p>
<p>This fact is easy to verify in light of Example <a href="mult.html#exm:diagmult">2.8</a>. Since the identity is simply a diagonal matrix with ones on the diagonal, when we multiply it by any matrix it merely scales each row or column of that matrix by 1. The size of the identity matrix is generally implied in context. If <span class="math inline">\(\mathbf{A}\)</span> is <span class="math inline">\(m\times n\)</span> then writing <span class="math inline">\(\mathbf{A}\I\)</span> implies that <span class="math inline">\(\I\)</span> is <span class="math inline">\(n \times n\)</span>, where as writing <span class="math inline">\(\I\mathbf{A}\)</span> implies <span class="math inline">\(\I\)</span> is <span class="math inline">\(m\times m\)</span>.</p>
For <em>certain</em> square matrices <span class="math inline">\(\mathbf{A}\)</span>, an inverse matrix, written <span class="math inline">\(\mathbf{A}^{-1}\)</span>, exists such that
<span class="math display">\[\mathbf{A}\mathbf{A}^{-1} = \I\]</span>
<span class="math display">\[\mathbf{A}^{-1}\mathbf{A} = \I\]</span>
It is very important to understand that not all matrices have inverses. There are 2 very important conditions that must be satisfied:
<p>We have not yet discussed the notion of matrix rank, so the present discussion is aimed only at defining the concept of a matrix inverse rather than defining when it exists or how it is determined. For now, we want to see the analogy of the matrix inverse to our previous understanding of scalar algebra. Recall that the inverse of a non-zero scalar number is its reciprocal:
<span class="math display">\[a^{-1} = \frac{1}{a}\]</span>
Multiplying a scalar by its inverse yields the multiplicative identity, 1:
<span class="math display">\[(a)(a^{-1}) = (a)(\frac{1}{a}) = 1\]</span>
All scalars have an inverse with the exception of 0. For matrices, the idea of an inverse is quite the same - multiply a matrix on the left or right by its inverse to get the multiplicative identity, <span class="math inline">\(\I\)</span>. However, as previously stated, the matrix inverse only exists for a small subset of matrices, those that are square and full rank. Such matrices are equivalently called <strong>invertible</strong> or <strong>non-singular</strong>.</p>

<div class="example">
<p><span id="exm:dontcancel" class="example"><strong>Example 2.10  (Don’t Cancel That!!) </strong></span>We must be careful in linear algebra to remember the basics and not confuse our equations with scalar equations. When we see an equation like
<span class="math display">\[\mathbf{A}\x=\lambda\x\]</span>
We <font-color:red><strong> CANNOT </strong> <font-color:black>cancel terms from both sides. Mathematically, this operation amounts to multiplying both sides by an inverse. When the term we are canceling is a non-zero scalar, then we can proceed as usual. However, we must be careful not to assume that a matrix/vector quantity has an inverse. For example, the following operation is <strong>nonsense:</strong>
<span class="math display">\[\require{cancel}\]</span>
<span class="math display">\[\mathbf{A}\cancel{\mathbf{x}}=\lambda \cancel{\mathbf{x}}\]</span>
Note that, while this equation made sense to begin with, after erroneously canceling terms, it no longer makes sense as it equates a matrix, <span class="math inline">\(\mathbf{A}\)</span>, with a scalar, <span class="math inline">\(\lambda\)</span>.</p>
</div>
</div>
<div id="exercises-1" class="section level2" number="2.7">
<h2><span class="header-section-number">2.7</span> Exercises</h2>
<ol>
<li>
On a coordinate plane, draw the vectors <span class="math inline">\(\a = \left(\begin{matrix} 1\\2\end{matrix}\right)\)</span> and <span class="math inline">\(\b=\left(\begin{matrix} 0\\1\end{matrix}\right)\)</span> and then draw <span class="math inline">\(\bo{c}=\a+\b\)</span>. Make dotted lines which illustrate how the point/vector <span class="math inline">\(\bo{c}\)</span> can be reached by connecting the vectors a and b “tail-to-head.”
<li>
Use the following vectors to answer the questions:
<span class="math display">\[
\v=\left(\begin{matrix} 6\\-1\end{matrix}\right) \quad \bo{u}=\left(\begin{matrix} -2\\1\end{matrix}\right) \quad \x=\left(\begin{matrix} 4\\2\\1\end{matrix}\right) \quad \y=\left(\begin{matrix}-1\\-2\\-3\end{matrix}\right) \quad \e=\left(\begin{matrix} 1\\1\\1\end{matrix}\right)
\]</span>
<ol style="list-style-type:lower-alpha">
<li>
Compute the following linear combinations, if possible:
<ol style="list-style-type:lower-roman">
<li>
<span class="math inline">\(2\u+3\v=\)</span>
<li>
<span class="math inline">\(\x-2\y+\e=\)</span>
<li>
<span class="math inline">\(-2\u-\v+\e=\)</span>
<li>
<span class="math inline">\(\u+\e=\)</span>
</ol>
<li>
Compute the following inner products, if possible:
<ol style="list-style-type:lower-roman">
<li>
<span class="math inline">\(\u^T\v=\)</span>
<li>
<span class="math inline">\(\x^T\x=\)</span>
<li>
<span class="math inline">\(\e^T\y=\)</span>
<li>
<span class="math inline">\(\x^T\u=\)</span>
<li>
<span class="math inline">\(\x^T\e=\)</span>
<li>
<span class="math inline">\(\y^T\e=\)</span>
<li>
<span class="math inline">\(\v^T\x=\)</span>
<li>
<span class="math inline">\(\e^T\v=\)</span><br />

</ol>
<li>
What happens when you take the inner product of a vector with <span class="math inline">\(\e\)</span>?<br />

<li>
What happens when you take the inner product of a vector with itself (as in <span class="math inline">\(\x^T\x\)</span>)?<br />

</ol>
<li>
Use the following matrices to answer the questions:
<span class="math display">\[\mathbf{A}=\left(\begin{matrix} 1&amp;3&amp;8\\3&amp;0&amp;-2\\8&amp;-2&amp;-3 \end{matrix}\right) \quad \bo{M}=\left(\begin{matrix} 1&amp;8&amp;-2&amp;5\\2&amp;8&amp;1&amp;7 \end{matrix}\right) \quad 
\D = \left(\begin{matrix} 1&amp;0&amp;0\\0&amp;5&amp;0\\0&amp;0&amp;3\end{matrix}\right) \]</span>
<span class="math display">\[
\bo{H}=\left(\begin{matrix} 2&amp;-1\\1&amp;3 \end{matrix}\right) \quad \bo{W}=\left(\begin{matrix} 1&amp;1&amp;1&amp;1\\2&amp;2&amp;2&amp;2\\3&amp;3&amp;3&amp;3\end{matrix}\right)
\]</span>
<ol style="list-style-type:lower-alpha">
<li>
Circle the matrix products that are possible and specify their resulting dimensions:
<ol style="list-style-type:lower-roman">
<li>
<span class="math inline">\(\mathbf{A}\M\)</span>
<li>
<span class="math inline">\(\mathbf{A}\bo{W}\)</span>
<li>
<span class="math inline">\(\bo{W}\D\)</span>
<li>
<span class="math inline">\(\bo{W}^T\D\)</span>
<li>
<span class="math inline">\(\bo{H}\M\)</span>
<li>
<span class="math inline">\(\bo{M}\bo{H}\)</span>
<li>
<span class="math inline">\(\bo{M}^T\bo{H}^T\)</span>
<li>
<span class="math inline">\(\D\bo{W}\)</span><br />

</ol>
<li>
<p>Compute the following matrix products:</p>
<span class="math display">\[\bo{H}\M\quad \mbox{and} \quad \mathbf{A}\D\]</span>
<li>
From the previous computation, <span class="math inline">\(\mathbf{A}\D\)</span>, do you notice anything interesting about multiplying a matrix by a diagonal matrix on the right? Can you generalize what happens in words? (<em>Hint:</em> see Example <a href="mult.html#exm:diagmult">2.8</a> and Exercise <a href="mult.html#exr:diagmultexer">2.5</a>.<br />

</ol>
<li>
Is matrix multiplication commutative?
<li>
<strong>Different Views of Matrix Multiplication:</strong> Consider the matrix product
<span class="math inline">\(\mathbf{A}\mathbf{B}\)</span> where <span class="math display">\[\mathbf{A} = \left(\begin{matrix} 1 &amp; 2\\3&amp;4 \end{matrix}\right) \quad \mathbf{B} = \left(\begin{matrix} 2&amp;5\\1&amp;3\end{matrix}\right)\]</span>
Let <span class="math inline">\(\C=\mathbf{A}\mathbf{B}\)</span>.
<ol style="list-style-type:lower-roman">
<li>
Compute the matrix product <span class="math inline">\(\C\)</span>.
<li>
Compute the matrix-vector product <span class="math inline">\(\mathbf{A}\mathbf{B}_{\star 1}\)</span> and show that this is the first column of <span class="math inline">\(\C\)</span>. (Likewise, <span class="math inline">\(\mathbf{A}\mathbf{B}_{\star 2}\)</span> is the second column of <span class="math inline">\(\C\)</span>.) (<em>Matrix multiplication can be viewed as a collection of matrix-vector products.</em>)
<li>
Compute the two outer products using columns of <span class="math inline">\(\mathbf{A}\)</span> and rows of <span class="math inline">\(\mathbf{B}\)</span> and show that
<span class="math display">\[\acol{1}\brow{1} + \acol{2}\brow{2} = \C\]</span> (<em>Matrix multiplication can be viewed as the sum of outer products.</em>)
<li>
Since <span class="math inline">\(\mathbf{A}\mathbf{B}_{\star 1}\)</span> is the first column of <span class="math inline">\(\C\)</span>, show how <span class="math inline">\(\C_{\star 1}\)</span> can be written as a linear combination of columns of <span class="math inline">\(\mathbf{A}\)</span>. (<em>Matrix multiplication can be viewed as a collection of linear combinations of columns of the first matrix.</em>)
<li>
Finally, note that <span class="math inline">\(\arow{1}\mathbf{B}\)</span> will give the first row of <span class="math inline">\(\C\)</span>. (<em>This amounts to a linear combination of rows - can you see that?</em>)
</ol>
</ol>
</div>
<div id="list-of-key-terms" class="section level2 unnumbered">
<h2>List of Key Terms</h2>
<ul>
<li>addition</li>
<li>subtraction</li>
<li>equal matrices</li>
<li>scalar multiplication</li>
<li>inner product</li>
<li>matrix product</li>
<li>linear combination</li>
<li>outer product</li>
<li>multiplicative identity</li>
<li>matrix inverse</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="multapp.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/shainarace/LinearAlgebra/edit/master/015-mult.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/shainarace/LinearAlgebra/blob/master/015-mult.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": {
"engine": "lunr"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
