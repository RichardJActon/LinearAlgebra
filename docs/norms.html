<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Norms, Similarity, and Distance | Linear Algebra for Data Science</title>
  <meta name="description" content="A traditional textbook fused with a collection of data science case studies that was engineered to weave practicality and applied problem solving into a linear algebra curriculum" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Norms, Similarity, and Distance | Linear Algebra for Data Science" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A traditional textbook fused with a collection of data science case studies that was engineered to weave practicality and applied problem solving into a linear algebra curriculum" />
  <meta name="github-repo" content="shainarace/linearalgebra" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Norms, Similarity, and Distance | Linear Algebra for Data Science" />
  
  <meta name="twitter:description" content="A traditional textbook fused with a collection of data science case studies that was engineered to weave practicality and applied problem solving into a linear algebra curriculum" />
  

<meta name="author" content="Shaina Race Bennett, PhD" />


<meta name="date" content="2021-07-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="solvesys.html"/>
<link rel="next" href="linind.html"/>
<script src="libs/header-attrs-2.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.4.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.57.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.57.1/plotly-latest.min.js"></script>
<script src="libs/d3-4.5.0/d3.min.js"></script>
<script src="libs/forceNetwork-binding-0.4/forceNetwork.js"></script>
<!DOCTYPE html>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  loader: {load: ['[tex]/cancel', '[tex]/systeme']},
  TeX: {
    packages: {'[+]': ['cancel','systeme','boldsymbol']}
  }
});
</script>


<script type="text/javascript" id="MathJax-script"
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

<span class="math" style="display:none">
\(\usepackage{amsfonts}
\usepackage{cancel}
\usepackage{amsmath}
\usepackage{systeme}
\usepackage{amsthm}
\usepackage{xcolor}
\usepackage{boldsymbol}
\newenvironment{am}[1]{%
  \left(\begin{array}{@{}*{#1}{c}|c@{}}
}{%
  \end{array}\right)
}
\newcommand{\bordermatrix}[3]{\begin{matrix} ~ & \begin{matrix} #1 \end{matrix} \\ \begin{matrix} #2 \end{matrix}\hspace{-1em} & #3 \end{matrix}}
\newcommand{\eref}[1]{Example~\ref{#1}}
\newcommand{\fref}[1]{Figure~\ref{#1}}
\newcommand{\tref}[1]{Table~\ref{#1}}
\newcommand{\sref}[1]{Section~\ref{#1}}
\newcommand{\cref}[1]{Chapter~\ref{#1}}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{fact}{Fact}
\newtheorem{thm}{Theorem}
\newtheorem{example}{Example}[section]
\newcommand{\To}{\Rightarrow}
\newcommand{\del}{\nabla}
\renewcommand{\Re}{\mathbb{R}}
\renewcommand{\O}{\mathcal{O}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\eps}{\epsilon}
\newcommand{\cont}{\Rightarrow \Leftarrow}
\newcommand{\back}{\backslash}
\newcommand{\norm}[1]{\|{#1}\|}
\newcommand{\abs}[1]{|{#1}|}
\newcommand{\ip}[1]{\langle{#1}\rangle}
\newcommand{\bo}{\mathbf}
\newcommand{\mean}{\boldsymbol\mu}
\newcommand{\cov}{\boldsymbol\Sigma}
\newcommand{\wt}{\widetilde}
\newcommand{\p}{\textbf{p}}
\newcommand{\ff}{\textbf{f}}
\newcommand{\aj}{\textbf{a}_j}
\newcommand{\ajhat}{\widehat{\textbf{a}_j}}
\newcommand{\I}{\textbf{I}}
\newcommand{\A}{\textbf{A}}
\newcommand{\B}{\textbf{B}}
\newcommand{\bL}{\textbf{L}}
\newcommand{\bP}{\textbf{P}}
\newcommand{\bD}{\textbf{D}}
\newcommand{\bS}{\textbf{S}}
\newcommand{\bW}{\textbf{W}}
\newcommand{\id}{\textbf{I}}
\newcommand{\M}{\textbf{M}}
\renewcommand{\B}{\textbf{B}}
\newcommand{\V}{\textbf{V}}
\newcommand{\U}{\textbf{U}}
\newcommand{\y}{\textbf{y}}
\newcommand{\bv}{\textbf{v}}
\renewcommand{\v}{\textbf{v}}
\newcommand{\cC}{\mathscr{C}}
\newcommand{\e}{\textbf{e}}
\newcommand{\w}{\textbf{w}}
\newcommand{\h}{\textbf{h}}
\renewcommand{\b}{\textbf{b}}
\renewcommand{\a}{\textbf{a}}
\renewcommand{\u}{\textbf{u}}
\newcommand{\C}{\textbf{C}}
\newcommand{\D}{\textbf{D}}
\newcommand{\cc}{\textbf{c}}
\newcommand{\Q}{\textbf{Q}}
\renewcommand{\S}{\textbf{S}}
\newcommand{\X}{\textbf{X}}
\newcommand{\Z}{\textbf{Z}}
\newcommand{\z}{\textbf{z}}
\newcommand{\Y}{\textbf{Y}}
\newcommand{\plane}{\textit{P}}
\newcommand{\mxn}{$m\mbox{x}n$}
\newcommand{\kmeans}{\textit{k}-means\,}
\newcommand{\bbeta}{\boldsymbol\beta}
\newcommand{\ssigma}{\boldsymbol\Sigma}
\newcommand{\xrow}[1]{\mathbf{X}_{{#1}\star}}
\newcommand{\xcol}[1]{\mathbf{X}_{\star{#1}}}
\newcommand{\yrow}[1]{\mathbf{Y}_{{#1}\star}}
\newcommand{\ycol}[1]{\mathbf{Y}_{\star{#1}}}
\newcommand{\crow}[1]{\mathbf{C}_{{#1}\star}}
\newcommand{\ccol}[1]{\mathbf{C}_{\star{#1}}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\arow}[1]{\mathbf{A}_{{#1}\star}}
\newcommand{\acol}[1]{\mathbf{A}_{\star{#1}}}
\newcommand{\brow}[1]{\mathbf{B}_{{#1}\star}}
\newcommand{\bcol}[1]{\mathbf{B}_{\star{#1}}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\renewcommand{\t}{ \indent}
\newcommand{\nt}{ \indent}
\newcommand{\x}{\mathbf{x}}
\renewcommand{\Y}{\mathbf{Y}}
\newcommand{\ep}{\mathbf{\epsilon}}
\renewcommand{\pm}{\left(\begin{matrix}}
\renewcommand{\mp}{\end{matrix}\right)}
\newcommand{\bm}{\bordermatrix}
\usepackage{pdfpages,cancel}
\newenvironment{code}{\Verbatim [formatcom=\color{blue}]}{\endVerbatim}
\)
</span>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><center><img src="figs/matrixlogo.jpg" width="50"></center></li>
<li><center><strong> Linear Algebra for Data Science </strong></center></li>
<li><center><strong> with examples in R </strong></center></li>

<li class="divider"></li>
<li><a href="index.html#section"></a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#structure-of-the-book"><i class="fa fa-check"></i>Structure of the book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i>About the author</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#what-is-linear-algebra"><i class="fa fa-check"></i><b>1.1</b> What is Linear Algebra?</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#why-linear-algebra"><i class="fa fa-check"></i><b>1.2</b> Why Linear Algebra</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#describing-matrices-and-vectors"><i class="fa fa-check"></i><b>1.3</b> Describing Matrices and Vectors</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="intro.html"><a href="intro.html#dimensionsize-of-a-matrix"><i class="fa fa-check"></i><b>1.3.1</b> Dimension/Size of a Matrix</a></li>
<li class="chapter" data-level="1.3.2" data-path="intro.html"><a href="intro.html#ij-notation"><i class="fa fa-check"></i><b>1.3.2</b> <span class="math inline">\((i,j)\)</span> Notation</a></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#example-defining-social-networks"><i class="fa fa-check"></i>Example: Defining social networks</a></li>
<li class="chapter" data-level="1.3.3" data-path="intro.html"><a href="intro.html#introcorr"><i class="fa fa-check"></i><b>1.3.3</b> Example: Correlation matrices</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#vectors"><i class="fa fa-check"></i><b>1.4</b> Vectors</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="intro.html"><a href="intro.html#vector-geometry-n-space"><i class="fa fa-check"></i><b>1.4.1</b> Vector Geometry: <span class="math inline">\(n\)</span>-space</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#matrix-operations"><i class="fa fa-check"></i><b>1.5</b> Matrix Operations</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="intro.html"><a href="intro.html#transpose"><i class="fa fa-check"></i><b>1.5.1</b> Transpose</a></li>
<li class="chapter" data-level="1.5.2" data-path="intro.html"><a href="intro.html#trace-of-a-matrix"><i class="fa fa-check"></i><b>1.5.2</b> Trace of a Matrix</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#special"><i class="fa fa-check"></i><b>1.6</b> Special Matrices and Vectors</a></li>
<li class="chapter" data-level="1.7" data-path="intro.html"><a href="intro.html#summary-of-conventional-notation"><i class="fa fa-check"></i><b>1.7</b> Summary of Conventional Notation</a></li>
<li class="chapter" data-level="1.8" data-path="intro.html"><a href="intro.html#exercises"><i class="fa fa-check"></i><b>1.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="mult.html"><a href="mult.html"><i class="fa fa-check"></i><b>2</b> Matrix Arithmetic</a>
<ul>
<li class="chapter" data-level="2.1" data-path="mult.html"><a href="mult.html#matrix-addition-subtraction-and-scalar-multiplication"><i class="fa fa-check"></i><b>2.1</b> Matrix Addition, Subtraction, and Scalar Multiplication</a></li>
<li class="chapter" data-level="2.2" data-path="mult.html"><a href="mult.html#sec:vectoradd"><i class="fa fa-check"></i><b>2.2</b> Geometry of Vector Addition and Scalar Multiplication</a></li>
<li class="chapter" data-level="2.3" data-path="mult.html"><a href="mult.html#linear-combinations"><i class="fa fa-check"></i><b>2.3</b> Linear Combinations</a></li>
<li class="chapter" data-level="2.4" data-path="mult.html"><a href="mult.html#matrix-multiplication"><i class="fa fa-check"></i><b>2.4</b> Matrix Multiplication</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="mult.html"><a href="mult.html#the-inner-product"><i class="fa fa-check"></i><b>2.4.1</b> The Inner Product</a></li>
<li class="chapter" data-level="2.4.2" data-path="mult.html"><a href="mult.html#matrix-product"><i class="fa fa-check"></i><b>2.4.2</b> Matrix Product</a></li>
<li class="chapter" data-level="2.4.3" data-path="mult.html"><a href="mult.html#matrix-vector-product"><i class="fa fa-check"></i><b>2.4.3</b> Matrix-Vector Product</a></li>
<li class="chapter" data-level="" data-path="mult.html"><a href="mult.html#linear-combination-view-of-matrix-products"><i class="fa fa-check"></i>Linear Combination view of Matrix Products</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="mult.html"><a href="mult.html#vector-outer-products"><i class="fa fa-check"></i><b>2.5</b> Vector Outer Products</a></li>
<li class="chapter" data-level="2.6" data-path="mult.html"><a href="mult.html#the-identity-and-the-matrix-inverse"><i class="fa fa-check"></i><b>2.6</b> The Identity and the Matrix Inverse</a></li>
<li class="chapter" data-level="2.7" data-path="mult.html"><a href="mult.html#exercises-1"><i class="fa fa-check"></i><b>2.7</b> Exercises</a></li>
<li class="chapter" data-level="" data-path="mult.html"><a href="mult.html#list-of-key-terms"><i class="fa fa-check"></i>List of Key Terms</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="multapp.html"><a href="multapp.html"><i class="fa fa-check"></i><b>3</b> Applications of Matrix Multiplication</a>
<ul>
<li class="chapter" data-level="3.1" data-path="multapp.html"><a href="multapp.html#systems-of-equations"><i class="fa fa-check"></i><b>3.1</b> Systems of Equations</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="multapp.html"><a href="multapp.html#big-systems-of-equations"><i class="fa fa-check"></i><b>3.1.1</b> <em>Big</em> Systems of Equations</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="multapp.html"><a href="multapp.html#regression-analysis"><i class="fa fa-check"></i><b>3.2</b> Regression Analysis</a></li>
<li class="chapter" data-level="3.3" data-path="multapp.html"><a href="multapp.html#linear-combinations-1"><i class="fa fa-check"></i><b>3.3</b> Linear Combinations</a></li>
<li class="chapter" data-level="3.4" data-path="multapp.html"><a href="multapp.html#multapp-ex"><i class="fa fa-check"></i><b>3.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="r-programming-basics.html"><a href="r-programming-basics.html"><i class="fa fa-check"></i><b>4</b> R Programming Basics</a></li>
<li class="chapter" data-level="5" data-path="solvesys.html"><a href="solvesys.html"><i class="fa fa-check"></i><b>5</b> Solving Systems of Equations</a>
<ul>
<li class="chapter" data-level="5.1" data-path="solvesys.html"><a href="solvesys.html#gaussian-elimination"><i class="fa fa-check"></i><b>5.1</b> Gaussian Elimination</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="solvesys.html"><a href="solvesys.html#row-operations"><i class="fa fa-check"></i><b>5.1.1</b> Row Operations</a></li>
<li class="chapter" data-level="5.1.2" data-path="solvesys.html"><a href="solvesys.html#the-augmented-matrix"><i class="fa fa-check"></i><b>5.1.2</b> The Augmented Matrix</a></li>
<li class="chapter" data-level="5.1.3" data-path="solvesys.html"><a href="solvesys.html#gaussian-elimination-summary"><i class="fa fa-check"></i><b>5.1.3</b> Gaussian Elimination Summary</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="solvesys.html"><a href="solvesys.html#gauss-jordan-elimination"><i class="fa fa-check"></i><b>5.2</b> Gauss-Jordan Elimination</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="solvesys.html"><a href="solvesys.html#gauss-jordan-elimination-summary"><i class="fa fa-check"></i><b>5.2.1</b> Gauss-Jordan Elimination Summary</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="solvesys.html"><a href="solvesys.html#three-types-of-systems"><i class="fa fa-check"></i><b>5.3</b> Three Types of Systems</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="solvesys.html"><a href="solvesys.html#uniquesol"><i class="fa fa-check"></i><b>5.3.1</b> The Unique Solution Case</a></li>
<li class="chapter" data-level="5.3.2" data-path="solvesys.html"><a href="solvesys.html#inconsistent"><i class="fa fa-check"></i><b>5.3.2</b> The Inconsistent Case</a></li>
<li class="chapter" data-level="5.3.3" data-path="solvesys.html"><a href="solvesys.html#infinitesol"><i class="fa fa-check"></i><b>5.3.3</b> The Infinite Solutions Case</a></li>
<li class="chapter" data-level="5.3.4" data-path="solvesys.html"><a href="solvesys.html#matrix-rank"><i class="fa fa-check"></i><b>5.3.4</b> Matrix Rank</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="solvesys.html"><a href="solvesys.html#solving-matrix-equations"><i class="fa fa-check"></i><b>5.4</b> Solving Matrix Equations</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="solvesys.html"><a href="solvesys.html#solving-for-the-inverse-of-a-matrix"><i class="fa fa-check"></i><b>5.4.1</b> Solving for the Inverse of a Matrix</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="solvesys.html"><a href="solvesys.html#exercises-2"><i class="fa fa-check"></i><b>5.5</b> Exercises</a></li>
<li class="chapter" data-level="5.6" data-path="solvesys.html"><a href="solvesys.html#list-of-key-terms-1"><i class="fa fa-check"></i><b>5.6</b> List of Key Terms</a></li>
<li class="chapter" data-level="5.7" data-path="solvesys.html"><a href="solvesys.html#gauss-jordan-elimination-in-r"><i class="fa fa-check"></i><b>5.7</b> Gauss-Jordan Elimination in R</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="norms.html"><a href="norms.html"><i class="fa fa-check"></i><b>6</b> Norms, Similarity, and Distance</a>
<ul>
<li class="chapter" data-level="6.1" data-path="norms.html"><a href="norms.html#sec-norms"><i class="fa fa-check"></i><b>6.1</b> Norms and Distances</a></li>
<li class="chapter" data-level="6.2" data-path="norms.html"><a href="norms.html#other-useful-norms-and-distances"><i class="fa fa-check"></i><b>6.2</b> Other useful norms and distances</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="norms.html"><a href="norms.html#norm-star_1."><i class="fa fa-check"></i><b>6.2.1</b> 1-norm, <span class="math inline">\(\|\star\|_1\)</span>.</a></li>
<li class="chapter" data-level="6.2.2" data-path="norms.html"><a href="norms.html#infty-norm-star_infty."><i class="fa fa-check"></i><b>6.2.2</b> <span class="math inline">\(\infty\)</span>-norm, <span class="math inline">\(\|\star\|_{\infty}\)</span>.</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="norms.html"><a href="norms.html#inner-products"><i class="fa fa-check"></i><b>6.3</b> Inner Products</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="norms.html"><a href="norms.html#covariance"><i class="fa fa-check"></i><b>6.3.1</b> Covariance</a></li>
<li class="chapter" data-level="6.3.2" data-path="norms.html"><a href="norms.html#mahalanobis-distance"><i class="fa fa-check"></i><b>6.3.2</b> Mahalanobis Distance</a></li>
<li class="chapter" data-level="6.3.3" data-path="norms.html"><a href="norms.html#angular-distance"><i class="fa fa-check"></i><b>6.3.3</b> Angular Distance</a></li>
<li class="chapter" data-level="6.3.4" data-path="norms.html"><a href="norms.html#correlation"><i class="fa fa-check"></i><b>6.3.4</b> Correlation</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="norms.html"><a href="norms.html#exercises-3"><i class="fa fa-check"></i><b>6.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="linind.html"><a href="linind.html"><i class="fa fa-check"></i><b>7</b> Linear Independence</a>
<ul>
<li class="chapter" data-level="7.1" data-path="linind.html"><a href="linind.html#linear-independence"><i class="fa fa-check"></i><b>7.1</b> Linear Independence</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="linind.html"><a href="linind.html#determining-linear-independence"><i class="fa fa-check"></i><b>7.1.1</b> Determining Linear Independence</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="linind.html"><a href="linind.html#span"><i class="fa fa-check"></i><b>7.2</b> Span of Vectors</a></li>
<li class="chapter" data-level="7.3" data-path="linind.html"><a href="linind.html#exercises-4"><i class="fa fa-check"></i><b>7.3</b> Exercises</a></li>
<li class="chapter" data-level="" data-path="linind.html"><a href="linind.html#list-of-key-terms-2"><i class="fa fa-check"></i>List of Key Terms</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="basis.html"><a href="basis.html"><i class="fa fa-check"></i><b>8</b> Basis and Change of Basis</a>
<ul>
<li class="chapter" data-level="8.1" data-path="basis.html"><a href="basis.html#exercises-5"><i class="fa fa-check"></i><b>8.1</b> Exercises</a></li>
<li class="chapter" data-level="" data-path="basis.html"><a href="basis.html#list-of-key-terms-3"><i class="fa fa-check"></i>List of Key Terms</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="orthog.html"><a href="orthog.html"><i class="fa fa-check"></i><b>9</b> Orthogonality</a>
<ul>
<li class="chapter" data-level="9.1" data-path="orthog.html"><a href="orthog.html#orthonormal-basis"><i class="fa fa-check"></i><b>9.1</b> Orthonormal Basis</a></li>
<li class="chapter" data-level="9.2" data-path="orthog.html"><a href="orthog.html#orthogonal-projection"><i class="fa fa-check"></i><b>9.2</b> Orthogonal Projection</a></li>
<li class="chapter" data-level="9.3" data-path="orthog.html"><a href="orthog.html#why"><i class="fa fa-check"></i><b>9.3</b> Why??</a></li>
<li class="chapter" data-level="9.4" data-path="orthog.html"><a href="orthog.html#exercises-6"><i class="fa fa-check"></i><b>9.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="leastsquares.html"><a href="leastsquares.html"><i class="fa fa-check"></i><b>10</b> Least Squares</a>
<ul>
<li class="chapter" data-level="10.1" data-path="leastsquares.html"><a href="leastsquares.html#introducing-error"><i class="fa fa-check"></i><b>10.1</b> Introducing Error</a></li>
<li class="chapter" data-level="10.2" data-path="leastsquares.html"><a href="leastsquares.html#why-the-normal-equations"><i class="fa fa-check"></i><b>10.2</b> Why the normal equations?</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="leastsquares.html"><a href="leastsquares.html#geometrical-interpretation"><i class="fa fa-check"></i><b>10.2.1</b> Geometrical Interpretation</a></li>
<li class="chapter" data-level="10.2.2" data-path="leastsquares.html"><a href="leastsquares.html#calculus-derivation"><i class="fa fa-check"></i><b>10.2.2</b> Calculus Derivation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="lsapp.html"><a href="lsapp.html"><i class="fa fa-check"></i><b>11</b> Applications of Least Squares</a>
<ul>
<li class="chapter" data-level="11.1" data-path="lsapp.html"><a href="lsapp.html#simple-linear-regression"><i class="fa fa-check"></i><b>11.1</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="lsapp.html"><a href="lsapp.html#cars-data"><i class="fa fa-check"></i><b>11.1.1</b> Cars Data</a></li>
<li class="chapter" data-level="11.1.2" data-path="lsapp.html"><a href="lsapp.html#setting-up-the-normal-equations"><i class="fa fa-check"></i><b>11.1.2</b> Setting up the Normal Equations</a></li>
<li class="chapter" data-level="11.1.3" data-path="lsapp.html"><a href="lsapp.html#solving-for-parameter-estimates-and-statistics"><i class="fa fa-check"></i><b>11.1.3</b> Solving for Parameter Estimates and Statistics</a></li>
<li class="chapter" data-level="11.1.4" data-path="lsapp.html"><a href="lsapp.html#ols-in-r-via-lm"><i class="fa fa-check"></i><b>11.1.4</b> OLS in R via <code>lm()</code></a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="lsapp.html"><a href="lsapp.html#multiple-linear-regression"><i class="fa fa-check"></i><b>11.2</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="lsapp.html"><a href="lsapp.html#bike-sharing-dataset"><i class="fa fa-check"></i><b>11.2.1</b> Bike Sharing Dataset</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="eigen.html"><a href="eigen.html"><i class="fa fa-check"></i><b>12</b> Eigenvalues and Eigenvectors</a>
<ul>
<li class="chapter" data-level="12.1" data-path="eigen.html"><a href="eigen.html#diagonalization"><i class="fa fa-check"></i><b>12.1</b> Diagonalization</a></li>
<li class="chapter" data-level="12.2" data-path="eigen.html"><a href="eigen.html#geometric-interpretation-of-eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>12.2</b> Geometric Interpretation of Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="12.3" data-path="eigen.html"><a href="eigen.html#exercises-7"><i class="fa fa-check"></i><b>12.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>13</b> Principal Components Analysis</a>
<ul>
<li class="chapter" data-level="13.1" data-path="pca.html"><a href="pca.html#geometrical-comparison-with-least-squares"><i class="fa fa-check"></i><b>13.1</b> Geometrical comparison with Least Squares</a></li>
<li class="chapter" data-level="13.2" data-path="pca.html"><a href="pca.html#covariance-or-correlation-matrix"><i class="fa fa-check"></i><b>13.2</b> Covariance or Correlation Matrix?</a></li>
<li class="chapter" data-level="13.3" data-path="pca.html"><a href="pca.html#pca-in-r"><i class="fa fa-check"></i><b>13.3</b> PCA in R</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="pca.html"><a href="pca.html#covariance-pca"><i class="fa fa-check"></i><b>13.3.1</b> Covariance PCA</a></li>
<li class="chapter" data-level="13.3.2" data-path="pca.html"><a href="pca.html#principal-components-loadings-and-variance-explained"><i class="fa fa-check"></i><b>13.3.2</b> Principal Components, Loadings, and Variance Explained</a></li>
<li class="chapter" data-level="13.3.3" data-path="pca.html"><a href="pca.html#scores-and-pca-projection"><i class="fa fa-check"></i><b>13.3.3</b> Scores and PCA Projection</a></li>
<li class="chapter" data-level="13.3.4" data-path="pca.html"><a href="pca.html#pca-functions-in-r"><i class="fa fa-check"></i><b>13.3.4</b> PCA functions in R</a></li>
<li class="chapter" data-level="13.3.5" data-path="pca.html"><a href="pca.html#the-biplot"><i class="fa fa-check"></i><b>13.3.5</b> The Biplot</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="pca.html"><a href="pca.html#variable-clustering-with-pca"><i class="fa fa-check"></i><b>13.4</b> Variable Clustering with PCA</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="pca.html"><a href="pca.html#correlation-pca"><i class="fa fa-check"></i><b>13.4.1</b> Correlation PCA</a></li>
<li class="chapter" data-level="13.4.2" data-path="pca.html"><a href="pca.html#which-projection-is-better"><i class="fa fa-check"></i><b>13.4.2</b> Which Projection is Better?</a></li>
<li class="chapter" data-level="13.4.3" data-path="pca.html"><a href="pca.html#beware-of-biplots"><i class="fa fa-check"></i><b>13.4.3</b> Beware of biplots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="pcaapp.html"><a href="pcaapp.html"><i class="fa fa-check"></i><b>14</b> Applications of Principal Components</a>
<ul>
<li class="chapter" data-level="14.1" data-path="pcaapp.html"><a href="pcaapp.html#dimension-reduction"><i class="fa fa-check"></i><b>14.1</b> Dimension reduction</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="pcaapp.html"><a href="pcaapp.html#feature-selection"><i class="fa fa-check"></i><b>14.1.1</b> Feature Selection</a></li>
<li class="chapter" data-level="14.1.2" data-path="pcaapp.html"><a href="pcaapp.html#feature-extraction"><i class="fa fa-check"></i><b>14.1.2</b> Feature Extraction</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="pcaapp.html"><a href="pcaapp.html#exploratory-analysis"><i class="fa fa-check"></i><b>14.2</b> Exploratory Analysis</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="pcaapp.html"><a href="pcaapp.html#uk-food-consumption"><i class="fa fa-check"></i><b>14.2.1</b> UK Food Consumption</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="pcaapp.html"><a href="pcaapp.html#fifa-soccer-players"><i class="fa fa-check"></i><b>14.3</b> FIFA Soccer Players</a></li>
<li class="chapter" data-level="14.4" data-path="pcaapp.html"><a href="pcaapp.html#cancer-genetics"><i class="fa fa-check"></i><b>14.4</b> Cancer Genetics</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="pcaapp.html"><a href="pcaapp.html#computing-the-pca"><i class="fa fa-check"></i><b>14.4.1</b> Computing the PCA</a></li>
<li class="chapter" data-level="14.4.2" data-path="pcaapp.html"><a href="pcaapp.html#d-plot-with-package"><i class="fa fa-check"></i><b>14.4.2</b> 3D plot with  package</a></li>
<li class="chapter" data-level="14.4.3" data-path="pcaapp.html"><a href="pcaapp.html#d-plot-with-package-1"><i class="fa fa-check"></i><b>14.4.3</b> 3D plot with  package</a></li>
<li class="chapter" data-level="14.4.4" data-path="pcaapp.html"><a href="pcaapp.html#variance-explained"><i class="fa fa-check"></i><b>14.4.4</b> Variance explained</a></li>
<li class="chapter" data-level="14.4.5" data-path="pcaapp.html"><a href="pcaapp.html#using-correlation-pca"><i class="fa fa-check"></i><b>14.4.5</b> Using Correlation PCA</a></li>
<li class="chapter" data-level="14.4.6" data-path="pcaapp.html"><a href="pcaapp.html#range-standardization-as-an-alternative-to-covariance-pca"><i class="fa fa-check"></i><b>14.4.6</b> Range standardization as an alternative to covariance PCA</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="svd.html"><a href="svd.html"><i class="fa fa-check"></i><b>15</b> The Singular Value Decomposition (SVD)</a>
<ul>
<li class="chapter" data-level="15.1" data-path="svd.html"><a href="svd.html#resolving-a-matrix-into-components"><i class="fa fa-check"></i><b>15.1</b> Resolving a Matrix into Components</a></li>
<li class="chapter" data-level="15.2" data-path="svd.html"><a href="svd.html#data-compression"><i class="fa fa-check"></i><b>15.2</b> Data Compression</a></li>
<li class="chapter" data-level="15.3" data-path="svd.html"><a href="svd.html#noise-reduction"><i class="fa fa-check"></i><b>15.3</b> Noise Reduction</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="svdapp.html"><a href="svdapp.html"><i class="fa fa-check"></i><b>16</b> Applications of SVD</a>
<ul>
<li class="chapter" data-level="16.1" data-path="svdapp.html"><a href="svdapp.html#tm"><i class="fa fa-check"></i><b>16.1</b> Text Mining</a>
<ul>
<li class="chapter" data-level="16.1.1" data-path="svdapp.html"><a href="svdapp.html#note-about-rows-vs.-columns"><i class="fa fa-check"></i><b>16.1.1</b> Note About Rows vs. Columns</a></li>
<li class="chapter" data-level="16.1.2" data-path="svdapp.html"><a href="svdapp.html#term-weighting"><i class="fa fa-check"></i><b>16.1.2</b> Term Weighting</a></li>
<li class="chapter" data-level="16.1.3" data-path="svdapp.html"><a href="svdapp.html#other-considerations"><i class="fa fa-check"></i><b>16.1.3</b> Other Considerations</a></li>
<li class="chapter" data-level="16.1.4" data-path="svdapp.html"><a href="svdapp.html#latent-semantic-indexing"><i class="fa fa-check"></i><b>16.1.4</b> Latent Semantic Indexing</a></li>
<li class="chapter" data-level="16.1.5" data-path="svdapp.html"><a href="svdapp.html#example"><i class="fa fa-check"></i><b>16.1.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="svdapp.html"><a href="svdapp.html#rappasvd"><i class="fa fa-check"></i><b>16.2</b> Image Compression</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="svdapp.html"><a href="svdapp.html#image-data-in-r"><i class="fa fa-check"></i><b>16.2.1</b> Image data in R</a></li>
<li class="chapter" data-level="16.2.2" data-path="svdapp.html"><a href="svdapp.html#computing-the-svd-of-dr.-rappa"><i class="fa fa-check"></i><b>16.2.2</b> Computing the SVD of Dr. Rappa</a></li>
<li class="chapter" data-level="16.2.3" data-path="svdapp.html"><a href="svdapp.html#the-noise"><i class="fa fa-check"></i><b>16.2.3</b> The Noise</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="fa.html"><a href="fa.html"><i class="fa fa-check"></i><b>17</b> Factor Analysis</a>
<ul>
<li class="chapter" data-level="17.1" data-path="fa.html"><a href="fa.html#assumptions-of-factor-analysis"><i class="fa fa-check"></i><b>17.1</b> Assumptions of Factor Analysis</a></li>
<li class="chapter" data-level="17.2" data-path="fa.html"><a href="fa.html#determining-factorability"><i class="fa fa-check"></i><b>17.2</b> Determining Factorability</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="fa.html"><a href="fa.html#visual-examination-of-correlation-matrix"><i class="fa fa-check"></i><b>17.2.1</b> Visual Examination of Correlation Matrix</a></li>
<li class="chapter" data-level="17.2.2" data-path="fa.html"><a href="fa.html#barletts-sphericity-test"><i class="fa fa-check"></i><b>17.2.2</b> Barlett’s Sphericity Test</a></li>
<li class="chapter" data-level="17.2.3" data-path="fa.html"><a href="fa.html#kaiser-meyer-olkin-kmo-measure-of-sampling-adequacy"><i class="fa fa-check"></i><b>17.2.3</b> Kaiser-Meyer-Olkin (KMO) Measure of Sampling Adequacy</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="fa.html"><a href="fa.html#communalities"><i class="fa fa-check"></i><b>17.3</b> Communalities</a></li>
<li class="chapter" data-level="17.4" data-path="fa.html"><a href="fa.html#number-of-factors"><i class="fa fa-check"></i><b>17.4</b> Number of Factors</a></li>
<li class="chapter" data-level="17.5" data-path="fa.html"><a href="fa.html#rotation-of-factors"><i class="fa fa-check"></i><b>17.5</b> Rotation of Factors</a></li>
<li class="chapter" data-level="17.6" data-path="fa.html"><a href="fa.html#fa-apps"><i class="fa fa-check"></i><b>17.6</b> Methods of Factor Analysis</a>
<ul>
<li class="chapter" data-level="17.6.1" data-path="fa.html"><a href="fa.html#pca-rotations"><i class="fa fa-check"></i><b>17.6.1</b> PCA Rotations</a></li>
</ul></li>
<li class="chapter" data-level="17.7" data-path="fa.html"><a href="fa.html#case-study-personality-tests"><i class="fa fa-check"></i><b>17.7</b> Case Study: Personality Tests</a>
<ul>
<li class="chapter" data-level="17.7.1" data-path="fa.html"><a href="fa.html#raw-pca-factors"><i class="fa fa-check"></i><b>17.7.1</b> Raw PCA Factors</a></li>
<li class="chapter" data-level="17.7.2" data-path="fa.html"><a href="fa.html#rotated-principal-components"><i class="fa fa-check"></i><b>17.7.2</b> Rotated Principal Components</a></li>
<li class="chapter" data-level="17.7.3" data-path="fa.html"><a href="fa.html#visualizing-rotation-via-biplots"><i class="fa fa-check"></i><b>17.7.3</b> Visualizing Rotation via BiPlots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="otherdimred.html"><a href="otherdimred.html"><i class="fa fa-check"></i><b>18</b> Dimension Reduction for Visualization</a>
<ul>
<li class="chapter" data-level="18.1" data-path="otherdimred.html"><a href="otherdimred.html#multidimensional-scaling"><i class="fa fa-check"></i><b>18.1</b> Multidimensional Scaling</a>
<ul>
<li class="chapter" data-level="18.1.1" data-path="otherdimred.html"><a href="otherdimred.html#mds-of-leukemia-dataset"><i class="fa fa-check"></i><b>18.1.1</b> MDS of Leukemia dataset</a></li>
<li class="chapter" data-level="" data-path="otherdimred.html"><a href="otherdimred.html#a-note-on-standardization"><i class="fa fa-check"></i>A note on standardization</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="sna.html"><a href="sna.html"><i class="fa fa-check"></i><b>19</b> Social Network Analysis</a>
<ul>
<li class="chapter" data-level="19.1" data-path="sna.html"><a href="sna.html#working-with-network-data"><i class="fa fa-check"></i><b>19.1</b> Working with Network Data</a></li>
<li class="chapter" data-level="19.2" data-path="sna.html"><a href="sna.html#network-visualization---igraph-package"><i class="fa fa-check"></i><b>19.2</b> Network Visualization - <code>igraph</code> package</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="sna.html"><a href="sna.html#adding-attribute-information-to-your-visualization"><i class="fa fa-check"></i><b>19.2.1</b> Adding attribute information to your visualization</a></li>
<li class="chapter" data-level="19.2.2" data-path="sna.html"><a href="sna.html#preparing-the-data-for-networkd3"><i class="fa fa-check"></i><b>19.2.2</b> Preparing the data for <code>networkD3</code></a></li>
<li class="chapter" data-level="19.2.3" data-path="sna.html"><a href="sna.html#creating-an-interactive-visualization-with-networkd3"><i class="fa fa-check"></i><b>19.2.3</b> Creating an Interactive Visualization with <code>networkD3</code></a></li>
<li class="chapter" data-level="19.2.4" data-path="sna.html"><a href="sna.html#saving-your-interactive-visualization-to-.html"><i class="fa fa-check"></i><b>19.2.4</b> Saving your Interactive Visualization to .html</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Clustering</b></span></li>
<li class="chapter" data-level="20" data-path="clusintro.html"><a href="clusintro.html"><i class="fa fa-check"></i><b>20</b> Introduction</a>
<ul>
<li class="chapter" data-level="20.1" data-path="clusintro.html"><a href="clusintro.html#mathematical-setup"><i class="fa fa-check"></i><b>20.1</b> Mathematical Setup</a>
<ul>
<li class="chapter" data-level="20.1.1" data-path="clusintro.html"><a href="clusintro.html#data"><i class="fa fa-check"></i><b>20.1.1</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="clusintro.html"><a href="clusintro.html#the-number-of-clusters-k"><i class="fa fa-check"></i><b>20.2</b> The Number of Clusters, <span class="math inline">\(k\)</span></a></li>
<li class="chapter" data-level="20.3" data-path="clusintro.html"><a href="clusintro.html#partitioning-of-graphs-and-networks"><i class="fa fa-check"></i><b>20.3</b> Partitioning of Graphs and Networks</a></li>
<li class="chapter" data-level="20.4" data-path="clusintro.html"><a href="clusintro.html#history-of-data-clustering"><i class="fa fa-check"></i><b>20.4</b> History of Data Clustering</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="clusteralgos.html"><a href="clusteralgos.html"><i class="fa fa-check"></i><b>21</b> Algorithms for Data Clustering</a>
<ul>
<li class="chapter" data-level="21.1" data-path="clusteralgos.html"><a href="clusteralgos.html#hc"><i class="fa fa-check"></i><b>21.1</b> Hierarchical Algorithms</a>
<ul>
<li class="chapter" data-level="21.1.1" data-path="clusteralgos.html"><a href="clusteralgos.html#agglomerative-hierarchical-clustering"><i class="fa fa-check"></i><b>21.1.1</b> Agglomerative Hierarchical Clustering</a></li>
<li class="chapter" data-level="21.1.2" data-path="clusteralgos.html"><a href="clusteralgos.html#principal-direction-divisive-partitioning-pddp"><i class="fa fa-check"></i><b>21.1.2</b> Principal Direction Divisive Partitioning (PDDP)</a></li>
</ul></li>
<li class="chapter" data-level="21.2" data-path="clusteralgos.html"><a href="clusteralgos.html#kmeanshistory"><i class="fa fa-check"></i><b>21.2</b> Iterative Partitional Algorithms</a>
<ul>
<li class="chapter" data-level="21.2.1" data-path="clusteralgos.html"><a href="clusteralgos.html#early-partitional-algorithms"><i class="fa fa-check"></i><b>21.2.1</b> Early Partitional Algorithms</a></li>
<li class="chapter" data-level="21.2.2" data-path="clusteralgos.html"><a href="clusteralgos.html#kmeans"><i class="fa fa-check"></i><b>21.2.2</b> <span class="math inline">\(k\)</span>-means</a></li>
<li class="chapter" data-level="21.2.3" data-path="clusteralgos.html"><a href="clusteralgos.html#the-expectation-maximization-em-clustering-algorithm"><i class="fa fa-check"></i><b>21.2.3</b> The Expectation-Maximization (EM) Clustering Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="21.3" data-path="clusteralgos.html"><a href="clusteralgos.html#density-search-algorithms"><i class="fa fa-check"></i><b>21.3</b> Density Search Algorithms</a>
<ul>
<li class="chapter" data-level="21.3.1" data-path="clusteralgos.html"><a href="clusteralgos.html#density-based-spacial-clustering-of-applications-with-noise-dbscan"><i class="fa fa-check"></i><b>21.3.1</b> Density Based Spacial Clustering of Applications with Noise (DBSCAN)</a></li>
</ul></li>
<li class="chapter" data-level="21.4" data-path="clusteralgos.html"><a href="clusteralgos.html#conclusion"><i class="fa fa-check"></i><b>21.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="chap1-5.html"><a href="chap1-5.html"><i class="fa fa-check"></i><b>22</b> Algorithms for Graph Partitioning</a>
<ul>
<li class="chapter" data-level="22.1" data-path="chap1-5.html"><a href="chap1-5.html#spectral"><i class="fa fa-check"></i><b>22.1</b> Spectral Clustering</a></li>
<li class="chapter" data-level="22.2" data-path="chap1-5.html"><a href="chap1-5.html#fiedler-partitioning"><i class="fa fa-check"></i><b>22.2</b> Fiedler Partitioning</a>
<ul>
<li class="chapter" data-level="22.2.1" data-path="chap1-5.html"><a href="chap1-5.html#linear-algebraic-motivation-for-the-fiedler-vector"><i class="fa fa-check"></i><b>22.2.1</b> Linear Algebraic Motivation for the Fiedler vector</a></li>
<li class="chapter" data-level="22.2.2" data-path="chap1-5.html"><a href="chap1-5.html#graph-cuts"><i class="fa fa-check"></i><b>22.2.2</b> Graph Cuts</a></li>
<li class="chapter" data-level="22.2.3" data-path="chap1-5.html"><a href="chap1-5.html#pic"><i class="fa fa-check"></i><b>22.2.3</b> Power Iteration Clustering</a></li>
<li class="chapter" data-level="22.2.4" data-path="chap1-5.html"><a href="chap1-5.html#modularity"><i class="fa fa-check"></i><b>22.2.4</b> Clustering via Modularity Maximization</a></li>
</ul></li>
<li class="chapter" data-level="22.3" data-path="chap1-5.html"><a href="chap1-5.html#stochastic-clustering"><i class="fa fa-check"></i><b>22.3</b> Stochastic Clustering</a>
<ul>
<li class="chapter" data-level="22.3.1" data-path="chap1-5.html"><a href="chap1-5.html#stochastic-clustering-algorithm-sca"><i class="fa fa-check"></i><b>22.3.1</b> Stochastic Clustering Algorithm (SCA)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="validation.html"><a href="validation.html"><i class="fa fa-check"></i><b>23</b> Cluster Validation</a>
<ul>
<li class="chapter" data-level="23.1" data-path="validation.html"><a href="validation.html#internal-validity-metrics"><i class="fa fa-check"></i><b>23.1</b> Internal Validity Metrics</a>
<ul>
<li class="chapter" data-level="23.1.1" data-path="validation.html"><a href="validation.html#common-measures-of-cohesion-and-separation"><i class="fa fa-check"></i><b>23.1.1</b> Common Measures of Cohesion and Separation</a></li>
</ul></li>
<li class="chapter" data-level="23.2" data-path="validation.html"><a href="validation.html#external"><i class="fa fa-check"></i><b>23.2</b> External Validity Metrics</a>
<ul>
<li class="chapter" data-level="23.2.1" data-path="validation.html"><a href="validation.html#accuracy"><i class="fa fa-check"></i><b>23.2.1</b> Accuracy</a></li>
<li class="chapter" data-level="23.2.2" data-path="validation.html"><a href="validation.html#entropy"><i class="fa fa-check"></i><b>23.2.2</b> Entropy</a></li>
<li class="chapter" data-level="23.2.3" data-path="validation.html"><a href="validation.html#purity"><i class="fa fa-check"></i><b>23.2.3</b> Purity</a></li>
<li class="chapter" data-level="23.2.4" data-path="validation.html"><a href="validation.html#mutual-information-mi-and-normalized-mutual-information-nmi"><i class="fa fa-check"></i><b>23.2.4</b> Mutual Information (MI) and <br> Normalized Mutual Information (NMI)</a></li>
<li class="chapter" data-level="23.2.5" data-path="validation.html"><a href="validation.html#other-external-measures-of-validity"><i class="fa fa-check"></i><b>23.2.5</b> Other External Measures of Validity</a></li>
<li class="chapter" data-level="23.2.6" data-path="validation.html"><a href="validation.html#summary-table"><i class="fa fa-check"></i><b>23.2.6</b> Summary Table</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="24" data-path="findk.html"><a href="findk.html"><i class="fa fa-check"></i><b>24</b> Determining the Number of Clusters <span class="math inline">\(k\)</span></a>
<ul>
<li class="chapter" data-level="24.1" data-path="findk.html"><a href="findk.html#methods-based-on-cluster-validity-stopping-rules"><i class="fa fa-check"></i><b>24.1</b> Methods based on Cluster Validity (Stopping Rules)</a></li>
<li class="chapter" data-level="24.2" data-path="findk.html"><a href="findk.html#sum-squared-error-sse-cohesion-plots"><i class="fa fa-check"></i><b>24.2</b> Sum Squared Error (SSE) Cohesion Plots</a>
<ul>
<li class="chapter" data-level="24.2.1" data-path="findk.html"><a href="findk.html#cosine-cohesion-plots-for-text-data"><i class="fa fa-check"></i><b>24.2.1</b> Cosine-Cohesion Plots for Text Data</a></li>
<li class="chapter" data-level="24.2.2" data-path="findk.html"><a href="findk.html#ray-and-turis-method"><i class="fa fa-check"></i><b>24.2.2</b> Ray and Turi’s Method</a></li>
<li class="chapter" data-level="24.2.3" data-path="findk.html"><a href="findk.html#the-gap-statistic"><i class="fa fa-check"></i><b>24.2.3</b> The Gap Statistic</a></li>
</ul></li>
<li class="chapter" data-level="24.3" data-path="findk.html"><a href="findk.html#perroncluster"><i class="fa fa-check"></i><b>24.3</b> Graph Methods Based on Eigenvalues <br> (Perron Cluster Analysis)</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>25</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Linear Algebra for Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="norms" class="section level1" number="6">
<h1><span class="header-section-number">Chapter 6</span> Norms, Similarity, and Distance</h1>
<div id="sec-norms" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Norms and Distances</h2>
<p>In applied mathematics, Norms are functions which measure the magnitude or length of a vector. They are commonly used to determine similarities between observations by measuring the distance between them. As we will see, there are many ways to define distance between two points.</p>
<div class="definition">
<p><span id="def:normdef" class="definition"><strong>Definition 6.1  (Vector Norms and Distance Metrics) </strong></span>A Norm, or distance metric, is a function that takes a vector as input and returns a scalar quantity (<span class="math inline">\(f: \Re^n \to \Re\)</span>). A vector norm is typically denoted by two vertical bars surrounding the input vector, <span class="math inline">\(\|\bo{x}\|\)</span>, to signify that it is not just any function, but one that satisfies the following criteria:</p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(c\)</span> is a scalar, then <span class="math display">\[\|c\x\|=|c|\|x\|\]</span><br />
</li>
<li>The triangle inequality: <span class="math display">\[\|\x+\bo{y}\| \leq \|\x\|+\|\bo{y}\|\]</span><br />
</li>
<li><span class="math inline">\(\|\x\|=0\)</span> if and only if <span class="math inline">\(\x=0\)</span>.<br />
</li>
<li><span class="math inline">\(\|\x\|\geq 0\)</span> for any vector <span class="math inline">\(\x\)</span><br />
</li>
</ol>
</div>
<p>We will not spend any time on these axioms or on the theoretical aspects of norms, but we will put a couple of these functions to good use in our studies, the first of which is the <strong>Euclidean norm</strong> or <strong>2-norm</strong>.</p>

<div class="definition">
<p><span id="def:twonormdef" class="definition"><strong>Definition 6.2  (Euclidean Norm, <span class="math inline">\(\|\star\|_2\)</span>) </strong></span>The <strong>Euclidean Norm</strong>, also known as the <strong>2-norm</strong> simply measures the Euclidean length of a vector (i.e. a point’s distance from the origin). Let <span class="math inline">\(\x = (x_1,x_2,\dots,x_n)\)</span>. Then,
<span class="math display">\[\|\x\|_2 = \sqrt{x_1^2 + x_2^2 + \dots + x_n^2} \]</span>
If <span class="math inline">\(\x\)</span> is a column vector, then <span class="math display">\[\|\x\|_2= \sqrt{\x^T\x}.\]</span>
Often we will simply write <span class="math inline">\(\|\star\|\)</span> rather than <span class="math inline">\(\|\star\|_2\)</span> to denote the <span class="math inline">\(2\)</span>-norm, as it is by far the most commonly used norm.</p>
</div>
<p>This is merely the distance formula from undergraduate mathematics, measuring the distance between the point <span class="math inline">\(\x\)</span> and the origin. To compute the distance between two different points, say <span class="math inline">\(\x\)</span> and <span class="math inline">\(\y\)</span>, we’d calculate
<span class="math display">\[\|\x-\y\|_2= \sqrt{(x_1-y_1)^2 + (x_2-y_2)^2 + \dots + (x_n-y_n)^2}\]</span></p>
<div class="example">
<p><span id="exm:twonorm" class="example"><strong>Example 6.1  (Euclidean Norm and Distance) </strong></span>Suppose I have two vectors in <span class="math inline">\(3\)</span>-space:
<span class="math display">\[\x=(1,1,1) \mbox{   and   } \bo{y}=(1,0,0)\]</span>
Then the magnitude of <span class="math inline">\(\x\)</span> (i.e. its length or distance from the origin) is
<span class="math display">\[\|\x\|_2=\sqrt{1^2+1^2+1^2}=\sqrt{3}\]</span>
and the magnitude of <span class="math inline">\(\bo{y}\)</span> is
<span class="math display">\[\|\bo{y}\|_2=\sqrt{1^2+0^2+0^2}=1\]</span>
and the distance between point <span class="math inline">\(\x\)</span> and point <span class="math inline">\(\bo{y}\)</span> is
<span class="math display">\[ \|\x-\bo{y}\|_2=\sqrt{(1-1)^2 + (1-0)^2 + (1-0)^2} =\sqrt{2}.\]</span>
The Euclidean norm is crucial to many methods in data analysis as it measures the closeness of two data points.</p>
</div>
<p>Thus, to turn any vector into a <strong>unit vector</strong>, a vector with a length of 1, we need only to divide each of the entries in the vector by its Euclidean norm. This is a simple form of standardization used in many areas of data analysis. For a unit vector <span class="math inline">\(\x\)</span>, <span class="math inline">\(\x^T\x=1\)</span>.</p>
<p>Perhaps without knowing it, we’ve already seen many formulas involving the norm of a vector. Examples <a href="norms.html#exm:sdnorm">6.2</a> and <a href="norms.html#exm:lsreg">6.3</a> show how some of the most important concepts in statistics can be represented using vector norms.</p>
<div class="example">
<p><span id="exm:sdnorm" class="example"><strong>Example 6.2  (Standard Deviation and Variance) </strong></span>Suppose a group of individuals has the following heights, measured in inches: (60, 70, 65, 50, 55). The mean height for this group is 60 inches. The formula for the <strong>sample standard deviation</strong> is typically given as
<span class="math display">\[s = \frac{\sqrt{\sum_{i=1}^n (x_i-\bar{x})^2}}{\sqrt{n-1}}\]</span></p>
<p>We want to subtract the mean from each observation, square the numbers, sum the result, take the square root and divide by <span class="math inline">\(\sqrt{n-1}\)</span>.</p>
<p>If we let <span class="math inline">\(\bar{\x}=\bar{x}\e=(60,60,60,60,60)\)</span> be a vector containing the mean, and <span class="math inline">\(\x=(60, 70, 65, 50, 55)\)</span> be the vector of data then the standard deviation in matrix notation is:
<span class="math display">\[s=\frac{1}{\sqrt{n-1}}\|\x-\bar{\x}\|_2=7.9\]</span></p>
<p>The <strong>sample variance</strong> of this data is merely the square of the sample standard deviation:
<span class="math display">\[s^2 = \frac{1}{n-1}\|\x-\bar{\x}\|_2^2\]</span></p>
</div>
<div class="example">
<p><span id="exm:lsreg" class="example"><strong>Example 6.3  (Residual Sums of Squares) </strong></span>Another place we’ve seen a similar calculation is in linear regression. You’ll recall the objective of our regression line is to minimize the sum of squared residuals between the predicted value <span class="math inline">\(\hat{y}\)</span> and the observed value <span class="math inline">\(y\)</span>:
<span class="math display">\[\sum_{i=1}^n (\hat{y}_i-y_i )^2.\]</span>
In vector notation, we’d let <span class="math inline">\(\y\)</span> be a vector containing the observed data and <span class="math inline">\(\hat{\y}\)</span> be a vector containing the corresponding predictions and write this summation as
<span class="math display">\[\|\hat{\y}-\y\|_2^2\]</span></p>
<p>In fact, any situation where the phrase “sum of squares” is encountered, the <span class="math inline">\(2\)</span>-norm is generally implicated.</p>
</div>

<div class="example">
<p><span id="exm:rsquared" class="example"><strong>Example 6.4  (Coefficient of Determination, <span class="math inline">\(R^2\)</span>) </strong></span>Since variance can be expressed using the Euclidean norm, so can the <strong>coefficient of determination</strong> or <span class="math inline">\(R^2\)</span>.
<span class="math display">\[R^2 = \frac{SS_{reg}}{SS_{tot}}= \frac{\sum_{i=1}^n (\hat{y_i}-\bar{y})^2}{\sum_{i=1}^n (y_i-\bar{y})^2} = \frac{\|\hat{\y}-\bar{\y}\|^2}{\|\y-\bar{\y}\|^2}\]</span></p>
</div>
</div>
<div id="other-useful-norms-and-distances" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> Other useful norms and distances</h2>
<div id="norm-star_1." class="section level3" number="6.2.1">
<h3><span class="header-section-number">6.2.1</span> 1-norm, <span class="math inline">\(\|\star\|_1\)</span>.</h3>
<p>If <span class="math inline">\(\x=\pm x_1 &amp; x_2 &amp; \dots &amp;x_n \mp\)</span> then the <span class="math inline">\(1\)</span>-norm of <span class="math inline">\(\X\)</span> is
<span class="math display">\[\|\x\|_1 = \sum_{i=1}^n |x_i|.\]</span>
This metric is often referred to as <em>Manhattan distance</em>, <em>city block distance</em>, or <em>taxicab distance</em> because it measures the distance between points along a rectangular grid (as a taxicab must travel on the streets of Manhattan, for example). When <span class="math inline">\(\x\)</span> and <span class="math inline">\(\y\)</span> are binary vectors, the <span class="math inline">\(1\)</span>-norm is called the <strong>Hamming Distance</strong>, and simply measures the number of elements that are different between the two vectors.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-24"></span>
<img src="figs/1norm.png" alt="The lengths of the red, yellow, and blue paths represent the 1-norm distance between the two points. The green line shows the Euclidean measurement (2-norm)." width="50%" />
<p class="caption">
Figure 6.1: The lengths of the red, yellow, and blue paths represent the 1-norm distance between the two points. The green line shows the Euclidean measurement (2-norm).
</p>
</div>
</div>
<div id="infty-norm-star_infty." class="section level3" number="6.2.2">
<h3><span class="header-section-number">6.2.2</span> <span class="math inline">\(\infty\)</span>-norm, <span class="math inline">\(\|\star\|_{\infty}\)</span>.</h3>
<p>The infinity norm, also called the Supremum, or Max distance, is:
<span class="math display">\[\|\x\|_{\infty} =  \max\{|x_1|,|x_2|,\dots,|x_p|\}\]</span></p>
</div>
</div>
<div id="inner-products" class="section level2" number="6.3">
<h2><span class="header-section-number">6.3</span> Inner Products</h2>
<p>The inner product of vectors is a notion that we’ve already seen in Chapter <a href="mult.html#mult">2</a>, it is what’s called the <em>dot product</em> in most physics and calculus text books.</p>
<div class="definition">
<p><span id="def:innerproddef" class="definition"><strong>Definition 2.3  (Vector Inner Product) </strong></span>The inner product of two <span class="math inline">\(n\times 1\)</span> vectors <span class="math inline">\(\x\)</span> and <span class="math inline">\(\y\)</span> is written <span class="math inline">\(\x^T\y\)</span> (or sometimes as <span class="math inline">\(\langle \x,\y \rangle\)</span>) and is the sum of the product of corresponding elements.
<span class="math display">\[\x^T\y = \pm x_1 &amp; x_2 &amp; \dots &amp; x_n \mp \pm y_1 \\y_2 \\ \vdots \\ y_n \mp = x_1y_1+x_2y_2+\dots+x_ny_n=\sum_{i=1}^n x_i y_i.\]</span></p>
<p>When we take the inner product of a vector with itself, we get the square of the 2-norm:
<span class="math display">\[\x^T\x=\|\x\|_2^2.\]</span></p>
</div>
<p>Inner products are at the heart of every matrix product. When we multiply two matrices, <span class="math inline">\(\X_{m\times n}\)</span> and <span class="math inline">\(\bo{Y}_{n\times p}\)</span>, we can represent the individual elements of the result as inner products of rows of <span class="math inline">\(\X\)</span> and columns of <span class="math inline">\(\Y\)</span> as follows:</p>
<p><span class="math display">\[
\X\Y = \pm \xrow{1} \\ \xrow{2}    \\ \vdots \\ \xrow{m}   \mp 
\pm \ycol{1}&amp;\ycol{2}&amp;\dots&amp;\ycol{p}  \mp \\
= \pm \xrow{1}\ycol{1} &amp; \xrow{1}\ycol{2}   &amp; \dots &amp; \xrow{1}\ycol{p}  \\
\xrow{2}\ycol{1} &amp; \xrow{2}\ycol{2}    &amp; \dots &amp; \xrow{2}\ycol{p}  \\
\xrow{3}\ycol{1}  &amp;\xrow{3}\ycol{2}    &amp;\dots &amp; \xrow{3}\ycol{p}  \\
\vdots &amp; \vdots  &amp; \ddots &amp; \vdots \\
\xrow{m}\ycol{1} &amp; \dots &amp; \ddots &amp; \xrow{m}\ycol{p}  \mp 
\]</span></p>
<div id="covariance" class="section level3" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> Covariance</h3>
<p>Another important statistical measurement that is represented by an inner product is <strong>covariance.</strong> Covariance is a measure of how much two random variables change together. The statistical formula for covariance is given as
<span class="math display" id="eq:cov">\[\begin{equation}
Covariance(\x,\y)=E[(\x-E[\x])(\y-E[\y])]
  \tag{6.1}
\end{equation}\]</span>
where <span class="math inline">\(E[\star]\)</span> is the expected value of the variable.
If larger values of one variable correspond to larger values of the other variable and at the same time smaller values of one correspond to smaller values of the other, then the covariance between the two variables is positive. In the opposite case, if larger values of one variable correspond to smaller values of the other and vice versa, then the covariance is negative. Thus, the <em>sign</em> of the covariance shows the tendency of the linear relationship between variables, however the <em>magnitude</em> of the covariance is not easy to interpret. Covariance is a population parameter - it is a property of the joint distribution of the random variables <span class="math inline">\(\x\)</span> and <span class="math inline">\(\y\)</span>. Definition <a href="norms.html#def:covariancedef">6.3</a> provides the mathematical formulation for the <em>sample</em> covariance. This is our best estimate for the population parameter when we have data sampled from a population.</p>
<div class="definition">
<p><span id="def:covariancedef" class="definition"><strong>Definition 6.3  (Sample Covariance) </strong></span>If <span class="math inline">\(\x\)</span> and <span class="math inline">\(\y\)</span> are <span class="math inline">\(n\times 1\)</span> vectors containing <span class="math inline">\(n\)</span> observations for two different variables, then the <strong>sample covariance</strong> of <span class="math inline">\(\x\)</span> and <span class="math inline">\(\y\)</span> is given by
<span class="math display">\[\frac{1}{n-1}\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y}) = \frac{1}{n-1}(\x-\bar{\x})^T(\y-\bar{\y})\]</span>
Where again <span class="math inline">\(\bar{\x}\)</span> and <span class="math inline">\(\bar{\y}\)</span> are vectors that contain <span class="math inline">\(\bar{x}\)</span> and <span class="math inline">\(\bar{y}\)</span> repeated <span class="math inline">\(n\)</span> times. It should be clear from this formulation that <span class="math display">\[cov(\x,\y)=cov(\y,\x).\]</span></p>
<p>When we have <span class="math inline">\(p\)</span> vectors, <span class="math inline">\(\v_1,\v_2,\dots,\v_p\)</span>, each containing <span class="math inline">\(n\)</span> observations for <span class="math inline">\(p\)</span> different variables, the sample covariances are most commonly given by the <strong>sample covariance matrix</strong>, <span class="math inline">\(\ssigma\)</span>, where <span class="math display">\[\ssigma_{ij}=cov(\v_i,\v_j).\]</span> This matrix is symmetric, since <span class="math inline">\(\ssigma_{ij}=\ssigma_{ji}\)</span>. If we create a matrix <span class="math inline">\(\V\)</span> whose columns are the vectors <span class="math inline">\(\v_1,\v_2,\dots \v_p\)</span> <em>once the variables have been centered to have mean 0</em>, then the covariance matrix is given by:
<span class="math display">\[cov(\V)=\ssigma = \frac{1}{n-1}\V^T\V.\]</span>
The <span class="math inline">\(j^{th}\)</span> diagonal element of this matrix gives the variance <span class="math inline">\(\v_j\)</span> since
<span class="math display">\[\begin{eqnarray}
\ssigma_{jj}=cov(\v_j,\v_j) &amp;=&amp;\frac{1}{n-1}(\v_j-\bar{\v}_j)^T(\v_j-\bar{\v}_j) \\
&amp;=&amp;\frac{1}{n-1}\|\v_j-\bar{\v}_j\|_2^2\\
&amp;=&amp; var(\v_j)
\end{eqnarray}\]</span></p>
</div>
<p>When two variables are completely uncorrelated, their covariance is zero. This lack of correlation would be seen in a covariance matrix with a diagonal structure. That is, if <span class="math inline">\(\v_1, \v_2,\dots, \v_p\)</span> are uncorrelated with individual variances <span class="math inline">\(\sigma_1^2,\sigma_2^2,\dots,\sigma_p^2\)</span> respectively then the corresponding covariance matrix is:
<span class="math display">\[\ssigma = \pm \sigma_1^2 &amp; 0 &amp; 0&amp; \dots &amp; 0\\
                                        0 &amp; \sigma_2^2 &amp; 0 &amp; \dots &amp; 0\\
                                        0 &amp; 0 &amp; \ddots &amp; \vdots &amp; 0 \\
                                        \vdots &amp; \vdots &amp;\vdots &amp; \ddots &amp; \vdots \\
                                        0 &amp; 0 &amp; 0 &amp; 0 &amp; \sigma_p^2 \mp\]</span>
Furthermore, for variables which are independent and identically distributed (take for instance the error terms in a linear regression model, which are assumed to independent and normally distributed with mean 0 and constant variance <span class="math inline">\(\sigma\)</span>), the covariance matrix is a multiple of the identity matrix:
<span class="math display">\[\ssigma = \pm \sigma^2 &amp; 0 &amp; 0&amp; \dots &amp; 0\\
                                        0 &amp; \sigma^2 &amp; 0 &amp; \dots &amp; 0\\
                                        0 &amp; 0 &amp; \ddots &amp; \vdots &amp; 0 \\
                                        \vdots &amp; \vdots &amp;\vdots &amp; \ddots &amp; \vdots \\
                                        0 &amp; 0 &amp; 0 &amp; 0 &amp; \sigma^2 \mp =\sigma^2\bo{I}\]</span></p>
<p>Transforming our variables in a such a way that their covariance matrix becomes diagonal will be our goal in Chapter <a href="pca.html#pca">13</a>.</p>
<div class="theorem">
<p><span id="thm:propcov" class="theorem"><strong>Theorem 6.1  (Properties of Covariance Matrices) </strong></span>The following mathematical properties stem from Equation <a href="norms.html#eq:cov">(6.1)</a>. Let <span class="math inline">\(\X_{n\times p}\)</span> be a matrix of data containing <span class="math inline">\(n\)</span> observations on <span class="math inline">\(p\)</span> variables. If <span class="math inline">\(\A\)</span> is a constant matrix (or vector, in the first case) then
<span class="math display">\[cov(\X\A)=\A^Tcov(\X)\A \quad \mbox{ and } \quad cov(\X+\A)=cov(\X)\]</span></p>
</div>
</div>
<div id="mahalanobis-distance" class="section level3" number="6.3.2">
<h3><span class="header-section-number">6.3.2</span> Mahalanobis Distance</h3>
<p>Mahalanobis Distance is similar to Euclidean distance, but takes into account the correlation of the variables. This metric is relatively common in data mining applications like classification. Suppose we have <span class="math inline">\(p\)</span> variables which have some covariance matrix, <span class="math inline">\(\cov\)</span>. Then the Mahalanobis distance between two observations, <span class="math inline">\(\x=\pm x_1&amp; x_2 &amp;\dots &amp; x_p \mp^T\)</span> and <span class="math inline">\(\y = \pm y_1 &amp; y_2 &amp; \dots &amp; y_p \mp^T\)</span> is given by
<span class="math display">\[d(\x,\y)=\sqrt{(\x-\y)^T\cov^{-1}(\x-\y)}.\]</span>
If the covariance matrix is diagonal (meaning the variables are uncorrelated) then the Mahalanobis distance reduces to Euclidean distance normalized by the variance of each variable:
<span class="math display">\[d(\x,\y)=\sqrt{\sum_{i=1}^p\frac{(x_i-y_i)^2}{s_i^2}}=\|\cov^{-1/2}(\x-\y)\|_2.\]</span></p>
</div>
<div id="angular-distance" class="section level3" number="6.3.3">
<h3><span class="header-section-number">6.3.3</span> Angular Distance</h3>
<p>The inner product between two vectors can provide useful information about their relative orientation in space and about their similarity. For example, to find the cosine of the angle between two vectors in <span class="math inline">\(n\)</span>-space, the inner product of their corresponding unit vectors will provide the result. This cosine is often used as a measure of similarity or correlation between two vectors.</p>
<div class="definition">
<p><span id="def:cosine" class="definition"><strong>Definition 6.4  (Cosine of Angle between Vectors) </strong></span>The cosine of the angle between two vectors in <span class="math inline">\(n\)</span>-space is given by
<span class="math display">\[\cos(\theta)=\frac{\x^T\y}{\|\x\|_2\|\y\|_2}\]</span>
<img src="figs/cos.gif" /></p>
</div>
<p>This angular distance is at the heart of <strong>Pearson’s correlation coefficient</strong>.</p>
</div>
<div id="correlation" class="section level3" number="6.3.4">
<h3><span class="header-section-number">6.3.4</span> Correlation</h3>
<p>Pearson’s correlation is a normalized version of the covariance, so that not only the <em>sign</em> of the coefficient is meaningful, but its <em>magnitude</em> is meaningful in measuring the strength of the linear association.</p>

<div class="example">
<p><span id="exm:cosinecorr" class="example"><strong>Example 6.5  (Pearson’s Correlation and Cosine Distance) </strong></span>You may recall the formula for Pearson’s correlation between variable <span class="math inline">\(\x\)</span> and <span class="math inline">\(\y\)</span> with a sample size of <span class="math inline">\(n\)</span> to be as follows:
<span class="math display">\[r = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^{n} (x_i - \bar{x})^2}\sqrt{\sum_{i=1}^{n} (y_i - \bar{y})^2}}\]</span>
If we let <span class="math inline">\(\bar{\x}\)</span> be a vector that contains <span class="math inline">\(\bar{x}\)</span> repeated <span class="math inline">\(n\)</span> times, like we did in Example <a href="norms.html#exm:sdnorm">6.2</a>, and let <span class="math inline">\(\bar{\y}\)</span> be a vector that contains <span class="math inline">\(\bar{y}\)</span> then Pearson’s coefficient can be written as:
<span class="math display">\[r=\frac{(\x-\bar{\x})^T(\y-\bar{\y})}{\|\x-\bar{\x}\|\|\y-\bar{\y}\|}\]</span>
In other words, it is just the cosine of the angle between the two vectors once they have been <em>centered</em> to have mean 0.</p>
<p>This makes sense: correlation is a measure of the extent to which the two variables share a line in space. If the cosine of the angle is positive or negative one, this means the angle between the two vectors is <span class="math inline">\(0^{\circ}\)</span> or <span class="math inline">\(180^{\circ}\)</span>, thus, the two vectors are perfectly correlated or <em>collinear.</em></p>
</div>
<p>It is difficult to visualize the angle between two variable vectors because they exist in <span class="math inline">\(n\)</span>-space, where <span class="math inline">\(n\)</span> is the number of observations in the dataset. Unless we have fewer than 3 observations, we cannot draw these vectors or even picture them in our minds. As it turns out, this angular measurement does translate into something we can conceptualize: Pearson’s correlation coefficient is the angle formed between the two possible regression lines using the centered data: <span class="math inline">\(\y\)</span> regressed on <span class="math inline">\(\x\)</span> and <span class="math inline">\(\x\)</span> regressed on <span class="math inline">\(\y\)</span>. This is illustrated in Figure <a href="norms.html#fig:corrangle">6.2</a>.</p>

<div class="figure" style="text-align: center"><span id="fig:corrangle"></span>
<img src="figs/corrangle.jpg" alt="Correlation Coefficient \(r\) and Angle between Regression Lines" width="40%" />
<p class="caption">
Figure 6.2: Correlation Coefficient <span class="math inline">\(r\)</span> and Angle between Regression Lines
</p>
</div>
<p>To compute the matrix of pairwise correlations between variables <span class="math inline">\(\x_1,\x_2,\x_3,\dots,\x_p\)</span> (columns containing <span class="math inline">\(n\)</span> observations for each variable), we’d first center them to have mean zero, then normalize them to have length <span class="math inline">\(\|\x_i\|=1\)</span> and then compose the matrix
<span class="math display">\[\X=[\x_1|\x_2|\x_3|\dots|\x_p].\]</span></p>
<p>Using this centered and normalized data, the correlation matrix is simply
<span class="math display">\[\C=\X^T\X.\]</span>
## Outer Products</p>
<p>The outer product of two vectors <span class="math inline">\(\x \in \Re^m\)</span> and <span class="math inline">\(\y \in \Re^n\)</span>, written <span class="math inline">\(\x\y^T\)</span>, is an $mn $ matrix with rank 1. To see this basic fact, lets just look at an example.</p>
<div class="example">
<p><span id="exm:outerprod" class="example"><strong>Example 2.9  (Outer Product) </strong></span>Let <span class="math inline">\(\x=\pm 1\\2\\3\\4\mp\)</span> and let <span class="math inline">\(\y=\pm2\\1\\3\mp\)</span>. Then the outer product of <span class="math inline">\(\x\)</span> and <span class="math inline">\(\y\)</span> is:
<span class="math display">\[\x\y^T = \pm 1\\2\\3\\4\mp \pm 2&amp;1&amp;3\mp = \pm 2&amp;1&amp;3\\4&amp;2&amp;6\\6&amp;3&amp;9\\8&amp;4&amp;12 \mp\]</span>
which clearly has rank 1. It should be clear from this example that computing an outer product will always result in a matrix whose rows and columns are multiples of each other.</p>
</div>
<div class="example">
<p><span id="exm:centerouter" class="example"><strong>Example 6.6  (Centering Data with an Outer Product) </strong></span>As we’ve seen in previous examples, many statistical formulas involve the <em>centered</em> data, that is, data from which the mean has been subtracted so that the new mean is zero. Suppose we have a matrix of data containing observations of individuals’ heights (h) in inches, weights (w), in pounds and wrist sizes (s), in inches:</p>
<p><span class="math display">\[\A=\bm{ ~ &amp; h &amp; w &amp; s \cr 
            person_1 &amp; 60 &amp; 102 &amp; 5.5 \cr
            person_2 &amp; 72 &amp; 170 &amp;  7.5 \cr
            person_3 &amp; 66 &amp; 110 &amp; 6.0\cr
            person_4 &amp; 69 &amp; 128 &amp; 6.5\cr
            person_5 &amp; 63 &amp; 130 &amp;  7.0\cr}\]</span></p>
<p>The average values for height, weight, and wrist size are as follows:
<span class="math display">\[\begin{eqnarray}
\bar{h}&amp;=&amp;66\\
\bar{w}&amp;=&amp;128\\
\bar{s}&amp;=&amp;6.5
\end{eqnarray}\]</span></p>
<p>To center all of the variables in this data set simultaneously, we could compute an outer product using a vector containing the means and a vector of all ones:</p>
<p><span class="math display">\[\pm 60 &amp; 102 &amp; 5.5 \cr
             72 &amp; 170 &amp;  7.5 \cr
            66 &amp; 110 &amp; 6.0\cr
            69 &amp; 128 &amp; 6.5\cr
            63 &amp; 130 &amp;  7.0\cr \mp - \pm 1\\1\\1\\1\\1 \mp \pm 66 &amp; 128 &amp; 6.5 \mp\]</span>
<span class="math display">\[= \pm 60 &amp; 102 &amp; 5.5 \cr
             72 &amp; 170 &amp;  7.5 \cr
            66 &amp; 110 &amp; 6.0\cr
            69 &amp; 128 &amp; 6.5\cr
            63 &amp; 130 &amp;  7.0\cr \mp - \pm  66 &amp; 128 &amp; 6.5 \\66 &amp; 128 &amp; 6.5 \\66 &amp; 128 &amp; 6.5 \\66 &amp; 128 &amp; 6.5 \\66 &amp; 128 &amp; 6.5 \mp\]</span></p>
<p><span class="math display">\[= \pm    -6.0000 &amp; -26.0000  &amp; -1.0000\\
    6.0000 &amp;  42.0000   &amp; 1.0000\\
         0 &amp; -18.0000 &amp;  -0.5000\\
    3.0000    &amp;     0       &amp;  0\\
   -3.0000 &amp;   2.0000 &amp;   0.5000 \mp\]</span></p>
</div>
</div>
</div>
<div id="exercises-3" class="section level2" number="6.4">
<h2><span class="header-section-number">6.4</span> Exercises</h2>
<ol>
<li>
Let <span class="math inline">\(\u=\pm 1\\2\\-4\\-2\mp\)</span> and <span class="math inline">\(\v=\pm 1\\-1\\1\\-1\mp\)</span>.
<ol style="list-style-type:lower-alpha">
<li>
Determine the Euclidean distance between <span class="math inline">\(\u\)</span> and <span class="math inline">\(\v\)</span>.
<li>
Find a vector of unit length in the direction of <span class="math inline">\(\u\)</span>.
<li>
Determine the cosine of the angle between <span class="math inline">\(\u\)</span> and <span class="math inline">\(\v\)</span>.
<li>
Find the 1- and <span class="math inline">\(\infty\)</span>-norms of <span class="math inline">\(\u\)</span> and <span class="math inline">\(\v\)</span>.
<li>
Suppose these vectors are observations on four independent variables, which have the following covariance matrix:
<span class="math display">\[\cov=\pm 2&amp;0&amp;0&amp;0\\0&amp;1&amp;0&amp;0\\0&amp;0&amp;2&amp;0\\0&amp;0&amp;0&amp;1 \mp\]</span>
Determine the Mahalanobis distance between <span class="math inline">\(\u\)</span> and <span class="math inline">\(\v\)</span>.
</ol>
<li>
<p>Write a matrix expression for the correlation matrix, <span class="math inline">\(\C\)</span>, for a matrix of <em>centered</em> data, <span class="math inline">\(\X\)</span>, where <span class="math inline">\(\C_{ij}=r_{ij}\)</span> is Pearson’s correlation measure between variables <span class="math inline">\(\x_i\)</span> and <span class="math inline">\(\x_j\)</span>. To do this, we need more than an inner product, we need to normalize the rows and columns by
the norms <span class="math inline">\(\|\x_i\|\)</span>. For a hint, revisit the exercises in Chapter <a href="mult.html#mult">2</a>.</p>
<li>
<p>Suppose you have a matrix of data, <span class="math inline">\(\A_{n\times p}\)</span>, containing <span class="math inline">\(n\)</span> observations on <span class="math inline">\(p\)</span> variables. Develop a matrix formula for the standardized data (where the mean of each variable should be subtracted from the corresponding column before dividing by the standard deviation). <em>Hint: use Exercises 1(f) and 4 from Chapter <a href="mult.html#mult">2</a> along with Example <a href="norms.html#exm:centerouter">6.6</a>.</em></p>
<li>
<p>Explain why, for any norm or distance metric,
<span class="math display">\[\|\x-\y\|=\|\y-\x\|\]</span></p>
</ol>
<!-- % -->
<!-- % -->
<!-- %\section{Homogeneous Linear Equations} -->
<!-- % -->
<!-- %Linear Algebra is the study of linear equations. As you may recall, a homogeneous linear equation is one in which the right hand side of the equation is 0 (whether it be the scalar 0, a vector of zeros, or a matrix of zeros). In two dimensions, the set of solutions to a linear equation is a one-dimensional line, something we are all familiar with, for example: -->
<!-- % -->
<!-- %$$3x_1 + 2x_2 = 0.$$ -->
<!-- % -->
<!-- %We can write this simple homogeneous equation in  matrix notation by letting $\bo{a}=\pm 3 \\ 2 \mp$ and $\x=\pm x_1 \\ x_2 \mp$ so our equation becomes -->
<!-- %$$\bo{a}^T\x=\pm 3 & 2 \mp \pm x_1 \\x_2 \mp = 0.$$ -->
<!-- % -->
<!-- %This equation has infinitely many solutions - any point on the line will do. Gaussian Elimination will obviously not change this simple equation.  In this situation, $x_1$ is called a basic variable (because it corresponds to a basic column) and $x_2$ is called a free variable (because it does \textit{not} correspond to a basic column and it is free to take on any value). And so to represent the solution set, we have the following: -->
<!-- %$$\pm -\frac{2}{3} x_1 \\ x_1 \mp = x_1 \pm -frac{2}{3} \\ 1 \mp = \alpha \pm -\\frac{2}{3} \\ 1 \mp = span \left\lbrace\pm -\frac{2}{3} \\ 1 \mp \right\rbrace$$  -->
<!-- % -->

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="solvesys.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="linind.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/shainarace/LinearAlgebra/edit/master/019-norms.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/shainarace/LinearAlgebra/blob/master/019-norms.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": {
"engine": "lunr"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
