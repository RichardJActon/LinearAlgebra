<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10 Eigenvalues and Eigenvectors |  Linear Algebra for Data Science   with examples in R and Python</title>
  <meta name="description" content="Linear Algebra for Data Science with examples in R." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 10 Eigenvalues and Eigenvectors |  Linear Algebra for Data Science   with examples in R and Python" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="MSALogo.png" />
  <meta property="og:description" content="Linear Algebra for Data Science with examples in R." />
  <meta name="github-repo" content="rstudio/linalg-master" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 Eigenvalues and Eigenvectors |  Linear Algebra for Data Science   with examples in R and Python" />
  
  <meta name="twitter:description" content="Linear Algebra for Data Science with examples in R." />
  <meta name="twitter:image" content="MSALogo.png" />

<meta name="author" content="Shaina Race Bennett, PhD" />


<meta name="date" content="2021-05-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="basis.html"/>
<link rel="next" href="norms.html"/>
<script src="libs/header-attrs-2.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.2.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.52.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.52.2/plotly-latest.min.js"></script>
<script src="libs/d3-4.5.0/d3.min.js"></script>
<script src="libs/forceNetwork-binding-0.4/forceNetwork.js"></script>
<!DOCTYPE html>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  loader: {load: ['[tex]/cancel', '[tex]/systeme']},
  TeX: {
    packages: {'[+]': ['cancel','systeme','boldsymbol']}
  }
});
</script>

<script src="https://unpkg.com/@webcreate/infinite-ajax-scroll@^3.0.0-beta.6/dist/infinite-ajax-scroll.min.js"></script>

<script type="text/javascript" id="MathJax-script"
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

<span class="math" style="display:none">
\(\usepackage{amsfonts}
\usepackage{cancel}
\usepackage{amsmath}
\usepackage{systeme}
\usepackage{amsthm}
\usepackage{xcolor}
\usepackage{boldsymbol}
\newenvironment{am}[1]{%
  \left(\begin{array}{@{}*{#1}{c}|c@{}}
}{%
  \end{array}\right)
}
\newcommand{\bordermatrix}[3]{\begin{matrix} ~ & \begin{matrix} #1 \end{matrix} \\ \begin{matrix} #2 \end{matrix}\hspace{-1em} & #3 \end{matrix}}
\newcommand{\eref}[1]{Example~\ref{#1}}
\newcommand{\fref}[1]{Figure~\ref{#1}}
\newcommand{\tref}[1]{Table~\ref{#1}}
\newcommand{\sref}[1]{Section~\ref{#1}}
\newcommand{\cref}[1]{Chapter~\ref{#1}}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{fact}{Fact}
\newtheorem{thm}{Theorem}
\newtheorem{example}{Example}[section]
\newcommand{\To}{\Rightarrow}
\newcommand{\del}{\nabla}
\renewcommand{\Re}{\mathbb{R}}
\renewcommand{\O}{\mathcal{O}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\eps}{\epsilon}
\newcommand{\cont}{\Rightarrow \Leftarrow}
\newcommand{\back}{\backslash}
\newcommand{\norm}[1]{\|{#1}\|}
\newcommand{\abs}[1]{|{#1}|}
\newcommand{\ip}[1]{\langle{#1}\rangle}
\newcommand{\bo}{\mathbf}
\newcommand{\mean}{\boldsymbol\mu}
\newcommand{\cov}{\boldsymbol\Sigma}
\newcommand{\wt}{\widetilde}
\newcommand{\p}{\textbf{p}}
\newcommand{\ff}{\textbf{f}}
\newcommand{\aj}{\textbf{a}_j}
\newcommand{\ajhat}{\widehat{\textbf{a}_j}}
\newcommand{\I}{\textbf{I}}
\newcommand{\A}{\textbf{A}}
\newcommand{\B}{\textbf{B}}
\newcommand{\bL}{\textbf{L}}
\newcommand{\bP}{\textbf{P}}
\newcommand{\bD}{\textbf{D}}
\newcommand{\bS}{\textbf{S}}
\newcommand{\bW}{\textbf{W}}
\newcommand{\id}{\textbf{I}}
\newcommand{\M}{\textbf{M}}
\renewcommand{\B}{\textbf{B}}
\newcommand{\V}{\textbf{V}}
\newcommand{\U}{\textbf{U}}
\newcommand{\y}{\textbf{y}}
\newcommand{\bv}{\textbf{v}}
\renewcommand{\v}{\textbf{v}}
\newcommand{\cC}{\mathscr{C}}
\newcommand{\e}{\textbf{e}}
\newcommand{\w}{\textbf{w}}
\newcommand{\h}{\textbf{h}}
\renewcommand{\b}{\textbf{b}}
\renewcommand{\a}{\textbf{a}}
\renewcommand{\u}{\textbf{u}}
\newcommand{\C}{\textbf{C}}
\newcommand{\D}{\textbf{D}}
\newcommand{\cc}{\textbf{c}}
\newcommand{\Q}{\textbf{Q}}
\renewcommand{\S}{\textbf{S}}
\newcommand{\X}{\textbf{X}}
\newcommand{\Z}{\textbf{Z}}
\newcommand{\z}{\textbf{z}}
\newcommand{\Y}{\textbf{Y}}
\newcommand{\plane}{\textit{P}}
\newcommand{\mxn}{$m\mbox{x}n$}
\newcommand{\kmeans}{\textit{k}-means\,}
\newcommand{\bbeta}{\boldsymbol\beta}
\newcommand{\ssigma}{\boldsymbol\Sigma}
\newcommand{\xrow}[1]{\mathbf{X}_{{#1}\star}}
\newcommand{\xcol}[1]{\mathbf{X}_{\star{#1}}}
\newcommand{\yrow}[1]{\mathbf{Y}_{{#1}\star}}
\newcommand{\ycol}[1]{\mathbf{Y}_{\star{#1}}}
\newcommand{\crow}[1]{\mathbf{C}_{{#1}\star}}
\newcommand{\ccol}[1]{\mathbf{C}_{\star{#1}}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\arow}[1]{\mathbf{A}_{{#1}\star}}
\newcommand{\acol}[1]{\mathbf{A}_{\star{#1}}}
\newcommand{\brow}[1]{\mathbf{B}_{{#1}\star}}
\newcommand{\bcol}[1]{\mathbf{B}_{\star{#1}}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\renewcommand{\t}{ \indent}
\newcommand{\nt}{ \indent}
\newcommand{\x}{\mathbf{x}}
\renewcommand{\Y}{\mathbf{Y}}
\newcommand{\ep}{\mathbf{\epsilon}}
\renewcommand{\pm}{\left(\begin{matrix}}
\renewcommand{\mp}{\end{matrix}\right)}
\newcommand{\bm}{\bordermatrix}
\usepackage{pdfpages,cancel}
\newenvironment{code}{\Verbatim [formatcom=\color{blue}]}{\endVerbatim}
\)
</span>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><center><img src="figs/iaaicon.png" width="50"></center></li>
<li><center><strong> Linear Algebra for Data Science </strong></center></li>
<li><center><strong> with examples in R and Python </strong></center></li>

<li class="divider"></li>
<li><a href="index.html#section"></a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>0.1</b> Preface</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#structure-of-the-book"><i class="fa fa-check"></i>Structure of the book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i>About the author</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#what-is-linear-algebra"><i class="fa fa-check"></i><b>1.1</b> What is Linear Algebra?</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#why-linear-algebra"><i class="fa fa-check"></i><b>1.2</b> Why Linear Algebra</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#describing-matrices-and-vectors"><i class="fa fa-check"></i><b>1.3</b> Describing Matrices and Vectors</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#vectors"><i class="fa fa-check"></i><b>1.4</b> Vectors</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#matrix-operations"><i class="fa fa-check"></i><b>1.5</b> Matrix Operations</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#special"><i class="fa fa-check"></i><b>1.6</b> Special Matrices and Vectors</a></li>
<li class="chapter" data-level="1.7" data-path="intro.html"><a href="intro.html#summary-of-conventional-notation"><i class="fa fa-check"></i><b>1.7</b> Summary of Conventional Notation</a></li>
<li class="chapter" data-level="1.8" data-path="intro.html"><a href="intro.html#exercises"><i class="fa fa-check"></i><b>1.8</b> Exercises</a></li>
<li class="chapter" data-level="1.9" data-path="intro.html"><a href="intro.html#list-of-key-terms"><i class="fa fa-check"></i><b>1.9</b> List of Key Terms</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="mult.html"><a href="mult.html"><i class="fa fa-check"></i><b>2</b> Matrix Arithmetic</a>
<ul>
<li class="chapter" data-level="2.1" data-path="mult.html"><a href="mult.html#matrix-addition-subtraction-and-scalar-multiplication"><i class="fa fa-check"></i><b>2.1</b> Matrix Addition, Subtraction, and Scalar Multiplication</a></li>
<li class="chapter" data-level="2.2" data-path="mult.html"><a href="mult.html#sec:vectoradd"><i class="fa fa-check"></i><b>2.2</b> Geometry of Vector Addition and Scalar Multiplication</a></li>
<li class="chapter" data-level="2.3" data-path="mult.html"><a href="mult.html#linear-combinations"><i class="fa fa-check"></i><b>2.3</b> Linear Combinations</a></li>
<li class="chapter" data-level="2.4" data-path="mult.html"><a href="mult.html#matrix-multiplication"><i class="fa fa-check"></i><b>2.4</b> Matrix Multiplication</a></li>
<li class="chapter" data-level="2.5" data-path="mult.html"><a href="mult.html#vector-outer-products"><i class="fa fa-check"></i><b>2.5</b> Vector Outer Products</a></li>
<li class="chapter" data-level="2.6" data-path="mult.html"><a href="mult.html#the-identity-and-the-matrix-inverse"><i class="fa fa-check"></i><b>2.6</b> The Identity and the Matrix Inverse</a></li>
<li class="chapter" data-level="2.7" data-path="mult.html"><a href="mult.html#exercises-1"><i class="fa fa-check"></i><b>2.7</b> Exercises</a></li>
<li class="chapter" data-level="" data-path="mult.html"><a href="mult.html#list-of-key-terms-1"><i class="fa fa-check"></i>List of Key Terms</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="multapp.html"><a href="multapp.html"><i class="fa fa-check"></i><b>3</b> Applications of Matrix Multiplication</a>
<ul>
<li class="chapter" data-level="3.1" data-path="multapp.html"><a href="multapp.html#systems-of-equations"><i class="fa fa-check"></i><b>3.1</b> Systems of Equations</a></li>
<li class="chapter" data-level="3.2" data-path="multapp.html"><a href="multapp.html#regression-analysis"><i class="fa fa-check"></i><b>3.2</b> Regression Analysis</a></li>
<li class="chapter" data-level="3.3" data-path="multapp.html"><a href="multapp.html#linear-combinations-1"><i class="fa fa-check"></i><b>3.3</b> Linear Combinations</a></li>
<li class="chapter" data-level="3.4" data-path="multapp.html"><a href="multapp.html#multapp-ex"><i class="fa fa-check"></i><b>3.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="r-programming-basics.html"><a href="r-programming-basics.html"><i class="fa fa-check"></i><b>4</b> R Programming Basics</a></li>
<li class="chapter" data-level="5" data-path="solvesys.html"><a href="solvesys.html"><i class="fa fa-check"></i><b>5</b> Solving Systems of Equations</a>
<ul>
<li class="chapter" data-level="5.1" data-path="solvesys.html"><a href="solvesys.html#gaussian-elimination"><i class="fa fa-check"></i><b>5.1</b> Gaussian Elimination</a></li>
<li class="chapter" data-level="5.2" data-path="solvesys.html"><a href="solvesys.html#three-types-of-systems"><i class="fa fa-check"></i><b>5.2</b> Three Types of Systems</a></li>
<li class="chapter" data-level="5.3" data-path="solvesys.html"><a href="solvesys.html#solving-matrix-equations"><i class="fa fa-check"></i><b>5.3</b> Solving Matrix Equations</a></li>
<li class="chapter" data-level="5.4" data-path="solvesys.html"><a href="solvesys.html#exercises-2"><i class="fa fa-check"></i><b>5.4</b> Exercises</a></li>
<li class="chapter" data-level="5.5" data-path="solvesys.html"><a href="solvesys.html#list-of-key-terms-2"><i class="fa fa-check"></i><b>5.5</b> List of Key Terms</a></li>
<li class="chapter" data-level="5.6" data-path="solvesys.html"><a href="solvesys.html#gauss-jordan-elimination-in-r"><i class="fa fa-check"></i><b>5.6</b> Gauss-Jordan Elimination in R</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="leastsquares.html"><a href="leastsquares.html"><i class="fa fa-check"></i><b>6</b> Least Squares</a>
<ul>
<li class="chapter" data-level="6.1" data-path="leastsquares.html"><a href="leastsquares.html#why-the-normal-equations"><i class="fa fa-check"></i><b>6.1</b> Why the normal equations?</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="lsapp.html"><a href="lsapp.html"><i class="fa fa-check"></i><b>7</b> Applications of Least Squares</a>
<ul>
<li class="chapter" data-level="7.1" data-path="lsapp.html"><a href="lsapp.html#simple-linear-regression"><i class="fa fa-check"></i><b>7.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="7.2" data-path="lsapp.html"><a href="lsapp.html#multiple-linear-regression"><i class="fa fa-check"></i><b>7.2</b> Multiple Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="linind.html"><a href="linind.html"><i class="fa fa-check"></i><b>8</b> Linear Independence</a>
<ul>
<li class="chapter" data-level="8.1" data-path="linind.html"><a href="linind.html#linear-independence"><i class="fa fa-check"></i><b>8.1</b> Linear Independence</a></li>
<li class="chapter" data-level="8.2" data-path="linind.html"><a href="linind.html#span"><i class="fa fa-check"></i><b>8.2</b> Span of Vectors</a></li>
<li class="chapter" data-level="8.3" data-path="linind.html"><a href="linind.html#exercises-3"><i class="fa fa-check"></i><b>8.3</b> Exercises</a></li>
<li class="chapter" data-level="" data-path="linind.html"><a href="linind.html#list-of-key-terms-3"><i class="fa fa-check"></i>List of Key Terms</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="basis.html"><a href="basis.html"><i class="fa fa-check"></i><b>9</b> Basis and Change of Basis</a>
<ul>
<li class="chapter" data-level="9.1" data-path="basis.html"><a href="basis.html#exercises-4"><i class="fa fa-check"></i><b>9.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="eigen.html"><a href="eigen.html"><i class="fa fa-check"></i><b>10</b> Eigenvalues and Eigenvectors</a>
<ul>
<li class="chapter" data-level="10.1" data-path="eigen.html"><a href="eigen.html#diagonalization"><i class="fa fa-check"></i><b>10.1</b> Diagonalization</a></li>
<li class="chapter" data-level="10.2" data-path="eigen.html"><a href="eigen.html#geometric-interpretation-of-eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>10.2</b> Geometric Interpretation of Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="10.3" data-path="eigen.html"><a href="eigen.html#exercises-5"><i class="fa fa-check"></i><b>10.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="norms.html"><a href="norms.html"><i class="fa fa-check"></i><b>11</b> Norms, Inner Products and Orthogonality</a>
<ul>
<li class="chapter" data-level="11.1" data-path="norms.html"><a href="norms.html#sec-norms"><i class="fa fa-check"></i><b>11.1</b> Norms and Distances</a></li>
<li class="chapter" data-level="11.2" data-path="norms.html"><a href="norms.html#other-useful-norms-and-distances"><i class="fa fa-check"></i><b>11.2</b> Other useful norms and distances</a></li>
<li class="chapter" data-level="11.3" data-path="norms.html"><a href="norms.html#inner-products"><i class="fa fa-check"></i><b>11.3</b> Inner Products</a></li>
<li class="chapter" data-level="11.4" data-path="norms.html"><a href="norms.html#orthogonality"><i class="fa fa-check"></i><b>11.4</b> Orthogonality</a></li>
<li class="chapter" data-level="11.5" data-path="norms.html"><a href="norms.html#outer-products"><i class="fa fa-check"></i><b>11.5</b> Outer Products</a></li>
<li class="chapter" data-level="11.6" data-path="norms.html"><a href="norms.html#exercises-6"><i class="fa fa-check"></i><b>11.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>12</b> Principal Components Analysis</a>
<ul>
<li class="chapter" data-level="12.1" data-path="pca.html"><a href="pca.html#geometrical-comparison-with-least-squares"><i class="fa fa-check"></i><b>12.1</b> Geometrical comparison with Least Squares</a></li>
<li class="chapter" data-level="12.2" data-path="pca.html"><a href="pca.html#covariance-or-correlation-matrix"><i class="fa fa-check"></i><b>12.2</b> Covariance or Correlation Matrix?</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="pca-in-r.html"><a href="pca-in-r.html"><i class="fa fa-check"></i><b>13</b> PCA in R</a>
<ul>
<li class="chapter" data-level="13.1" data-path="pca-in-r.html"><a href="pca-in-r.html#variable-clustering-with-pca"><i class="fa fa-check"></i><b>13.1</b> Variable Clustering with PCA</a></li>
<li class="chapter" data-level="13.2" data-path="pca-in-r.html"><a href="pca-in-r.html#pca-as-svd"><i class="fa fa-check"></i><b>13.2</b> PCA as SVD</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="pcaapp.html"><a href="pcaapp.html"><i class="fa fa-check"></i><b>14</b> Applications of Principal Components</a>
<ul>
<li class="chapter" data-level="14.1" data-path="pcaapp.html"><a href="pcaapp.html#dimension-reduction"><i class="fa fa-check"></i><b>14.1</b> Dimension reduction</a></li>
<li class="chapter" data-level="14.2" data-path="pcaapp.html"><a href="pcaapp.html#exploratory-analysis"><i class="fa fa-check"></i><b>14.2</b> Exploratory Analysis</a></li>
<li class="chapter" data-level="14.3" data-path="pcaapp.html"><a href="pcaapp.html#fifa-soccer-players"><i class="fa fa-check"></i><b>14.3</b> FIFA Soccer Players</a></li>
<li class="chapter" data-level="14.4" data-path="pcaapp.html"><a href="pcaapp.html#cancer-genetics"><i class="fa fa-check"></i><b>14.4</b> Cancer Genetics</a></li>
<li class="chapter" data-level="14.5" data-path="pcaapp.html"><a href="pcaapp.html#rappasvd"><i class="fa fa-check"></i><b>14.5</b> Image Compression</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="fa.html"><a href="fa.html"><i class="fa fa-check"></i><b>15</b> Factor Analysis</a>
<ul>
<li class="chapter" data-level="15.1" data-path="fa.html"><a href="fa.html#assumptions-of-factor-analysis"><i class="fa fa-check"></i><b>15.1</b> Assumptions of Factor Analysis</a></li>
<li class="chapter" data-level="15.2" data-path="fa.html"><a href="fa.html#determining-factorability"><i class="fa fa-check"></i><b>15.2</b> Determining Factorability</a></li>
<li class="chapter" data-level="15.3" data-path="fa.html"><a href="fa.html#communalities"><i class="fa fa-check"></i><b>15.3</b> Communalities</a></li>
<li class="chapter" data-level="15.4" data-path="fa.html"><a href="fa.html#number-of-factors"><i class="fa fa-check"></i><b>15.4</b> Number of Factors</a></li>
<li class="chapter" data-level="15.5" data-path="fa.html"><a href="fa.html#rotation-of-factors"><i class="fa fa-check"></i><b>15.5</b> Rotation of Factors</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="fa-apps.html"><a href="fa-apps.html"><i class="fa fa-check"></i><b>16</b> Factor Analysis</a>
<ul>
<li class="chapter" data-level="16.1" data-path="fa-apps.html"><a href="fa-apps.html#pca-rotations"><i class="fa fa-check"></i><b>16.1</b> PCA Rotations</a></li>
<li class="chapter" data-level="16.2" data-path="fa-apps.html"><a href="fa-apps.html#ex-personality-tests"><i class="fa fa-check"></i><b>16.2</b> Ex: Personality Tests</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="dimension-reduction-for-visualization.html"><a href="dimension-reduction-for-visualization.html"><i class="fa fa-check"></i><b>17</b> Dimension Reduction for Visualization</a>
<ul>
<li class="chapter" data-level="17.1" data-path="dimension-reduction-for-visualization.html"><a href="dimension-reduction-for-visualization.html#multidimensional-scaling"><i class="fa fa-check"></i><b>17.1</b> Multidimensional Scaling</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="social-network-analysis.html"><a href="social-network-analysis.html"><i class="fa fa-check"></i><b>18</b> Social Network Analysis</a>
<ul>
<li class="chapter" data-level="18.1" data-path="social-network-analysis.html"><a href="social-network-analysis.html#working-with-network-data"><i class="fa fa-check"></i><b>18.1</b> Working with Network Data</a></li>
<li class="chapter" data-level="18.2" data-path="social-network-analysis.html"><a href="social-network-analysis.html#network-visualization---igraph-package"><i class="fa fa-check"></i><b>18.2</b> Network Visualization - <code>igraph</code> package</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAP///wAAACH5BAEAAAAALAAAAAABAAEAAAICRAEAOw==" width="2" height="200" />
Linear Algebra for Data Science <br>
with examples in R and Python</p></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="eigen" class="section level1" number="10">
<h1><span class="header-section-number">Chapter 10</span> Eigenvalues and Eigenvectors</h1>
<div class="Smain">
<div class="Scontainer">
<div class="Stext-header">
Definition <a href="#0"><strong>??</strong></a>.1: Eigenvalues and Eigenvectors
</div>
<div class="Stext">
For a square matrix <span class="math inline">\(\A_{n\times n}\)</span>, a scalar <span class="math inline">\(\lambda\)</span> is called an <strong>eigenvalue</strong> of <span class="math inline">\(\A\)</span> if there is a nonzero vector <span class="math inline">\(\x\)</span> such that <span class="math display">\[\A\x=\lambda\x.\]</span> Such a vector, <span class="math inline">\(\x\)</span> is called an <strong>eigenvector</strong> of <span class="math inline">\(\A\)</span> corresponding to the <strong>eigenvalue</strong> <span class="math inline">\(\lambda\)</span>. We sometimes refer to the pair <span class="math inline">\((\lambda,\x)\)</span> as an <strong>eigenpair.</strong>
</div>
</div>
</div>
<p>Eigenvalues and eigenvectors have numerous applications throughout mathematics, statistics and other fields. First, we must get a handle on the definition which we will do through some examples.</p>
<div class="S2main">
<div class="S2container">
<div class="S2text-header">
Example <a href="#0"><strong>??</strong></a>.1: Eigenvalues and Eigenvectors
</div>
<div class="S2text">
<p>Determine whether <span class="math inline">\(\x=\pm 1\\1 \mp\)</span> is an eigenvector of <span class="math inline">\(\A=\pm 3 &amp; 1 \\1&amp;3 \mp\)</span> and if so, find the corresponding eigenvalue.\
To determine whether <span class="math inline">\(\x\)</span> is an eigenvector, we want to compute <span class="math inline">\(\A\x\)</span> and observe whether the result is a multiple of <span class="math inline">\(\x\)</span>. If this is the case, then the multiplication factor is the corresponding eigenvalue:
<span class="math display">\[\A\x=\pm  3 &amp; 1 \\1&amp;3 \mp \pm 1\\1 \mp =\pm 4\\4 \mp=4\pm 1\\1 \mp\]</span>
From this it follows that <span class="math inline">\(\x\)</span> <em>is</em> an eigenvector of <span class="math inline">\(\A\)</span> and the corresponding eigenvalue is <span class="math inline">\(\lambda = 4\)</span>.\</p>
Is the vector <span class="math inline">\(\y=\pm 2\\2 \mp\)</span> an eigenvector?
<span class="math display">\[\A\y=\pm  3 &amp; 1 \\1&amp;3 \mp \pm 2\\2 \mp =\pm 8\\8 \mp=4\pm 2\\2 \mp = 4\y\]</span>
Yes, it is and it corresponds to the <em>same</em> eigenvalue, <span class="math inline">\(\lambda=4\)</span>
</div>
</div>
</div>
<p>Example <a href="#ex-eig1"><strong>??</strong></a> shows a very important property of eigenvalue-eigenvector pairs. If <span class="math inline">\((\lambda,\x)\)</span> is an eigenpair then any scalar multiple of <span class="math inline">\(\x\)</span> is also an eigenvector corresponding to <span class="math inline">\(\lambda\)</span>. To see this, let <span class="math inline">\((\lambda,\x)\)</span> be an eigenpair for a matrix <span class="math inline">\(\A\)</span> (which means that <span class="math inline">\(\A\x=\lambda\x\)</span>) and let <span class="math inline">\(\y=\alpha\x\)</span> be any scalar multiple of <span class="math inline">\(\x\)</span>. Then we have,
<span class="math display">\[\A\y = \A(\alpha\x)=\alpha(\A\x) = \alpha(\lambda\x) = \lambda(\alpha\x)=\lambda\y\]</span>
which shows that <span class="math inline">\(\y\)</span> (or any scalar multiple of <span class="math inline">\(\x\)</span>) is also an eigenvector associated with the eigenvalue <span class="math inline">\(\lambda\)</span>.</p>
<p>Thus, for each eigenvalue we have infinitely many eigenvectors. In the preceding example, the eigenvectors associated with <span class="math inline">\(\lambda = 4\)</span> will be scalar multiples of <span class="math inline">\(\x=\pm 1\\1 \mp\)</span>. You may recall from Chapter <a href="linind.html#linind">8</a> that the set of all scalar multiples of <span class="math inline">\(\x\)</span> is denoted <span class="math inline">\(span(\x)\)</span>. The <span class="math inline">\(span(\x)\)</span> in this example represents the <strong>eigenspace</strong> of <span class="math inline">\(\lambda\)</span>.
<em>Note: when using software to compute eigenvectors, it is standard practice for the software to provide the normalized/unit eigenvector.</em></p>
<p>In some situations, an eigenvalue can have multiple eigenvectors which are linearly independent. The number of linearly independent eigenvectors associated with an eigenvalue is called the <strong>geometric multiplicity</strong> of the eigenvalue. Example <a href="#ex-eig2"><strong>??</strong></a> clarifies this concept.</p>
<div class="S2main">
<div class="S2container">
<div class="S2text-header">
Example <a href="#0"><strong>??</strong></a>.2: Geometric Multiplicity
</div>
<div class="S2text">
<p>Consider the matrix <span class="math inline">\(\A=\pm 3 &amp; 0 \\0 &amp; 3 \mp\)</span>. It should be straightforward to see that <span class="math inline">\(\x_1 =\pm 1 \\0 \mp\)</span> and <span class="math inline">\(\x_2=\pm 0\\1\mp\)</span> are both eigenvectors corresponding to the eigenvalue <span class="math inline">\(\lambda = 3\)</span>. <span class="math inline">\(\x_1\)</span> and <span class="math inline">\(\x_2\)</span> are linearly independent, therefore the geometric multiplicity of <span class="math inline">\(\lambda=3\)</span> is 2.\</p>
<p>What happens if we take a linear combination of <span class="math inline">\(\x_1\)</span> and <span class="math inline">\(\x_2\)</span>? Is that also an eigenvector?
Consider <span class="math inline">\(\y=\pm 2 \\ 3 \mp = 2\x_1+3\x_2\)</span>. Then
<span class="math display">\[\A\y = \pm 3 &amp; 0 \\0 &amp; 3 \mp \pm 2 \\ 3 \mp = \pm 6 \\ 9 \mp = 3 \pm 2\\3\mp = 3\y\]</span>
shows that <span class="math inline">\(\y\)</span> is also an eigenvector associated with <span class="math inline">\(\lambda=3\)</span>.</p>
The <strong>eigenspace</strong> corresponding to <span class="math inline">\(\lambda=3\)</span> is the set of all linear combinations of <span class="math inline">\(\x_1\)</span> and <span class="math inline">\(\x_2\)</span>, i.e. the <span class="math inline">\(span(\x_1,\x_2)\)</span>.
</div>
</div>
</div>
<p>We can generalize the result that we saw in Example <a href="#ex-eig2"><strong>??</strong></a> for any square matrix and any geometric multiplicity. Let <span class="math inline">\(\A_{n\times n}\)</span> have an eigenvalue <span class="math inline">\(\lambda\)</span> with geometric multiplicity <span class="math inline">\(k\)</span>. This means there are <span class="math inline">\(k\)</span> linearly independent eigenvectors, <span class="math inline">\(\x_1,\x_2,\dots,\x_k\)</span> such that <span class="math inline">\(\A\x_i=\lambda\x_i\)</span> for each eigenvector <span class="math inline">\(\x_i\)</span>. Now if we let <span class="math inline">\(\y\)</span> be a vector in the <span class="math inline">\(span(\x_1,\x_2,\dots,\x_k)\)</span> then <span class="math inline">\(\y\)</span> is some linear combination of the <span class="math inline">\(\x_i\)</span>’s:
<span class="math display">\[\y=\alpha_1\x_2+\alpha_2\x_2+\dots+\alpha_k\x_k\]</span>
Observe what happens when we multiply <span class="math inline">\(\y\)</span> by <span class="math inline">\(\A\)</span>:
<span class="math display">\[\begin{eqnarray*}
\A\y &amp;=&amp;\A(\alpha_1\x_2+\alpha_2\x_2+\dots+\alpha_k\x_k) \\  
&amp;=&amp; \alpha_1(\A\x_1)+\alpha_2(\A\x_2)+\dots +\alpha_k(\A\x_k) \\ 
&amp;=&amp; \alpha_1(\lambda\x_1)+\alpha_2(\lambda\x_2)+\dots +\alpha_k(\lambda\x_k) \\ 
&amp;=&amp; \lambda(\alpha_1\x_2+\alpha_2\x_2+\dots+\alpha_k\x_k) \\
&amp;=&amp; \lambda\y
\end{eqnarray*}\]</span>
which shows that <span class="math inline">\(\y\)</span> (or any vector in the <span class="math inline">\(span(\x_1,\x_2,\dots,\x_k)\)</span>) is an eigenvector of <span class="math inline">\(\A\)</span> corresponding to <span class="math inline">\(\lambda\)</span>.</p>
<p>This proof allows us to formally define the concept of an eigenspace.</p>
<div class="Smain">
<div class="Scontainer">
<div class="Stext-header">
Definition <a href="#0"><strong>??</strong></a>.2: Eigenspace
</div>
<div class="Stext">
Let <span class="math inline">\(\A\)</span> be a square matrix and let <span class="math inline">\(\lambda\)</span> be an eigenvalue of <span class="math inline">\(\A\)</span>. The set of all eigenvectors corresponding to <span class="math inline">\(\lambda\)</span>, together with the zero vector, is called the <strong>eigenspace</strong> of <span class="math inline">\(\lambda\)</span>. The number of basis vectors required to form the eigenspace is called the <strong>geometric multiplicity</strong> of <span class="math inline">\(\lambda\)</span>.
</div>
</div>
</div>
<p>Now, let’s attempt the eigenvalue problem from the other side. Given an eigenvalue, we will find the corresponding eigenspace in Example <a href="#ex-eig3"><strong>??</strong></a>.</p>
<div class="S2main">
<div class="S2container">
<div class="S2text-header">
Example <a href="#0"><strong>??</strong></a>.2: Eigenvalues and Eigenvectors
</div>
<div class="S2text">
<p>Show that <span class="math inline">\(\lambda=5\)</span> is an eigenvalue of <span class="math inline">\(\A=\pm 1 &amp; 2 \\4&amp;3 \mp\)</span> and determine the eigenspace of <span class="math inline">\(\lambda=5\)</span>.\</p>
Attempting the problem from this angle requires slightly more work. We want to find a vector <span class="math inline">\(\x\)</span> such that <span class="math inline">\(\A\x=5\x\)</span>. Setting this up, we have:
<span class="math display">\[\A\x = 5\x.\]</span>
What we want to do is move both terms to one side and factor out the vector <span class="math inline">\(x\)</span>. In order to do this, we must use an identity matrix, otherwise the equation wouldn’t make sense (we’d be subtracting a constant from a matrix).
<span class="math display">\[\begin{eqnarray*}
\A\x-5\x &amp;=&amp; \bo{0}\\
(\A-5\bo{I})\x &amp;=&amp; \bo{0} \\
\left( \pm 1 &amp; 2 \\4&amp;3 \mp - \pm 5 &amp; 0 \\ 0 &amp; 5 \mp \right) \pm x_1 \\ x_2 \mp &amp;=&amp; \pm 0 \\0 \mp \\
\pm -4 &amp; 2\\ 4 &amp; -2 \mp  \pm x_1 \\ x_2 \mp &amp;=&amp; \pm 0 \\0 \mp \\
\end{eqnarray*}\]</span>
Clearly, the matrix <span class="math inline">\(\A-\lambda\bo{I}\)</span> is singular (i.e. does not have linearly independent rows/columns). This will always be the case by the definition <span class="math inline">\(\A\x=\lambda\x\)</span>, and is often used as an alternative definition.\
In order to solve this homogeneous system of equations, we use Gaussian elimination:
<span class="math display">\[\begin{amatrix}{2} -4 &amp; 2 &amp; 0 \\4 &amp; -2 &amp; 0 \end{amatrix}\longrightarrow\begin{amatrix}{2} 1 &amp; -\frac{1}{2} &amp; 0 \\0 &amp; 0 &amp; 0 \end{amatrix}\]</span>
This implies that any vector <span class="math inline">\(\x\)</span> for which <span class="math inline">\(x_1-\frac{1}{2}\x_2=0\)</span> satisfies the eigenvector equation. We can pick any such vector, for example <span class="math inline">\(\x=\pm 1\\2\mp\)</span>, and say that the eigenspace of <span class="math inline">\(\lambda=5\)</span> is
<span class="math display">\[span\left\lbrace\pm 1\\2 \mp\right\rbrace\]</span>
</div>
</div>
</div>
<p>If we didn’t know either an eigenvalue or eigenvector of <span class="math inline">\(\A\)</span> and instead wanted to find both, we would first find eigenvalues by determining all possible <span class="math inline">\(\lambda\)</span> such that <span class="math inline">\(\A-\lambda\bo{I}\)</span> is singular and then find the associated eigenvectors. There are some tricks which allow us to do this by hand for <span class="math inline">\(2\times 2\)</span> and <span class="math inline">\(3\times 3\)</span> matrices, but after that the computation time is unworthy of the effort. Now that we have a good understanding of how to interpret eigenvalues and eigenvectors algebraically, let’s take a look at some of the things that they can do, starting with one important fact.</p>
<div class="Smain">
<div class="Scontainer">
<div class="Stext-header">
Definition <a href="#0"><strong>??</strong></a>.3: Eigenvalues and the Trace of a Matrix
</div>
<div class="Stext">
Let <span class="math inline">\(\A\)</span> be an <span class="math inline">\(n\times n\)</span> matrix with eigenvalues <span class="math inline">\(\lambda_1,\lambda_2,\dots,\lambda_n\)</span>. Then the sum of the eigenvalues is equal to the trace of the matrix (recall that the trace of a matrix is the sum of its diagonal elements).
<span class="math display">\[Trace(\A)=\sum_{i=1}^n \lambda_i.\]</span>
</div>
</div>
</div>
<div class="S2main">
<div class="S2container">
<div class="S2text-header">
Example <a href="#0"><strong>??</strong></a>.3: Trace of Covariance Matrix
</div>
<div class="S2text">
<p>Suppose that we had a collection of <span class="math inline">\(n\)</span> observations on <span class="math inline">\(p\)</span> variables, <span class="math inline">\(\x_1,\x_2,\dots,\x_p\)</span>. After centering the data to have zero mean, we can compute the sample variances as:
<span class="math display">\[var(\x_i)=\frac{1}{n-1}\x_i^T\x_i =\|\x_i\|^2\]</span>
These variances form the diagonal elements of the sample covariance matrix,
<span class="math display">\[\ssigma = \frac{1}{n-1}\X^T\X\]</span>
Thus, the total variance of this data is
<span class="math display">\[\frac{1}{n-1}\sum_{i=1}^n \|\x_i\|^2 = Trace(\ssigma) = \sum_{i=1}^n \lambda_i.\]</span></p>
In other words, the sum of the eigenvalues of a covariance matrix provides the total variance in the variables <span class="math inline">\(\x_1,\dots,\x_p\)</span>.
</div>
</div>
</div>
<div id="diagonalization" class="section level2" number="10.1">
<h2><span class="header-section-number">10.1</span> Diagonalization</h2>
<p>Let’s take another look at Example <a href="#ex-eig3"><strong>??</strong></a>. We already showed that <span class="math inline">\(\lambda_1=5\)</span> and <span class="math inline">\(\v_1=\pm 1\\2\mp\)</span> is an eigenpair for the matrix <span class="math inline">\(\A=\pm 1 &amp; 2 \\4&amp;3 \mp\)</span>. You may verify that <span class="math inline">\(\lambda_2=-1\)</span> and <span class="math inline">\(\v_2=\pm 1\\-1 \mp\)</span> is another eigenpair. Suppose we create a matrix of eigenvectors:
<span class="math display">\[\V=(\v_1,\v_2) = \pm 1&amp;1\\2&amp;-1 \mp\]</span>
and a diagonal matrix containing the corresponding eigenvalues:
<span class="math display">\[\D=\pm 5 &amp; 0 \\ 0 &amp; -1 \mp\]</span>
Then it is easy to verify that <span class="math inline">\(\A\V=\V\D\)</span>:
<span class="math display">\[\begin{eqnarray*}
\A\V &amp;=&amp; \pm 1 &amp; 2 \\4&amp;3 \mp \pm 1&amp;1\\2&amp;-1 \mp\\
        &amp;=&amp; \pm 5&amp;-1\\10&amp;1 \mp\\
        &amp;=&amp;  \pm 1&amp;1\\2&amp;-1 \mp\pm 5 &amp; 0 \\ 0 &amp; -1 \mp\\
        &amp;=&amp;\V\D
\end{eqnarray*}\]</span>
If the columns of <span class="math inline">\(\V\)</span> are linearly independent, which they are in this case, we can write:
<span class="math display">\[\V^{-1}\A\V = \D\]</span></p>
<p>What we have just done is develop a way to transform a matrix <span class="math inline">\(\A\)</span> into a diagonal matrix <span class="math inline">\(\D\)</span>. This is known as <strong>diagonalization.</strong></p>
<div class="Smain">
<div class="Scontainer">
<div class="Stext-header">
Definition <a href="#0"><strong>??</strong></a>.4: Diagonalizable
</div>
<div class="Stext">
An <span class="math inline">\(n\times n\)</span> matrix <span class="math inline">\(\A\)</span> is said to be <strong>diagonalizable</strong> if there exists an invertible matrix <span class="math inline">\(\bP\)</span> and a diagonal matrix <span class="math inline">\(\D\)</span> such that
<span class="math display">\[\bP^{-1}\A\bP=\D\]</span>
This is possible if and only if the matrix <span class="math inline">\(\A\)</span> has <span class="math inline">\(n\)</span> linearly independent eigenvectors (known as a complete set of eigenvectors). The matrix <span class="math inline">\(\bP\)</span> is then the matrix of eigenvectors and the matrix <span class="math inline">\(\D\)</span> contains the corresponding eigenvalues on the diagonal.
</div>
</div>
</div>
<p>Determining whether or not a matrix <span class="math inline">\(\A_{n\times n}\)</span> is diagonalizable is a little tricky. Having <span class="math inline">\(rank(\A)=n\)</span> is <em>not</em> a sufficient condition for having <span class="math inline">\(n\)</span> linearly independent eigenvectors. The following matrix stands as a counter example:
<span class="math display">\[\A=\pm -3&amp; 1 &amp; -3 \\20&amp; 3 &amp; 10 \\2&amp; -2 &amp; 4 \mp\]</span>
This matrix has full rank but only two linearly independent eigenvectors. Fortunately, for our primary application of diagonalization, we will be dealing with a symmetric matrix, which can always be diagonalized. In fact, symmetric matrices have an additional property which makes this diagonalization particularly nice, as we will see in Chapter <a href="pca.html#pca">12</a>.</p>
</div>
<div id="geometric-interpretation-of-eigenvalues-and-eigenvectors" class="section level2" number="10.2">
<h2><span class="header-section-number">10.2</span> Geometric Interpretation of Eigenvalues and Eigenvectors</h2>
<p>Since any scalar multiple of an eigenvector is still an eigenvector, let’s consider for the present discussion unit eigenvectors <span class="math inline">\(\x\)</span> of a square matrix <span class="math inline">\(\A\)</span> - those with length <span class="math inline">\(\|\x\|=1\)</span>. By the definition, we know that
<span class="math display">\[\A\x = \lambda\x\]</span>
We know that geometrically, if we multiply <span class="math inline">\(\x\)</span> by <span class="math inline">\(\A\)</span>, the resulting vector points in the same direction as <span class="math inline">\(\x\)</span>. Geometrically, it turns out that multiplying the unit circle or unit sphere by a matrix <span class="math inline">\(\A\)</span> carves out an ellipse, or an ellipsoid. We can see eigenvectors visually by watching how multiplication by a matrix <span class="math inline">\(\A\)</span> changes the unit vectors. Figure <a href="#eigenarrows"><strong>??</strong></a> illustrates this. The blue arrows represent (a sampling of) the unit circle, all vectors <span class="math inline">\(\x\)</span> for which <span class="math inline">\(\|\x\|=1\)</span>. The red arrows represent the image of the blue arrows after multiplication by <span class="math inline">\(\A\)</span>, or <span class="math inline">\(\A\x\)</span> for each vector <span class="math inline">\(\x\)</span>. We can see how almost every vector changes direction when multiplied by <span class="math inline">\(\A\)</span>, except the eigenvector directions which are marked in black. Such a picture provides a nice geometrical interpretation of eigenvectors for a general matrix, but we will see in Chapter <a href="pca.html#pca">12</a> just how powerful these eigenvector directions are when we look at symmetric matrix.</p>

<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-189"></span>
<img src="figs/eigenarrows.jpg" alt="Visualizing eigenvectors (in black) using the image (in red) of the unit sphere (in blue) after multiplication by \(\A\)." width="36%" />
<p class="caption">
Figure 10.1: Visualizing eigenvectors (in black) using the image (in red) of the unit sphere (in blue) after multiplication by <span class="math inline">\(\A\)</span>.
</p>
</div>
</div>
<div id="exercises-5" class="section level2" number="10.3">
<h2><span class="header-section-number">10.3</span> Exercises</h2>
<ol>
<li>
Show that <span class="math inline">\(\v\)</span> is an eigenvector of <span class="math inline">\(\A\)</span> and find the corresponding eigenvalue:
<ol style="list-style-type:lower-alpha">
<li>
$= &amp; 2 \2 &amp; 1 \-3 $
<li>
$= &amp; 1 \6 &amp; 0 \-2 $
<li>
$= &amp; -2 \5 &amp; -7 \2 $
</ol>
<li>
Show that <span class="math inline">\(\lambda\)</span> is an eigenvalue of <span class="math inline">\(\A\)</span> and list two eigenvectors corresponding to this eigenvalue:
<ol style="list-style-type:lower-alpha">
<li>
<span class="math inline">\(\A=\pm 0&amp; 4\\-1&amp;5\mp \quad \lambda = 4\)</span>
<li>
<span class="math inline">\(\A=\pm 0&amp; 4\\-1&amp;5\mp \quad \lambda = 1\)</span>
</ol>
<li>
Based on the eigenvectors you found in exercises 2, can the matrix <span class="math inline">\(\A\)</span> be diagonalized? Why or why not? If diagonalization is possible, explain how it would be done.
<li>
Can a rectangular matrix have eigenvalues/eigenvectors?
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="basis.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="norms.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/shainarace/LinearAlgebra/edit/master/027-eigen.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/shainarace/LinearAlgebra/blob/master/027-eigen.Rmd",
"text": null
},
"download": ["bookdownproj.pdf", "bookdownproj.epub"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
