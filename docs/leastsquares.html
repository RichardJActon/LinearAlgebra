<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 11 Least Squares | bookdownproj.utf8</title>
  <meta name="description" content="" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 11 Least Squares | bookdownproj.utf8" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 11 Least Squares | bookdownproj.utf8" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="eigen.html"/>
<link rel="next" href="lsapp.html"/>
<script src="libs/header-attrs-2.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.4.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.57.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.57.1/plotly-latest.min.js"></script>
<script src="libs/d3-4.5.0/d3.min.js"></script>
<script src="libs/forceNetwork-binding-0.4/forceNetwork.js"></script>
<!DOCTYPE html>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  loader: {load: ['[tex]/cancel', '[tex]/systeme']},
  TeX: {
    packages: {'[+]': ['cancel','systeme','boldsymbol']}
  }
});
</script>


<script type="text/javascript" id="MathJax-script"
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

<span class="math" style="display:none">
\(\usepackage{amsfonts}
\usepackage{cancel}
\usepackage{amsmath}
\usepackage{systeme}
\usepackage{amsthm}
\usepackage{xcolor}
\usepackage{boldsymbol}
\newenvironment{am}[1]{%
  \left(\begin{array}{@{}*{#1}{c}|c@{}}
}{%
  \end{array}\right)
}
\newcommand{\bordermatrix}[3]{\begin{matrix} ~ & \begin{matrix} #1 \end{matrix} \\ \begin{matrix} #2 \end{matrix}\hspace{-1em} & #3 \end{matrix}}
\newcommand{\eref}[1]{Example~\ref{#1}}
\newcommand{\fref}[1]{Figure~\ref{#1}}
\newcommand{\tref}[1]{Table~\ref{#1}}
\newcommand{\sref}[1]{Section~\ref{#1}}
\newcommand{\cref}[1]{Chapter~\ref{#1}}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{fact}{Fact}
\newtheorem{thm}{Theorem}
\newtheorem{example}{Example}[section]
\newcommand{\To}{\Rightarrow}
\newcommand{\del}{\nabla}
\renewcommand{\Re}{\mathbb{R}}
\renewcommand{\O}{\mathcal{O}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\eps}{\epsilon}
\newcommand{\cont}{\Rightarrow \Leftarrow}
\newcommand{\back}{\backslash}
\newcommand{\norm}[1]{\|{#1}\|}
\newcommand{\abs}[1]{|{#1}|}
\newcommand{\ip}[1]{\langle{#1}\rangle}
\newcommand{\bo}{\mathbf}
\newcommand{\mean}{\boldsymbol\mu}
\newcommand{\cov}{\boldsymbol\Sigma}
\newcommand{\wt}{\widetilde}
\newcommand{\p}{\textbf{p}}
\newcommand{\ff}{\textbf{f}}
\newcommand{\aj}{\textbf{a}_j}
\newcommand{\ajhat}{\widehat{\textbf{a}_j}}
\newcommand{\I}{\textbf{I}}
\newcommand{\A}{\textbf{A}}
\newcommand{\B}{\textbf{B}}
\newcommand{\bL}{\textbf{L}}
\newcommand{\bP}{\textbf{P}}
\newcommand{\bD}{\textbf{D}}
\newcommand{\bS}{\textbf{S}}
\newcommand{\bW}{\textbf{W}}
\newcommand{\id}{\textbf{I}}
\newcommand{\M}{\textbf{M}}
\renewcommand{\B}{\textbf{B}}
\newcommand{\V}{\textbf{V}}
\newcommand{\U}{\textbf{U}}
\newcommand{\y}{\textbf{y}}
\newcommand{\bv}{\textbf{v}}
\renewcommand{\v}{\textbf{v}}
\newcommand{\cC}{\mathscr{C}}
\newcommand{\e}{\textbf{e}}
\newcommand{\w}{\textbf{w}}
\newcommand{\h}{\textbf{h}}
\renewcommand{\b}{\textbf{b}}
\renewcommand{\a}{\textbf{a}}
\renewcommand{\u}{\textbf{u}}
\newcommand{\C}{\textbf{C}}
\newcommand{\D}{\textbf{D}}
\newcommand{\cc}{\textbf{c}}
\newcommand{\Q}{\textbf{Q}}
\renewcommand{\S}{\textbf{S}}
\newcommand{\X}{\textbf{X}}
\newcommand{\Z}{\textbf{Z}}
\newcommand{\z}{\textbf{z}}
\newcommand{\Y}{\textbf{Y}}
\newcommand{\plane}{\textit{P}}
\newcommand{\mxn}{$m\mbox{x}n$}
\newcommand{\kmeans}{\textit{k}-means\,}
\newcommand{\bbeta}{\boldsymbol\beta}
\newcommand{\ssigma}{\boldsymbol\Sigma}
\newcommand{\xrow}[1]{\mathbf{X}_{{#1}\star}}
\newcommand{\xcol}[1]{\mathbf{X}_{\star{#1}}}
\newcommand{\yrow}[1]{\mathbf{Y}_{{#1}\star}}
\newcommand{\ycol}[1]{\mathbf{Y}_{\star{#1}}}
\newcommand{\crow}[1]{\mathbf{C}_{{#1}\star}}
\newcommand{\ccol}[1]{\mathbf{C}_{\star{#1}}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\arow}[1]{\mathbf{A}_{{#1}\star}}
\newcommand{\acol}[1]{\mathbf{A}_{\star{#1}}}
\newcommand{\brow}[1]{\mathbf{B}_{{#1}\star}}
\newcommand{\bcol}[1]{\mathbf{B}_{\star{#1}}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\renewcommand{\t}{ \indent}
\newcommand{\nt}{ \indent}
\newcommand{\x}{\mathbf{x}}
\renewcommand{\Y}{\mathbf{Y}}
\newcommand{\ep}{\mathbf{\epsilon}}
\renewcommand{\pm}{\left(\begin{matrix}}
\renewcommand{\mp}{\end{matrix}\right)}
\newcommand{\bm}{\bordermatrix}
\usepackage{pdfpages,cancel}
\newenvironment{code}{\Verbatim [formatcom=\color{blue}]}{\endVerbatim}
\)
</span>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><center><img src="figs/matrixlogo.jpg" width="50"></center></li>
<li><center><strong> Linear Algebra for Data Science </strong></center></li>
<li><center><strong> with examples in R </strong></center></li>

<li class="divider"></li>
<li><a href="index.html#section"></a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#structure-of-the-book"><i class="fa fa-check"></i>Structure of the book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i>About the author</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#what-is-linear-algebra"><i class="fa fa-check"></i><b>1.1</b> What is Linear Algebra?</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#why-linear-algebra"><i class="fa fa-check"></i><b>1.2</b> Why Linear Algebra</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#describing-matrices-and-vectors"><i class="fa fa-check"></i><b>1.3</b> Describing Matrices and Vectors</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#vectors"><i class="fa fa-check"></i><b>1.4</b> Vectors</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#matrix-operations"><i class="fa fa-check"></i><b>1.5</b> Matrix Operations</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#special"><i class="fa fa-check"></i><b>1.6</b> Special Matrices and Vectors</a></li>
<li class="chapter" data-level="1.7" data-path="intro.html"><a href="intro.html#summary-of-conventional-notation"><i class="fa fa-check"></i><b>1.7</b> Summary of Conventional Notation</a></li>
<li class="chapter" data-level="1.8" data-path="intro.html"><a href="intro.html#exercises"><i class="fa fa-check"></i><b>1.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="mult.html"><a href="mult.html"><i class="fa fa-check"></i><b>2</b> Matrix Arithmetic</a>
<ul>
<li class="chapter" data-level="2.1" data-path="mult.html"><a href="mult.html#matrix-addition-subtraction-and-scalar-multiplication"><i class="fa fa-check"></i><b>2.1</b> Matrix Addition, Subtraction, and Scalar Multiplication</a></li>
<li class="chapter" data-level="2.2" data-path="mult.html"><a href="mult.html#sec:vectoradd"><i class="fa fa-check"></i><b>2.2</b> Geometry of Vector Addition and Scalar Multiplication</a></li>
<li class="chapter" data-level="2.3" data-path="mult.html"><a href="mult.html#linear-combinations"><i class="fa fa-check"></i><b>2.3</b> Linear Combinations</a></li>
<li class="chapter" data-level="2.4" data-path="mult.html"><a href="mult.html#matrix-multiplication"><i class="fa fa-check"></i><b>2.4</b> Matrix Multiplication</a></li>
<li class="chapter" data-level="2.5" data-path="mult.html"><a href="mult.html#vector-outer-products"><i class="fa fa-check"></i><b>2.5</b> Vector Outer Products</a></li>
<li class="chapter" data-level="2.6" data-path="mult.html"><a href="mult.html#the-identity-and-the-matrix-inverse"><i class="fa fa-check"></i><b>2.6</b> The Identity and the Matrix Inverse</a></li>
<li class="chapter" data-level="2.7" data-path="mult.html"><a href="mult.html#exercises-1"><i class="fa fa-check"></i><b>2.7</b> Exercises</a></li>
<li class="chapter" data-level="" data-path="mult.html"><a href="mult.html#list-of-key-terms"><i class="fa fa-check"></i>List of Key Terms</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="multapp.html"><a href="multapp.html"><i class="fa fa-check"></i><b>3</b> Applications of Matrix Multiplication</a>
<ul>
<li class="chapter" data-level="3.1" data-path="multapp.html"><a href="multapp.html#systems-of-equations"><i class="fa fa-check"></i><b>3.1</b> Systems of Equations</a></li>
<li class="chapter" data-level="3.2" data-path="multapp.html"><a href="multapp.html#regression-analysis"><i class="fa fa-check"></i><b>3.2</b> Regression Analysis</a></li>
<li class="chapter" data-level="3.3" data-path="multapp.html"><a href="multapp.html#linear-combinations-1"><i class="fa fa-check"></i><b>3.3</b> Linear Combinations</a></li>
<li class="chapter" data-level="3.4" data-path="multapp.html"><a href="multapp.html#multapp-ex"><i class="fa fa-check"></i><b>3.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="r-programming-basics.html"><a href="r-programming-basics.html"><i class="fa fa-check"></i><b>4</b> R Programming Basics</a></li>
<li class="chapter" data-level="5" data-path="solvesys.html"><a href="solvesys.html"><i class="fa fa-check"></i><b>5</b> Solving Systems of Equations</a>
<ul>
<li class="chapter" data-level="5.1" data-path="solvesys.html"><a href="solvesys.html#gaussian-elimination"><i class="fa fa-check"></i><b>5.1</b> Gaussian Elimination</a></li>
<li class="chapter" data-level="5.2" data-path="solvesys.html"><a href="solvesys.html#gauss-jordan-elimination"><i class="fa fa-check"></i><b>5.2</b> Gauss-Jordan Elimination</a></li>
<li class="chapter" data-level="5.3" data-path="solvesys.html"><a href="solvesys.html#three-types-of-systems"><i class="fa fa-check"></i><b>5.3</b> Three Types of Systems</a></li>
<li class="chapter" data-level="5.4" data-path="solvesys.html"><a href="solvesys.html#solving-matrix-equations"><i class="fa fa-check"></i><b>5.4</b> Solving Matrix Equations</a></li>
<li class="chapter" data-level="5.5" data-path="solvesys.html"><a href="solvesys.html#exercises-2"><i class="fa fa-check"></i><b>5.5</b> Exercises</a></li>
<li class="chapter" data-level="5.6" data-path="solvesys.html"><a href="solvesys.html#list-of-key-terms-1"><i class="fa fa-check"></i><b>5.6</b> List of Key Terms</a></li>
<li class="chapter" data-level="5.7" data-path="solvesys.html"><a href="solvesys.html#gauss-jordan-elimination-in-r"><i class="fa fa-check"></i><b>5.7</b> Gauss-Jordan Elimination in R</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="norms.html"><a href="norms.html"><i class="fa fa-check"></i><b>6</b> Norms, Inner Products and Orthogonality</a>
<ul>
<li class="chapter" data-level="6.1" data-path="norms.html"><a href="norms.html#sec-norms"><i class="fa fa-check"></i><b>6.1</b> Norms and Distances</a></li>
<li class="chapter" data-level="6.2" data-path="norms.html"><a href="norms.html#other-useful-norms-and-distances"><i class="fa fa-check"></i><b>6.2</b> Other useful norms and distances</a></li>
<li class="chapter" data-level="6.3" data-path="norms.html"><a href="norms.html#inner-products"><i class="fa fa-check"></i><b>6.3</b> Inner Products</a></li>
<li class="chapter" data-level="6.4" data-path="norms.html"><a href="norms.html#exercises-3"><i class="fa fa-check"></i><b>6.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="linind.html"><a href="linind.html"><i class="fa fa-check"></i><b>7</b> Linear Independence</a>
<ul>
<li class="chapter" data-level="7.1" data-path="linind.html"><a href="linind.html#linear-independence"><i class="fa fa-check"></i><b>7.1</b> Linear Independence</a></li>
<li class="chapter" data-level="7.2" data-path="linind.html"><a href="linind.html#span"><i class="fa fa-check"></i><b>7.2</b> Span of Vectors</a></li>
<li class="chapter" data-level="7.3" data-path="linind.html"><a href="linind.html#exercises-4"><i class="fa fa-check"></i><b>7.3</b> Exercises</a></li>
<li class="chapter" data-level="" data-path="linind.html"><a href="linind.html#list-of-key-terms-2"><i class="fa fa-check"></i>List of Key Terms</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="basis.html"><a href="basis.html"><i class="fa fa-check"></i><b>8</b> Basis and Change of Basis</a>
<ul>
<li class="chapter" data-level="8.1" data-path="basis.html"><a href="basis.html#exercises-5"><i class="fa fa-check"></i><b>8.1</b> Exercises</a></li>
<li class="chapter" data-level="" data-path="basis.html"><a href="basis.html#list-of-key-terms-3"><i class="fa fa-check"></i>List of Key Terms</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="orthog.html"><a href="orthog.html"><i class="fa fa-check"></i><b>9</b> Orthogonality</a>
<ul>
<li class="chapter" data-level="9.1" data-path="orthog.html"><a href="orthog.html#orthonormal-basis"><i class="fa fa-check"></i><b>9.1</b> Orthonormal Basis</a></li>
<li class="chapter" data-level="9.2" data-path="orthog.html"><a href="orthog.html#orthogonal-projection"><i class="fa fa-check"></i><b>9.2</b> Orthogonal Projection</a></li>
<li class="chapter" data-level="9.3" data-path="orthog.html"><a href="orthog.html#why"><i class="fa fa-check"></i><b>9.3</b> Why??</a></li>
<li class="chapter" data-level="9.4" data-path="orthog.html"><a href="orthog.html#exercises-6"><i class="fa fa-check"></i><b>9.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="eigen.html"><a href="eigen.html"><i class="fa fa-check"></i><b>10</b> Eigenvalues and Eigenvectors</a>
<ul>
<li class="chapter" data-level="10.1" data-path="eigen.html"><a href="eigen.html#diagonalization"><i class="fa fa-check"></i><b>10.1</b> Diagonalization</a></li>
<li class="chapter" data-level="10.2" data-path="eigen.html"><a href="eigen.html#geometric-interpretation-of-eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>10.2</b> Geometric Interpretation of Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="10.3" data-path="eigen.html"><a href="eigen.html#exercises-7"><i class="fa fa-check"></i><b>10.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="leastsquares.html"><a href="leastsquares.html"><i class="fa fa-check"></i><b>11</b> Least Squares</a>
<ul>
<li class="chapter" data-level="11.1" data-path="leastsquares.html"><a href="leastsquares.html#why-the-normal-equations"><i class="fa fa-check"></i><b>11.1</b> Why the normal equations?</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="lsapp.html"><a href="lsapp.html"><i class="fa fa-check"></i><b>12</b> Applications of Least Squares</a>
<ul>
<li class="chapter" data-level="12.1" data-path="lsapp.html"><a href="lsapp.html#simple-linear-regression"><i class="fa fa-check"></i><b>12.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="12.2" data-path="lsapp.html"><a href="lsapp.html#multiple-linear-regression"><i class="fa fa-check"></i><b>12.2</b> Multiple Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="pca.html"><a href="pca.html"><i class="fa fa-check"></i><b>13</b> Principal Components Analysis</a>
<ul>
<li class="chapter" data-level="13.1" data-path="pca.html"><a href="pca.html#geometrical-comparison-with-least-squares"><i class="fa fa-check"></i><b>13.1</b> Geometrical comparison with Least Squares</a></li>
<li class="chapter" data-level="13.2" data-path="pca.html"><a href="pca.html#covariance-or-correlation-matrix"><i class="fa fa-check"></i><b>13.2</b> Covariance or Correlation Matrix?</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="pca-in-r.html"><a href="pca-in-r.html"><i class="fa fa-check"></i><b>14</b> PCA in R</a>
<ul>
<li class="chapter" data-level="14.1" data-path="pca-in-r.html"><a href="pca-in-r.html#variable-clustering-with-pca"><i class="fa fa-check"></i><b>14.1</b> Variable Clustering with PCA</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="pcaapp.html"><a href="pcaapp.html"><i class="fa fa-check"></i><b>15</b> Applications of Principal Components</a>
<ul>
<li class="chapter" data-level="15.1" data-path="pcaapp.html"><a href="pcaapp.html#dimension-reduction"><i class="fa fa-check"></i><b>15.1</b> Dimension reduction</a></li>
<li class="chapter" data-level="15.2" data-path="pcaapp.html"><a href="pcaapp.html#exploratory-analysis"><i class="fa fa-check"></i><b>15.2</b> Exploratory Analysis</a></li>
<li class="chapter" data-level="15.3" data-path="pcaapp.html"><a href="pcaapp.html#fifa-soccer-players"><i class="fa fa-check"></i><b>15.3</b> FIFA Soccer Players</a></li>
<li class="chapter" data-level="15.4" data-path="pcaapp.html"><a href="pcaapp.html#cancer-genetics"><i class="fa fa-check"></i><b>15.4</b> Cancer Genetics</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="svd.html"><a href="svd.html"><i class="fa fa-check"></i><b>16</b> The Singular Value Decomposition (SVD)</a>
<ul>
<li class="chapter" data-level="16.1" data-path="svd.html"><a href="svd.html#resolving-a-matrix-into-components"><i class="fa fa-check"></i><b>16.1</b> Resolving a Matrix into Components</a></li>
<li class="chapter" data-level="16.2" data-path="svd.html"><a href="svd.html#noise-reduction"><i class="fa fa-check"></i><b>16.2</b> Noise Reduction</a></li>
<li class="chapter" data-level="16.3" data-path="svd.html"><a href="svd.html#latent-semantic-indexing"><i class="fa fa-check"></i><b>16.3</b> Latent Semantic Indexing</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="applications-of-svd.html"><a href="applications-of-svd.html"><i class="fa fa-check"></i><b>17</b> Applications of SVD</a>
<ul>
<li class="chapter" data-level="17.1" data-path="applications-of-svd.html"><a href="applications-of-svd.html#latent-semantic-indexing-1"><i class="fa fa-check"></i><b>17.1</b> Latent Semantic Indexing</a></li>
<li class="chapter" data-level="17.2" data-path="applications-of-svd.html"><a href="applications-of-svd.html#rappasvd"><i class="fa fa-check"></i><b>17.2</b> Image Compression</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="fa.html"><a href="fa.html"><i class="fa fa-check"></i><b>18</b> Factor Analysis</a>
<ul>
<li class="chapter" data-level="18.1" data-path="fa.html"><a href="fa.html#assumptions-of-factor-analysis"><i class="fa fa-check"></i><b>18.1</b> Assumptions of Factor Analysis</a></li>
<li class="chapter" data-level="18.2" data-path="fa.html"><a href="fa.html#determining-factorability"><i class="fa fa-check"></i><b>18.2</b> Determining Factorability</a></li>
<li class="chapter" data-level="18.3" data-path="fa.html"><a href="fa.html#communalities"><i class="fa fa-check"></i><b>18.3</b> Communalities</a></li>
<li class="chapter" data-level="18.4" data-path="fa.html"><a href="fa.html#number-of-factors"><i class="fa fa-check"></i><b>18.4</b> Number of Factors</a></li>
<li class="chapter" data-level="18.5" data-path="fa.html"><a href="fa.html#rotation-of-factors"><i class="fa fa-check"></i><b>18.5</b> Rotation of Factors</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="fa-apps.html"><a href="fa-apps.html"><i class="fa fa-check"></i><b>19</b> Factor Analysis</a>
<ul>
<li class="chapter" data-level="19.1" data-path="fa-apps.html"><a href="fa-apps.html#pca-rotations"><i class="fa fa-check"></i><b>19.1</b> PCA Rotations</a></li>
<li class="chapter" data-level="19.2" data-path="fa-apps.html"><a href="fa-apps.html#ex-personality-tests"><i class="fa fa-check"></i><b>19.2</b> Ex: Personality Tests</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="otherdimred.html"><a href="otherdimred.html"><i class="fa fa-check"></i><b>20</b> Dimension Reduction for Visualization</a>
<ul>
<li class="chapter" data-level="20.1" data-path="otherdimred.html"><a href="otherdimred.html#multidimensional-scaling"><i class="fa fa-check"></i><b>20.1</b> Multidimensional Scaling</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="sna.html"><a href="sna.html"><i class="fa fa-check"></i><b>21</b> Social Network Analysis</a>
<ul>
<li class="chapter" data-level="21.1" data-path="sna.html"><a href="sna.html#working-with-network-data"><i class="fa fa-check"></i><b>21.1</b> Working with Network Data</a></li>
<li class="chapter" data-level="21.2" data-path="sna.html"><a href="sna.html#network-visualization---igraph-package"><i class="fa fa-check"></i><b>21.2</b> Network Visualization - <code>igraph</code> package</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="leastsquares" class="section level1" number="11">
<h1><span class="header-section-number">Chapter 11</span> Least Squares</h1>
<p>The least squares problem arises in almost all areas of applied mathematics. In data science, the idea is generally to find an approximate mathematical relationship between predictor and target variables such that the sum of squared errors between the true target values and the predicted target values is minimized. In two dimensions, the goal would be to develop a line as depicted in Figure <a href="leastsquares.html#fig:leastsquaresillustrated">11.1</a> such that the sum of squared vertical distances (the residuals, in green) between the true data (in red) and the mathematical prediction (in blue) is minimized.</p>
<div class="figure" style="text-align: center"><span id="fig:leastsquaresillustrated"></span>
<img src="figs/lsreg.pdf" alt="Least Squares Illustrated in 2 dimensions"  />
<p class="caption">
Figure 11.1: Least Squares Illustrated in 2 dimensions
</p>
</div>
<p>If we let <span class="math inline">\(\bo{r}\)</span> be a vector containing the residual values <span class="math inline">\((r_1,r_2,\dots,r_n)\)</span> then the sum of squared residuals can be written in linear algebraic notation as
<span class="math display">\[\sum_{i=1}^n r_i^2 = \bo{r}^T\bo{r}=(\y-\hat{\y})^T(\y-\hat{\y}) = \|\y-\hat{\y}\|^2\]</span></p>
<p>Suppose we want to regress our target variable <span class="math inline">\(\y\)</span> on <span class="math inline">\(p\)</span> predictor variables, <span class="math inline">\(\x_1,\x_2,\dots,\x_p\)</span>. If we have <span class="math inline">\(n\)</span> observations, then the ideal situation would be to find a vector of parameters <span class="math inline">\(\boldsymbol\beta\)</span> containing an intercept, <span class="math inline">\(\beta_0\)</span> along with <span class="math inline">\(p\)</span> slope parameters, <span class="math inline">\(\beta_1,\dots,\beta_p\)</span> such that
<span class="math display" id="eq:lssystem">\[\begin{equation}
\tag{11.1}
\underbrace{\bordermatrix{1&amp;\x_1 &amp; \x_2 &amp; \dots &amp; \x_p}{obs_1\\obs_2\\ \vdots \\ obs_n}{\left(\begin{matrix}
                      1 &amp;  x_{11} &amp; x_{12} &amp; \dots &amp; x_{1p}\\
                     1 &amp; x_{21} &amp; x_{22} &amp; \dots &amp; x_{2p}\\
                      \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp;\vdots\\
                     1 &amp; x_{n1} &amp; x_{n2} &amp; \dots &amp; x_{np} \end{matrix}\right) }}_{\LARGE\mathbf{X}}
                      \underbrace{\left(\begin{matrix} \beta_0\\ \beta_1\\  \vdots \\ \beta_p \end{matrix}\right)}_{\LARGE \boldsymbol\beta} 
                      =  \underbrace{\pm y_0\\ y_1 \\ \vdots \\ y_n \mp}_{\LARGE \y} 
\end{equation}\]</span></p>
<p>With many more observations than variables, this system of equations will not, in practice, have a solution. Thus, our goal becomes finding a vector of parameters <span class="math inline">\(\hat{\bbeta}\)</span> such that <span class="math inline">\(\X\hat{\bbeta}=\hat{\y}\)</span> comes as close to <span class="math inline">\(\y\)</span> as possible.
Using the design matrix, <span class="math inline">\(\X\)</span>, the least squares solution <span class="math inline">\(\hat{\boldsymbol\beta}\)</span> is the one for which
<span class="math display">\[\|\y-\X\hat{\boldsymbol\beta} \|^2=\|\y-\hat{\y}\|^2\]</span> is minimized. Theorem <a href="leastsquares.html#thm:leastsquares">11.1</a> characterizes the solution to the least squares problem.</p>
<div class="theorem">
<p><span id="thm:leastsquares" class="theorem"><strong>Theorem 11.1  (Least Squares Problem and Solution) </strong></span>For an <span class="math inline">\(n\times m\)</span> matrix <span class="math inline">\(\X\)</span> and <span class="math inline">\(n\times 1\)</span> vector <span class="math inline">\(\y\)</span>, let <span class="math inline">\(\bo{r} = \X\widehat{\boldsymbol\beta} - \y\)</span>. The least squares problem is to find a vector <span class="math inline">\(\widehat{\boldsymbol\beta}\)</span> that minimizes the quantity
<span class="math display">\[\sum_{i=1}^n r_i^2 = \|\y-\X\widehat{\boldsymbol\beta} \|^2.\]</span></p>
<p>Any vector <span class="math inline">\(\widehat{\bbeta}\)</span> which provides a minimum value for this expression is called a <em>least-squares solution</em>.</p>
<ul>
<li>The set of all least squares solutions is precisely the set of solutions to the so-called <strong>normal equations</strong>, <span class="math display">\[\X^T\X\widehat{\bbeta} = \X^T\y.\]</span></li>
<li>There is a unique least squares solution if and only if <span class="math inline">\(rank(\X)=m\)</span> (i.e. linear independence of variables or no perfect multicollinearity!), in which case <span class="math inline">\(\X^T\X\)</span> is invertible and the solution is given by
<span class="math display">\[\widehat{\bbeta} = (\X^T\X)^{-1}\X^T\y\]</span></li>
</ul>
</div>
<div class="example">
<span id="exm:lsex" class="example"><strong>Example 11.1  (Solving a Least Squares Problem) </strong></span>In 2014, data was collected regarding the percentage of linear algebra exercises done by students and the grade they received on their examination. Based on this data, what is the expected effect of completing an additional 10% of the exercises on a students exam grade?\
<p>To find the least squares regression line, we want to solve the equation <span class="math inline">\(\X\bbeta = \y\)</span>:
<span class="math display">\[
\pm 1 &amp; 20\\
 1 &amp; 100\\
  1 &amp; 90\\
   1 &amp; 70\\
    1 &amp; 50\\
     1 &amp; 10\\
      1 &amp; 30\mp \pm \beta_0 \\ \beta_1 \mp = \pm 55\\100\\100\\70\\75\\25\\60 \mp
\]</span></p>
<p>This system is obviously inconsistent. Thus, we want to find the least squares solution <span class="math inline">\(\hat{\bbeta}\)</span> by solving <span class="math inline">\(\X^T\X\hat{\bbeta}=\X^T\y\)</span>:</p>
<p><span class="math display">\[\begin{eqnarray}
\small
\pm 1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1 \\20&amp;100&amp;90&amp;70&amp;50&amp;10&amp;30 \mp\pm 1 &amp; 20\\
 1 &amp; 100\\
  1 &amp; 90\\
   1 &amp; 70\\
    1 &amp; 50\\
     1 &amp; 10\\
      1 &amp; 30\mp \pm \beta_0 \\ \beta_1 \mp &amp;=&amp;\pm 1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1 \\20&amp;100&amp;90&amp;70&amp;50&amp;10&amp;30 \mp \pm 55\\100\\100\\70\\75\\25\\60 \mp \cr
\pm 7 &amp; 370\\370&amp;26900 \mp     \pm \beta_0 \\ \beta_1 \mp &amp;=&amp; \pm 485 \\ 30800\mp
\end{eqnarray}\]</span></p>
<p>Now, since multicollinearity was not a problem, we can simply find the inverse of <span class="math inline">\(\X^T\X\)</span> and multiply it on both sides of the equation:
<span class="math display">\[\pm 7 &amp; 370\\370&amp;26900 \mp^{-1}= \pm 0.5233 &amp;  -0.0072\\ -0.0072 &amp; 0.0001 \mp\]</span>
and so <span class="math display">\[\pm \beta_0 \\ \beta_1 \mp = \pm  0.5233 &amp;  -0.0072\\ -0.0072 &amp; 0.0001 \mp \pm  485 \\ 30800\mp = \pm 32.1109 \\0.7033\mp\]</span></p>
<p>Thus, for each additional 10% of exercises completed, exam grades are expected to increase by about 7 points. The data along with the regression line <span class="math display">\[grade=32.1109+0.7033percent\_exercises\]</span> is shown below.</p>
<center>
<img src=figs/grades.jpg width=50%/>
</center>
</div>
<div id="why-the-normal-equations" class="section level2" number="11.1">
<h2><span class="header-section-number">11.1</span> Why the normal equations?</h2>
<p>The normal equations can be derived using matrix calculus (demonstrated at the end of this section) but the solution of the normal equations also has a nice geometrical interpretation. It involves the idea of orthogonal projection, a concept which will be useful for understanding future topics.</p>
<div id="geometrical-interpretation" class="section level3" number="11.1.1">
<h3><span class="header-section-number">11.1.1</span> Geometrical Interpretation</h3>
<p>In order for a system of equations, <span class="math inline">\(\A\x=\b\)</span> to have a solution, <span class="math inline">\(\b\)</span> must be a linear combination of columns of <span class="math inline">\(\A\)</span>. That is simply the definition of matrix multiplication and equality. If <span class="math inline">\(\A\)</span> is <span class="math inline">\(m\times n\)</span> then
<span class="math display">\[\A\x=\b \Longrightarrow \b = x_1\A_1+x_2\A_2+\dots+x_n\A_n.\]</span>
As discussed in Chapter <a href="linind.html#linind">7</a>, another way to say this is that <span class="math inline">\(\b\)</span> is in the <span class="math inline">\(span\)</span> of the columns of <span class="math inline">\(\A\)</span>. The <span class="math inline">\(span\)</span> of the columns of <span class="math inline">\(\A\)</span> is called the <strong>column space</strong> of <span class="math inline">\(\A\)</span>. In Least-Squares applications, the problem is that <span class="math inline">\(\b\)</span> is <em>not</em> in the column space of <span class="math inline">\(\A\)</span>. In essence, we want to find the vector <span class="math inline">\(\hat{\b}\)</span> that is <em>closest</em> to <span class="math inline">\(\b\)</span> but exists in the column space of <span class="math inline">\(\A\)</span>. Then we know that <span class="math inline">\(\A\hat{\x}=\hat{\b}\)</span> <em>does</em> have a unique solution, and that the right hand side of the equation comes as close to the original data as possible. By multiplying both sides of the original equation by <span class="math inline">\(\A^T\)</span> what we are really doing is <em>projecting</em> <span class="math inline">\(\b\)</span> orthogonally onto the column space of <span class="math inline">\(\A\)</span>. We should think of the column space as a flat surface (perhaps a plane) in space, and <span class="math inline">\(\b\)</span> as a point that exists off of that flat surface. There are many ways to draw a line from a point to plane, but the shortest distance would always be travelled perpendicular (orthogonal) to the plane. You may recall from undergraduate calculus or physics that a <em>normal</em> vector to a plane is a vector that is orthogonal to that plane. The normal equations, <span class="math inline">\(\A^T\A\x=\A^T\b\)</span>, help us find the closest point to <span class="math inline">\(\b\)</span> that belongs to the column space of <span class="math inline">\(\A\)</span> by means of an orthogonal projection. This geometrical vantage point is depicted in Figure <a href="leastsquares.html#fig:lsproj">11.2</a>.</p>

<div class="figure" style="text-align: center"><span id="fig:lsproj"></span>
<img src="figs/lsproj.jpg" alt="The normal equations yield the vector \(\hat{\b}\) in the column space of \(\A\) which is closest to the original right hand side \(\b\) vector." width="60%" />
<p class="caption">
Figure 11.2: The normal equations yield the vector <span class="math inline">\(\hat{\b}\)</span> in the column space of <span class="math inline">\(\A\)</span> which is closest to the original right hand side <span class="math inline">\(\b\)</span> vector.
</p>
</div>
</div>
<div id="calculus-derivation" class="section level3" number="11.1.2">
<h3><span class="header-section-number">11.1.2</span> Calculus Derivation</h3>
<p>If you’ve taken a course in undergraduate calculus, you recall that finding minima and maxima of functions typically involves taking their derivatives and setting them equal to zero. That approach to the derivation of the normal equations will be fruitful, but we first need to understand how to take derivatives of matrix equations. Without teaching vector calculus, we will simply provide the following required formulas for matrix derivatives. If you’ve taken some undergraduate calculus, perhaps you’ll see some parallels.</p>
<table>
<tr>
<td>
Condition
<td>
Formula
<tr>
<td style="text-align:center">
<span class="math inline">\(\mathbf{a}\)</span> is not a function of <span class="math inline">\(\x\)</span>
<td>
<span class="math inline">\(\frac{\partial \mathbf{a}}{\partial \mathbf{x}}= \mathbf{0}\)</span>
<tr>
<tr>
<td style="text-align:center">
<span class="math inline">\(\mathbf{a}\)</span> is not a function of <span class="math inline">\(\x\)</span>
<td>
<span class="math inline">\(\frac{\partial \mathbf{x}}{\partial \mathbf{x}}= \mathbf{I}\)</span>
<tr>
<tr>
<td style="text-align:center">
<span class="math inline">\(\A\)</span> is not a function of <span class="math inline">\(\x\)</span>
<td>
<span class="math inline">\(\frac{\partial \mathbf{A}\mathbf{x}}{\partial \mathbf{x}}= \mathbf{A}\)</span>
<tr>
<td style="text-align:center">
<span class="math inline">\(\A\)</span> is not a function of <span class="math inline">\(\x\)</span>
<td>
<span class="math inline">\(\frac{\partial \mathbf{x}^T\mathbf{A}}{\partial \mathbf{x}}= \mathbf{A}^T\)</span>
<tr>
<td style="text-align:center">
<span class="math inline">\(\A\)</span> is not a function of <span class="math inline">\(\x\)</span>
<td>
<span class="math inline">\(\frac{\partial \mathbf{x}^T\mathbf{A}\mathbf{x}}{\partial \mathbf{x}}= (\mathbf{A}+\mathbf{A}^T)\mathbf{x}\)</span>
<tr>
<td style="text-align:center">
<span class="math inline">\(\A\)</span> is not a function of <span class="math inline">\(\x\)</span> <br> <span class="math inline">\(\A\)</span> is symmetric
<td>
<span class="math inline">\(\frac{\partial \mathbf{x}^T\mathbf{A}\mathbf{x}}{\partial \mathbf{x}}= 2\mathbf{A}\mathbf{x}\)</span>
</table

Now let's start with our objective, which is to minimize sum of squared error, by writing it as the inner product of the vector of residuals with itself:
$$\boldsymbol \varepsilon^T \boldsymbol \varepsilon = (\mathbf{y}-\mathbf{X}\boldsymbol \beta)^T(\mathbf{y}-\mathbf{X}\boldsymbol \beta)$$
We'd like to minimize this function with respect to $\boldsymbol \beta$, our vector of unknowns. Thus, our procedure will be to take the derivative with respect to $\boldsymbol \beta$ and set it equal to 0. 

In Chapter \@ref(lsapp), we'll take a deeper dive into the utility of least squares for applied data science. 


</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="eigen.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="lsapp.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/shainarace/LinearAlgebra/edit/master/029-OLStext.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/shainarace/LinearAlgebra/blob/master/029-OLStext.Rmd",
"text": null
},
"download": ["bookdownproj.pdf", "bookdownproj.epub"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
