[["index.html", "Linear Algebra for Data Science Preface Structure of the book About the author Acknowledgements", " Linear Algebra for Data Science Shaina Race Bennett, PhD 2021-07-29 Preface This course is meant to instill a working knowledge of linear algebra terminology and to lay the foundations of advanced data mining techniques like Principal Component Analysis, Factor Analysis, Collaborative Filtering, Correspondence Analysis, Network Analysis, Support Vector Machines and many more. In order to fully comprehend these important tools and techniques, we will need to understand the language in which they are presented: Linear Algebra. This is NOT a rigorous proof-based mathematics course. It is an intuitive introduction to the most important definitions and concepts that are needed to understand and effectively implement these important data mining methodologies. So that we know how to stir the pile… Image source: https://xkcd.com/1838/ Structure of the book This project is the fusion of a traditional textbook (with definitions, theorems, examples and exercises) with a collection of interactive programming exercises (designed for in-class demonstration) that was engineered to weave practicality and applied problem solving into the curriculum right from the start. This is a work in progress; please check back frequently for updates. About the author Shaina Race Bennett earned her PhD in Operations Research from NC State in 2014 where she focused on matrix theory and clustering high-dimensional data sets. She was a teaching assistant professor at the Institute for Advanced Analytics from 2014 until 2021 when she joined Fidelity Investments as a Principal Data Scientist. She enjoys bringing linear algebra to life with animations and applications, and would love to hear from you about your experience with this text. Acknowledgements The author would like to acknowledge and celebrate the work of her PhD advisor Dr. Carl Meyer who wrote the most thorough and complete proof-based presentation of the material in this book. If you’re looking for more details, we strongly suggest the book that contains all of them: Meyer, Carl D. Matrix analysis and applied linear algebra. Vol. 71. Siam, 2000. "],["intro.html", "Chapter 1 Introduction 1.1 What is Linear Algebra? 1.2 Why Linear Algebra 1.3 Describing Matrices and Vectors 1.4 Vectors 1.5 Matrix Operations 1.6 Special Matrices and Vectors 1.7 Summary of Conventional Notation 1.8 Exercises", " Chapter 1 Introduction 1.1 What is Linear Algebra? At its heart, Linear Algebra is the study of linear functions. The term linear should be understood to mean straight or flat. A line on the two dimensional coordinate plane, written \\[y=mx+b\\] represents a linear equation - the set of points \\((x,y)\\) satisfying this equation form a straight line. On the other hand, a quadratic equation like \\[y=ax^2+bx+c\\] is nonlinear - it is not straight, it has curvature. You may have learned that the equation of a plane in 3-dimensional space (the coordinate cube \\((x,y,z)\\)) is written as \\[ax+by+cz=d.\\] Such a plane is a classic example - the set of points \\((x,y,z)\\) satisfying this equation form a flat surface (a plane). A linear function involves only multiplication by scalars and addition/subtraction, for example: \\[2x+3y+6z=9 \\quad \\mbox{or} \\quad 4x_1-3x_2+9x_3+x_4-x_5+2x_6 = 2.\\] The second equation above brings us to an important point: we don’t have to restrict ourselves to a 3-dimensional world. While the “flatness” of linear equations is evident when we can graph them in 2 and 3-dimensions, with 6 variables we can no longer conceptualize the “flatness” of our equation. We take on principal that the surface that contains all solutions to the equation \\(4x_1-3x_2+9x_3+x_4-x_5+2x_6 = 2\\) is flat, without curvature, existing in a 6-dimensional space (or higher!). You may be asking now: what is 6-dimensional space? We’ll get to the the definition of \\(n\\)-space soon (Definition 1.5), but it should satisfy your intuition to extend your basic notion of coordinate axes. If you have 3 variables of interest, say height, weight and circumference, you can make a 3-d scatter plot because we have 3 physical dimensions to map to each characteristic. Add in a forth variable, say cost, and suddenly we cannot physically consider the plot (because our perception is limited to 3-dimensions) but we ought to be able to suspend our disbelief and assume that a “forth axes” (forth dimension) can be considered to represent cost. In some cases, this course will challenge you to think geometrically about data. Not in terms of the geometry you learned in high school regarding polygons and circles, but in terms of the layout and patterns of data. Linear algebra allows us to develop concepts of distance and similarity when our data has more than 3 variables and we can no longer look at a scatter plot and use our eyeballs to declare “these two observations are far away from each other.” The second term in the phrase is, of course, algebra. While you are likely familiar with the term, this course will challenge your initial familiarity. Linear Algebra deals with the algebra of matrices, which is likely different from any algebra you’ve experienced before. For example, when given an equation for an unknown value, like \\[2x=3,\\] you probably immediately think “I should divide both sides by 2 to determine the unknown value x.” Our equations in this course will be quite different because the expressions will involve matrices and vectors. For example, \\[\\left(\\begin{matrix} 2 &amp; 3\\\\1&amp;4 \\end{matrix}\\right) \\left(\\begin{matrix} x_1\\\\x_2 \\end{matrix}\\right) = \\left(\\begin{matrix} 5\\\\6 \\end{matrix}\\right)\\] is one type of equation we will learn to solve in this course. In this situation, we cannot simply divide both sides by the left hand matrix to solve for the unknowns - in fact it should look quite confusing to consider what that would even mean! Learning to work with matrices will be like learning a new language - the only way to succeed will be to practice and struggle and practice and struggle. Keep pace with the course and learn the terminology and definitions foremost - without the language and notation firmly in place, the techniques will seem far more difficult than they actually are. 1.2 Why Linear Algebra If you want to understand the foundations of data science, it is imperative that you be familiar with Linear Algebra. As you’ve probably already noticed, data tends to come in rows and columns. By its very nature, data forms a matrix. A matrix is just an array of numbers, logically ordered by rows and columns. For example take the following data: To do anything to this data, we need a way to store it mathematically. This is done by creating a matrix: The rows of this matrix correspond to observations, in this case a sample of people for whom we have collected data. The columns of this matrix correspond to the variables we are measuring. Some manipulation and pre-processing is usually required to turn nominal/categorical/qualitative variables into numerical data, often using binary dummy variables. Most every tool in data science involves some form of linear algebra on a data matrix. In this course, we will learn some of the foundations of these tools. If you master the material in this course, you will be able to understand many more advanced data techniques as you progress through your careers. With that in mind, let’s start at the beginning. 1.3 Describing Matrices and Vectors As we alluded to earlier, matrices are simply arrays of numbers which correspond in some way to their rows and columns. The following are examples of matrices: \\[\\A=\\left(\\begin{matrix} 1 &amp; 2\\\\3&amp;5 \\end{matrix}\\right) \\qquad \\mathbf{H}= \\left(\\begin{matrix} 6 &amp; 5&amp; 10\\\\0.1 &amp; 0.5 &amp; 0.9\\\\1&amp;4&amp;1\\\\1&amp;1&amp;1\\\\2&amp;0.4&amp;9.99 \\end{matrix}\\right)\\] In this text, when we denote a matrix, we will always use a bold capital letter like \\(\\mathbf{A}, \\mathbf{M}, \\mbox{or } \\mathbf{D}\\). To start, lets cover some basic properties and notation. 1.3.1 Dimension/Size of a Matrix Definition 1.1 (Dimension/Size of a Matrix) The dimension of a matrix is the number of rows and number of columns in the matrix. This is sometimes referred to as the size of the matrix. We say a matrix in \\(m\\times n\\) if it has \\(m\\) rows and \\(n\\) columns. If \\(\\A\\) is a matrix, we might specify the dimensions of \\(\\A\\) with a subscript: \\(\\A_{m\\times n}\\) should be read \\(\\A\\) is an \\(m\\times n\\) matrix. An \\(n\\times n\\) matrix is called a square matrix because it has the same number of rows and columns. A matrix without this characteristic is called a rectangular matrix. Example 1.1 (Dimensions of a Matrix) Consider the data matrix presented previously, containing observations on the two variables Credit Score and Income: \\[ \\A\\,=\\left(\\begin{matrix} 780 &amp; 95000\\cr 600 &amp; 60000\\cr 550 &amp; 65000\\cr 400 &amp; 35000\\cr 450 &amp; 40000\\cr 750 &amp; 80000\\end{matrix}\\right) \\] The dimension of the matrix \\(\\A\\) is \\(6\\times 2\\) because \\(\\A\\) has 6 rows and 2 columns. Thus when referring to \\(\\A\\) we might write \\(\\A_{6\\times 2}\\) when the size is important. Note that the number of rows _always} comes first when specifying the size of a matrix! Exercise 1.1 (Determining dimension of a matrix) For the following matrices, determine the dimension: \\[\\mathbf{B} = \\left( \\begin{matrix}1 &amp; 2 &amp; 0 \\cr 2&amp;1&amp;0\\cr3&amp;1&amp;1 \\end{matrix}\\right) \\quad \\mathbf{C} = \\left(\\begin{matrix} .01&amp;.5&amp;1.6&amp;1.7\\\\ .1&amp;3.5&amp;4&amp;2\\\\.61&amp;.55&amp;.46&amp;.17\\cr1.2&amp;1.5&amp;1.6&amp;1\\cr.31&amp;.35&amp;1.3&amp;2.3\\\\2.3&amp;3.5&amp;.06&amp;.7\\\\.3&amp;.2&amp;2.1&amp;1.8\\end{matrix}\\right) \\quad \\mathbf{T} = \\left(\\begin{matrix} 1\\cr1.3\\cr0.8\\cr2\\cr2.5\\cr0.8\\cr0.9 \\end{matrix} \\right)\\] 1.3.2 \\((i,j)\\) Notation Now beyond just knowing how many rows and columns a matrix has, we frequently want to refer to a specific row or column that matrix. In Example 1.1 above, you may have noticed that the names of the individuals did not appear in our matrix - those names were merely identifiers. Of course we did not throw them away and erase them from our hard drive, we merely replaced them with an implied numerical index \\(i=\\{1,2,3,4,5,6\\}\\) that corresponds to a row of the matrix for every person. As long as we don’t reorder the rows of our matrix, we will know that the correspondence is as follows: \\[\\left\\lbrace \\begin{array}{c} row 1\\\\ row 2\\\\ row 3\\\\ row 4\\\\ row 5\\\\ row 6 \\end{array} \\right\\rbrace \\Longleftrightarrow \\left\\lbrace \\begin{array}{c} John\\\\ Sam\\\\ Elena\\\\ Jim\\\\ Eric\\\\ Helen \\end{array} \\right\\rbrace\\] So, if I’d like to focus on data about Eric in particular, I’d have to look no further than the \\(5^{th}\\) row of my matrix \\(\\A\\). Similarly, if I want to know the Credit Scores of all individuals, I’d simply focus on the \\(1^{st}\\) column of the matrix. Hence, to find Eric’s Credit Score, I’d aim my sight on the element of the matrix located in the \\(5^{th}\\) row and \\(1^{st}\\) column of the matrix \\(\\A\\). Now, rather than write an entire sentence each time we want to refer to an individual element or row, we will develop some notation. As a general rule, the letter \\(i\\) is used to index the rows of a matrix and the letter \\(j\\) is used to index the columns. Definition 1.2 (Notation for Elements of a Matrix) Two subscripts are used to identify individual elements of a matrix: - The element of matrix \\(\\A\\) corresponding to row \\(i\\) and column \\(j\\) is written \\(A_{ij}\\) - The diagonal elements of a square matrix are those that have identical row and column indices: \\(\\A_{ii}\\) Beyond these basic conventions, there are other common notational tricks that we will become familiar with. The first of these is writing a partitioned matrix. We will often want to consider a matrix as a collection of either rows or columns rather than individual elements. As we will see in the future, when we partition matrices in this form, we can view their multiplication in simplified form. This often leads us to a new view of the data which can be helpful for interpretation. When we write \\(\\A=( \\A_1 | \\A_2 | \\dots | \\A_n )\\) we are viewing the matrix \\(\\A\\) as collection of column vectors, \\(\\A_i\\), in the following way: \\[\\A=( \\A_1 | \\A_2 | \\dots | \\A_n )=\\left(\\begin{matrix} \\uparrow &amp; \\uparrow &amp;\\uparrow&amp;\\dots &amp; \\uparrow \\\\ \\A_1&amp;\\A_2&amp;\\A_3&amp;\\dots&amp;\\A_p \\\\ \\downarrow &amp;\\downarrow &amp;\\downarrow &amp;\\dots&amp;\\downarrow \\end{matrix}\\right) \\] Similarly, we can write \\(\\A\\) as a collection of row vectors: \\[\\A=\\left(\\begin{matrix} \\A_1 \\\\ \\A_2 \\\\ \\vdots \\\\ \\A_m \\end{matrix}\\right) = \\left(\\begin{matrix} \\longleftarrow &amp; \\A_1 &amp; \\longrightarrow \\\\ \\longleftarrow &amp; \\A_2 &amp; \\longrightarrow \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ \\longleftarrow &amp; \\A_m &amp; \\longrightarrow \\end{matrix}\\right)\\] Sometimes, we will want to refer to both rows and columns in the same context. The above notation is not sufficient for this as we have \\(\\A_j\\) referring to either a column or a row. In these situations, we may use \\(\\A_{\\star j}\\) to reference the \\(j^{th}\\) column and \\(\\A_{i \\star}\\) to reference the \\(i^{th}\\) row: \\[\\bordermatrix{\\blue{\\acol{1}}&amp;\\acol{2}&amp;\\dots&amp;\\dots &amp;\\acol{n}}{~\\\\~\\\\~\\\\~\\\\~}{\\pm \\blue{a_{11}} &amp; a_{12} &amp;\\dots &amp; \\dots &amp; a_{1n} \\\\ \\blue{\\vdots} &amp; \\vdots &amp; &amp; &amp; \\vdots \\\\ \\blue{a_{i1}} &amp; \\dots&amp; a_{ij} &amp; \\dots &amp; a_{in}\\\\ \\blue{\\vdots} &amp; \\vdots &amp; &amp; &amp; \\vdots \\\\ \\blue{a_{m1}} &amp; \\dots&amp; \\dots &amp; \\dots &amp; a_{mn} \\mp}\\] \\[\\bordermatrix{&amp;&amp;&amp;&amp;&amp;}{\\blue{\\arow{1}} \\\\\\vdots \\\\ \\arow{i} \\\\ \\vdots \\\\\\arow{m}}{\\pm \\blue{a_{11}} &amp; \\blue{a_{12} }&amp;\\blue{\\dots} &amp; \\blue{\\dots} &amp; \\blue{a_{1n}} \\cr \\vdots&amp; \\vdots &amp; &amp; &amp; \\vdots \\cr a_{i1} &amp; \\dots&amp; a_{ij} &amp; \\dots &amp; a_{in}\\cr \\vdots &amp; \\vdots &amp; &amp; &amp; \\vdots \\cr a_{m1} &amp; \\dots&amp; \\dots &amp; \\dots &amp; a_{mn} \\mp}\\] Definition 1.3 (Rows and Columns of a Matrix) To refer to entire rows or columns, a placeholder \\(\\star\\) is often used to represent the idea that we take all rows or columns after narrowing in on a given column or row respectively: - To refer to the \\(k^{th}\\) row of \\(\\A\\) we will use the notation \\(\\arow{k}\\) (“$k^{th} row, all columns”) - Similarly, to refer to the \\(k^{th}\\) column of \\(\\A\\) we will use the notation \\(\\acol{k}\\). Often, when there is no confusion about whether we are referring to rows or columns, the above notation will be shortened to simply \\(\\A_k\\). In such a scenario it will be made clear from context whether this represents the \\(k^{th}\\) row or \\(k^{th}\\) column. For example, \\[\\A=( \\A_1 | \\A_2 | \\dots | \\A_n )\\] represents a matrix \\(\\A\\) with \\(n\\) columns \\(\\{\\A_1,\\A_2,\\dots,\\A_n\\}\\). Example 1.2 (Rows, Columns, and Elements of a Matrix) Consider the following table of data and corresponding matrix: Element \\(B_{42}=150\\) corresponds to the weight (second column) of observation 4 (forth row). The column \\[\\bcol{3} = \\left(\\begin{matrix} 23\\\\30\\\\41\\\\27\\\\35\\\\25\\\\21 \\end{matrix}\\right)\\] corresponds to the variable age and the row \\[\\brow{6} = \\left(\\begin{matrix} 68&amp;165&amp;25 \\end{matrix}\\right)\\] corresponds to the measurements for observation 6. Exercise 1.2 (Rows, Columns, and Elements of a Matrix) For the matrix \\[ \\mathbf{L}=\\left(\\begin{matrix} 3 &amp; -6 &amp; -2\\\\1 &amp; -4 &amp; 5\\\\0 &amp; 4 &amp; -5 \\end{matrix}\\right)\\] Write the following elements, rows, or columns: \\[L_{23}= \\qquad\\qquad L_{31}= \\qquad\\qquad \\mathbf{L}_{\\star 2}= \\qquad\\qquad \\mathbf{L}_{1 \\star}=\\qquad\\qquad\\] Definition 1.4 (Equality of Matrices) Two matrices are equal if and only if they have the same size and all of their corresponding entries are equal. That is, \\[\\A=\\B \\Longleftrightarrow A_{ij} = B_{ij} \\quad \\mbox{for all i and j}\\] Example: Defining social networks One advantage to using (i,j)-notation is that it allows us to define an entire matrix by describing one arbitrary element according to its row (\\(i\\)) and column (\\(j\\)). Let’s consider a classic example from network analysis. Suppose we have a group of 6 students and we want to examine how often they have worked together in teams. Rather than use the students names, let’s just number them Student 1 through Student 6. These students were put into teams in the summer semester and then reassigned to new teams for the fall and the spring semester: Now, we can define what is called an adjacency or association matrix for this data as follows: \\[\\A_{ij} =\\left\\{ \\begin{array}{l} \\mbox{# of times Student i has worked with Student j}\\,\\,\\,\\mbox{if } i\\neq j\\\\ 0 \\,\\,\\,\\mbox{ if } i=j \\end{array} \\right.\\] Breaking apart this matrix definition, we see both the rows (indexed by \\(i\\)) and columns (indexed by \\(j\\)) will correspond to students. For example, the element in the \\(2^{nd}\\) row and \\(3^{rd}\\) column, (\\(A_{23}\\)), will be the number of times Student 2 has worked with Student 3. Thus, \\[A_{23}=2.\\] When the row and column numbers are the same (\\(i=j\\)), which happens along the diagonal of the matrix, the entries will be 0 (this number was chosen arbitrarily - one could also use `3’ along the diagonal). The result is a square matrix with 6 rows and columns: \\[\\A=\\left(\\begin{matrix} 0&amp;2&amp;1&amp;1&amp;1&amp;1\\\\2&amp;0&amp;2&amp;2&amp;0&amp;0\\\\1&amp;2&amp;0&amp;1&amp;1&amp;1\\\\1&amp;2&amp;1&amp;0&amp;1&amp;1\\\\1&amp;0&amp;1&amp;1&amp;0&amp;3\\\\1&amp;0&amp;1&amp;1&amp;3&amp;0\\end{matrix}\\right)\\] We can quickly see that there is symmetry in this matrix because the number of times Students \\(i\\) and \\(j\\) worked together is, of course, the same as the number of times Students \\(j\\) and \\(i\\) worked together. Formally, a matrix is called symmetric if \\(A_{ij}=A_{ji}\\) (this important concept will be defined again on page ). We can also see straight from this matrix that Students 5 and 6 worked together every semester while neither of them worked with Student 2 at all. An adjacency matrix like the one listed above is often used to describe a network or graph. A graph is a collection of vertices (nodes) that each represent some entity, and edges which connect vertices that are related in some way. Below is a network of the students, where the thickness of the edge between two vertices indicates the number of times they have worked together. Figure 1.1: Network of Student Team Membership If we had chosen to put 3’s along the diagonal, each vertex in this graph would have a bold edge that loops back to itself. Graphs and the adjacency matrices which define them are at the heart of many problems in social network analysis. For example, websites like Facebook and LinkedIn are just enormous networks of nodes (individual users) connected by edges (“friendships” or “connections”). In the case of Twitter, the network is even more complicated because the relationships are directed: the “follower” connection is not symmetric, the people I “follow” do not have to follow me back. We may have a chance to discuss more of this later; in general, it makes the analysis much more difficult! 1.3.3 Example: Correlation matrices When we have several variables to analyze, it is good practice to measure the pairwise correlations between variables. Suppose we have 4 variables, \\(x_1, x_2, x_3,\\,\\mbox{and}\\, x_4\\). We will often examine the correlation matrix, \\(\\C\\), which is defined as follows: \\[\\C_{ij}=correlation(x_i,x_j).\\] For example, suppose our correlation matrix is as follows: \\[\\C=\\left(\\begin{matrix} 1 &amp; 0.3 &amp; -0.9 &amp; 0.1\\\\0.3 &amp; 1 &amp; 0.8 &amp; -0.5\\\\-0.9&amp;0.8&amp;1&amp;-0.6\\\\0.1&amp;-0.5&amp;-0.6&amp;1 \\end{matrix}\\right)\\] It is clear that the diagonal elements of this matrix, \\(C_{ii}\\), should always equal 1 because every variable is perfectly correlated with itself. We can also see that \\[C_{ij}=C_{ji}\\quad \\mbox{Because }\\,correlation(x_i,x_j)=correlation(x_j,x_i)\\] And in this particular example, \\[C_{13}=-0.9 \\quad \\mbox{Indicates that }\\,x_1\\,\\,\\mbox{and}\\,\\,x_3\\,\\,\\mbox{have a strong negative correlation.}\\] 1.4 Vectors Now that we’ve introduced the concept of a matrix, let’s talk about vectors. A vector are merely a special type of matrix, one that consists of only a single row or column. They are essentially ordered lists of numbers. For example: \\[\\x=\\left(\\begin{matrix} 5\\\\6\\\\7\\\\8 \\end{matrix}\\right) \\qquad \\mathbf{z}=\\left(\\begin{matrix} 3 &amp; 5 &amp; 1 &amp; 0 &amp; 2 \\end{matrix}\\right) \\qquad \\y = \\left(\\begin{matrix} y_1\\\\y_2\\\\y_3 \\end{matrix}\\right)\\] are all examples of vectors; \\(\\x\\) and \\(\\y\\) are column vectors and \\(\\mathbf{z}\\) is a row vector. In this text, to denote a vector we will always use a bold lower-case letter like \\(\\x, \\y, \\mbox{or } \\mathbf{a}\\). Since vectors only have one row or column, we do not need two subscripts to refer to their elements - we can use a single subscript to refer to the elements, as shown in the vector \\(\\y\\). Thus, \\(x_3\\) refers to the \\(3^{rd}\\) element in the vector \\(\\x\\) above: \\[x_3 = 7.\\] Notice that the elements of a vector are not written in bold. In fact, all scalar quantities (a quantity that is just a single number (like 2 or \\(\\sqrt{5}\\)), not a matrix) will be represented by unbolded, usually lowercase (often times greek) letters. For example, letters like \\(\\alpha, \\beta, x_i, \\mbox{or } a\\) will be used to refer to scalars. You may wonder now why the notation is so particular, but as we get into the subject it will be come clear that some convention must be retained if we are going to understand an equation. If we write an equation like \\[\\A\\x=\\b \\qquad \\mbox{or} \\qquad \\mathbf{M}\\y=\\alpha\\y\\] We have to know what it represents - in the first case, we are dealing with a matrix \\(\\A\\), and two vectors \\(\\x\\) and \\(\\b\\); in the second case we are dealing with a matrix \\(\\mathbf{M}\\), a vector \\(\\y\\), and a scalar, \\(\\alpha\\). Exercise 1.3 (Matrix, Vector, and Scalar Notation) For the following quantities, indicate whether the notation indicates a Matrix, Scalar, or Vector. \\[\\A \\qquad \\qquad A_{ij} \\qquad \\qquad \\v \\qquad \\qquad p_2 \\qquad \\qquad \\lambda \\qquad \\qquad \\p_2\\] In the previous exercise, we see an important difference: \\(p_2\\) is not bold, and thus is automatically assumed to a scalar, while \\(\\p_2\\) is bold and lowercase meaning, despite the subscript, it refers to a vector. 1.4.1 Vector Geometry: \\(n\\)-space You are already familiar with the concept of “ordered pairs” or coordinates \\((x_1,x_2)\\) on the two-dimensional plane (in Linear Algebra, we call this plane “\\(2\\)-space”). Fortunately, we do not live in a two-dimensional world! Our data will more often consist of measurements on a number (lets call that number \\(n\\)) of variables. Thus, our data points belong to what is known as \\(n\\)-space. They are represented by \\(n\\)-tuples which are nothing more than ordered lists of numbers: \\[(x_1, x_2, x_3, \\dots, x_n).\\] An \\(n\\)-tuple defines a vector with the same \\(n\\) elements, and so these two concepts should be thought of interchangeably. The only difference is that the vector has a direction (usually depicted with an arrow), away from the origin and toward the \\(n\\)-tuple. This concept is obviously very difficult to visualize when \\(n&gt;3\\), but our mental visualizations of \\(2-\\) and \\(3-\\)space will usually be sufficient when we want to consider higher dimensional spaces. Example 1.3 (2-space) Whereas we once thought of ordered pairs as points in space, we can also consider the coordinates as defining vectors. In almost every scenario, these two concepts can be thought of equivalently, however the direction of vectors helps us define their arithmetic geometrically, as we will see in the next chapter. You will recall that the symbol \\(\\Re\\) is used to denote the set of real numbers. \\(\\Re\\) is simply \\(1\\)-space. It is a set of vectors with a single element. In this sense any real number, \\(x\\), has a direction: if it is positive, it is to one side of the origin, if it is negative it is to the opposite side. That number, \\(x\\), also has a magnitude: \\(|x|\\) is the distance between \\(x\\) and the origin, 0. Definition 1.5 (n-space) \\(n\\)-space (the set of real \\(n\\)-tuples) is denoted \\(\\Re^n\\). In set notation, the formal mathematical definition is simply: \\[\\Re^n = \\left\\lbrace (x_1,x_2,\\dots,x_n) : x_i \\in \\Re, i=1,\\dots, n\\right\\rbrace.\\] We will often use this notation to define the size of an arbitrary vector. For example, \\(\\x \\in \\Re^p\\) simply means that \\(\\x\\) is a vector with \\(p\\) entries: \\(\\x=(x_1,x_2,\\dots,x_p)\\). Many (all, really) of the concepts we have previously considered in \\(2\\)- or \\(3\\)-space extend naturally to \\(n\\)-space and a few new concepts become useful as well. One very important concept is that of a norm or distance metric, as we will see in Chapter 6. Before we get into the details of the vector space model, let’s continue with some of the basic definitions we will need on that journey. 1.5 Matrix Operations 1.5.1 Transpose One important transformation we will have to perform on a matrix is to switch the columns into rows. It is not necessary that you see the importance of this transformation right now, but trust that it is something we will need quite frequently. Definition 1.6 (Transpose of a Matrix or Vector) For a given \\(m\\times n\\) matrix \\(\\A\\), the transpose of \\(\\A\\), written \\(\\A^T\\) (read as “\\(\\A\\)-transpose”) in this course and sometimes elsewhere denoted as \\(\\A&#39;\\) is the \\(n\\times m\\) matrix whose rows are the corresponding columns of \\(\\A\\). Thus, if \\(\\A\\) is a \\(3\\times 4\\) matrix then \\(\\A^T\\) is a \\(4\\times 3\\) matrix as follows: \\[\\A = \\left(\\begin{matrix} A_{11} &amp; A_{12} &amp; A_{13} &amp;A_{14}\\\\ A_{21} &amp; A_{22} &amp; A_{23} &amp;A_{24}\\\\ A_{31} &amp; A_{32} &amp; A_{33} &amp;A_{34}\\end{matrix}\\right) \\quad \\A^T = \\left(\\begin{matrix} A_{11} &amp; A_{21} &amp; A_{31}\\\\A_{12} &amp; A_{22} &amp; A_{32}\\\\A_{13} &amp; A_{23} &amp; A_{33}\\\\A_{14} &amp; A_{24} &amp; A_{34}\\end{matrix}\\right)\\] An equivalent way to state this definition is to say that \\[(A^T)_{ij} = A_{ji}\\] Note: If we transpose the transpose of a matrix, we will get back the original matrix. That is, \\[(\\A^T)^T = \\A.\\] Example 1.4 (Transpose of a Matrix or Vector) For the following matrices and vectors, determine the transpose: \\[\\B=\\left(\\begin{matrix} 2 &amp; -3 &amp; -4 \\\\5&amp;-6&amp;-7\\\\-8&amp;9&amp;0 \\end{matrix}\\right) \\qquad \\mathbf{M}=\\left(\\begin{matrix} -1&amp;2\\\\-3&amp;6\\\\7&amp;-9\\\\5&amp;-1 \\end{matrix}\\right) \\qquad \\x=\\left(\\begin{matrix}3\\\\-4\\\\5\\\\6\\end{matrix}\\right)\\] To find the transpose, we simply create new matrices whose rows are the corresponding columns of each matrix or vector: \\[\\B^T=\\left(\\begin{matrix} 2 &amp;5&amp; -8\\\\-3&amp;-6&amp;9\\\\-4&amp;-7&amp;0\\end{matrix}\\right) \\qquad \\mathbf{M}^T = \\left(\\begin{matrix}-1&amp;-3&amp;7&amp;5\\\\2&amp;6&amp;-9&amp;-1 \\end{matrix}\\right) \\] \\[\\x^T = \\left(\\begin{matrix} 3&amp;-4&amp;5&amp;6 \\end{matrix}\\right)\\] Definition 1.7 (Symmetric Matrix) Defining the matrix transpose allows us to define the notion of a symmetric matrix. A matrix \\(\\A\\) is called symmetric if and only if \\[\\A=\\A^T.\\] This is equivalent to saying that \\(\\A\\) is symmetric if and only if \\[A_{ij}=A_{ji}.\\] It should be clear from the definition that in order for \\(\\A\\) to be a symmetric matrix, it must be a square matrix - otherwise \\(\\A\\) and \\(\\A^T\\) would not even have the same size! Example 1.5 (Symmetric Matrices) The following matrix is symmetric because \\(\\B^T = \\B\\). \\[\\B=\\left(\\begin{matrix} 2 &amp; 0 &amp; 1 \\\\0&amp;-6&amp;-7\\\\1&amp;-7&amp;0 \\end{matrix}\\right)\\] Exercise 1.4 (Transpose and Symmetry) Given that, \\[\\A=\\left(\\begin{matrix} 2 &amp; -4 \\\\-1&amp;2\\\\3&amp;-6 \\end{matrix}\\right) \\quad \\v^T = \\left(\\begin{matrix} 1 &amp; 0 &amp; -2 &amp; 5\\end{matrix}\\right) \\quad \\B=\\left(\\begin{matrix} B_{11}&amp;B_{12}&amp;B_{13}\\\\B_{21}&amp;B_{22}&amp;B_{23}\\\\B_{31}&amp;B_{32}&amp;B_{33}\\\\B_{41}&amp;B_{42}&amp;B_{43}\\end{matrix}\\right)\\] compute the following matrices: \\[\\A^T= \\qquad \\qquad \\qquad \\qquad (\\A^T)^T= \\qquad \\qquad \\qquad \\qquad\\] \\[ \\v= \\qquad \\qquad \\qquad \\qquad \\B^T = \\qquad \\qquad\\qquad \\qquad \\] Give an example of a \\(4\\times 4\\) symmetric matrix:\\ Is a correlation matrix symmetric? (For a hint, see section 1.3.3)\\ 1.5.2 Trace of a Matrix Another important matrix operation that will come into play later is the trace of a matrix. The trace of a square matrix is the sum of the diagonal elements of this matrix. Definition 1.8 (Trace of a Matrix) For an \\(n\\times n\\) square matrix, \\(\\A\\), the trace of \\(\\A\\), written \\[trace(\\A)\\,\\,\\,\\mbox{or}\\,\\,\\,tr(\\A)\\] is the sum of the diagonal elements: \\[tr(\\A)=\\sum_{i=1}^n A_{ii}\\] The trace of a rectangular matrix is undefined. Example 1.6 (Trace of a Matrix) Let \\[\\A=\\left(\\begin{matrix} 3&amp;4&amp;1\\\\0&amp;1&amp;-2\\\\-1&amp;\\sqrt{2}&amp;3\\end{matrix}\\right)\\] Then, the trace of \\(\\A\\) is the sum of the diagonal elements: \\[tr(\\A)=\\sum_{i=1}^3 A_{ii} = 3+1+3 = 7.\\] While we’re on the subject of diagonal elements, let’s take the opportunity to introduce some special types of matrices, namely the identity matrix and a diagonal matrix. 1.6 Special Matrices and Vectors Definition 1.9 (Identity Matrix) The bold capital letter \\(\\mathbf{I}\\) will always to denote the identity matrix. Sometimes this matrix has a single subscript to specify the size of the matrix. More often, the size of the identity is implied by the matrix equation in which it appears. \\[\\mathbf{I}_4 = \\left(\\begin{matrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0\\\\ 0&amp;0&amp;1&amp;0\\\\ 0&amp;0&amp;0&amp;1 \\end{matrix}\\right)\\] Definition 1.10 (Elementary Vectors) The bold lowercase \\(\\mathbf{e}_j\\) is used to refer to the \\(j^{th}\\) column of \\(\\mathbf{I}\\). It is simply a vector of zeros with a one in the \\(j^{th}\\) position. We do not often specify the size of the vector \\(\\mathbf{e}_j\\), the number of elements is generally assumed from the context of the problem. \\[\\mathbf{e}_j=\\begin{matrix} \\begin{matrix} ~ \\\\~\\\\~\\\\~\\\\j^{th}\\text{row} \\rightarrow \\\\ ~\\\\~\\\\~ \\end{matrix} &amp; \\begin{pmatrix}0\\\\0\\\\ \\vdots \\\\0\\\\1\\\\0\\\\ \\vdots\\\\0\\end{pmatrix}\\\\\\\\ \\end{matrix}\\] The vector \\(\\mathbf{e}\\) with no subscript refers to a vector of all ones. In some texts, this vector is written as a bold faced \\(\\textbf{1}\\). \\[\\mathbf{e} = \\left(\\begin{matrix} 1\\\\1\\\\1\\\\ \\vdots \\\\ 1\\end{matrix}\\right)\\] The elementary vectors described in Definition 1.10 create the coordinate axes of \\(n\\)-space. For illustrative purposes, let’s consider \\(2\\)-space. The elementary vectors in \\(2\\)-space are: \\[\\e_1 = \\left(\\begin{matrix} 1\\\\0\\end{matrix}\\right) \\quad \\mbox{and} \\quad \\e_2 = \\left(\\begin{matrix} 0\\\\1 \\end{matrix}\\right) \\] These vectors correspond to the directions of the coordinate axes as illustrated in Figure 1.2 Figure 1.2: Elementary cectors represent our “usual” coordinate axes Definition 1.11 (Diagonal Matrix) A diagonal matrix is a matrix for which off-diagonal elements, \\(\\A_{ij},\\,i\\ne j\\) are zero. For example: \\[\\mathbf{D} = \\left(\\begin{matrix} \\sigma_1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_2 &amp; 0 &amp; 0\\\\ 0&amp;0&amp;\\sigma_3&amp;0\\\\ 0&amp;0&amp;0&amp;\\sigma_4 \\end{matrix}\\right)\\] Since the off diagonal elements are 0, we need only define the diagonal elements for such a matrix. Thus, we will frequently write \\[\\mathbf{D}=diag\\{\\sigma_1,\\sigma_2,\\sigma_3,\\sigma_4\\}\\] or simply \\[D_{ii} = \\sigma_i.\\] From the preceding definitions, it should be clear that diagonal and identity matrices are always square, meaning they have the same number of rows and columns, and {symmetric}, meaning they do not change under the transpose operation. Exercise 1.5 (Trace and Special Matrices) Write out the following matrices and then compute their Trace, if possible: \\[\\I_5 \\qquad \\mathbf{D}=diag\\{2,6,1\\} \\qquad \\e_2 \\in \\Re^4\\] Definition 1.12 (Triangular Matrices) Triangular matrices are square matrices whose elements are zero either below or above the main diagonal. If the matrix has zeros below the main diagonal, then it’s called upper triangular. The following illustration depicts an upper triangular matrix, where the asterisk symbol is used to denote any number (including potential zeros). \\[\\left(\\begin{matrix} *&amp;*&amp;*&amp;\\dots&amp;*\\\\0&amp;*&amp;*&amp;\\dots&amp;*\\\\0&amp;0&amp;*&amp;\\dots&amp;*\\\\ \\vdots &amp;\\vdots &amp;\\ddots &amp;\\ddots&amp;*\\\\0&amp;0&amp;0&amp;0&amp;*\\end{matrix}\\right)\\] If a matrix has zeros above the main diagonal, then it’s called lower triangular. The following illustration depicts a lower triangular matrix. \\[\\left(\\begin{matrix} 0&amp;0&amp;0&amp;\\dots&amp;0\\\\ *&amp;0&amp;0&amp;\\dots&amp;0\\\\ *&amp;*&amp;0&amp;\\dots&amp;0\\\\ \\vdots &amp;\\vdots &amp;\\ddots &amp;\\ddots&amp;0\\\\ *&amp;*&amp;*&amp;*&amp;0\\end{matrix}\\right)\\] Exercise 1.6 (Triangular Matrices) The following are examples of triangular matrices. Are they upper or lower triangular? \\[\\left(\\begin{matrix} 1&amp;2&amp;3&amp;4\\\\0&amp;5&amp;6&amp;7\\\\0&amp;0&amp;8&amp;9\\\\0&amp;0&amp;0&amp;1 \\end{matrix}\\right) \\quad \\left(\\begin{matrix} -1&amp;0&amp;0\\\\0&amp;-2&amp;0\\\\1&amp;-1&amp;2 \\end{matrix}\\right) \\quad \\left(\\begin{matrix} 0&amp;0&amp;0\\\\0&amp;0&amp;0\\\\0&amp;0&amp;0 \\end{matrix}\\right)\\] 1.7 Summary of Conventional Notation Linear Algebra has some conventional ways of representing certain types of numerical objects. Throughout this course, we will stick to the following basic conventions: Bold and uppercase letters like \\(\\A\\), \\(\\X\\), and \\(\\U\\) will be used to refer to matrices. Occasionally, the size of the matrix will be specified by subscripts, like \\(\\A_{m\\times n}\\), which means that \\(\\A\\) is a matrix with \\(m\\) rows and \\(n\\) columns. Bold and lowercase letters like \\(\\x\\) and \\(\\y\\) will be used to reference vectors. Unless otherwise specified, these vectors will be thought of as columns, with \\(\\x^T\\) and \\(\\y^T\\) referring to the row equivalent. The individual elements of a vector or matrix will often be referred to with subscripts, so that \\(A_{ij}\\) (or sometimes \\(a_{ij}\\)) denotes the element in the \\(i^{th}\\) row and \\(j^{th}\\) column of the matrix \\(\\A\\). Similarly, \\(x_k\\) denotes the \\(k^{th}\\) element of the vector \\(\\x\\). These references to individual elements are not generally bolded because they refer to scalar quantities. Scalar quantities are written as unbolded greek letters like \\(\\alpha\\), \\(\\delta\\), and \\(\\lambda\\). The notation \\(\\x \\in \\Re^n\\) simply means that \\(\\x\\) is a vector in \\(n\\)-space, or a vector with \\(n\\) elements. The transpose of an \\(m\\times n\\) matrix \\(\\A\\) is the \\(n\\times m\\) matrix \\(\\A^T\\) whose rows are the columns of \\(\\A\\). The trace of a square matrix \\(\\A_{n\\times n}\\), denoted \\(Tr(\\A)\\) or \\(Trace(\\A)\\), is the sum of the diagonal elements of \\(\\A\\), \\[Tr(\\A)=\\sum_{i=1}^n A_{ii}.\\] 1.8 Exercises Use the following matrices or vectors to answer the following questions: \\[\\begin{equation*} \\mathbf{A}=\\left(\\begin{array}{ccc} 1&amp;3&amp;8\\\\3&amp;0&amp;-2\\\\4&amp;1&amp;-3 \\end{array}\\right) \\quad \\mathbf{M}=\\left(\\begin{array}{cccc} 1&amp;8&amp;-2&amp;5\\\\2&amp;8&amp;1&amp;7 \\end{array}\\right) \\quad \\mathbf{D} = \\left(\\begin{array}{ccc} 1&amp;0&amp;0\\\\0&amp;5&amp;0\\\\0&amp;0&amp;3\\end{array}\\right) \\end{equation*}\\] \\[\\begin{equation*} \\mathbf{X}=\\left(\\begin{array}{cc} 780 &amp; 95000\\\\600 &amp; 60000\\\\550 &amp; 65000\\\\400 &amp; 35000\\\\450 &amp; 40000\\\\750 &amp;80000\\end{array}\\right) \\quad \\mathbf{t} = \\left(\\begin{array}{c} 1\\\\1.3\\\\0.8\\\\2\\\\2.5\\\\0.8\\\\0.9 \\end{array}\\right) \\quad \\mathbf{v}=\\left(\\begin{array}{c} 6\\\\3\\\\-1\\\\2\\end{array}\\right) \\quad \\mathbf{u}=\\left(\\begin{array}{cccc} 6&amp;4&amp;8&amp;1\\end{array}\\right) \\end{equation*}\\] Write the appropriate size/dimensions next to each matrix: \\(\\mathbf{A}\\) \\(\\mathbf{M}\\) \\(\\mathbf{D}\\) \\(\\mathbf{X}\\) \\(\\mathbf{t}\\) \\(\\mathbf{v}\\) \\(\\mathbf{u}\\) Which of these matrices are square? Which are rectangular? Give the following quantities: \\(A_{12}=\\) \\(M_{21}=\\) \\(\\D_{\\star3}=\\) \\(\\mathbf{M}_{2\\star}=\\) \\(X_{42}=\\) \\(t_5=\\) \\(v_3=\\) What are the diagonal elements of \\(\\A\\)? For each of the following matrices and vectors, give their dimension. Label each as a matrix or vector. For each matrix, indicate whether the matrix is square or rectangular. \\[\\A=\\left(\\begin{matrix} 2 &amp; 3 &amp; -1\\\\1&amp;-1&amp;1\\\\2&amp;2&amp;1 \\end{matrix}\\right)\\] \\[\\mathbf{h}=\\left(\\begin{matrix} -1\\\\-4\\\\1\\\\2 \\end{matrix}\\right)\\] \\[\\B=\\left(\\begin{matrix} B_{11}&amp;B_{12}&amp;B_{13}\\\\B_{21}&amp;B_{22}&amp;B_{23}\\\\B_{31}&amp;B_{32}&amp;B_{33}\\\\B_{41}&amp;B_{42}&amp;B_{43}\\end{matrix}\\right)\\] \\[\\C=\\left(\\begin{matrix} 1 &amp; 0.3 &amp; -0.9 &amp; 0.1\\\\0.3 &amp; 1 &amp; 0.8 &amp; -0.5\\\\-0.9&amp;0.8&amp;1&amp;-0.6\\\\0.1&amp;-0.5&amp;-0.6&amp;1 \\end{matrix}\\right)\\] \\[\\A = [A_{ij}] \\quad \\mbox{where } i=1,2,3 \\quad \\mbox{and } j=1,2\\] For the following quantities, use what you know about notation to tell if they are matrices, vectors, or scalars: \\(\\mathbf{H}\\) \\(\\mathbf{W}\\) \\(n\\) \\(\\v_2\\) \\(v_2\\) \\(\\mathbf{M}_{\\star 2}\\) \\(\\lambda\\) \\(A_{ij}\\) \\(\\mathbf{r}\\) The matrix \\(\\C\\) from exercise 2d is the correlation matrix discussed earlier in this chapter. What is the trace of \\(\\C\\)? For any correlation matrix computed using \\(p\\) variables, what should we expect the trace to be? If \\[\\v^T = \\left(\\begin{matrix} 1&amp;6&amp;-1&amp;\\sqrt{2} \\end{matrix}\\right),\\] then what is \\(\\v\\)? For each of the following, write the vector or matrix that is specified: \\(\\e_3 \\in \\Re^4\\) \\(\\D=diag\\{2, \\sqrt{3}, -1\\}\\) \\(\\e \\in \\Re^3\\) \\(\\I_2\\) How do you know if a matrix is symmetric? Give an example of a symmetric matrix. Give an example of a \\(4\\times 4\\) upper triangular matrix and a \\(3\\times 3\\) lower triangular matrix. Suppose we measure the heights of 10 people, \\(person_1, person_2, \\dots, person_{10}\\). If we define a matrix \\(\\S\\) as \\[S_{ij} = height(person_i) - height(person_j)\\] is the matrix \\(\\S\\) symmetric? What is the trace(\\(\\S\\))? If instead we create a matrix \\(\\mathbf{G}\\) where \\[G_{ij} = [height(person_i) - height(person_j)]^2\\] is the matrix \\(\\mathbf{G}\\) symmetric? What is the trace(\\(\\mathbf{G}\\))? Refer to the network/graph shown below. This particular network has 6 numbered vertices (the circles) and edges which connect the vertices. Each edge has a certain {weight} (perhaps reflecting some level of association between the vertices) which is given as a number. Use the network shown below to answer the following questions. Figure 1.3: Graph (Network) for exercise 11 Write down the adjacency matrix, \\(\\A\\), for this graph where \\(\\A_{ij}\\) reflects the weight of the edge connecting vertex \\(i\\) and vertex \\(j\\). The degree of a vertex is defined as the sum of the weights of the edges connected to that vertex. Create a vector \\(\\mathbf{d}\\) such that \\(d_i\\) is the degree of node \\(i\\). "],["mult.html", "Chapter 2 Matrix Arithmetic 2.1 Matrix Addition, Subtraction, and Scalar Multiplication 2.2 Geometry of Vector Addition and Scalar Multiplication 2.3 Linear Combinations 2.4 Matrix Multiplication 2.5 Vector Outer Products 2.6 The Identity and the Matrix Inverse 2.7 Exercises List of Key Terms", " Chapter 2 Matrix Arithmetic 2.1 Matrix Addition, Subtraction, and Scalar Multiplication Addition, subtraction, and scalar multiplication are the only operations which act element-wise on matrices - they are performed in a way you might expect given your previous studies. Definition 2.1 (Addition, Subtraction, and Scalar Multiplication) Two matrices can be added or subtracted only when they have the same dimensions. If \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are both \\(m\\times n\\) matrices then the (i,j) element of the sum (or difference), written \\((\\mathbf{A}_-^+ \\mathbf{B})_{ij}\\) is: Two matrices can be added or subtracted only when they have the same dimensions. If \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are both \\(m\\times n\\) matrices then the (i,j) element of the sum (or difference), written \\((\\mathbf{A}_-^+ \\mathbf{B})_{ij}\\) is: Example 2.1 (Addition, Subtraction, and Scalar Multiplication) a. Compute \\(\\mathbf{A}+\\mathbf{B}\\), if possible: \\[\\mathbf{A}=\\left(\\begin{matrix} 2 &amp; 3 &amp; -1\\\\1&amp;-1&amp;1\\\\2&amp;2&amp;1 \\end{matrix}\\right) \\quad \\mathbf{B}=\\left(\\begin{matrix} 4 &amp; 5 &amp; 6\\\\-1&amp;0&amp;4\\\\3&amp;4&amp;3 \\end{matrix}\\right)\\] We can add the matrices because they have the same size. \\[\\mathbf{A}+\\mathbf{B} = \\left(\\begin{matrix} 6 &amp; 8 &amp; 5\\\\0&amp;-1&amp;5\\\\5&amp;6&amp;4\\end{matrix}\\right)\\] b. Compute \\(\\mathbf{A}-\\bo{H}\\), if possible: \\[\\mathbf{A}=\\left(\\begin{matrix} 1 &amp; 2\\\\3&amp;5 \\end{matrix}\\right) \\qquad \\bo{H}= \\left(\\begin{matrix} 6 &amp; 5&amp; 10\\\\0.1 &amp; 0.5 &amp; 0.9 \\end{matrix}\\right)\\] We cannot subtract these matrices because they don’t have the same size. c. Compute \\(2\\mathbf{A}\\): \\[\\mathbf{A}=\\left(\\begin{matrix} 2 &amp; 3 &amp; -1\\\\1&amp;-1&amp;1\\\\2&amp;2&amp;1 \\end{matrix}\\right)\\] We simply multiply every element in \\(\\mathbf{A}\\) by 2, \\[2\\mathbf{A}=\\left(\\begin{matrix} 4 &amp; 6 &amp; -2\\\\2&amp;-2&amp;2\\\\4&amp;4&amp;2 \\end{matrix}\\right)\\] Exercise 2.1 (Addition, Subtraction, and Scalar Multiplication) a. Compute \\(\\v-\\y\\), if possible: \\[\\v=\\left(\\begin{matrix} 2\\\\-3\\\\4 \\end{matrix}\\right) \\quad \\y=\\left(\\begin{matrix} 1\\\\4\\\\1 \\end{matrix}\\right)\\] b. Compute \\(\\v+\\bo{h}\\), if possible: \\[\\v=\\left(\\begin{matrix} 4\\\\-5\\\\3 \\end{matrix}\\right) \\quad \\bo{h}=\\left(\\begin{matrix} -1\\\\-4\\\\1\\\\2 \\end{matrix}\\right)\\] c. Compute \\(\\frac{1}{\\sqrt{2}}\\v\\): \\[\\v=\\left(\\begin{matrix} 4\\\\-5\\\\3 \\end{matrix}\\right)\\] 2.2 Geometry of Vector Addition and Scalar Multiplication You’ve already learned how vector addition works algebraically: it occurs element-wise between two vectors of the same length: \\[ \\a+\\b =\\left(\\begin{matrix} a_1\\\\ a_2\\\\ a_3\\\\ \\vdots \\\\ a_n \\end{matrix}\\right) +\\left(\\begin{matrix} b_1\\\\ b_2\\\\ b_3\\\\ \\vdots \\\\ b_n \\end{matrix}\\right) = \\left(\\begin{matrix} a_1+b_1\\\\a_2+b_2\\\\a_3+b_3\\\\ \\vdots \\\\a_n+b_n \\end{matrix}\\right) \\] Geometrically, vector addition is witnessed by placing the two vectors, \\(\\a\\) and \\(\\b\\), tail-to-head. The result, \\(\\a+\\b\\), is the vector from the open tail to the open head. This is called the parallelogram law and is demonstrated in Figure 2.1. Figure 2.1: Geometry of Vector Addition When subtracting vectors as \\(\\a-\\b\\) we simply add \\(-\\b\\) to \\(\\a\\). The vector \\(-\\b\\) has the same length as \\(\\b\\) but points in the opposite direction. This vector has the same length as the one which connects the two heads of \\(\\a\\) and \\(\\b\\) as shown in Figure 2.2. Figure 2.2: Geometry of Vector Subtraction Example 2.2 (Centering Data) One thing we will do frequently in this course is deal with centered and/or standardized data. To center a group of data points, we merely subtract the mean of each variable from each measurement on that variable. Geometrically, this amounts to a translation (shift) of the data so that its center (or mean) is at the origin. Figure 2.3 illustrates this process using 4 data points. Figure 2.3: Centering a Data Cloud as a Geometric Translation Scalar multiplication is another operation which acts element-wise: \\[\\alpha \\a = \\alpha \\left(\\begin{matrix} a_1\\\\a_2\\\\a_3\\\\ \\vdots \\\\a_n \\end{matrix}\\right) = \\left(\\begin{matrix} \\alpha a_1 \\\\ \\alpha a_2\\\\ \\alpha a_3 \\\\ \\vdots \\\\ \\alpha a_n\\end{matrix}\\right) \\] Scalar multiplication changes the length of a vector but not the overall direction (although a negative scalar will scale the vector in the opposite direction through the origin). We can see this geometric interpretation of scalar multiplication in Figure 2.4. Figure 2.4: Geometric Effect of Scalar Multiplication 2.3 Linear Combinations Definition 2.2 (Linear Combination) A linear combination is constructed from a set of terms \\(\\v_1, \\v_2, \\dots, \\v_n\\) by multiplying each term by a scalar constant and adding the result: \\[\\bo{c}=\\alpha_1\\v_1+\\alpha_2 \\v_2+ \\dots+ \\alpha_n\\v_n = \\sum_{i=1}^n \\alpha_i \\v_n\\] The coefficients \\(\\alpha_i\\) are scalar constants and the terms, \\(\\{\\v_i\\}\\) can be scalars, vectors, or matrices. Most often, we will consider linear combinations where the terms \\(\\{\\v_i\\}\\) are vectors. Linear combinations are quite simple to understand. Once the equation is written, we can consider the expression as a breakdown into parts. Example 2.3 (Linear Combination) The simplest linear combination might involve columns of the identity matrix: \\[\\left(\\begin{matrix} 3 \\\\ -2\\\\4 \\end{matrix}\\right) = 3\\left(\\begin{matrix} 1\\\\0\\\\0 \\end{matrix}\\right) -2 \\left(\\begin{matrix} 0\\\\1\\\\0 \\end{matrix}\\right) +4 \\left(\\begin{matrix} 0\\\\0\\\\1 \\end{matrix}\\right)\\] We can easily picture this linear combination as a \"breakdown into parts where the parts give directions along the 3 coordinate axis with which we are all familiar. We don’t necessarily have to use vectors as the terms for a linear combination. Example 2.4 shows how we can write any \\(m\\times n\\) matrix as a linear combination of \\(nm\\) elementary matrices. Example 2.4 (Linear Combination of Matrices) Write the matrix \\(\\mathbf{A}=\\left(\\begin{matrix} 1 &amp; 3\\\\4&amp;2 \\end{matrix}\\right)\\) as a linear combination of the following matrices: \\[\\left\\lbrace \\left(\\begin{matrix} 1 &amp; 0\\\\0&amp;0 \\end{matrix}\\right),\\left(\\begin{matrix} 0 &amp; 1\\\\0&amp;0 \\end{matrix}\\right),\\left(\\begin{matrix} 0 &amp; 0\\\\1&amp;0 \\end{matrix}\\right),\\left(\\begin{matrix} 0 &amp; 0\\\\0&amp;1 \\end{matrix}\\right) \\right\\rbrace\\] Solution: \\[\\mathbf{A}=\\left(\\begin{matrix} 1 &amp; 3\\\\4&amp;2 \\end{matrix}\\right) = 1\\left(\\begin{matrix} 1 &amp; 0\\\\0&amp;0 \\end{matrix}\\right)+3\\left(\\begin{matrix} 0 &amp; 1\\\\0&amp;0 \\end{matrix}\\right)+4\\left(\\begin{matrix} 0 &amp; 0\\\\1&amp;0 \\end{matrix}\\right)+2\\left(\\begin{matrix} 0 &amp; 0\\\\0&amp;1 \\end{matrix}\\right)\\] 2.4 Matrix Multiplication When we multiply matrices, we do not perform the operation element-wise as we did with addition and scalar multiplication. Matrix multiplication is, in itself, a very powerful tool for summarizing information. In fact, many of the analytical tools we will focus on in this course, like Markov Chains, Principal Components Analysis, Factor Analysis, and the Singular Value Decomposition, can all be understood more clearly with a firm grasp on matrix multiplication. Because this operation is so important, we will spend a considerable amount of energy breaking it down in many ways. 2.4.1 The Inner Product We’ll begin by defining the multiplication of a row vector times a column vector, known as an inner product (sometimes called the dot product in applied sciences). For the remainder of this course, unless otherwise specified, we will consider vectors to be columns rather than rows. This makes the notation more simple because if \\(\\x\\) is a column vector, \\[\\x=\\left(\\begin{matrix} x_1\\\\x_2\\\\\\vdots\\\\ x_n\\end{matrix}\\right)\\] then we can automatically assume that \\(\\x^T\\) is a row vector: \\[\\x^T = \\left(\\begin{matrix} x_1&amp;x_2&amp;\\dots&amp;x_n\\end{matrix}\\right).\\] Definition 2.3 (Inner Product) The inner product of two vectors, \\(\\x\\) and \\(\\y\\), written \\(\\x^T\\y\\), is defined as the sum of the product of corresponding elements in \\(\\x\\) and \\(\\y\\): \\[\\x^T\\y = \\sum_{i=1}^n x_i y_i.\\] If we write this out for two vectors with 4 elements each, we’d have: \\[\\x^T\\y=\\left(\\begin{matrix} x_1 &amp; x_2 &amp; x_3 &amp; x_4 \\end{matrix}\\right) \\left(\\begin{matrix} y_1\\\\y_2\\\\y_3\\\\y_4 \\end{matrix}\\right) = x_1y_1+x_2y_2+x_3y_3+x_4y_4\\] Note: The inner product between vectors is only possible when the two vectors have the same number of elements! Example 2.5 (Vector Inner Product) Let \\[\\x=\\left(\\begin{matrix} -1 \\\\2\\\\4\\\\0 \\end{matrix}\\right) \\quad \\y=\\left(\\begin{matrix} 3 \\\\5\\\\1\\\\7 \\end{matrix}\\right) \\quad \\v=\\left(\\begin{matrix} -3 \\\\-2\\\\5\\\\3\\\\-2 \\end{matrix}\\right) \\quad \\u= \\left(\\begin{matrix} 2\\\\-1\\\\3\\\\-3\\\\-2 \\end{matrix}\\right)\\] If possible, compute the following inner products: \\(\\x^T\\y\\) \\[\\begin{eqnarray} \\x^T\\y &amp;=&amp;\\left(\\begin{matrix} -1 &amp;2&amp;4&amp;0 \\end{matrix}\\right) \\left(\\begin{matrix} 3 \\\\5\\\\1\\\\7 \\end{matrix}\\right) \\cr &amp;=&amp; (-1)(3)+(2)(5)+(4)(1)+(0)(7) \\cr &amp;=&amp; -3+10+4=\\framebox{11} \\end{eqnarray}\\] \\(\\x^T\\v\\) This is not possible because \\(\\x\\) and \\(\\v\\) do not have the same number of elements \\(\\v^T\\u\\) \\[\\begin{eqnarray} \\v^T\\u &amp;=&amp; \\left(\\begin{matrix} -3 &amp;-2&amp;5&amp;3&amp;-2 \\end{matrix}\\right) \\left(\\begin{matrix} 2\\\\-1\\\\3\\\\-3\\\\-2 \\end{matrix}\\right) \\cr &amp;=&amp; (-3)(2)+(-2)(-1)+(5)(3)+(3)(-3)+(-2)(-2) \\cr &amp;=&amp; -6+2+15-9+4 = \\framebox{6} \\end{eqnarray}\\] Exercise 2.2 (Vector Inner Product) Let \\[\\bo{v}=\\left(\\begin{matrix} 1 \\\\2\\\\3\\\\4\\\\5 \\end{matrix}\\right) \\quad \\e=\\left(\\begin{matrix} 1 \\\\1\\\\1\\\\1\\\\1 \\end{matrix}\\right) \\quad \\bo{p}=\\left(\\begin{matrix} 0.5 \\\\0.1\\\\0.2\\\\0\\\\0.2 \\end{matrix}\\right) \\quad \\u= \\left(\\begin{matrix} 10\\\\4\\\\3\\\\2\\\\1 \\end{matrix}\\right) \\quad \\bo{s} = \\left(\\begin{matrix} 2\\\\2\\\\-3 \\end{matrix}\\right)\\] If possible, compute the following inner products: \\(\\bo{v}^T\\e\\) \\(\\bo{e}^T\\bo{v}\\) \\(\\bo{v}^T\\bo{s}\\) \\(\\bo{p}^T\\u\\) \\(\\bo{v}^T\\bo{v}\\) It should be clear from the definition and from the previous exercise, that for all vectors \\(\\x\\) and \\(\\y\\), \\[\\x^T\\y = \\y^T\\x.\\] Also, if we take the inner product of a vector with itself, the result is the sum of squared elements in that vector: \\[\\x^T\\x = \\sum_{i=1}^n x_i^2 = x_1^2 + x_2^2+ \\dots + x_n^2.\\] Now that we are comfortable multiplying a row vector (\\(\\x^T\\) in the definition) and a column vector (\\(\\y\\) in the definition), we can define multiplication for matrices in general. 2.4.2 Matrix Product Matrix multiplication is nothing more than a collection of inner products done simultaneously in one operation. We must be careful when multiplying matrices because, as with vectors, the operation is not always possible. Unlike the vector inner product, the order in which you multiply matrices makes a big difference! Definition 2.4 (Matrix Multiplication) Let \\(\\mathbf{A}_{m\\times n}\\) and \\(\\mathbf{B}_{k\\times p}\\) be matrices. The matrix product \\(\\mathbf{A}\\mathbf{B}\\) is possible if and only if \\(n=k\\); that is, when the number of columns in \\(\\mathbf{A}\\) is the same as the number of rows in \\(\\mathbf{B}\\). If this condition holds, then the dimension of the product, \\(\\mathbf{A}\\mathbf{B}\\) is \\(m\\times p\\) and the (ij)-entry of the product \\(\\mathbf{A}\\mathbf{B}\\) is the inner product of the \\(i^{th}\\) row of \\(\\mathbf{A}\\) and the \\(j^{th}\\) column of \\(\\mathbf{B}\\): \\[(\\mathbf{A}\\mathbf{B})_{ij} = \\mathbf{A}_{i\\star}\\mathbf{B}_{\\star j}\\] This definition may be easier to dissect using an example: Example 2.6 (Steps to Compute a Matrix Product) Let \\[\\mathbf{A}=\\left(\\begin{matrix} 2 &amp; 3 \\\\ -1 &amp; 4 \\\\ 5 &amp; 1 \\end{matrix}\\right) \\quad \\mbox{and} \\quad \\mathbf{B}=\\left(\\begin{matrix} 0 &amp; -2 \\\\ 2 &amp; -3 \\end{matrix}\\right)\\] When we first get started with matrix multiplication, we often follow a few simple steps: Write down the matrices and their dimensions. Make sure the “inside” dimensions match - those corresponding to the columns of the first matrix and the rows of the second matrix: \\[\\underset{(3\\times \\red{2})}{\\mathbf{A}} \\underset{(\\red{2} \\times 2)}{\\mathbf{B}}\\] If these dimensions match, then we can multiply the matrices. If they don’t, we stop right there - multiplication is not possible. Now, look at the “outer” dimensions - this will tell you the size of the resulting matrix. \\[\\underset{(\\blue{3}\\times 2)}{\\mathbf{A}} \\underset{(2\\times \\blue{2})}{\\mathbf{B}}\\] So the product \\(\\mathbf{A}\\mathbf{B}\\) is a \\(3\\times 2\\) matrix. Finally, we compute the product of the matrices by multiplying each row of \\(\\mathbf{A}\\) by each column of \\(\\mathbf{B}\\) using inner products. The element in the first row and first column of the product (written \\((\\mathbf{A}\\mathbf{B})_{11}\\)) will be the inner product of the first row of \\(\\mathbf{A}\\) and the first column of \\(\\mathbf{B}\\). Then, \\((\\mathbf{A}\\mathbf{B})_{12}\\) will be the inner product of the first row of \\(\\mathbf{A}\\) and the second column of \\(\\mathbf{B}\\), etc. \\[\\begin{eqnarray} \\mathbf{A}\\mathbf{B} &amp;=&amp;\\left(\\begin{matrix} (2)(0)+(3)(2) &amp; (2)(-2)+(3)(-3)\\\\ (-1)(0)+(4)(2) &amp; (-1)(-2)+(4)(-3)\\\\ (5)(0)+(1)(2) &amp; (5)(-2)+(1)(-3) \\end{matrix}\\right) \\cr &amp;=&amp; \\left(\\begin{matrix} 6&amp;-13\\\\8 &amp; -10\\\\2&amp;-13\\end{matrix}\\right) \\end{eqnarray}\\] Matrix multiplication is incredibly important for data analysis. You may not see why all these multiplications and additions are so useful at this point, but we will visit some basic applications shortly. For now, let’s practice so that we are prepared for the applications! Exercise 2.3 (Matrix Multiplication) Suppose we have \\[\\mathbf{A}_{4\\times 6} \\quad \\mathbf{B}_{5\\times 5} \\quad \\M_{5\\times 4} \\quad \\bP_{6\\times 5}\\] Circle the matrix products that are possible to compute and write the dimension of the result. \\[\\mathbf{A}\\M \\qquad \\M\\mathbf{A} \\qquad \\mathbf{B}\\M \\qquad \\M\\mathbf{B} \\qquad \\bP\\mathbf{A} \\qquad \\bP\\M \\qquad \\mathbf{A}\\bP \\qquad \\mathbf{A}^T\\bP \\qquad \\M^T\\mathbf{B}\\] Let \\[\\begin{equation} \\mathbf{A}=\\left(\\begin{matrix} 1&amp;1&amp;0&amp;1\\\\0&amp;1&amp;1&amp;1\\\\1&amp;0&amp;1&amp;0\\end{matrix}\\right) \\quad \\M = \\left(\\begin{matrix} -2&amp;1&amp;-1&amp;2&amp;-2\\\\1&amp;-2&amp;0&amp;-1&amp;2\\\\2&amp;1&amp;-3&amp;-2&amp;3 \\\\ 1&amp;3&amp;2&amp;-1&amp;2\\end{matrix}\\right) \\end{equation}\\] \\[\\begin{equation} \\C=\\left(\\begin{matrix} -1&amp;0&amp;1&amp;0\\\\1&amp;-1&amp;0&amp;0\\\\0&amp;0&amp;1&amp;-1 \\end{matrix}\\right) \\end{equation}\\] Determine the following matrix products, if possible: a \\(\\mathbf{A}\\C\\) b \\(\\mathbf{A}\\M\\) \\(\\mathbf{A}^T\\C\\) One very important thing to keep in mind is this: matrix multiplication is NOT commutative! As we see from the previous exercises, it’s quite common to be able to compute a product \\(\\mathbf{A}\\mathbf{B}\\) where the reverse product, \\(\\mathbf{B}\\mathbf{A}\\) is not even possible to compute. Even if both products are possible it is almost never the case that \\(\\mathbf{A}\\mathbf{B}\\) equals \\(\\mathbf{B}\\mathbf{A}\\). 2.4.2.1 Multiplication by a Diagonal Matrix As we will see in the next example, multiplication by a diagonal matrix causes a very specific effect on a matrix. Example 2.7 (Multiplication by a Diagonal Matrix) Compute the following matrix product and comment on what you find in the results: \\[\\D=\\left(\\begin{matrix} 2&amp;0&amp;0\\\\0&amp;3&amp;0\\\\0&amp;0&amp;-2 \\end{matrix}\\right) \\mathbf{A}= \\left(\\begin{matrix} 1&amp;2&amp;3\\\\1&amp;1&amp;2\\\\2&amp;1&amp;3 \\end{matrix}\\right)\\] \\[\\D\\mathbf{A}=\\left(\\begin{matrix} 2&amp;4&amp;6\\\\3&amp;3&amp;6\\\\-4&amp;-2&amp;-6 \\end{matrix}\\right)\\] In doing this multiplication, we see that the effect of multiplying the matrix \\(\\mathbf{A}\\) by a diagonal matrix on the left is that the rows of the matrix \\(\\mathbf{A}\\) are simply scaled by the entries in the diagonal matrix. You should work this computation out by hand to convince yourself that this effect will happen every time. Diagonal scaling can be important, and from now on when you see a matrix product like \\(\\D\\mathbf{A}\\) where \\(\\D\\) is diagonal, you should automatically put together that the result is just a row-scaled version of \\(\\mathbf{A}\\). Exercise 2.4 (Multiplication by a Diagonal Matrix) What happens if we were to compute the product from Example 2.7 in the reversed order, with the diagonal matrix on the right: \\[\\mathbf{A}\\D?\\] Would we expect the same result? Is multiplication by a diagonal matrix commutative? Work out the calculation and comment on what you’ve found. 2.4.3 Matrix-Vector Product A matrix-vector product works exactly the same way as matrix multiplication; after all, a vector \\(\\x\\) is nothing but an \\(n\\times 1\\) matrix. In order to multiply a matrix by a vector, again we must match the dimensions to make sure they line up correctly. For example, if we have an \\(m\\times n\\) matrix \\(\\mathbf{A}\\), we can multiply by a \\(1\\times m\\) row vector \\(\\v^T\\) on the left: \\[\\v^T\\mathbf{A} \\quad \\mbox{works because } \\underset{ (1\\times \\red{m})}{\\v^T} \\underset{(\\red{m}\\times n)}{\\mathbf{A}}\\] \\[\\Longrightarrow \\mbox{The result will be a } 1 \\times n \\mbox{ row vector.}\\] or we can multiply by an \\(n\\times 1\\) column vector \\(\\x\\) on the right: \\[\\mathbf{A}\\x \\quad \\mbox{works because } \\underset{(m\\times \\red{n})}{\\mathbf{A}}\\underset{(\\red{n}\\times 1)}{\\x} \\] \\[\\Longrightarrow \\mbox{The result will be a } m\\times 1 \\mbox{ column vector.}\\] Matrix-vector multiplication works the same way as matrix multiplication: we simply multiply rows by columns until we’ve completed the answer. In the case of \\(\\v^T\\mathbf{A}\\), we’d multiply the row \\(\\v\\) by each of the \\(n\\) columns of \\(\\mathbf{A}\\), carving out our solution, one entry at a time : \\[\\v^T\\mathbf{A} = \\left(\\begin{matrix} \\v^T\\mathbf{A}_{*1} &amp; \\v^T\\acol{2} &amp; \\dots &amp; \\v^T\\acol{n} \\end{matrix}\\right).\\] In the case of \\(\\mathbf{A}\\x\\), we’d multiply each of the \\(m\\) rows of \\(\\mathbf{A}\\) by the column \\(\\x\\): \\[\\mathbf{A}\\x = \\left(\\begin{matrix} \\arow{1}\\x \\\\ \\arow2{x} \\\\ \\vdots \\\\ \\arow{m}\\x \\end{matrix}\\right).\\] Let’s see an example of this: Example 2.8 (Matrix-Vector Products) Let \\[\\mathbf{A}=\\left(\\begin{matrix} 2 &amp; 3 \\\\ -1 &amp; 4 \\\\ 5 &amp; 1 \\end{matrix}\\right) \\quad \\v=\\left(\\begin{matrix} 3\\\\2 \\end{matrix}\\right) \\quad \\bo{q}=\\left(\\begin{matrix} 2\\\\-1\\\\3\\end{matrix}\\right)\\] Determine whether the following matrix-vector products are possible. When possible, compute the product. \\(\\mathbf{A}\\bo{q}\\) \\[\\mbox{Not Possible: Inner dimensions do not match} \\quad \\underset{(3\\times \\red{2})}{\\mathbf{A}}\\underset{(\\red{3}\\times 1)}{\\bo{q}}\\] \\(\\mathbf{A}\\v\\) \\[ \\left(\\begin{matrix} 2 &amp; 3 \\\\ -1 &amp; 4 \\\\ 5 &amp; 1 \\end{matrix}\\right) \\left(\\begin{matrix} 3\\\\2 \\end{matrix}\\right) = \\left(\\begin{matrix} 2(3)+3(2) \\\\ -1(3)+4(2)\\\\5(3)+1(2) \\end{matrix}\\right) = \\left(\\begin{matrix} 12\\\\5\\\\17\\end{matrix}\\right) \\] \\(\\bo{q}^T\\mathbf{A}\\) Rather than write out the entire calculation, the blue text highlights one of the two inner products required: \\[ \\left(\\begin{matrix} \\blue{2} &amp; \\blue{-1} &amp; \\blue{3}\\end{matrix}\\right) \\left(\\begin{matrix} \\blue{2} &amp; 3 \\\\ \\blue{-1} &amp; 4 \\\\ \\blue{5} &amp; 1 \\end{matrix}\\right) = \\left(\\begin{matrix} \\blue{20} &amp; 5 \\end{matrix}\\right) \\] \\(\\v^T\\mathbf{A}\\) \\[\\mbox{Not Possible: Inner dimensions do not match} \\quad \\underset{(1\\times \\red{2})}{\\v^T}\\underset{(\\red{3}\\times 2)}{\\mathbf{A}}\\] Exercise 2.5 (Matrix-Vector Products) Let \\[ \\mathbf{A}=\\left(\\begin{matrix} 1&amp;1&amp;0&amp;1\\\\0&amp;1&amp;1&amp;1\\\\1&amp;0&amp;1&amp;0\\end{matrix}\\right) \\quad \\mathbf{B}=\\left(\\begin{matrix} 0 &amp; -2 \\\\ 1 &amp; -3 \\end{matrix}\\right) \\] \\[ \\x=\\left(\\begin{matrix} 2\\\\1\\\\3 \\end{matrix}\\right) \\quad \\y = \\left(\\begin{matrix} 1\\\\1 \\end{matrix}\\right) \\quad \\z = \\left(\\begin{matrix} 3\\\\1\\\\2\\\\3 \\end{matrix}\\right)\\] Determine whether the following matrix-vector products are possible. When possible, compute the product. \\(\\mathbf{A}\\z\\) \\(\\z^T\\mathbf{A}\\) \\(\\y^T\\mathbf{B}\\) \\(\\mathbf{B}\\y\\) \\(\\x^T\\mathbf{A}\\) Linear Combination view of Matrix Products All matrix products can be viewed as linear combinations or a collection of linear combinations. This vantage point is extremely crucial to our understanding of data science techniques that are based on matrix-factorization. Let’s start with matrix-vector product and see how we can depict it as a linear combination of the columns of the matrix. Definition 2.5 (Matrix-Vector Product as a Linear Combination) Let \\(\\mathbf{A}\\) be an \\(m\\times n\\) matrix partitioned into columns, \\[\\mathbf{A} = [\\mathbf{A}_1 | \\mathbf{A}_2 | \\dots | \\mathbf{A}_n]\\] and let \\(\\x\\) be a vector in \\(\\Re^n\\). Then, \\[\\mathbf{A}\\x = x_1\\mathbf{A}_1 + x_2\\mathbf{A}_2 + \\dots + x_n\\mathbf{A}_n\\] We use the animation in Figure 2.5 to illustrate Definition 2.5. Figure 2.5: Illustration of Definition 2.5 Definition 2.5 extends to any matrix product. If \\(\\mathbf{A}\\mathbf{B}=\\mathbf{C}\\) then the columns of \\(\\mathbf{C}\\) can be viewed as linear combinations of the columns of \\(\\mathbf{A}\\) and, likewise, the rows of \\(\\C\\) can be viewed as linear combinations of the rows of \\(\\mathbf{B}\\). We leave the latter fact for the reader to explore independently (see end-of-chapter exercise 5), and animate the former in Figure 2.6. Figure 2.6: Illustration of Definition 2.5 2.5 Vector Outer Products Whereas inner products were the product of a row vector with a column vector (think \\(\\x^T\\y\\)), outer products are the product of a column vector with a row vector (think \\(\\x\\y^T\\)). Let’s first consider the dimensions of the outcome: \\[\\underset{(m\\times \\red{1})}{\\x} \\underset{(\\red{1} \\times n)}{\\y^T} = \\bo{M}_{m\\times n}\\] So the result is a matrix! We’ll want to treat this product in the same way we treat any matrix product, by multiplying row \\(\\times\\) column until we’ve run out of rows and columns. Let’s take a look at an example: Example 2.9 (Vector Outer Product) Let \\(\\x = \\left(\\begin{matrix} 3\\\\4\\\\-2 \\end{matrix}\\right)\\) and \\(\\y=\\left(\\begin{matrix} 1\\\\5\\\\3 \\end{matrix}\\right)\\). Then, \\[\\x\\y^T = \\left(\\begin{matrix} \\red{3}\\\\4\\\\-2 \\end{matrix}\\right) \\left(\\begin{matrix} \\red{1}&amp;5&amp;3 \\end{matrix}\\right) = \\left(\\begin{matrix} \\red{3}&amp;15&amp;9\\\\4&amp;20&amp;12\\\\-2&amp;-10&amp;-3\\end{matrix}\\right)\\] As you can see by performing this calculation, a vector outer product will always produce a matrix whose rows are multiples of each other! 2.6 The Identity and the Matrix Inverse The identity matrix, introduced in Section 1.6, is to matrices as the number `1’ is to scalars. It is the multiplicative identity. For any matrix (or vector) \\(\\mathbf{A}\\), multiplying \\(\\mathbf{A}\\) by the identity matrix on either side does not change \\(\\mathbf{A}\\): \\[\\begin{align*} \\mathbf{A}\\I&amp;=\\mathbf{A} \\\\ \\I\\mathbf{A} &amp;= \\mathbf{A} \\end{align*}\\] This fact is easy to verify in light of Example 2.7. Since the identity is simply a diagonal matrix with ones on the diagonal, when we multiply it by any matrix it merely scales each row or column of that matrix by 1. The size of the identity matrix is generally implied in context. If \\(\\mathbf{A}\\) is \\(m\\times n\\) then writing \\(\\mathbf{A}\\I\\) implies that \\(\\I\\) is \\(n \\times n\\), where as writing \\(\\I\\mathbf{A}\\) implies \\(\\I\\) is \\(m\\times m\\). For certain square matrices \\(\\mathbf{A}\\), an inverse matrix, written \\(\\mathbf{A}^{-1}\\), exists such that \\[\\mathbf{A}\\mathbf{A}^{-1} = \\I\\] \\[\\mathbf{A}^{-1}\\mathbf{A} = \\I\\] It is very important to understand that not all matrices have inverses. There are 2 very important conditions that must be satisfied: We have not yet discussed the notion of matrix rank, so the present discussion is aimed only at defining the concept of a matrix inverse rather than defining when it exists or how it is determined. For now, we want to see the analogy of the matrix inverse to our previous understanding of scalar algebra. Recall that the inverse of a non-zero scalar number is its reciprocal: \\[a^{-1} = \\frac{1}{a}\\] Multiplying a scalar by its inverse yields the multiplicative identity, 1: \\[(a)(a^{-1}) = (a)(\\frac{1}{a}) = 1\\] All scalars have an inverse with the exception of 0. For matrices, the idea of an inverse is quite the same - multiply a matrix on the left or right by its inverse to get the multiplicative identity, \\(\\I\\). However, as previously stated, the matrix inverse only exists for a small subset of matrices, those that are square and full rank. Such matrices are equivalently called invertible or non-singular. Example 2.10 (Don’t Cancel That!!) We must be careful in linear algebra to remember the basics and not confuse our equations with scalar equations. When we see an equation like \\[\\mathbf{A}\\x=\\lambda\\x\\] We CANNOT cancel terms from both sides. Mathematically, this operation amounts to multiplying both sides by an inverse. When the term we are canceling is a non-zero scalar, then we can proceed as usual. However, we must be careful not to assume that a matrix/vector quantity has an inverse. For example, the following operation is nonsense: \\[\\require{cancel} \\mathbf{A}\\cancel{\\x}=\\lambda\\cancel{\\x}\\] Note that, while this equation made sense to begin with, after erroneously canceling terms, it no longer makes sense as it equates a matrix, \\(\\mathbf{A}\\), with a scalar, \\(\\lambda\\). 2.7 Exercises On a coordinate plane, draw the vectors \\(\\a = \\left(\\begin{matrix} 1\\\\2\\end{matrix}\\right)\\) and \\(\\b=\\left(\\begin{matrix} 0\\\\1\\end{matrix}\\right)\\) and then draw \\(\\bo{c}=\\a+\\b\\). Make dotted lines which illustrate how the point/vector \\(\\bo{c}\\) can be reached by connecting the vectors a and b “tail-to-head.” Use the following vectors to answer the questions: \\[ \\v=\\left(\\begin{matrix} 6\\\\-1\\end{matrix}\\right) \\quad \\bo{u}=\\left(\\begin{matrix} -2\\\\1\\end{matrix}\\right) \\quad \\x=\\left(\\begin{matrix} 4\\\\2\\\\1\\end{matrix}\\right) \\quad \\y=\\left(\\begin{matrix}-1\\\\-2\\\\-3\\end{matrix}\\right) \\quad \\e=\\left(\\begin{matrix} 1\\\\1\\\\1\\end{matrix}\\right) \\] Compute the following linear combinations, if possible: \\(2\\u+3\\v=\\) \\(\\x-2\\y+\\e=\\) \\(-2\\u-\\v+\\e=\\) \\(\\u+\\e=\\) Compute the following inner products, if possible: \\(\\u^T\\v=\\) \\(\\x^T\\x=\\) \\(\\e^T\\y=\\) \\(\\x^T\\u=\\) \\(\\x^T\\e=\\) \\(\\y^T\\e=\\) \\(\\v^T\\x=\\) \\(\\e^T\\v=\\) What happens when you take the inner product of a vector with \\(\\e\\)? What happens when you take the inner product of a vector with itself (as in \\(\\x^T\\x\\))? Use the following matrices to answer the questions: \\[\\mathbf{A}=\\left(\\begin{matrix} 1&amp;3&amp;8\\\\3&amp;0&amp;-2\\\\8&amp;-2&amp;-3 \\end{matrix}\\right) \\quad \\bo{M}=\\left(\\begin{matrix} 1&amp;8&amp;-2&amp;5\\\\2&amp;8&amp;1&amp;7 \\end{matrix}\\right) \\quad \\D = \\left(\\begin{matrix} 1&amp;0&amp;0\\\\0&amp;5&amp;0\\\\0&amp;0&amp;3\\end{matrix}\\right) \\] \\[ \\bo{H}=\\left(\\begin{matrix} 2&amp;-1\\\\1&amp;3 \\end{matrix}\\right) \\quad \\bo{W}=\\left(\\begin{matrix} 1&amp;1&amp;1&amp;1\\\\2&amp;2&amp;2&amp;2\\\\3&amp;3&amp;3&amp;3\\end{matrix}\\right) \\] Circle the matrix products that are possible and specify their resulting dimensions: \\(\\mathbf{A}\\M\\) \\(\\mathbf{A}\\bo{W}\\) \\(\\bo{W}\\D\\) \\(\\bo{W}^T\\D\\) \\(\\bo{H}\\M\\) \\(\\bo{M}\\bo{H}\\) \\(\\bo{M}^T\\bo{H}^T\\) \\(\\D\\bo{W}\\) Compute the following matrix products: \\[\\bo{H}\\M\\quad \\mbox{and} \\quad \\mathbf{A}\\D\\] From the previous computation, \\(\\mathbf{A}\\D\\), do you notice anything interesting about multiplying a matrix by a diagonal matrix on the right? Can you generalize what happens in words? (Hint: see Example 2.7 and Exercise 2.4. Is matrix multiplication commutative? Different Views of Matrix Multiplication: Consider the matrix product \\(\\mathbf{A}\\mathbf{B}\\) where \\[\\mathbf{A} = \\left(\\begin{matrix} 1 &amp; 2\\\\3&amp;4 \\end{matrix}\\right) \\quad \\mathbf{B} = \\left(\\begin{matrix} 2&amp;5\\\\1&amp;3\\end{matrix}\\right)\\] Let \\(\\C=\\mathbf{A}\\mathbf{B}\\). Compute the matrix product \\(\\C\\). Compute the matrix-vector product \\(\\mathbf{A}\\mathbf{B}_{\\star 1}\\) and show that this is the first column of \\(\\C\\). (Likewise, \\(\\mathbf{A}\\mathbf{B}_{\\star 2}\\) is the second column of \\(\\C\\).) (Matrix multiplication can be viewed as a collection of matrix-vector products.) Compute the two outer products using columns of \\(\\mathbf{A}\\) and rows of \\(\\mathbf{B}\\) and show that \\[\\acol{1}\\brow{1} + \\acol{2}\\brow{2} = \\C\\] (Matrix multiplication can be viewed as the sum of outer products.) Since \\(\\mathbf{A}\\mathbf{B}_{\\star 1}\\) is the first column of \\(\\C\\), show how \\(\\C_{\\star 1}\\) can be written as a linear combination of columns of \\(\\mathbf{A}\\). (Matrix multiplication can be viewed as a collection of linear combinations of columns of the first matrix.) Finally, note that \\(\\arow{1}\\mathbf{B}\\) will give the first row of \\(\\C\\). (This amounts to a linear combination of rows - can you see that?) List of Key Terms addition subtraction equal matrices scalar multiplication inner product matrix product linear combination outer product multiplicative identity matrix inverse "],["multapp.html", "Chapter 3 Applications of Matrix Multiplication 3.1 Systems of Equations 3.2 Regression Analysis 3.3 Linear Combinations 3.4 Exercises", " Chapter 3 Applications of Matrix Multiplication As we will begin to see here, matrix multiplication has a number of uses in data modeling and problem solving. It expresses a rather large number of operations in a surprisingly compact way. The more comfortable we can be with this compact notation and what it entails, the more understanding we can have with analytical tools like Principal Components Analysis, Factor Analysis, Markov Chains, and Optimization (to name a few). 3.1 Systems of Equations Matrix multiplication creates a system of equations, which is nothing more than a collection of equations which hold true simultaneously. Suppose we take the matrix-vector product: \\[\\A\\x=\\b\\] Where \\[\\A=\\pm 1&amp;2&amp;3&amp;1\\\\0&amp;3&amp;2&amp;1\\\\1&amp;1&amp;1&amp;4\\mp \\quad \\x=\\pm x_1\\\\x_2\\\\x_3\\\\x_4 \\mp \\quad \\mbox{and} \\quad \\b=\\pm 10\\\\15\\\\6\\mp\\] Let’s take a look at what happens when we write the equation \\(\\A\\x=\\b\\) the old-fashioned way, without matrices: \\[ \\pm 1&amp;2&amp;3&amp;1\\\\0&amp;3&amp;2&amp;1\\\\1&amp;1&amp;1&amp;4\\mp\\pm x_1\\\\x_2\\\\x_3\\\\x_4 \\mp = \\pm 10\\\\15\\\\6\\mp \\] \\[ \\Longrightarrow\\begin{cases}\\begin{align} x_1+2x_2+3x_3+x_4 = 10\\\\ 3x_2+2x_3+x_4 = 15\\\\ x_1+x_2+x_3+4x_4 = 6\\end{align}\\end{cases} \\] We get a system of three equations. In general, a system of equations is nothing more than a matrix equation, \\(\\A\\x=\\b\\) where the matrix \\(\\A\\) contains the coefficients on the parameters you wish you find, \\(\\x\\) is a vector containing those unknown parameters and \\(\\b\\) is a vector containing the right hand sides of the equations. These systems of equations pop-up in all types of data applications from regression analysis to optimization. Let’s consider a scenario which mimics the real-world and try to model it using a matrix-vector product. Example 3.1 (System of Equations) A large manufacturing company has recently signed a deal to manufacture trail mix for a well-known food label. This label makes 3 versions of its product - one for airlines, one for grocery stores, and one for gas stations. Each version has a different mixture of peanuts, raisins, and chocolate which serves as the base of the trail mix. The base mixtures are made in 15 kg batches and sent to a second building for packaging. The following table contains the information about the mixes, each row containing the recipe for a 15 kg batch. There is also some additional information on the costs of the ingredients, the price the manufacturer can charge for the mixtures and the amount of storage allocated for each ingredient. Raisins (kg/batch) Peanuts (kg/batch) Chocolate (kg/batch) Sale Price($/kg) Airline (a) 7 6 2 4.99 Grocery (g) 2 5 8 6.50 Gas Station (s) 6 4 5 5.50 |Storage (kg) | 380 | 500| 620| | |Cost ($/kg) | 2.55|4.65|4.80| | a. If the manufacturer wanted to use up all the ingredients in storage each day, how many batches of each mixture (airline, gas station, and grocery) should be made? We can gather from the table that 1 batch of the airline mixture contains 7 kgs of raisins. We want the total number of kgs of raisins from each of the 3 mixtures to match the storage capacity of raisins, which is 380 kg. We can set this up as a system of equations, one for each ingredient, where \\[\\begin{eqnarray} a&amp;=&amp;\\mbox{batches of airline mixture}\\\\ g&amp;=&amp;\\mbox{batches of grocery mixture}\\\\ s&amp;=&amp;\\mbox{batches of gas station mixture}\\\\ a,g,s &amp;\\geq &amp; 0 \\end{eqnarray}\\] as follows: \\[\\begin{cases}\\begin{eqnarray} 7 a+6 s+2g &amp;=&amp; 380 \\quad \\mbox{(Raisins)}\\\\ 6 a+4 s+5g &amp;=&amp; 500 \\quad \\mbox{(Peanuts)}\\\\ 2 a+5 s+8g &amp;=&amp; 620 \\quad \\mbox{(Chocolate)}\\end{eqnarray}\\end{cases}\\] We can then transform this system of equations into matrix form: \\[\\pm 7&amp;2&amp;6\\\\6&amp;5&amp;4\\\\2&amp;8&amp;5 \\mp \\pm a\\\\g\\\\s \\mp = \\pm 380\\\\500\\\\620 \\mp\\] While we haven’t yet discussed how to solve such a system of equations, you can verify that \\[a=20\\,batches \\quad g=60\\,batches \\quad s=20\\,batches\\] is indeed a solution - in fact, it is the only possible solution. Determining solutions such as this, and establishing that they are unique (i.e. that they are the only possible solution) is one of the many tools that the study of linear algebra will provide. b. Use matrix-vector multiplication to determine how much it costs the manufacturer to produce 1 batch of each mixture For 1 batch of airline mixture, the manufacturer will spend \\[7 kg\\times \\$2.55/kg = \\$17.85\\,\\,\\mbox{ on raisins.}\\] Of course, we need to add in the cost of peanuts and chocolate and then repeat this calculation for both grocery and gas station mixtures. This is conveniently done in one matrix-vector multiplication: \\[\\pm 7&amp;6&amp;2\\\\2&amp;5&amp;8 \\\\6&amp;4&amp;5 \\mp \\pm 2.55\\\\4.65\\\\4.80 \\mp = \\pm 55.35\\\\66.75\\\\57.90\\mp\\] Thus, the cost of 1 batch of each type of mixture is: Mixture Cost ($/batch) airline 55.35 grocery 66.75 gas station 57.90 3.1.1 Big Systems of Equations When we expand our minds to the possibilities associated with matrix-matrix products, the systems that we can generate get very large very quickly. For example, let’s consider a \\(2\\times 2\\) example, \\[\\A\\X=\\B\\] where \\[\\A=\\pm 2&amp;3\\\\1&amp;4 \\mp \\qquad \\X=\\pm x_{11} &amp; x_{12} \\\\x_{21} &amp; x_{22} \\mp \\qquad \\B=\\pm 7 &amp; 6 \\\\ 5&amp; 9 \\mp\\] Let’s take a look at what the simple equation \\(\\A\\X=\\B\\) is really saying in terms of all the matrix values that we have. All we have to do is write out the multiplication “the long way.” For example, to get the element in the first row and first column of \\(\\B\\) (in this case, 7) we would compute the inner product of the first row of \\(\\A\\) with the first column of \\(\\X\\): \\[\\pm 2 &amp; 3 \\mp \\pm x_{11} \\\\ x_{21} \\mp = \\red{2x_{11}+3x_{21} = 7}\\] Now the equation in red above is just one of 4. We have one equation for each element of \\(\\B\\)!. Let’s make sure we understand how to get all 4 of these equations: \\[\\begin{eqnarray} 2x_{11}+3x_{21} &amp;=&amp; 7 \\\\ 2x_{12}+3x_{22} &amp;=&amp;6\\\\ 1x_{11}+4x_{21} &amp;=&amp; 5 \\\\ 1x_{12}+4x_{22} &amp;=&amp;9\\\\ \\end{eqnarray}\\] Now, a list of 4 equations does not seem that big. But what if the dimensions of the matrix \\(\\B\\) were \\(9\\times10\\)? By now you should be able to see that we’d have a system of 90 equations! The number of equations generated will always equal the number of elements in the right hand side matrix \\(\\B\\). 3.2 Regression Analysis In statistics, the solution to these systems of equations is exactly what we are trying to find when we do regression analysis. Take, for example, a regression analysis with some dependent variable, \\(\\y\\), and two independent variables, \\(\\h,\\w\\). The preliminary goal of this analysis is to find unknown parameters \\(\\beta_0, \\beta_1, \\dots\\) such that \\[\\begin{equation} \\y= \\beta_0+\\beta_1\\h+\\beta_2\\w \\tag{3.1} \\end{equation}\\] This is the single equation we usually consider when talking about regression analysis - but what about all those data points? Suppose, for simplicity, we have only 4 observations as listed in the following table: \\(\\h\\) \\(\\w\\) \\(\\y\\) 3 3 6 2 3 6 5 6 10 6 5 9 When we write the model from Equation (3.1), what we are really saying is that the equation holds true for each of the 4 observations in our dataset. So rather than 1 single equation, what we really have here is 4 equations - 1 for each observation: \\[\\begin{eqnarray} \\beta_0 + 3 \\beta_1 + 3 \\beta_2 &amp;=&amp; 6 \\quad \\mbox{(obs. 1)}\\\\ \\beta_0 + 2 \\beta_1 + 3 \\beta_2 &amp;=&amp; 6 \\quad \\mbox{(obs. 2)}\\\\ \\beta_0 + 5 \\beta_1 + 6 \\beta_2 &amp;=&amp; 10 \\,\\,\\, \\mbox{(obs. 3)}\\\\ \\beta_0 + 6 \\beta_1 + 5 \\beta_2 &amp;=&amp; 9 \\quad \\mbox{(obs. 4)}\\\\ \\end{eqnarray}\\] Rather than writing all these equations out, we instead represent the situation in matrix format as \\[\\X\\bbeta = \\y,\\] Where \\[\\X=\\pm 1&amp;3&amp;3\\\\1&amp;2&amp;3\\\\1&amp;5&amp;6\\\\1&amp;6&amp;5 \\mp \\quad \\bbeta = \\pm \\beta_0 \\\\\\beta_1 \\\\ \\beta_2 \\mp \\quad \\mbox{and}\\quad \\y = \\pm 6 \\\\6 \\\\10\\\\9\\mp\\] We know from our experiences with data that this situation will not have an exact solution: our data does not fall exactly on some straight line or surface. Instead, we have to consider some error, \\(\\boldsymbol\\epsilon\\) and try to minimize it: \\[\\X\\bbeta + \\boldsymbol\\epsilon = \\y,\\] where \\[\\boldsymbol\\epsilon = \\pm \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_4 \\mp\\] is a vector containing the residuals. We will get into the exact details of this shortly, but for now it is important that we see how to set up the regression equation in terms of matrices and vectors. The typical regression equation with the intercept will always involve adding a column of 1’s to the matrix of independent variables, as seen in the previous example. 3.3 Linear Combinations Let’s revisit the second part of Example 3.1, where the task was to use matrix-vector multiplication to determine how much it would cost for the manufacturer to produce 1 batch of each mixture. Essentially what we want to do is take a linear combination of the amounts of raisins, peanuts, and chocolate where the scalar weights are the cost of each ingredient: \\[\\bordermatrix{~&amp; \\mbox{Cost of 1kg}}{}{\\begin{pmatrix}\\mbox{airline}\\\\ \\mbox{grocery}\\\\ \\mbox{gas station} \\end{pmatrix}} = \\$2.55 \\bordermatrix{~&amp; raisins}{}{ \\begin{pmatrix} 7\\\\ 2\\\\ 6\\end{pmatrix}} + \\$4.65 \\bordermatrix{~&amp; peanuts}{}{ \\begin{pmatrix} 6\\\\ 5\\\\ 4\\end{pmatrix}} + \\$4.80 \\bordermatrix{~&amp; chocolate}{}{ \\begin{pmatrix} 2\\\\ 8\\\\ 5\\end{pmatrix}}\\] This linear combination is exactly the same as the matrix-vector product originally used: \\[\\pm 7&amp;6&amp;2\\\\2&amp;5&amp;8 \\\\6&amp;4&amp;5 \\mp \\pm 2.55\\\\4.65\\\\4.80 \\mp = \\pm 55.35\\\\66.75\\\\57.90\\mp\\] Matrix multiplication is nothing more than a series of linear combinations. Let’s develop another quick example. Example 3.2 (Linear Combinations of Variables) Suppose we have data for 100 postal packages using 3 variables: height \\(\\bo{h}\\), weight \\(\\bo{w}\\), and volume \\(\\bo{v}\\). If we create a data matrix, \\(\\X\\), the size of the matrix will be \\(100\\times 3\\) and the three columns will be composed of the variables height, weight, and volume. The previous sentence is written mathematically by creating a partitioned matrix: \\[\\X = \\left( \\bo{h} | \\bo{w} | \\v \\right)\\] If we wanted to create a new variable vector, \\(\\bo{c}\\), which equaled the height plus twice the weight of the package, we’d want to compute the following linear combination: \\[\\bo{c} = \\bo{h} + 2\\bo{w} + 0\\v\\] This could be accomplished by multiplying our whole data matrix by the vector \\(\\pm 1\\\\2\\\\0\\mp\\). \\[\\begin{eqnarray*} \\bo{c}&amp;=&amp;\\underset{(100\\times 3)}{\\X}\\pm 1\\\\2\\\\0\\mp \\cr &amp;=&amp;\\left( \\bo{h} | \\bo{w} | \\v \\right)\\pm 1\\\\2\\\\0\\mp \\cr &amp;=&amp; \\bo{h} + 2\\bo{w} + 0\\v \\end{eqnarray*}\\] If this example confuses you, you ought to write out a smaller matrix of values for the three variables, height, weight and volume. Write down 3 observations or so and see how the linear combination of these columns is precisely the same as the matrix-vector product. Being able to think of these two ideas as interchangeable will be fundamental when we start talking about factor analysis and principal components analysis. If we dissect our formula for a system of linear equations, \\(\\A\\x=\\bo{b}\\), we will find that the right-hand side vector \\(\\bo{b}\\) can be expressed as a linear combination of the columns in the coefficient matrix, \\(\\A\\). \\[\\begin{eqnarray*} \\bo{b}&amp;=&amp; \\A\\x\\\\ \\bo{b}&amp;=&amp; (\\A_1|\\A_2|\\dots|\\A_n)\\pm x_1\\\\x_2\\\\ \\vdots\\\\x_3 \\mp \\\\ \\bo{b}&amp;=&amp; x_1\\A_1 + x_2\\A_2 + \\dots + x_n\\A_n \\end{eqnarray*}\\] A concrete example of this expression is given in Example 3.3. Example 3.3 (Systems of Equations as Linear Combinations) Consider the following system of equations: \\[\\begin{eqnarray} 3x_1 + 2x_2 + 9x_3 &amp;=&amp; 1\\\\ 4x_1 + 2x_2 + 3x_3 &amp;=&amp; 5\\\\ 2x_1 + 7x_2 + \\,x_3 &amp;=&amp; 0 \\end{eqnarray}\\] We can write this as a matrix vector product \\(\\A\\x=\\bo{b}\\) where \\[\\A=\\pm 3 &amp; 2 &amp; 9\\\\4 &amp; 2 &amp; 3\\\\2 &amp;7&amp;1\\mp \\,\\,\\,\\x=\\pm x_1\\\\x_2\\\\x_3\\mp \\mbox{ and } \\bo{b}=\\pm 1\\\\5\\\\0 \\mp\\] We can also write \\(\\bo{b}\\) as a linear combination of columns of \\(\\A\\): \\[x_1 \\pm 3\\\\4\\\\2 \\mp +x_2 \\pm 2\\\\2\\\\7\\mp + x_3 \\pm9\\\\3\\\\1 \\mp = \\pm 1\\\\5\\\\0 \\mp\\] Similarly, if we have a matrix-matrix product, we can write each column of the result as a linear combination of columns of the first matrix. Let \\(\\A_{m\\times n}\\), \\(\\X_{n\\times p}\\), and \\(\\B_{m\\times p}\\) be matrices. If we have \\(\\A\\X=\\B\\) then \\[ (\\A_1 | \\A_2 | \\dots | \\A_n) \\pm x_{11} &amp; x_{12} &amp; \\dots &amp; x_{1p} \\\\x_{21} &amp; x_{22} &amp; \\dots &amp; x_{2n}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n1} &amp; x_{n2}&amp;\\dots &amp;x_{np} \\mp = (\\B_1 | \\B_2 | \\dots | \\B_n) \\] and we can write \\[\\B_j = \\A\\X_j = x_{1j}\\A_1 + x_{2j}\\A_2 + x_{3j}\\A_3 + \\dots + x_{nj}\\A_n.\\] A concrete example of this expression is given in Example 3.4. Example 3.4 (Linear Combinations in Matrix-Matrix Products) Suppose we have the following matrix formula: \\[\\A\\X=\\B\\] Where \\(\\A=\\pm 2 &amp; 1 &amp; 3\\\\1 &amp; 4 &amp; 2\\\\ 3 &amp; 2 &amp; 1 \\mp\\), \\(\\X=\\pm 5&amp;6\\\\9&amp;5\\\\7&amp;8 \\mp\\). Then \\[\\begin{eqnarray} \\B &amp;=&amp;\\pm 2 &amp; 1 &amp; 3\\\\1 &amp; 4 &amp; 2\\\\ 3 &amp; 2 &amp; 1 \\mp \\pm 5&amp;6\\\\9&amp;5\\\\7&amp;8 \\mp \\\\ ~ &amp;=&amp; \\pm 2(5)+1(9)+3(7)&amp;2(6) +1(5)+3(8)\\\\1(5)+4(9)+2(7)&amp;1(6)+4(5)+2(8)\\\\3(5)+2(9)+1(7)&amp;3(6)+2(5)+1(8) \\mp \\end{eqnarray}\\] and we can immediately notice that the columns of \\(\\B\\) are linear combinations of columns of \\(\\A\\): \\[\\B_1 = 5\\pm 2\\\\1\\\\3\\mp+9\\pm 1\\\\4\\\\2 \\mp + 7 \\pm 3\\\\2\\\\1 \\mp\\] \\[\\B_2 = 6\\pm 2\\\\1\\\\3\\mp+5\\pm 1\\\\4\\\\2 \\mp + 8 \\pm 3\\\\2\\\\1 \\mp\\] We may also notice that the rows of \\(\\B\\) can be expressed as a linear combination of rows of \\(\\X\\): \\[\\B_{1\\star} = 2\\pm 5&amp; 6 \\mp + 1\\pm 9&amp; 5 \\mp + 3\\pm 7&amp; 8 \\mp \\] \\[\\B_{2\\star} = 1\\pm 5&amp; 6 \\mp + 4\\pm 9&amp; 5 \\mp + 2\\pm 7&amp; 8 \\mp \\] \\[\\B_{3\\star} = 3\\pm 5&amp; 6 \\mp + 2\\pm 9&amp; 5 \\mp + 1\\pm 7&amp; 8 \\mp \\] Linear combinations are everywhere, and they can provide subtle but important meaning in the sense that they can break data down into a sum of parts. You should convince yourself of one final view of matrix multiplication, as the sum of outer products. In this case \\(\\B\\) is the sum of 3 outer products (3 matrices of rank 1) involving the columns of \\(\\A\\) and corresponding rows of \\(\\X\\): \\[\\B=\\acol{1}\\X_{1\\star}+\\acol{2}\\X_{2\\star}+\\acol{3}\\X_{3\\star}.\\] Example 3.4 turns out to have important implications for our interpretation of matrix factorizations. In this context we’d call \\(\\A\\X\\) a factorization of the matrix \\(\\B\\). We will see how to use these expressions to our advantage in later chapters. 3.4 Exercises A florist offers three sizes of flower arrangements (small, medium, large) containing three types of flowers (roses, daisies, and chrysanthemums). The number of each type of flower in each size arrangement is given in the table below, along with the selling price of each arrangement and the cost of each individual flower. Roses Daisies Chrys. Price Small 1 3 3 $10 Medium 2 4 6 $15 Large 4 8 6 $20 Cost $0.50 $0.25 $0.10 Let \\[\\A = \\pm 1 &amp;3 &amp; 3\\\\2&amp; 4&amp; 6\\\\4&amp; 8&amp;6 \\mp \\quad \\bo{p}= \\pm 10\\\\15\\\\20 \\mp \\quad \\bo{c} = \\pm 0.50\\\\0.25\\\\0.10 \\mp.\\] Determine the matrix-vector product that produces a vector, \\(\\y\\) which gives the total cost of creating each size arrangement (small, medium, and large). Suppose that an order came in for 2 small arrangements and 2 large arrangements. Let \\[\\bo{v} = \\pm 2\\\\0\\\\2 \\mp\\] Using matrix arithmetic (and writing out the formula) determine both the price of this order and the total profit to the florist. Write the following system of equations as a matrix-vector product \\(\\A\\x=\\b\\): \\[\\begin{eqnarray} 2x_2 +3x_3&amp;=&amp; 8, 2x_1+3x_2+1x_3 &amp;=&amp; 5, x_1-x_2-2x_3 &amp;=&amp;-5 \\end{eqnarray}\\] A model is being developed to predict a student’s SAT score based upon some numeric attributes. The data being used for this model is provided below: Observation PSAT score Mother’s SAT score SAT Score 1 1600 1700 1750 2 1800 1250 1750 3 1750 1300 1600 4 1200 1800 1450 5 1350 1950 1500 If our regression model is \\[SAT\\_score = \\beta_0 + \\beta_1* PSAT\\_score + \\beta_2* Mothers\\_SAT\\_Score + \\epsilon\\] Show how we’d set up the underlying matrix equation for regression analysis, \\[\\y = \\X\\bbeta \\] by defining the matrices/vectors \\(\\X, \\bbeta, \\mbox{ and } \\y\\). Suppose a company collected daily data regarding the sales and revenue of particular products for which prices fluctuate daily: Monday Tuesday Wednesday Thursday Friday Product Sales Rev. Sales Rev. Sales Rev. Sales Rev. Sales Rev. Widgets 1 195 5 945 2 400 2 450 5 790 Gadgets 35 350 13 110 25 300 45 497 90 789 Show how you could use matrix addition to compute the total weekly sales and revenue of each product. Now suppose you find out that both the sales and the revenue numbers in the table above were listed in hundreds (i.e. that 100 widgets were sold on Monday, bringing in $19,500 in revenue). Using your answer from part a. show how you would use scalar multiplication to represent the exact weekly numbers for revenue and units sold. Let \\[\\A=\\pm 3 &amp; 2 &amp; 9\\\\4 &amp; 2 &amp; 3\\\\2 &amp;7&amp;1\\mp \\quad\\] For a general matrix \\(\\A_{m\\times n}\\) describe what the following products will provide. Also give the size of the result (i.e. “\\(n\\times 1\\) vector” or “scalar”). \\(\\A\\e_j\\) \\(\\e_i^T\\A\\) \\(\\e_i^T\\A\\e_j\\) \\(\\A\\e\\) \\(\\e^T\\A\\) \\(\\frac{1}{n}\\e^T\\A\\) Let \\(\\bo{D}_{n\\times n}\\) be a diagonal matrix with diagonal elements \\(D_{ii}\\). What effect does multiplying a matrix \\(\\A_{n\\times m}\\) on the left by \\(\\bo{D}\\) have? What effect does multiplying a matrix \\(\\A_{m\\times n}\\) on the right by \\(\\bo{D}\\) have? If you cannot see this effect in a general sense, try writing out a simple \\(3\\times 3\\) matrix as an example first. "],["r-programming-basics.html", "Chapter 4 R Programming Basics", " Chapter 4 R Programming Basics Before we get started, you will need to know the basics of matrix manipulation in the R programming language: Generally matrices are entered in as one vector, which R then breaks apart into rows and columns in they way that you specify (with nrow/ncol). The default way that R reads a vector into a matrix is down the columns. To read the data in across the rows, use the byrow=TRUE option). This is only relevant if you’re entering matrices from scratch. Y=matrix(c(1,2,3,4),nrow=2,ncol=2) Y ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 X=matrix(c(1,2,3,4),nrow=2,ncol=2,byrow=TRUE) X ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 The standard multiplication symbol, ‘*,’ will unfortunately provide unexpected results if you are looking for matrix multiplication. ‘*’ will multiply matrices elementwise. In order to do matrix multiplication, the function is ‘%*%.’ X*X ## [,1] [,2] ## [1,] 1 4 ## [2,] 9 16 X%*%X ## [,1] [,2] ## [1,] 7 10 ## [2,] 15 22 To transpose a matrix or a vector \\(\\X\\), use the function t(\\(\\X\\)). t(X) ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 R indexes vectors and matrices starting with \\(i=1\\) (as opposed to \\(i=0\\) in python). X[i,j] gives element \\(\\X_{ij}\\). You can alter individual elements this way. X[2,1] ## [1] 3 X[2,1]=100 X ## [,1] [,2] ## [1,] 1 2 ## [2,] 100 4 To create a vector of all ones, \\(\\e\\), use the rep() function e=rep(1,5) e ## [1] 1 1 1 1 1 To compute the mean of a vector, use the mean function. To compute the column means of a matrix (or data frame), use the colMeans() function. You can also use the apply function, which is necessary if you want column standard deviations (sd() function). apply(X,dim,function) applies the specified function to the specified dimension dim (1 for rows, 2 for columns) of the matrix or data frame X. # Start by generating random ~N(0,1) data: A=replicate(2,rnorm(5)) colMeans(A) ## [1] 0.09138574 -0.34148516 # (Why aren&#39;t the means close to zero?) A=replicate(2,rnorm(100)) colMeans(A) ## [1] -0.1930677 0.0874687 #LawOfLargeNumbers. apply(A,2,sd) ## [1] 0.9540342 1.0643603 # To apply a &quot;homemade function&quot; you must create it as a function # Here we apply a sum of squares function for the first 5 rows of A: apply(A[1:5, ],1,function(x) x%*%x) ## [1] 0.7242231 1.0520831 0.3397001 1.6553828 6.2912147 # Here we center the data by subtracting the mean vector: B=apply(A,2,function(x) x-mean(x)) colMeans(B) ## [1] 1.165734e-17 -1.221245e-17 # R doesn&#39;t tell you when things are zero to machine precision. &quot;Machine zero&quot; in # R is given by the internal variable .Machine$double.eps colMeans(B) &lt; .Machine$double.eps ## [1] TRUE TRUE To invert a matrix, use the solve() command. Xinv=solve(X) X%*%Xinv ## [,1] [,2] ## [1,] 1 0 ## [2,] 0 1 To determine size of a matrix, use the dim() function. The result is a vector with two values: dim(x)[1] provides the number of rows and dim(x)[2] provides the number of columns. You can label rows/columns of a matrix using the rownames() or colnames() functions. dim(A) ## [1] 100 2 nrows=dim(A)[1] ncols=dim(A)[2] colnames(A)=c(&quot;This&quot;,&quot;That&quot;) A[1:5, ] ## This That ## [1,] 0.5291381 -0.66651026 ## [2,] 0.8526387 0.57016697 ## [3,] -0.5807107 0.04975133 ## [4,] 0.1250208 -1.28052827 ## [5,] -1.9888993 -1.52823242 Most arithmetic functions you apply to a vector act elementwise. In R, \\(\\x^2\\) will be a vector containing the square of the elements in \\(\\x\\). You can add a column to a matrix (or a data frame) by using the cbind() function. # Add a column containing the square of the second column A=cbind(A,A[ ,2]^2) colnames(A) ## [1] &quot;This&quot; &quot;That&quot; &quot;&quot; colnames(A)[3]=&quot;That Squared&quot; colnames(A) ## [1] &quot;This&quot; &quot;That&quot; &quot;That Squared&quot; You can compute vector norms using the norm() function. Unfortunately, the default norm is not the \\(2\\)-norm (it should be!) so we must specify the type=\"2\" as the second argument to the function. x=c(1,1,1) y=c(1,0,0) norm(x,type=&quot;2&quot;) ## [1] 1.732051 # It&#39;s actually fewer characters to work from the equivalent definition: sqrt(x%*%x) ## [,1] ## [1,] 1.732051 norm(y,type=&quot;2&quot;) ## [1] 1 norm(x-y,type=&quot;2&quot;) ## [1] 1.414214 You’ll learn many additional R techniques throughout this course, but our strategy in this text will be to pick them up as we go as opposed to trying to remember them from the beginning. "],["solvesys.html", "Chapter 5 Solving Systems of Equations 5.1 Gaussian Elimination 5.2 Gauss-Jordan Elimination 5.3 Three Types of Systems 5.4 Solving Matrix Equations 5.5 Exercises 5.6 List of Key Terms 5.7 Gauss-Jordan Elimination in R", " Chapter 5 Solving Systems of Equations In this section we will learn about solving the systems of equations that were presented in Chapter 3. There are three general situations we may find ourselves in when attempting to solve systems of equations: The system could have one unique solution. The system could have infinitely many solutions (sometimes called underdetermined). The system could have no solutions (sometimes called overdetermined or inconsistent). Luckily, no matter what type of system we are dealing with, the method to arriving at the answer (should it exist) is the same. The process is called Gaussian (or Gauss-Jordan) Elimination. 5.1 Gaussian Elimination Gauss-Jordan Elimination is essentially the same process of elimination you may have used in an Algebra class in primary school. Suppose, for example, we have the following simple system of equations: \\[\\begin{cases}\\begin{eqnarray} x_1+2x_2 &amp;=&amp; 11\\\\ x_1+x_2 &amp;=&amp; 6\\end{eqnarray}\\end{cases}\\] One simple way to solve this system of equations is to subtract the second equation from the first. By this we mean that we’d perform subtraction on the left hand and right hand sides of the equation: \\[\\pm &amp;x_1&amp;+&amp;2x_2 \\\\ -&amp;(x_1&amp;+&amp;x_2) \\\\ \\hline &amp;&amp;&amp;x_2 \\mp = \\pm 11 \\\\-6\\\\\\hline 5 \\mp\\] This operation is clearly allowed because the two subtracted quantities are equal (by the very definition of an equation!). What we are left with is one much simpler equation, \\[x_2=5\\] using this information, we can return to the first equation, substitute and solve for \\(x_1\\): \\[\\begin{eqnarray} x_1+2(5)&amp;=&amp;11 \\\\ x_1 &amp;=&amp; 1 \\end{eqnarray}\\] This final process of substitution is often called back substitution. Once we have a sufficient amount of information, we can use that information to substitute and solve for the remainder. 5.1.1 Row Operations In the previous example, we demonstrated one operation that can be performed on systems of equations without changing the solution: one equation can be added to a multiple of another (in that example, the multiple was -1). For any system of equations, there are 3 operations which will not change the solution set: Interchanging the order of the equations. Multiplying both sides of one equation by a constant. Replace one equation by a linear combination of itself and of another equation. Taking our simple system from the previous example, we’ll examine these three operations concretely: \\[\\begin{cases}\\begin{eqnarray} x_1+2x_2 &amp;=&amp; 11\\\\ x_1+x_2 &amp;=&amp; 6\\end{eqnarray}\\end{cases}\\] Interchanging the order of the equations. \\[\\begin{cases}\\begin{align} x_1+2x_2 &amp;= 11\\\\ x_1+x_2 &amp;= 6\\end{align}\\end{cases}\\] \\(\\Leftrightarrow\\) \\[\\begin{cases}\\begin{align} x_1+x_2 =&amp; 6\\\\ x_1+2x_2 =&amp; 11\\end{align}\\end{cases}\\] Multiplying both sides of one equation by a constant. (Multiply the second equation by -1). \\[\\begin{cases}\\begin{align} x_1+2x_2 &amp;=&amp; 11\\\\ x_1+x_2 &amp;=&amp; 6\\end{align}\\end{cases}\\] \\(\\Leftrightarrow\\) \\[\\begin{cases}\\begin{align} x_1+2x_2 &amp;=&amp; 11\\\\ -1x_1-1x_2 &amp;=&amp; -6\\end{align}\\end{cases}\\] Replace one equation by a linear combination of itself and of another equation. (Replace the second equation by the first minus the second.) \\[\\begin{cases}\\begin{eqnarray} x_1+2x_2 &amp;=&amp; 11\\\\ x_1+x_2 &amp;=&amp; 6\\end{eqnarray}\\end{cases}\\] \\(\\Leftrightarrow\\) \\[\\begin{cases}\\begin{eqnarray} x_1+2x_2 &amp;=&amp; 11\\\\ x_2 &amp;=&amp; 5\\end{eqnarray}\\end{cases}\\] Using these 3 row operations, we can transform any system of equations into one that is triangular. A triangular system is one that can be solved by back substitution. For example, \\[\\begin{cases}\\begin{align} x_1+2x_2 +3x_3= 14\\\\ x_2+x_3 =6\\\\ x_3 = 1\\end{align}\\end{cases}\\] is a triangular system. Using substitution, the second equation will give us the value for \\(x_2\\), which will allow for further substitution into the first equation to solve for the value of \\(x_1\\). Let’s take a look at an example of how we can transform any system to a triangular system. Example 5.1 (Transforming a System to a Triangular System via 3 Operations) Solve the following system of equations: \\[\\begin{cases}\\begin{eqnarray} x_1+x_2 +x_3&amp;=&amp; 1\\\\ x_1-2x_2+2x_3 &amp;=&amp;4\\\\ x_1+2x_2-x_3 &amp;=&amp; 2\\end{eqnarray}\\end{cases}\\] To turn this into a triangular system, we will want to eliminate the variable \\(x_1\\) from two of the equations. We can do this by taking the following operations: Replace equation 2 with (equation 2 - equation 1). Replace equation 3 with (equation 3 - equation 1). Then, our system becomes: \\[\\begin{cases}\\begin{eqnarray} x_1+x_2 +x_3&amp;=&amp; 1\\\\ -3x_2+x_3 &amp;=&amp;3\\\\ x_2-2x_3 &amp;=&amp; 1\\end{eqnarray}\\end{cases}\\] Next, we will want to eliminate the variable \\(x_2\\) from the third equation. We can do this by replacing equation 3 with (equation 3 + \\(\\frac{1}{3}\\) equation 2). However, we can avoid dealing with fractions if instead we: Swap equations 2 and 3. \\[\\begin{cases}\\begin{eqnarray} x_1+x_2 +x_3 &amp;=&amp; 1\\\\ x_2-2x_3 &amp;=&amp; 1\\\\ -3x_2+x_3 &amp;=&amp;3\\end{eqnarray}\\end{cases}\\] Now, as promised our math is a little simpler: Replace equation 3 with (equation 3 + 3*equation 2). \\[\\begin{cases}\\begin{eqnarray} x_1+x_2 +x_3 &amp;=&amp; 1 \\\\ x_2-2x_3 &amp;=&amp; 1 \\\\ -5x_3 &amp;=&amp;6 \\end{eqnarray}\\end{cases}\\] Now that our system is in triangular form, we can use substitution to solve for all of the variables: \\[x_1 = 3.6 \\quad x_2 = -1.4 \\quad x_3 = -1.2 \\] This is the procedure for Gaussian Elimination, which we will now formalize in it’s matrix version. 5.1.2 The Augmented Matrix When solving systems of equations, we will commonly use the augmented matrix. Definition 5.1 (The Augmented Matrix) The augmented matrix of a system of equations is simply the matrix which contains all of the coefficients of the equations, augmented with an extra column holding the values on the right hand sides of the equations. If our system is: \\[\\begin{cases}\\begin{eqnarray} a_{11}x_1+a_{12}x_2 +a_{13}x_3&amp;=&amp; b_1 a_{21}x_1+a_{22}x_2 +a_{23}x_3&amp;=&amp; b_2 a_{31}x_1+a_{32}x_2 +a_{33}x_3&amp;=&amp;b_3 \\end{eqnarray}\\end{cases}\\] Then the corresponding augmented matrix is \\[\\left(\\begin{array}{rrr|r} a_{11}&amp;a_{12}&amp;a_{13}&amp; b_1\\\\ a_{21}&amp;a_{22}&amp;a_{23}&amp; b_2\\\\ a_{31}&amp;a_{12}&amp;a_{33}&amp; b_3\\\\ \\end{array}\\right)\\] Using this augmented matrix, we can contain all of the information needed to perform the three operations outlined in the previous section. We will formalize these operations as they pertain to the rows (i.e. individual equations) of the augmented matrix (i.e. the entire system) in the following definition. Definition 5.2 (Row Operations for Gaussian Elimination) Gaussian Elimination is performed on the rows, \\(\\arow{i},\\) of an augmented matrix, \\[\\A = \\pm \\arow{1}\\\\\\arow{2}\\\\\\arow{3}\\\\\\vdots\\\\\\arow{m}\\mp\\] by using the three elementary row operations: Swap rows \\(i\\) and \\(j\\). Replace row \\(i\\) by a nonzero multiple of itself. Replace row \\(i\\) by a linear combination of itself plus a multiple of row \\(j\\). The ultimate goal of Gaussian elimination is to transform an augmented matrix into an upper-triangular matrix which allows for backsolving. \\[\\A \\rightarrow \\left(\\begin{array}{rrrr|r} t_{11}&amp; t_{12}&amp; \\dots&amp; t_{1n}&amp;c_1\\cr 0&amp; t_{22}&amp; \\dots&amp; t_{2n}&amp;c_2\\cr \\vdots&amp; \\vdots&amp; \\ddots&amp; \\vdots&amp;\\vdots\\cr 0&amp; 0&amp; \\dots&amp; t_{nn}&amp;c_n\\end{array}\\right)\\] The key to this process at each step is to focus on one position, called the pivot position or simply the pivot, and try to eliminate all terms below this position using the three row operations. Only nonzero numbers are allowed to be pivots. If a coefficient in a pivot position is ever 0, then the rows of the matrix should be interchanged to find a nonzero pivot. If this is not possible then we continue on to the next possible column where a pivot position can be created. Let’s now go through a detailed example of Gaussian elimination using the augmented matrix. We will use the same example (and same row operations) from the previous section to demonstrate the idea. Example 5.2 (Row Operations on the Augmented Matrix) We will solve the system of equations from Example 5.1 using the Augmented Matrix. \\[\\begin{equation*}\\begin{cases}\\begin{align} x_1+x_2 +x_3= 1\\\\ x_1-2x_2+2x_3 =4\\\\ x_1+2x_2-x_3 = 2\\end{align}\\end{cases} \\end{equation*}\\] Our first step will be to write the augmented matrix and identify the current pivot. Here, a square is drawn around the pivot and the numbers below the pivot are circled. It is our goal to eliminate the circled numbers using the row with the pivot. \\[\\begin{equation*} \\left(\\begin{array}{rrr|r} 1 &amp; 1 &amp; 1 &amp; 1\\\\ 1 &amp; -2 &amp; 2 &amp;4\\\\ 1&amp;2&amp;-1 &amp;2 \\end{array}\\right) \\xrightarrow{Current Pivot}\\left(\\begin{array}{rrr|r} \\fbox{1} &amp; 1 &amp; 1 &amp; 1\\\\ \\enclose{circle}[mathcolor=&quot;red&quot;]{\\color{black}{1}} &amp; -2 &amp; 2 &amp;4\\\\ \\enclose{circle}[mathcolor=&quot;red&quot;]{\\color{black}{1}}&amp;2&amp;-1 &amp;2 \\end{array}\\right) \\end{equation*}\\] We can eliminate the circled elements by making combinations with those rows and the pivot row. For instance, we’d replace row 2 by the combination (row 2 - row 1). Our shorthand notation for this will be R2’ = R2-R1. Similarly we will replace row 3 in the next step. \\[\\begin{equation*} \\xrightarrow{R2&#39;=R2-R1} \\left(\\begin{array}{rrr|r} \\fbox{1} &amp; 1 &amp; 1 &amp; 1\\\\ \\red{0} &amp; \\red{-3} &amp; \\red{1} &amp;\\red{3}\\\\ \\enclose{circle}[mathcolor=&quot;red&quot;]{\\color{black}{1}}&amp;2&amp;-1 &amp;2 \\end{array}\\right) \\xrightarrow{R3&#39;=R3-R1} \\left(\\begin{array}{rrr|r} \\fbox{1} &amp; 1 &amp; 1 &amp; 1\\\\ 0 &amp; -3 &amp; 1 &amp;3\\\\ \\red{0}&amp;\\red{1}&amp;\\red{-2}&amp;\\red{1} \\end{array}\\right) \\end{equation*}\\] Now that we have eliminated each of the circled elements below the current pivot, we will continue on to the next pivot, which is -3. Looking into the future, we can either do the operation \\(R3&#39;=R3+\\frac{1}{3}R2\\) or we can interchange rows 2 and 3 to avoid fractions in our next calculation. To keep things neat, we will do the latter. (note: either way you proceed will lead you to the same solution!) \\[\\begin{equation*} \\xrightarrow{Next Pivot} \\left(\\begin{array}{rrr|r} \\fbox{1} &amp; 1 &amp; 1 &amp; 1\\\\ 0 &amp; \\fbox{-3} &amp; 1 &amp;3\\\\ 0&amp;\\enclose{circle}[mathcolor=&quot;red&quot;]{\\color{black}{1}}&amp;-2&amp;1 \\end{array}\\right) \\xrightarrow{R2 \\leftrightarrow R3} \\left(\\begin{array}{rrr|r} \\fbox{1} &amp; 1 &amp; 1 &amp; 1\\\\ 0&amp;\\fbox{1}&amp;-2&amp;1\\\\ 0 &amp; \\enclose{circle}[mathcolor=&quot;red&quot;]{\\color{black}{-3}} &amp; 1 &amp;3 \\end{array}\\right) \\end{equation*}\\] Now that the current pivot is equal to 1, we can easily eliminate the circled entries below it by replacing rows with combinations using the pivot row. We finish the process once the last pivot is identified (the final pivot has no eliminations to make below it). \\[\\begin{equation*} \\xrightarrow{R3&#39;=R3+3R2} \\left(\\begin{array}{rrr|r} \\fbox{1} &amp; 1 &amp; 1 &amp; 1\\\\ 0&amp;\\fbox{1}&amp;-2&amp;1\\\\ \\red{0} &amp; \\red{0} &amp; \\red{\\fbox{-5}} &amp;\\red{6} \\end{array}\\right) \\end{equation*}\\] At this point, when all the pivots have been reached, the augmented matrix is said to be in row-echelon form. This simply means that all of the entries below the pivots are equal to 0. The augmented matrix can be transformed back into equation form now that it is in a triangular form: \\[\\begin{equation*} \\begin{cases}\\begin{align} x_1+x_2 +x_3= 1\\\\ x_2-2x_3 = 1\\\\ 5x_3 =-6\\end{align}\\end{cases} \\end{equation*}\\] Which is the same system we finally solved in Example 5.1 to get the final solution: \\[x_1 = 3.6 \\quad x_2 = -1.4 \\quad x_3 = -1.2 \\] 5.1.3 Gaussian Elimination Summary Let’s summarize the process of Gaussian elimination step-by-step: We work from the upper-left-hand corner of the matrix to the lower-right-hand corner Focusing on the first column, identify the first pivot element. The first pivot element should be located in the first row (if this entry is zero, we must interchange rows so that it is non-zero). Eliminate (zero-out) all elements below the pivot using the combination row operation. Determine the next pivot and go back to step 2. Only nonzero numbers are allowed to be pivots. If a coefficient in the next pivot position is 0, then the rows of the matrix should be interchanged to find a nonzero pivot. If this is not possible then we continue on to the next column to determine a pivot. When the entries below all of the pivots are equal to zero, the process stops. The augmented matrix is said to be in row-echelon form, which corresponds to a triangular system of equations, suitable to solve using back substitution. Exercise 5.1 (Gaussian Elimination and Back Substitution) Use Gaussian Elimination and back substitution to solve the following system. \\[\\begin{cases}\\begin{align} 2x_1-x_2=1\\\\ -x_1+2x_2-x_3=0\\\\ -x_2+x_3=0\\end{align}\\end{cases}\\] 5.2 Gauss-Jordan Elimination Gauss-Jordan elimination is Gaussian elimination taken one step further. In Gauss-Jordan elimination, we do not stop when the augmented matrix is in row-echelon form. Instead, we force all the pivot elements to equal 1 and we continue to eliminate entries above the pivot elements to reach what’s called reduced row echelon form. Let’s take a look at another example: Example 5.3 (Gauss-Jordan elimination) We begin with a system of equations, and transform it into an augmented matrix: \\[\\begin{cases}\\begin{align} x_2 -x_3= 3\\\\ -2x_1+4x_2-x_3 = 1\\\\ -2x_1+5x_2-4x_3 =-2\\end{align}\\end{cases} \\Longrightarrow \\left(\\begin{array}{rrr|r}0&amp;1&amp;-1&amp;3\\\\-2&amp;4&amp;-1&amp;1\\\\-2&amp;5&amp;-4&amp;-2\\end{array}\\right) \\] We start by locating our first pivot element. This element cannot be zero, so we will have to swap rows to bring a non-zero element to the pivot position. \\[\\left(\\begin{array}{rrr|r}0&amp;1&amp;-1&amp;3\\\\-2&amp;4&amp;-1&amp;1\\\\-2&amp;5&amp;-4&amp;-2\\end{array}\\right) \\xrightarrow{R1\\leftrightarrow R2} \\left(\\begin{array}{rrr|r}\\fbox{-2}&amp;4&amp;-1&amp;1\\\\0&amp;1&amp;-1&amp;3\\\\-2&amp;5&amp;-4&amp;-2\\end{array}\\right)\\] Now that we have a non-zero pivot, we will want to do two things: It does not matter what order we perform these two tasks in. Here, we will have an easy time eliminating using the -2 pivot: \\[\\left(\\begin{array}{rrr|r}\\fbox{-2}&amp;4&amp;-1&amp;1\\\\0&amp;1&amp;-1&amp;3\\\\-2&amp;5&amp;-4&amp;-2\\end{array}\\right)\\xrightarrow{R3&#39;=R3-R1} \\left(\\begin{array}{rrr|r}\\fbox{-2}&amp;4&amp;-1&amp;1\\\\0&amp;1&amp;-1&amp;3\\\\\\red{0}&amp;\\red{1}&amp;\\red{-3}&amp;\\red{-3}\\end{array}\\right)\\] Now, as promised, we will make our pivot equal to 1. \\[\\left(\\begin{array}{rrr|r}\\fbox{-2}&amp;4&amp;-1&amp;1\\\\0&amp;1&amp;-1&amp;3\\\\0&amp;1&amp;-3&amp;-3\\end{array}\\right) \\xrightarrow{R1&#39;=-\\frac{1}{2} R1} \\left(\\begin{array}{rrr|r}\\red{\\fbox{1}}&amp;\\red{-2}&amp;\\red{\\frac{1}{2}}&amp;\\red{-\\frac{1}{2}}\\\\0&amp;1&amp;-1&amp;3\\\\0&amp;1&amp;-3&amp;-3\\end{array}\\right)\\] We have finished our work with this pivot, and now we move on to the next one. Since it is already equal to 1, the only thing left to do is use it to eliminate the entries below it: \\[\\left(\\begin{array}{rrr|r}1&amp;-2&amp;\\frac{1}{2}&amp;-\\frac{1}{2}\\\\0&amp;\\fbox{1}&amp;-1&amp;3\\\\0&amp;1&amp;-3&amp;-3\\end{array}\\right)\\xrightarrow{R3&#39;=R3-R2} \\left(\\begin{array}{rrr|r}1&amp;-2&amp;\\frac{1}{2}&amp;-\\frac{1}{2}\\\\0&amp;\\fbox{1}&amp;-1&amp;3\\\\\\red{0}&amp;\\red{0}&amp;\\red{-2}&amp;\\red{-6}\\end{array}\\right)\\] And then we move onto our last pivot. This pivot has no entries below it to eliminate, so all we must do is turn it into a 1: \\[\\left(\\begin{array}{rrr|r}1&amp;-2&amp;\\frac{1}{2}&amp;\\frac{-1}{2}\\\\0&amp;1&amp;-1&amp;3\\\\0&amp;0&amp;\\fbox{-2}&amp;-6\\end{array}\\right)\\xrightarrow{R3&#39;=-\\frac{1}{2}R3}\\left(\\begin{array}{rrr|r}1&amp;-2&amp;\\frac{1}{2}&amp;-\\frac{1}{2}\\\\0&amp;1&amp;-1&amp;3\\\\\\red{0}&amp;\\red{0}&amp;\\red{\\fbox{1}}&amp;\\red{3}\\end{array}\\right) \\] Now, what really differentiates Gauss-Jordan elimination from Gaussian elimination is the next few steps. Here, our goal will be to use the pivots to eliminate all of the entries above them. While this takes a little extra work, as we will see, it helps us avoid the tedious work of back substitution. We’ll start at the southeast corner on the current pivot. We will use that pivot to eliminate the elements above it: \\[\\left(\\begin{array}{rrr|r} 1&amp;-2&amp;\\frac{1}{2}&amp;-\\frac{1}{2}\\\\0&amp;1&amp;-1&amp;3\\\\0&amp;0&amp;\\fbox{1}&amp;3\\end{array}\\right) \\xrightarrow{R2&#39;=R2+R3} \\left(\\begin{array}{rrr|r} 1&amp;-2&amp;\\frac{1}{2}&amp;-\\frac{1}{2}\\\\\\red{0}&amp;\\red{1}&amp;\\red{0}&amp;\\red{6}\\\\0&amp;0&amp;\\fbox{1}&amp;3\\end{array}\\right)\\] \\[ \\left(\\begin{array}{rrr|r} 1&amp;-2&amp;\\frac{1}{2}&amp;-\\frac{1}{2}\\\\0&amp;1&amp;0&amp;6\\\\0&amp;0&amp;\\fbox{1}&amp;3\\end{array}\\right)\\xrightarrow{R1&#39;=R1-\\frac{1}{2}R3}\\left(\\begin{array}{rrr|r} \\red{1}&amp;\\red{-2}&amp;\\red{0}&amp;\\red{-2}\\\\0&amp;1&amp;0&amp;6\\\\0&amp;0&amp;\\fbox{1}&amp;3\\end{array}\\right)\\] We’re almost done! One more pivot with elements above it to be eliminated: \\[\\left(\\begin{array}{rrr|r} 1&amp;-2&amp;0&amp;-2\\\\0&amp;\\fbox{1}&amp;0&amp;6\\\\0&amp;0&amp;1&amp;3\\end{array}\\right) \\xrightarrow{R1&#39;=R1+2R2} \\left(\\begin{array}{rrr|r}\\red{1}&amp;\\red{0}&amp;\\red{0}&amp;\\red{10}\\\\0&amp;\\fbox{1}&amp;0&amp;6\\\\0&amp;0&amp;1&amp;3\\end{array}\\right)\\] And we’ve reached reduced row echelon form. How does this help us? Well, let’s transform back to a system of equations: \\[\\begin{cases}\\begin{align} x_1 = 10\\\\ x_2= 6\\\\ x_3 =3\\end{align}\\end{cases}\\] The solution is simply what’s left in the right hand column of the augmented matrix. As you can see, the steps to performing Gaussian elimination and Gauss-Jordan elimination are very similar. Gauss-Jordan elimination is merely an extension of Gaussian elimination which brings the problem as close to completion as possible. 5.2.1 Gauss-Jordan Elimination Summary Focusing on the first column, identify the first pivot element. The first pivot element should be located in the first row (if this entry is zero, we must interchange rows so that it is non-zero). Our goal will be to use this element to eliminate all of the elements below it. The pivot element should be equal to 1. If it is not, we simply multiply the row by a constant to make it equal 1 (or interchange rows, if possible). Eliminate (zero-out) all elements below the pivot using the combination row operation. Determine the next pivot and go back to step 2. Only nonzero numbers are allowed to be pivots. If a coefficient in a pivot position is ever 0, then the rows of the matrix should be interchanged to find a nonzero pivot. If this is not possible then we continue on to the next possible column where a pivot position can be created. When the last pivot is equal to 1, begin to eliminate all the entries above the pivot positions. When all entries above and below each pivot element are equal to zero, the augmented matrix is said to be in reduced row echelon form and the Gauss-Jordan elimination process is complete. Exercise 5.2 (Gauss-Jordan Elimination) Use the Gauss-Jordan method to solve the following system: \\[\\begin{cases}\\begin{align} 4x_2-3x_3=3\\\\ -x_1+7x_2-5x_3=4\\\\ -x_1+8x_2-6x_3=5\\end{align}\\end{cases}\\] 5.3 Three Types of Systems As was mentioned earlier, there are 3 situations that may arise when solving a system of equations: The system could have one unique solution (this is the situation of our examples thus far). The system could have no solutions (sometimes called overdetermined or inconsistent). The system could have infinitely many solutions (sometimes called underdetermined). 5.3.1 The Unique Solution Case Based on our earlier examples, we already have a sense for systems which fall into the first case. Theorem 5.1 (Case 1: Unique solution) A system of equations \\(\\A\\x=\\b\\) has a unique solution if and only if both of the following conditions hold: The number of equations is equal to the number of variables (i.e. the coefficient matrix \\(\\A\\) is square). The number of pivots is equal to the number of rows/columns. In other words, under Gauss-Jordan elimination, the coefficient matrix is transformed into the identity matrix: \\[\\A \\xrightarrow{Gauss-Jordan} I\\] In this case, we say that the matrix \\(\\A\\) is invertible because it is full-rank (the rank of a matrix is the number of pivots after Gauss-Jordan elimination) and square. 5.3.2 The Inconsistent Case The second case scenario is a very specific one. In order for a system of equations to be inconsistent and have no solutions, it must be that after Gaussian elimination, a situation occurs where at least one equation reduces to \\(0=\\alpha\\) where \\(\\alpha\\) is nonzero. Such a situation would look as follows (using asterisks to denote any nonzero numbers): \\[\\left(\\begin{array}{rrr|r} *&amp;*&amp;*&amp;*\\\\0&amp;*&amp;*&amp;*\\\\0&amp;0&amp;0&amp;\\alpha\\end{array}\\right) \\] The third row of this augmented system indicates that \\[0x_1+0x_2+0x_3=\\alpha\\] where \\(\\alpha\\neq 0\\), which is a contradiction. When we reach such a situation through Gauss-Jordan elimination, we know the system is inconsistent. Example 5.4 (Identifying an Inconsistent System) \\[\\begin{cases}\\begin{align} x-y+z=1\\\\ x-y-z=2\\\\ x+y-z=3\\\\ x+y+z=4\\end{align}\\end{cases}\\] Using the augmented matrix and Gaussian elimination, we take the following steps: \\[\\left(\\begin{array}{rrr|r} 1&amp;-1&amp;1&amp;1\\\\1&amp;-1&amp;-1&amp;2\\\\1&amp;1&amp;-1&amp;3\\\\1&amp;1&amp;1&amp;4\\end{array}\\right) \\xrightarrow{\\substack{R2&#39;=R2-R1 \\\\ R3&#39;=R3-R1 \\\\ R4&#39;=R4-R1}} \\left(\\begin{array}{rrr|r} 1&amp;-1&amp;1&amp;1\\\\0&amp;0&amp;-2&amp;1\\\\0&amp;2&amp;-2&amp;2\\\\0&amp;2&amp;0&amp;3\\end{array}\\right) \\] \\[\\xrightarrow{ R4\\leftrightarrow R2}\\left(\\begin{array}{rrr|r} 1&amp;-1&amp;1&amp;1\\\\0&amp;2&amp;0&amp;3\\\\0&amp;2&amp;-2&amp;2\\\\0&amp;0&amp;-2&amp;1\\end{array}\\right)\\xrightarrow{R3&#39;=R3-R2} \\left(\\begin{array}{rrr|r} 1&amp;-1&amp;1&amp;1\\\\0&amp;2&amp;0&amp;3\\\\0&amp;0&amp;-2&amp;-1\\\\0&amp;0&amp;-2&amp;1\\end{array}\\right)\\] \\[\\xrightarrow{R4&#39;=R4-R3} \\left(\\begin{array}{rrr|r} 1&amp;-1&amp;1&amp;1\\\\0&amp;2&amp;0&amp;3\\\\0&amp;0&amp;-2&amp;-1\\\\0&amp;0&amp;0&amp;2\\end{array}\\right)\\] In this final step, we see our contradiction equation, \\(0=2\\). Since this is obviously impossible, we conclude that the system is inconsistent. Sometimes inconsistent systems are referred to as over-determined. In this example, you can see that we had more equations than variables. This is a common characteristic of over-determined or inconsistent systems. You can think of it as holding too many demands for a small set of variables! In fact, this is precisely the situation in which we find ourselves when we approach linear regression. Regression systems do not have an exact solution: there are generally no set of \\(\\beta_i&#39;s\\) that we can find so that our regression equation exactly fits every observation in the dataset - the regression system is inconsistent. Thus, we need a way to get as close as possible to a solution; that is, we need to find a solution that minimizes the residual error. This is done using the Least Squares method, the subject of Chapter 10. 5.3.3 The Infinite Solutions Case For the third case, consider the following system of equations written as an augmented matrix, and its reduced row echelon form after Gauss-Jordan elimination. As an exercise, it is suggested that you confirm this result. \\[\\left(\\begin{array}{rrr|r} 1&amp;2&amp;3&amp;0\\\\2&amp;1&amp;3&amp;0\\\\1&amp;1&amp;2&amp;0\\end{array}\\right) \\xrightarrow{Gauss-Jordan} \\left(\\begin{array}{rrr|r} 1&amp;0&amp;1&amp;0\\\\0&amp;1&amp;1&amp;0\\\\0&amp;0&amp;0&amp;0\\end{array}\\right) \\] There are several things you should notice about this reduced row echelon form. For starters, it has a row that is completely 0. This means, intuitively, that one of the equations was able to be completely eliminated - it contained redundant information from the first two. The second thing you might notice is that there are only 2 pivot elements. Because there is no pivot in the third row, the last entries in the third column could not be eliminated! This is characteristic of what is called a free-variable. Let’s see what this means by translating our reduced system back to equations: \\[\\begin{cases}\\begin{align} x_1+x_3 = 0\\\\ x_2+x_3= 0\\end{align}\\end{cases}\\] Clearly, our answer to this problem depends on the variable \\(x_3\\), which is considered free to take on any value. Once we know the value of \\(x_3\\) we can easily determine that \\[\\begin{align} x_1 &amp;= -x_3 \\\\ x_2 &amp;= -x_3 \\end{align}\\] Our convention here is to parameterize the solution and simply declare that \\(x_3=s\\) (or any other placeholder variable for a constant). Then our solution becomes: \\[\\pm x_1\\\\x_2\\\\x_3 \\mp = \\pm -s \\\\ -s \\\\ s \\mp = s \\pm -1\\\\-1\\\\1 \\mp\\] What this means is that any scalar multiple of the vector \\(\\pm -1\\\\-1\\\\1 \\mp\\) is a solution to the system. Thus there are infinitely many solutions! Theorem 5.2 (Case 3: Infinitely Many Solutions) A system of equations \\(\\A\\x=\\b\\) has infinitely many solutions if the system is consistent and any of the following conditions hold: The number of variables is greater than the number of equations. There is at least one free variable presented in the reduced row echelon form. The number of pivots is less than the number of variables. Example 5.5 (Infinitely Many Solutions) For the following reduced system of equations, characterize the set of solutions in the same fashion as the previous example. \\[\\left(\\begin{array}{rrrr|r} 1&amp;0&amp;1&amp;2&amp;0\\\\0&amp;1&amp;1&amp;-1&amp;0\\\\0&amp;0&amp;0&amp;0&amp;0\\\\0&amp;0&amp;0&amp;0&amp;0\\end{array}\\right) \\] A good way to start is sometimes to write out the corresponding equations: \\[\\begin{cases}\\begin{align} x_1+x_3+2x_4 = 0\\\\ x_2+x_3-x_4= 0\\end{align}\\end{cases} \\Longrightarrow \\systeme{ x_1=-x_3-2x_4\\\\ x_2=-x_3+x_4\\end{align}\\end{cases}\\] Now we have two variables which are free to take on any value. Thus, let \\[x_3 = s \\quad \\mbox{and} \\quad x_4 = t\\] Then, our solution is: \\[\\pm x_1\\\\x_2\\\\x_3\\\\x_4 \\mp = \\pm -s-2t \\\\ -s+t\\\\s\\\\t \\mp = s\\pm -1\\\\-1\\\\1\\\\0 \\mp + t\\pm -2\\\\1\\\\0\\\\1 \\mp\\] so any linear combination of the vectors \\[\\pm -1\\\\-1\\\\1\\\\0 \\mp \\quad \\mbox{and} \\quad \\pm -2\\\\1\\\\0\\\\1 \\mp\\] will provide a solution to this system. 5.3.4 Matrix Rank The rank of a matrix is the number of linearly independent rows or columns in the matrix (the number of linearly independent rows will always be the same as the number of linearly independent columns). It can be determined by reducing a matrix to row-echelon form and counting the number of pivots. A matrix is said to be full rank when its rank is maximal, meaning that either all rows or all columns are linearly independent. In other words, an \\(m\\times n\\) matrix \\(\\A\\) is full rank when the rank(\\(\\A\\))\\(=\\min(m,n)\\). A square matrix that is full rank will always have an inverse. 5.4 Solving Matrix Equations One final piece to the puzzle is what happens when we have a matrix equation like \\[\\A\\X=\\B\\] This situation is an easy extension of our previous problem because we are essentially solving the same system of equation with several different right-hand-side vectors (the columns of \\(\\B\\)). Let’s look at a \\(2\\times 2\\) example to get a feel for this! We’ll dissect the following matrix equation into two different systems of equations: \\[\\pm 1&amp;1\\\\2&amp;1\\mp \\pm x_{11} &amp; x_{12} \\\\ x_{21} &amp; x_{22} \\mp = \\pm 3&amp;3\\\\4&amp;5 \\mp.\\] Based on our previous discussions, we ought to be able to see that this matrix equation represents 4 separate equations which we’ll combine into two systems: \\[\\pm 1&amp;1\\\\2&amp;1\\mp \\pm x_{11} \\\\x_{21} \\mp = \\pm 3\\\\4 \\mp \\quad \\mbox{and}\\quad \\pm 1&amp;1\\\\2&amp;1\\mp \\pm x_{12} \\\\x_{22} \\mp = \\pm 3\\\\5 \\mp\\] Once you convince yourself that the unknowns can be found in this way, let’s take a look at the augmented matrices for these two systems: \\[\\left(\\begin{array}{rr|r} 1&amp;1&amp;3\\\\2&amp;1&amp;4\\end{array}\\right) \\quad\\mbox{and}\\quad \\left(\\begin{array}{rr|r} 1&amp;1&amp;3\\\\2&amp;1&amp;5\\end{array}\\right)\\] When performing Gauss-Jordan elimination on these two augmented matrices, how are the row operations going to differ? They’re not! The same row operations will be used for each augmented matrix - the only thing that will differ is how these row operations will affect the right hand side vectors. Thus, it is possible for us to keep track of those differences in one larger augmented matrix : \\[\\begin{pmatrix} \\begin{array}{cc|cc} 1&amp;1&amp;3&amp;3\\\\ 2&amp;1&amp;4&amp;5 \\end{array} \\end{pmatrix}\\] We can then perform the row operations on both right-hand sides at once: \\[\\begin{pmatrix} \\begin{array}{cc|cc} 1&amp;1&amp;3&amp;3\\\\ 2&amp;1&amp;4&amp;5 \\end{array} \\end{pmatrix}\\xrightarrow{R2&#39;=R2-2R1}\\begin{pmatrix} \\begin{array}{cc|cc} 1&amp;1&amp;3&amp;3\\\\ 0&amp;-1&amp;-2&amp;-1 \\end{array} \\end{pmatrix} \\] \\[\\xrightarrow{R2&#39;=-1R2}\\begin{pmatrix} \\begin{array}{cc|cc} 1&amp;1&amp;3&amp;3\\\\ 0&amp;1&amp;2&amp;1 \\end{array} \\end{pmatrix}\\xrightarrow{R1&#39;=R1-R2}\\begin{pmatrix} \\begin{array}{cc|cc} 1&amp;0&amp;1&amp;2\\\\ 0&amp;1&amp;2&amp;1 \\end{array} \\end{pmatrix}\\] Now again, remembering the situation from which we came, we have the equivalent system: \\[\\pm 1&amp;0\\\\0&amp;1 \\mp \\pm x_{11} &amp; x_{12} \\\\ x_{21} &amp; x_{22} \\mp = \\pm 1&amp;2\\\\2&amp;1\\mp\\] So we can conclude that \\[\\pm x_{11} &amp; x_{12} \\\\ x_{21} &amp; x_{22} \\mp = \\pm 1&amp;2\\\\2&amp;1\\mp\\] and we have solved our system. This method is particularly useful when finding the inverse of a matrix. 5.4.1 Solving for the Inverse of a Matrix For any square matrix \\(\\A\\), we know the inverse matrix (\\(\\A^{-1}\\)), if it exists, satisfies the following matrix equation, \\[\\A\\A^{-1} = \\I.\\] Using the Gauss-Jordan method with multiple right hand sides, we can solve for the inverse of any matrix. We simply start with an augmented matrix with \\(\\A\\) on the left and the identity on the right, and then use Gauss-Jordan elimination to transform the matrix \\(\\A\\) into the identity matrix. \\[\\left(\\begin{array}{r|r} \\bo{A} &amp; \\I\\end{array}\\right)\\xrightarrow{Gauss-Jordan}\\left(\\begin{array}{r|r} \\bo{I} &amp; \\A^{-1}\\end{array}\\right)\\] If this is possible then the matrix on the right is the inverse of \\(\\A\\). If this is not possible then \\(\\A\\) does not have an inverse. Let’s see a quick example of this. Example 5.6 (Finding a Matrix Inverse) Find the inverse of \\[\\A = \\pm -1&amp;2&amp;-1\\\\0&amp;-1&amp;1\\\\2&amp;-1&amp;0 \\mp\\] using Gauss-Jordan Elimination. Since \\(\\A\\A^{-1} = \\I\\), we set up the augmented matrix as \\(\\left(\\begin{array}{r|r} \\bo{A} &amp; \\I\\end{array}\\right)\\): \\[\\begin{pmatrix} \\begin{array}{ccc|ccc}-1&amp;2&amp;-1&amp;1&amp;0&amp;0\\\\0&amp;-1&amp;1&amp;0&amp;1&amp;0\\\\2&amp;-1&amp;0&amp;0&amp;0&amp;1 \\end{array}\\end{pmatrix} \\xrightarrow{R3&#39;=R3+2R1} \\begin{pmatrix} \\begin{array}{ccc|ccc} -1&amp;2&amp;-1&amp;1&amp;0&amp;0\\\\0&amp;-1&amp;1&amp;0&amp;1&amp;0\\\\0&amp;3&amp;-2&amp;2&amp;0&amp;1 \\end{array}\\end{pmatrix}\\] \\[\\begin{pmatrix} \\begin{array}{ccc|ccc} -1&amp;2&amp;-1&amp;1&amp;0&amp;0\\\\0&amp;-1&amp;1&amp;0&amp;1&amp;0\\\\0&amp;3&amp;-2&amp;2&amp;0&amp;1 \\end{array}\\end{pmatrix} \\xrightarrow{\\substack{R1&#39;=-1R1\\\\R3&#39;=R3+3R2}}\\begin{pmatrix}\\begin{array}{ccc|ccc} 1&amp;-2&amp;1&amp;-1&amp;0&amp;0\\\\0&amp;-1&amp;1&amp;0&amp;1&amp;0\\\\0&amp;0&amp;1&amp;2&amp;3&amp;1 \\end{array}\\end{pmatrix}\\] \\[\\begin{pmatrix}\\begin{array}{ccc|ccc} 1&amp;-2&amp;1&amp;-1&amp;0&amp;0\\\\0&amp;-1&amp;1&amp;0&amp;1&amp;0\\\\0&amp;0&amp;1&amp;2&amp;3&amp;1 \\end{array}\\end{pmatrix}\\xrightarrow{\\substack{R1&#39;=R1-R3\\\\R2&#39;=R2-R3}}\\begin{pmatrix}\\begin{array}{ccc|ccc} 1&amp;-2&amp;0&amp;-3&amp;-3&amp;-1\\\\0&amp;-1&amp;0&amp;-2&amp;-2&amp;-1\\\\0&amp;0&amp;1&amp;2&amp;3&amp;1 \\end{array}\\end{pmatrix}\\] \\[\\begin{pmatrix}\\begin{array}{ccc|ccc} 1&amp;-2&amp;0&amp;-3&amp;-3&amp;-1\\\\0&amp;-1&amp;0&amp;-2&amp;-2&amp;-1\\\\0&amp;0&amp;1&amp;2&amp;3&amp;1 \\end{array}\\end{pmatrix}\\xrightarrow{\\substack{R2&#39;=-1R2\\\\R1&#39;=R1+2R2}}\\begin{pmatrix}\\begin{array}{ccc|ccc} 1&amp;0&amp;0&amp;1&amp;1&amp;1\\\\0&amp;1&amp;0&amp;2&amp;2&amp;1\\\\0&amp;0&amp;1&amp;2&amp;3&amp;1 \\end{array}\\end{pmatrix}\\] Finally, we have completed our task. The inverse of \\(\\A\\) is the matrix on the right hand side of the augmented matrix! \\[\\A^{-1} = \\pm 1&amp;1&amp;1\\\\2&amp;2&amp;1\\\\2&amp;3&amp;1 \\mp\\] Exercise 5.3 (Finding a Matrix Inverse) Use the same method to determine the inverse of \\[\\B=\\pm 1&amp;1&amp;1\\\\2&amp;2&amp;1\\\\2&amp;3&amp;1 \\mp\\] (hint: Example 5.6 should tell you the answer you expect to find!) Example 5.7 (Inverse of a Diagonal Matrix) A full rank diagonal matrix (one with no zero diagonal elements) has a particularly neat and tidy inverse. Here we motivate the definition by working through an example. Find the inverse of the digaonal matrix \\(\\D\\), \\[\\D = \\pm 3&amp;0&amp;0\\\\0&amp;-2&amp;0\\\\0&amp;0&amp;\\sqrt{5} \\mp \\] To begin the process, we start with an augmented matrix and proceed with Gauss-Jordan Elimination. In this case, the process is quite simple! The elements above and below the diagonal pivots are already zero, we simply need to make each pivot equal to 1! \\[\\pm\\begin{array}{ccc|ccc} 3&amp;0&amp;0&amp;1&amp;0&amp;0\\\\0&amp;-2&amp;0&amp;0&amp;1&amp;0\\\\0&amp;0&amp;\\sqrt{5}&amp;0&amp;0&amp;1 \\end{array}\\mp \\xrightarrow{\\substack{R1&#39;=\\frac{1}{3}R1 \\\\R2&#39; = -\\frac{1}{2} R2\\\\R3&#39;=\\frac{1}{\\sqrt{5}} R3}} \\pm\\begin{array}{ccc|ccc} 1&amp;0&amp;0&amp;\\frac{1}{3}&amp;0&amp;0\\\\0&amp;1&amp;0&amp;0&amp;-\\frac{1}{2}&amp;0\\\\0&amp;0&amp;1&amp;0&amp;0&amp;\\frac{1}{\\sqrt{5}} \\end{array}\\mp\\] Thus, the inverse of \\(\\D\\) is: \\[\\D^{-1} = \\pm \\frac{1}{3}&amp;0&amp;0\\\\0&amp;-\\frac{1}{2}&amp;0\\\\0&amp;0&amp;\\frac{1}{\\sqrt{5}} \\mp \\] As you can see, all we had to do is take the scalar inverse of each diagonal element! Definition 5.3 (Inverse of a Diagonal Matrix) An \\(n\\times n\\) diagonal matrix \\(\\D = diag\\{d_{11},d_{22},\\dots,d_{nn}\\}\\) with no nonzero diagonal elements is invertible with inverse \\[\\D^{-1} = diag\\{\\frac{1}{d_{11}},\\frac{1}{d_{22}},\\dots,\\frac{1}{d_{nn}}\\}\\] 5.5 Exercises Using Gaussian Elimination on the augmented matrices, reduce each system of equations to a triangular form and solve using back-substitution. \\[\\begin{cases} x_1 +2x_2= 3\\\\ -x_1+x_2=0\\end{cases}\\] \\[\\begin{cases} x_1+x_2 +2x_3= 7\\\\ x_1+x_3 = 4\\\\ -2x_1-2x_2 =-6\\end{cases}\\] \\[\\begin{cases}\\begin{align} 2x_1-x_2 +x_3= 1\\\\ -x_1+2x_2+3x_3 = 6\\\\ x_2+4x_3 =6 \\end{align}\\end{cases}\\] Using Gauss-Jordan Elimination on the augmented matrices, reduce each system of equations from the previous exercise to reduced row-echelon form and give the solution as a vector. Use either Gaussian or Gauss-Jordan Elimination to solve the following systems of equations. Indicate whether the systems have a unique solution, no solution, or infinitely many solutions. If the system has infinitely many solutions, exhibit a general solution in vector form as we did in Section 5.3.3. \\[\\begin{cases}\\begin{align} 2x_1+2x_2+6x_3=4\\\\ 2x_1+x_2+7x_3=6\\\\ -2x_1-6x_2-7x_3=-1\\end{align}\\end{cases}\\] \\[\\begin{cases}\\begin{align} 1x_1+2x_2+2x_3=0\\\\ 2x_1+5x_2+7x_3=0\\\\ 3x_1+6x_2+6x_3=0\\end{align}\\end{cases}\\] \\[\\begin{cases}\\begin{align} 1x_1+3x_2-5x_3=0\\\\ 1x_1-2x_2+4x_3=2\\\\ 2x_1+1x_2-1x_3=0\\end{align}\\end{cases}\\] \\[\\begin{cases}\\begin{align} w-h+l=1\\\\ w-h-l=2\\\\ w+h-l=3\\\\ w+h+l=4\\end{align}\\end{cases}\\] \\[\\begin{cases}\\begin{align} w-h+l=1\\\\ w-h-l=2\\\\ w+h-l=3\\\\ w+h+l=2\\end{align}\\end{cases}\\] \\[\\begin{cases}\\begin{align} x_1+2x_2+2x_3+3x_4=0\\\\ 2x_1+4x_2+x_3+3x_4=0\\\\ 3x_1+6x_2+x_3+4x_4=0\\end{align}\\end{cases}\\] Use Gauss-Jordan Elimination to find the inverse of the following matrices, if possible. \\(\\A=\\pm 2&amp;3\\\\2&amp;2\\mp\\) \\(\\B=\\pm 1&amp;2\\\\2&amp;4\\mp\\) \\(\\C=\\pm 1&amp;2&amp;3\\\\4&amp;5&amp;6\\\\7&amp;8&amp;9\\mp\\) \\(\\D=\\pm 4&amp;0&amp;0\\\\0&amp;-4&amp;0\\\\0&amp;0&amp;2 \\mp\\) What is the inverse of a diagonal matrix, \\(\\bo{D}=diag\\{\\sigma_{1},\\sigma_{2}, \\dots,\\sigma_{n}\\}\\)? Suppose you have a matrix of data, \\(\\A_{n\\times p}\\), containing \\(n\\) observations on \\(p\\) variables. Suppose the standard deviations of these variables are contained in a diagonal matrix \\[\\bo{S}= diag\\{\\sigma_1, \\sigma_2,\\dots,\\sigma_p\\}.\\] Give a formula for a matrix that contains the same data but with each variable divided by its standard deviation. Hint: This problem connects Text Exercise 2.4 and Example 5.7. 5.6 List of Key Terms systems of equations row operations row-echelon form pivot element Gaussian elimination Gauss-Jordan elimination reduced row-echelon form rank unique solution infinitely many solutions inconsistent back-substitution 5.7 Gauss-Jordan Elimination in R It is important that you understand what is happening in the process of Gauss-Jordan Elimination. Once you have a handle on how the procedure works, it is no longer necessary to do every calculation by hand. We can skip to the reduced row echelon form of a matrix using the pracma package in R. \\ We’ll start by creating our matrix as a variable in R. Matrices are entered in as one vector, which R then breaks apart into rows and columns in they way that you specify (with nrow/ncol). The default way that R reads a vector into a matrix is down the columns. To read the data in across the rows, use the byrow=TRUE option). Once a matrix is created, it is stored under the variable name you give it (below, we call our matrices \\(\\Y\\) and \\(\\X\\)). We can then print out the stored matrix by simply typing \\(\\Y\\) or \\(\\X\\) at the prompt: (Y=matrix(c(1,2,3,4),nrow=2,ncol=2)) ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 (X=matrix(c(1,2,3,4),nrow=2,ncol=2,byrow=TRUE)) ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 To perform Gauss-Jordan elimination, we need to install the pracma package which contains the code for this procedure. install.packages(&quot;pracma&quot;) After installing a package in R, you must always add it to your library (so that you can actually use it in the current session). This is done with the library command: library(&quot;pracma&quot;) Now that the library is accessible, we can use the command to get the reduced row echelon form of an augmented matrix, \\(\\A\\): A= matrix(c(1,1,1,1,-1,-1,1,1,1,-1,-1,1,1,2,3,4), nrow=4, ncol=4) A ## [,1] [,2] [,3] [,4] ## [1,] 1 -1 1 1 ## [2,] 1 -1 -1 2 ## [3,] 1 1 -1 3 ## [4,] 1 1 1 4 rref(A) ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 0 ## [2,] 0 1 0 0 ## [3,] 0 0 1 0 ## [4,] 0 0 0 1 And we have the reduced row echelon form for one of the problems from the worksheets! You can see this system of equations is inconsistent because the bottom row amounts to the equation \\[0\\x_1+0\\x_2+0\\x_3 = 1.\\] This should save you some time and energy by skipping the arithmetic steps in Gauss-Jordan Elimination. "],["norms.html", "Chapter 6 Norms, Similarity, and Distance 6.1 Norms and Distances 6.2 Other useful norms and distances 6.3 Inner Products 6.4 Exercises", " Chapter 6 Norms, Similarity, and Distance 6.1 Norms and Distances In applied mathematics, Norms are functions which measure the magnitude or length of a vector. They are commonly used to determine similarities between observations by measuring the distance between them. As we will see, there are many ways to define distance between two points. Definition 6.1 (Vector Norms and Distance Metrics) A Norm, or distance metric, is a function that takes a vector as input and returns a scalar quantity (\\(f: \\Re^n \\to \\Re\\)). A vector norm is typically denoted by two vertical bars surrounding the input vector, \\(\\|\\bo{x}\\|\\), to signify that it is not just any function, but one that satisfies the following criteria: If \\(c\\) is a scalar, then \\[\\|c\\x\\|=|c|\\|x\\|\\] The triangle inequality: \\[\\|\\x+\\bo{y}\\| \\leq \\|\\x\\|+\\|\\bo{y}\\|\\] \\(\\|\\x\\|=0\\) if and only if \\(\\x=0\\). \\(\\|\\x\\|\\geq 0\\) for any vector \\(\\x\\) We will not spend any time on these axioms or on the theoretical aspects of norms, but we will put a couple of these functions to good use in our studies, the first of which is the Euclidean norm or 2-norm. Definition 6.2 (Euclidean Norm, \\(\\|\\star\\|_2\\)) The Euclidean Norm, also known as the 2-norm simply measures the Euclidean length of a vector (i.e. a point’s distance from the origin). Let \\(\\x = (x_1,x_2,\\dots,x_n)\\). Then, \\[\\|\\x\\|_2 = \\sqrt{x_1^2 + x_2^2 + \\dots + x_n^2} \\] If \\(\\x\\) is a column vector, then \\[\\|\\x\\|_2= \\sqrt{\\x^T\\x}.\\] Often we will simply write \\(\\|\\star\\|\\) rather than \\(\\|\\star\\|_2\\) to denote the \\(2\\)-norm, as it is by far the most commonly used norm. This is merely the distance formula from undergraduate mathematics, measuring the distance between the point \\(\\x\\) and the origin. To compute the distance between two different points, say \\(\\x\\) and \\(\\y\\), we’d calculate \\[\\|\\x-\\y\\|_2= \\sqrt{(x_1-y_1)^2 + (x_2-y_2)^2 + \\dots + (x_n-y_n)^2}\\] Example 6.1 (Euclidean Norm and Distance) Suppose I have two vectors in \\(3\\)-space: \\[\\x=(1,1,1) \\mbox{ and } \\bo{y}=(1,0,0)\\] Then the magnitude of \\(\\x\\) (i.e. its length or distance from the origin) is \\[\\|\\x\\|_2=\\sqrt{1^2+1^2+1^2}=\\sqrt{3}\\] and the magnitude of \\(\\bo{y}\\) is \\[\\|\\bo{y}\\|_2=\\sqrt{1^2+0^2+0^2}=1\\] and the distance between point \\(\\x\\) and point \\(\\bo{y}\\) is \\[ \\|\\x-\\bo{y}\\|_2=\\sqrt{(1-1)^2 + (1-0)^2 + (1-0)^2} =\\sqrt{2}.\\] The Euclidean norm is crucial to many methods in data analysis as it measures the closeness of two data points. Thus, to turn any vector into a unit vector, a vector with a length of 1, we need only to divide each of the entries in the vector by its Euclidean norm. This is a simple form of standardization used in many areas of data analysis. For a unit vector \\(\\x\\), \\(\\x^T\\x=1\\). Perhaps without knowing it, we’ve already seen many formulas involving the norm of a vector. Examples 6.2 and 6.3 show how some of the most important concepts in statistics can be represented using vector norms. Example 6.2 (Standard Deviation and Variance) Suppose a group of individuals has the following heights, measured in inches: (60, 70, 65, 50, 55). The mean height for this group is 60 inches. The formula for the sample standard deviation is typically given as \\[s = \\frac{\\sqrt{\\sum_{i=1}^n (x_i-\\bar{x})^2}}{\\sqrt{n-1}}\\] We want to subtract the mean from each observation, square the numbers, sum the result, take the square root and divide by \\(\\sqrt{n-1}\\). If we let \\(\\bar{\\x}=\\bar{x}\\e=(60,60,60,60,60)\\) be a vector containing the mean, and \\(\\x=(60, 70, 65, 50, 55)\\) be the vector of data then the standard deviation in matrix notation is: \\[s=\\frac{1}{\\sqrt{n-1}}\\|\\x-\\bar{\\x}\\|_2=7.9\\] The sample variance of this data is merely the square of the sample standard deviation: \\[s^2 = \\frac{1}{n-1}\\|\\x-\\bar{\\x}\\|_2^2\\] Example 6.3 (Residual Sums of Squares) Another place we’ve seen a similar calculation is in linear regression. You’ll recall the objective of our regression line is to minimize the sum of squared residuals between the predicted value \\(\\hat{y}\\) and the observed value \\(y\\): \\[\\sum_{i=1}^n (\\hat{y}_i-y_i )^2.\\] In vector notation, we’d let \\(\\y\\) be a vector containing the observed data and \\(\\hat{\\y}\\) be a vector containing the corresponding predictions and write this summation as \\[\\|\\hat{\\y}-\\y\\|_2^2\\] In fact, any situation where the phrase “sum of squares” is encountered, the \\(2\\)-norm is generally implicated. Example 6.4 (Coefficient of Determination, \\(R^2\\)) Since variance can be expressed using the Euclidean norm, so can the coefficient of determination or \\(R^2\\). \\[R^2 = \\frac{SS_{reg}}{SS_{tot}}= \\frac{\\sum_{i=1}^n (\\hat{y_i}-\\bar{y})^2}{\\sum_{i=1}^n (y_i-\\bar{y})^2} = \\frac{\\|\\hat{\\y}-\\bar{\\y}\\|^2}{\\|\\y-\\bar{\\y}\\|^2}\\] 6.2 Other useful norms and distances 6.2.1 1-norm, \\(\\|\\star\\|_1\\). If \\(\\x=\\pm x_1 &amp; x_2 &amp; \\dots &amp;x_n \\mp\\) then the \\(1\\)-norm of \\(\\X\\) is \\[\\|\\x\\|_1 = \\sum_{i=1}^n |x_i|.\\] This metric is often referred to as Manhattan distance, city block distance, or taxicab distance because it measures the distance between points along a rectangular grid (as a taxicab must travel on the streets of Manhattan, for example). When \\(\\x\\) and \\(\\y\\) are binary vectors, the \\(1\\)-norm is called the Hamming Distance, and simply measures the number of elements that are different between the two vectors. Figure 6.1: The lengths of the red, yellow, and blue paths represent the 1-norm distance between the two points. The green line shows the Euclidean measurement (2-norm). 6.2.2 \\(\\infty\\)-norm, \\(\\|\\star\\|_{\\infty}\\). The infinity norm, also called the Supremum, or Max distance, is: \\[\\|\\x\\|_{\\infty} = \\max\\{|x_1|,|x_2|,\\dots,|x_p|\\}\\] 6.3 Inner Products The inner product of vectors is a notion that we’ve already seen in Chapter 2, it is what’s called the dot product in most physics and calculus text books. Definition 2.3 (Vector Inner Product) The inner product of two \\(n\\times 1\\) vectors \\(\\x\\) and \\(\\y\\) is written \\(\\x^T\\y\\) (or sometimes as \\(\\langle \\x,\\y \\rangle\\)) and is the sum of the product of corresponding elements. \\[\\x^T\\y = \\pm x_1 &amp; x_2 &amp; \\dots &amp; x_n \\mp \\pm y_1 \\\\y_2 \\\\ \\vdots \\\\ y_n \\mp = x_1y_1+x_2y_2+\\dots+x_ny_n=\\sum_{i=1}^n x_i y_i.\\] When we take the inner product of a vector with itself, we get the square of the 2-norm: \\[\\x^T\\x=\\|\\x\\|_2^2.\\] Inner products are at the heart of every matrix product. When we multiply two matrices, \\(\\X_{m\\times n}\\) and \\(\\bo{Y}_{n\\times p}\\), we can represent the individual elements of the result as inner products of rows of \\(\\X\\) and columns of \\(\\Y\\) as follows: \\[ \\X\\Y = \\pm \\xrow{1} \\\\ \\xrow{2} \\\\ \\vdots \\\\ \\xrow{m} \\mp \\pm \\ycol{1}&amp;\\ycol{2}&amp;\\dots&amp;\\ycol{p} \\mp \\\\ = \\pm \\xrow{1}\\ycol{1} &amp; \\xrow{1}\\ycol{2} &amp; \\dots &amp; \\xrow{1}\\ycol{p} \\\\ \\xrow{2}\\ycol{1} &amp; \\xrow{2}\\ycol{2} &amp; \\dots &amp; \\xrow{2}\\ycol{p} \\\\ \\xrow{3}\\ycol{1} &amp;\\xrow{3}\\ycol{2} &amp;\\dots &amp; \\xrow{3}\\ycol{p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\xrow{m}\\ycol{1} &amp; \\dots &amp; \\ddots &amp; \\xrow{m}\\ycol{p} \\mp \\] 6.3.1 Covariance Another important statistical measurement that is represented by an inner product is covariance. Covariance is a measure of how much two random variables change together. The statistical formula for covariance is given as \\[\\begin{equation} Covariance(\\x,\\y)=E[(\\x-E[\\x])(\\y-E[\\y])] \\tag{6.1} \\end{equation}\\] where \\(E[\\star]\\) is the expected value of the variable. If larger values of one variable correspond to larger values of the other variable and at the same time smaller values of one correspond to smaller values of the other, then the covariance between the two variables is positive. In the opposite case, if larger values of one variable correspond to smaller values of the other and vice versa, then the covariance is negative. Thus, the sign of the covariance shows the tendency of the linear relationship between variables, however the magnitude of the covariance is not easy to interpret. Covariance is a population parameter - it is a property of the joint distribution of the random variables \\(\\x\\) and \\(\\y\\). Definition 6.3 provides the mathematical formulation for the sample covariance. This is our best estimate for the population parameter when we have data sampled from a population. Definition 6.3 (Sample Covariance) If \\(\\x\\) and \\(\\y\\) are \\(n\\times 1\\) vectors containing \\(n\\) observations for two different variables, then the sample covariance of \\(\\x\\) and \\(\\y\\) is given by \\[\\frac{1}{n-1}\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y}) = \\frac{1}{n-1}(\\x-\\bar{\\x})^T(\\y-\\bar{\\y})\\] Where again \\(\\bar{\\x}\\) and \\(\\bar{\\y}\\) are vectors that contain \\(\\bar{x}\\) and \\(\\bar{y}\\) repeated \\(n\\) times. It should be clear from this formulation that \\[cov(\\x,\\y)=cov(\\y,\\x).\\] When we have \\(p\\) vectors, \\(\\v_1,\\v_2,\\dots,\\v_p\\), each containing \\(n\\) observations for \\(p\\) different variables, the sample covariances are most commonly given by the sample covariance matrix, \\(\\ssigma\\), where \\[\\ssigma_{ij}=cov(\\v_i,\\v_j).\\] This matrix is symmetric, since \\(\\ssigma_{ij}=\\ssigma_{ji}\\). If we create a matrix \\(\\V\\) whose columns are the vectors \\(\\v_1,\\v_2,\\dots \\v_p\\) once the variables have been centered to have mean 0, then the covariance matrix is given by: \\[cov(\\V)=\\ssigma = \\frac{1}{n-1}\\V^T\\V.\\] The \\(j^{th}\\) diagonal element of this matrix gives the variance \\(\\v_j\\) since \\[\\begin{eqnarray} \\ssigma_{jj}=cov(\\v_j,\\v_j) &amp;=&amp;\\frac{1}{n-1}(\\v_j-\\bar{\\v}_j)^T(\\v_j-\\bar{\\v}_j) \\\\ &amp;=&amp;\\frac{1}{n-1}\\|\\v_j-\\bar{\\v}_j\\|_2^2\\\\ &amp;=&amp; var(\\v_j) \\end{eqnarray}\\] When two variables are completely uncorrelated, their covariance is zero. This lack of correlation would be seen in a covariance matrix with a diagonal structure. That is, if \\(\\v_1, \\v_2,\\dots, \\v_p\\) are uncorrelated with individual variances \\(\\sigma_1^2,\\sigma_2^2,\\dots,\\sigma_p^2\\) respectively then the corresponding covariance matrix is: \\[\\ssigma = \\pm \\sigma_1^2 &amp; 0 &amp; 0&amp; \\dots &amp; 0\\\\ 0 &amp; \\sigma_2^2 &amp; 0 &amp; \\dots &amp; 0\\\\ 0 &amp; 0 &amp; \\ddots &amp; \\vdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp;\\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma_p^2 \\mp\\] Furthermore, for variables which are independent and identically distributed (take for instance the error terms in a linear regression model, which are assumed to independent and normally distributed with mean 0 and constant variance \\(\\sigma\\)), the covariance matrix is a multiple of the identity matrix: \\[\\ssigma = \\pm \\sigma^2 &amp; 0 &amp; 0&amp; \\dots &amp; 0\\\\ 0 &amp; \\sigma^2 &amp; 0 &amp; \\dots &amp; 0\\\\ 0 &amp; 0 &amp; \\ddots &amp; \\vdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp;\\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma^2 \\mp =\\sigma^2\\bo{I}\\] Transforming our variables in a such a way that their covariance matrix becomes diagonal will be our goal in Chapter 13. Theorem 6.1 (Properties of Covariance Matrices) The following mathematical properties stem from Equation (6.1). Let \\(\\X_{n\\times p}\\) be a matrix of data containing \\(n\\) observations on \\(p\\) variables. If \\(\\A\\) is a constant matrix (or vector, in the first case) then \\[cov(\\X\\A)=\\A^Tcov(\\X)\\A \\quad \\mbox{ and } \\quad cov(\\X+\\A)=cov(\\X)\\] 6.3.2 Mahalanobis Distance Mahalanobis Distance is similar to Euclidean distance, but takes into account the correlation of the variables. This metric is relatively common in data mining applications like classification. Suppose we have \\(p\\) variables which have some covariance matrix, \\(\\cov\\). Then the Mahalanobis distance between two observations, \\(\\x=\\pm x_1&amp; x_2 &amp;\\dots &amp; x_p \\mp^T\\) and \\(\\y = \\pm y_1 &amp; y_2 &amp; \\dots &amp; y_p \\mp^T\\) is given by \\[d(\\x,\\y)=\\sqrt{(\\x-\\y)^T\\cov^{-1}(\\x-\\y)}.\\] If the covariance matrix is diagonal (meaning the variables are uncorrelated) then the Mahalanobis distance reduces to Euclidean distance normalized by the variance of each variable: \\[d(\\x,\\y)=\\sqrt{\\sum_{i=1}^p\\frac{(x_i-y_i)^2}{s_i^2}}=\\|\\cov^{-1/2}(\\x-\\y)\\|_2.\\] 6.3.3 Angular Distance The inner product between two vectors can provide useful information about their relative orientation in space and about their similarity. For example, to find the cosine of the angle between two vectors in \\(n\\)-space, the inner product of their corresponding unit vectors will provide the result. This cosine is often used as a measure of similarity or correlation between two vectors. Definition 6.4 (Cosine of Angle between Vectors) The cosine of the angle between two vectors in \\(n\\)-space is given by \\[\\cos(\\theta)=\\frac{\\x^T\\y}{\\|\\x\\|_2\\|\\y\\|_2}\\] This angular distance is at the heart of Pearson’s correlation coefficient. 6.3.4 Correlation Pearson’s correlation is a normalized version of the covariance, so that not only the sign of the coefficient is meaningful, but its magnitude is meaningful in measuring the strength of the linear association. Example 6.5 (Pearson’s Correlation and Cosine Distance) You may recall the formula for Pearson’s correlation between variable \\(\\x\\) and \\(\\y\\) with a sample size of \\(n\\) to be as follows: \\[r = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}}\\] If we let \\(\\bar{\\x}\\) be a vector that contains \\(\\bar{x}\\) repeated \\(n\\) times, like we did in Example 6.2, and let \\(\\bar{\\y}\\) be a vector that contains \\(\\bar{y}\\) then Pearson’s coefficient can be written as: \\[r=\\frac{(\\x-\\bar{\\x})^T(\\y-\\bar{\\y})}{\\|\\x-\\bar{\\x}\\|\\|\\y-\\bar{\\y}\\|}\\] In other words, it is just the cosine of the angle between the two vectors once they have been centered to have mean 0. This makes sense: correlation is a measure of the extent to which the two variables share a line in space. If the cosine of the angle is positive or negative one, this means the angle between the two vectors is \\(0^{\\circ}\\) or \\(180^{\\circ}\\), thus, the two vectors are perfectly correlated or collinear. It is difficult to visualize the angle between two variable vectors because they exist in \\(n\\)-space, where \\(n\\) is the number of observations in the dataset. Unless we have fewer than 3 observations, we cannot draw these vectors or even picture them in our minds. As it turns out, this angular measurement does translate into something we can conceptualize: Pearson’s correlation coefficient is the angle formed between the two possible regression lines using the centered data: \\(\\y\\) regressed on \\(\\x\\) and \\(\\x\\) regressed on \\(\\y\\). This is illustrated in Figure 6.2. Figure 6.2: Correlation Coefficient \\(r\\) and Angle between Regression Lines To compute the matrix of pairwise correlations between variables \\(\\x_1,\\x_2,\\x_3,\\dots,\\x_p\\) (columns containing \\(n\\) observations for each variable), we’d first center them to have mean zero, then normalize them to have length \\(\\|\\x_i\\|=1\\) and then compose the matrix \\[\\X=[\\x_1|\\x_2|\\x_3|\\dots|\\x_p].\\] Using this centered and normalized data, the correlation matrix is simply \\[\\C=\\X^T\\X.\\] ## Outer Products The outer product of two vectors \\(\\x \\in \\Re^m\\) and \\(\\y \\in \\Re^n\\), written \\(\\x\\y^T\\), is an $mn $ matrix with rank 1. To see this basic fact, lets just look at an example. Example 2.9 (Outer Product) Let \\(\\x=\\pm 1\\\\2\\\\3\\\\4\\mp\\) and let \\(\\y=\\pm2\\\\1\\\\3\\mp\\). Then the outer product of \\(\\x\\) and \\(\\y\\) is: \\[\\x\\y^T = \\pm 1\\\\2\\\\3\\\\4\\mp \\pm 2&amp;1&amp;3\\mp = \\pm 2&amp;1&amp;3\\\\4&amp;2&amp;6\\\\6&amp;3&amp;9\\\\8&amp;4&amp;12 \\mp\\] which clearly has rank 1. It should be clear from this example that computing an outer product will always result in a matrix whose rows and columns are multiples of each other. Example 6.6 (Centering Data with an Outer Product) As we’ve seen in previous examples, many statistical formulas involve the centered data, that is, data from which the mean has been subtracted so that the new mean is zero. Suppose we have a matrix of data containing observations of individuals’ heights (h) in inches, weights (w), in pounds and wrist sizes (s), in inches: \\[\\A=\\bm{ ~ &amp; h &amp; w &amp; s \\cr person_1 &amp; 60 &amp; 102 &amp; 5.5 \\cr person_2 &amp; 72 &amp; 170 &amp; 7.5 \\cr person_3 &amp; 66 &amp; 110 &amp; 6.0\\cr person_4 &amp; 69 &amp; 128 &amp; 6.5\\cr person_5 &amp; 63 &amp; 130 &amp; 7.0\\cr}\\] The average values for height, weight, and wrist size are as follows: \\[\\begin{eqnarray} \\bar{h}&amp;=&amp;66\\\\ \\bar{w}&amp;=&amp;128\\\\ \\bar{s}&amp;=&amp;6.5 \\end{eqnarray}\\] To center all of the variables in this data set simultaneously, we could compute an outer product using a vector containing the means and a vector of all ones: \\[\\pm 60 &amp; 102 &amp; 5.5 \\cr 72 &amp; 170 &amp; 7.5 \\cr 66 &amp; 110 &amp; 6.0\\cr 69 &amp; 128 &amp; 6.5\\cr 63 &amp; 130 &amp; 7.0\\cr \\mp - \\pm 1\\\\1\\\\1\\\\1\\\\1 \\mp \\pm 66 &amp; 128 &amp; 6.5 \\mp\\] \\[= \\pm 60 &amp; 102 &amp; 5.5 \\cr 72 &amp; 170 &amp; 7.5 \\cr 66 &amp; 110 &amp; 6.0\\cr 69 &amp; 128 &amp; 6.5\\cr 63 &amp; 130 &amp; 7.0\\cr \\mp - \\pm 66 &amp; 128 &amp; 6.5 \\\\66 &amp; 128 &amp; 6.5 \\\\66 &amp; 128 &amp; 6.5 \\\\66 &amp; 128 &amp; 6.5 \\\\66 &amp; 128 &amp; 6.5 \\mp\\] \\[= \\pm -6.0000 &amp; -26.0000 &amp; -1.0000\\\\ 6.0000 &amp; 42.0000 &amp; 1.0000\\\\ 0 &amp; -18.0000 &amp; -0.5000\\\\ 3.0000 &amp; 0 &amp; 0\\\\ -3.0000 &amp; 2.0000 &amp; 0.5000 \\mp\\] 6.4 Exercises Let \\(\\u=\\pm 1\\\\2\\\\-4\\\\-2\\mp\\) and \\(\\v=\\pm 1\\\\-1\\\\1\\\\-1\\mp\\). Determine the Euclidean distance between \\(\\u\\) and \\(\\v\\). Find a vector of unit length in the direction of \\(\\u\\). Determine the cosine of the angle between \\(\\u\\) and \\(\\v\\). Find the 1- and \\(\\infty\\)-norms of \\(\\u\\) and \\(\\v\\). Suppose these vectors are observations on four independent variables, which have the following covariance matrix: \\[\\cov=\\pm 2&amp;0&amp;0&amp;0\\\\0&amp;1&amp;0&amp;0\\\\0&amp;0&amp;2&amp;0\\\\0&amp;0&amp;0&amp;1 \\mp\\] Determine the Mahalanobis distance between \\(\\u\\) and \\(\\v\\). Write a matrix expression for the correlation matrix, \\(\\C\\), for a matrix of centered data, \\(\\X\\), where \\(\\C_{ij}=r_{ij}\\) is Pearson’s correlation measure between variables \\(\\x_i\\) and \\(\\x_j\\). To do this, we need more than an inner product, we need to normalize the rows and columns by the norms \\(\\|\\x_i\\|\\). For a hint, revisit the exercises in Chapter 2. Suppose you have a matrix of data, \\(\\A_{n\\times p}\\), containing \\(n\\) observations on \\(p\\) variables. Develop a matrix formula for the standardized data (where the mean of each variable should be subtracted from the corresponding column before dividing by the standard deviation). Hint: use Exercises 1(f) and 4 from Chapter 2 along with Example 6.6. Explain why, for any norm or distance metric, \\[\\|\\x-\\y\\|=\\|\\y-\\x\\|\\] "],["linind.html", "Chapter 7 Linear Independence 7.1 Linear Independence 7.2 Span of Vectors 7.3 Exercises List of Key Terms", " Chapter 7 Linear Independence One of the most central ideas in all of Linear Algebra is that of linear independence. For regression problems, it is repeatedly stressed that multicollinearity is problematic. Multicollinearity is simply a statistical term for linear dependence. It’s bad. Having a firm understanding of linear combinations, we can develop the important concept of linear independence. 7.1 Linear Independence Definition 7.1 (Linear Dependence and Linear Independence) A set of vectors \\(\\{\\v_1,\\v_2,\\dots, \\v_n\\}\\) is linearly dependent if we can express the zero vector, \\(\\bo{0}\\), as non-trivial linear combination of the vectors. In other words there exist some constants \\(\\alpha_1,\\alpha_2,\\dots \\alpha_n\\) (non-trivial means that these constants are not all zero) for which \\[\\begin{equation} \\alpha_1\\v_1 +\\alpha_2\\v_2 + \\dots +\\alpha_n\\v_n=\\bo{0}. \\tag{7.1} \\end{equation}\\] A set of terms is linearly independent if Equation (7.1) has only the trivial solution (\\(\\alpha_1=\\alpha_2=\\dots=\\alpha_n = 0\\)). Another way to express linear dependence is to say that we can write one of the vectors as a linear combination of the others. If there exists a non-trivial set of coefficients \\(\\alpha_1,\\alpha_2, \\dots, \\alpha_n\\) for which \\[\\alpha_1\\v_1 +\\alpha_2\\v_2 + \\dots +\\alpha_n\\v_n=\\bo{0}\\] then for \\(\\alpha_j \\neq 0\\) we could write \\[\\v_j=-\\frac{1}{\\alpha_j} \\sum_{\\substack{i=1\\\\i \\neq j}}^n \\alpha_i \\v_i\\] Example 7.1 (Linearly Dependent Vectors) The vectors \\(\\v_1 =\\pm 1\\\\2\\\\2 \\mp, \\v_2 = \\pm 1\\\\2\\\\3 \\mp, \\mbox{ and } \\v_3 = \\pm 3\\\\6\\\\7 \\mp\\) are linearly dependent because \\[\\v_3 = 2\\v_1+\\v_2\\] or, equivalently, because \\[2\\v_1+\\v_2-\\v_3 = \\bo{0}\\] 7.1.1 Determining Linear Independence You should realize that the linear combination expressed Definition @ref{def:linind} can be written as a matrix vector product. Let \\(\\A_{m\\times n} = (\\A_1|\\A_2|\\dots|\\A_n)\\) be a matrix. Then by Definition @ref{def:linind}, the columns of \\(\\A\\) are linearly independent if and only if the equation \\[\\begin{equation} \\A\\x = \\bo{0} \\tag{7.2} \\end{equation}\\] has only the trivial solution, \\(\\x=0\\). Equation @ref{eq:homo} is commonly known as the homogeneous linear equation. For this equation to have only the trivial solution, it must be the case that under Gauss-Jordan elimination, the augmented matrix \\((\\A|\\bo{0})\\) reduces to \\((\\bo{I}|0)\\). We have already seen this condition in our discussion about matrix inverses - if a square matrix \\(\\A\\) reduces to the identity matrix under Gauss-Jordan elimination then it is equivalently called full rank, nonsingular, or invertible. Now we add an additional condition equivalent to the others - the matrix \\(\\A\\) has linearly independent columns (and rows). In Theorem 7.1 we provide an important list of equivalent conditions regarding the existence of a matrix inverse. Theorem 7.1 (Equivalent Conditions for Matrix Invertibility) Let \\(\\A\\) be an \\(n\\times n\\) matrix - a square matrix. Then the following statements are equivalent. (If one these statements is true, then all of these statements are true) \\(\\A\\) is invertible (\\(\\A^{-1} exists\\)) \\(\\A\\) has full rank (\\(rank(\\A)=n\\)) The columns of \\(\\A\\) are linearly independent The rows of \\(\\A\\) are linearly independent The system \\(\\A\\x=\\b\\), \\(\\b\\neq \\bo{0}\\) has a unique solution \\(\\A\\x=\\bo{0} \\Longrightarrow \\x=\\bo{0}\\) \\(\\A\\) is nonsingular \\(\\A \\xrightarrow{Gauss-Jordan} \\bo{I}\\) 7.2 Span of Vectors Definition 7.2 (Vector Span) The span of a single vector \\(\\v\\) is the set of all scalar multiples of \\(\\v\\): \\[span(\\v)=\\{\\alpha\\v\\ \\mbox{ for any constant } \\alpha\\}\\] The span of a collection of vectors, \\(\\V=\\{\\v_1,\\v_2,\\dots,\\v_n\\}\\) is the set of all linear combinations of these vectors: \\[span(\\V)=\\{\\alpha_1\\v_1+\\alpha_2\\v_2+\\dots+\\alpha_n\\v_n \\mbox{ for any constants }\\alpha_1,\\dots,\\alpha_n\\}\\] Recall that addition of vectors can be done geometrically using the head-to-tail method shown in Figure 7.1. Figure 7.1: Geometrical addition of vectors: Head-to-tail If we have two linearly independent vectors on a coordinate plane, then any third vector can be written as a linear combination of them. This is because two vectors is sufficient to span the entire 2-dimensional plane. You should take a moment to convince yourself of this geometrically. In 3-space, two linearly independent vectors can still only span a plane. Figure 7.2 depicts this situation. The set of all linearly combinations of the two vectors \\(\\a\\) and \\(\\b\\) (i.e. the \\(span(\\a,\\b)\\)) carves out a plane. We call this a two-dimensional collection of vectors a subspace of \\(\\Re^3\\). A subspace is formally defined in Definition 7.3. Figure 7.2: The \\(span(\\a,\\b)\\) in \\(\\Re^3\\) creates a plane (a 2-dimensional subspace) Definition 7.3 (Subspace) A subspace, \\(\\mathcal{S}\\) of \\(\\Re^n\\) is thought of as a “flat” (having no curvature) surface within \\(\\Re^n\\). It is a collection of vectors which satisfies the following conditions: The origin (\\(\\bo{0}\\) vector) is contained in \\(\\mathcal{S}\\) If \\(\\x\\) and \\(\\y\\) are in \\(\\mathcal{S}\\) then the sum \\(\\x+\\y\\) is also in \\(\\mathcal{S}\\) If \\(\\x\\) is in \\(\\mathcal{S}\\) and \\(\\alpha\\) is a constant then \\(\\alpha\\x\\) is also in \\(\\mathcal{S}\\) The span of two vectors \\(\\a\\) and \\(\\b\\) is a subspace because it satisfies these three conditions. (Can you prove it formally? See exercise 4). Example 7.2 (Span) Let \\(\\a=\\pm 1\\\\3\\\\4 \\mp\\) and \\(\\b=\\pm 3\\\\0\\\\1 \\mp\\). Explain why or why not each of the following vectors is contained in the \\(span(\\a,\\b)\\)? \\(\\x=\\pm 5\\\\6\\\\9 \\mp\\) To determine if \\(\\x\\) is in the \\(span(\\a,\\b)\\) we need to find coefficients \\(\\alpha_1, \\alpha_2\\) such that \\[\\alpha_1\\a+\\alpha_2\\b=\\x.\\] Thus, we attempt to solve the system \\[\\pm 1&amp;3\\\\3&amp;0\\\\4&amp;1 \\mp \\pm \\alpha_1\\\\ \\alpha_2 \\mp = \\pm 5\\\\6\\\\9\\mp.\\] After Gaussian Elimination, we find that the system is consistent with the solution \\[\\pm\\alpha_1\\\\ \\alpha_2 \\mp=\\pm 2\\\\1\\mp\\] and so \\(\\x\\) is in fact in the \\(span(\\a,\\b)\\). \\(\\y=\\pm 2\\\\4\\\\6 \\mp\\) We could follow the same procedure as we did in part (a) to learn that the corresponding system is not consistent and thus that \\(\\y\\) is not in the \\(span(\\a,\\b)\\). 7.3 Exercises Six views of matrix multiplication: This notational exercise turns out to contain one of (well, six of) the most fundamentally important concepts to grasp for applied linear algebra. We must be able to intuitively create matrix multiplications from every angle in order for us to be strong practitioners. This is not something that we develop immediately. It comes through practice, visualization, and experience. Do not skip this exercise and keep it in your pocket as you proceed through this book. Let \\(\\A_{m\\times k}\\), \\(\\B_{k\\times n}\\), and \\(\\C_{m\\times n}\\) be matrices such that \\[\\A\\B=\\C.\\] Express the first column of \\(\\C\\) as a linear combination of the columns of \\(\\A\\). Express the first column of \\(\\C\\) as a matrix-vector product. Express \\(\\C\\) as a sum of outer products. Express the first row of \\(\\C\\) as a linear combination of the rows of \\(\\B\\). Express the first row of \\(\\C\\) as a matrix-vector product. Express the element \\(\\C_{ij}\\) as an inner product of row or column vectors from \\(\\A\\) and \\(\\B\\). Determine whether or not the vectors \\[\\x_1=\\pm 1\\\\3\\\\1\\mp,\\x_2=\\pm 0\\\\1\\\\1\\mp,\\x_3=\\pm 2\\\\1\\\\0\\mp\\] are linearly independent. Let \\(\\a=\\pm 1\\\\3\\\\4 \\mp\\) and \\(\\b=\\pm 3\\\\0\\\\1 \\mp\\). Show that the zero vector, \\(\\pm 0\\\\0\\\\0 \\mp\\) is in the \\(span(\\a,\\b)\\). Determine whether or not the vector \\(\\pm 1\\\\0\\\\1 \\mp\\) is in the \\(span(\\a,\\b)\\). Describe the span of one vector in \\(\\Re^3\\). Describe the span of two linearly independent vectors in \\(\\Re^3\\). Describe the span of two linearly dependent vectors in \\(\\Re^3\\). What is the span of the zero vector, \\(\\bo{0}=(0,0,\\dots, 0)\\)? Compare the \\(span \\left{\\pm 1\\\\1 \\mp\\right}\\) to the \\(span \\left{\\pm 1\\\\1 \\mp , \\pm 2\\\\2 \\mp \\right}\\). What is the dimension of the \\(span \\left{\\pm 1\\\\1 \\mp , \\pm 2\\\\2 \\mp \\right}\\) What is the definition of the dimension of a subspace? How would you describe a hyperplane? Draw the \\(span(\\a,\\b)\\) if \\(\\a=\\pm 1\\\\2 \\mp\\) and \\(\\b=\\pm 3\\\\6 \\mp\\). Prove that the span of vectors is a subspace by showing that it satisfies the three conditions from Definition 7.2. To make a formal proof, the strategy should be as follows: (1) Take two arbitrary elements from the span and show that when you add them together, the resulting vector is also in the span. (2) Take one arbitrary vector from the span and show that when you multiply it by a constant, the resulting vector is also in the span. (3) Show that the zero vector is contained in the span. You can simply show this fact for the span of two vectors and notice how the concept will hold for more than two vectors. True/False Mark each statement as true or false. Justify your response. If \\(\\A\\x=\\b\\) has a solution then \\(\\b\\) can be written as a linear combination of the columns of \\(\\A\\). If \\(\\A\\x=\\b\\) has a solution then \\(\\b\\) is in the span of the columns of \\(\\A\\). If \\(\\v_1\\) is in the \\(span(\\v_2,\\v_3)\\), then \\(\\v_1, \\v_2, \\,\\mbox{ and},\\v_3\\) form a linearly independent set. Two vectors are linearly independent only if they are not perfectly correlated, \\(-1&lt;\\rho&lt;1\\), where \\(\\rho\\) is Pearson’s correlation coefficient. List of Key Terms linearly independent linearly dependent full rank perfect multicollinearity severe multicollinearity invertible nonsingular linear combination geometrically linear (in)dependence geometrically vector span subspace dimension of subspace hyperplane "],["basis.html", "Chapter 8 Basis and Change of Basis 8.1 Exercises List of Key Terms", " Chapter 8 Basis and Change of Basis When we think of coordinate pairs, or coordinate triplets, we tend to think of them as points on a grid where each axis represents one of the coordinate directions:\\ Figure 8.1: The Coordinate Plane We may not have previously formalized it, but even in this elementary setting, we are considering these points (vectors) as linear combinations of the elementary basis vectors \\[\\e_1 = \\pm 1\\\\0\\mp \\mbox{ and } \\e_2=\\pm 0\\\\1 \\mp.\\] For example, the point \\((2,3)\\) can be written as \\[\\begin{equation} \\pm 2\\\\3 \\mp = 2\\pm 1\\\\0\\mp+3\\pm 0\\\\1\\mp = 2\\e_1+3\\e_2. \\tag{8.1} \\end{equation}\\] We consider the coefficients (the scalars 2 and 3) in this linear combination as coordinates in the basis \\(\\mathcal{B}_1=\\{\\e_1,\\e_2\\}\\). The coordinates, in essence, tell us how much “information” from the vector/point \\((2 ,3 )\\) lies along each basis direction: to create this point, we must travel 2 units along the direction of \\(\\e_1\\) and then 3 units along the direction of \\(\\e_2\\). We can also view Equation (8.1) as a way to separate the vector \\((2,3)\\) into orthogonal components. Each component is an orthogonal projection of the vector onto the span of the corresponding basis vector. The orthogonal projection of vector \\(\\bo{a}\\) onto the span another vector \\(\\v\\) is simply the closest point to \\(\\bo{a}\\) contained on the span(\\(\\v\\)), found by projecting \\(\\bo{a}\\) onto \\(\\v\\) at a \\(90^\\circ\\) angle. Figure 8.2 shows this explicitly for \\(\\bo{a}=(2,3)\\). Figure 8.2: Orthogonal Projections onto basis vectors Definition 8.1 (Elementary Basis) For any vector \\(\\a=(a_1,a_2,\\dots, a_n)\\), the basis \\(\\mathcal{B} = \\{\\e_1,\\e_2,\\dots,\\e_n\\}\\) (recall \\(\\e_i\\) is the \\(i^{th}\\) column of the identity matrix \\(\\bo{I}_n\\)) is the and \\(\\a\\) can be written in this basis using the \\(a_1,a_2,\\dots, a_n\\) as follows: \\[\\a=a_1\\e_1+a_2\\e_2+\\dots a_n\\e_n.\\] The elementary basis \\(\\mathcal{B}_1\\) is convenient for many reasons, one being its orthonormality: \\[\\begin{eqnarray*} \\e_1^T\\e_1 &amp;=&amp; \\e_2^T\\e_2 = 1\\\\ \\e_1^T\\e_2 &amp;=&amp; \\e_2^T\\e_1 = 0 \\end{eqnarray*}\\] However, there are many (infinitely many, in fact) ways to represent the data points on different axes. If I wanted to view this data in a different way, I could use a different basis. Let’s consider, for example, the following orthonormal basis, drawn in green over the original grid in Figure 8.3: \\[\\mathcal{B}_2 = \\{\\v_1,\\v_2\\}=\\left\\lbrace {\\textstyle\\frac{\\sqrt{2}}{2}}\\pm 1\\\\ 1\\mp ,{\\textstyle\\frac{\\sqrt{2}}{2}}\\pm 1\\\\-1\\mp\\right\\rbrace\\] Figure 8.3: New basis vectors, \\(\\v_1\\) and \\(\\v_2\\), shown on original plane The scalar multipliers \\(\\frac{\\sqrt{2}}{2}\\) are simply normalizing factors so that the basis vectors have unit length. You can convince yourself that this is an orthonormal basis by confirming that \\[\\begin{eqnarray*} \\v_1^T\\v_1 &amp;=&amp; \\v_2^T\\v_2 = 1\\\\ \\v_1^T\\v_2 &amp;=&amp; \\v_2^T\\v_1 = 0 \\end{eqnarray*}\\] If we want to change the basis from the elementary \\(\\mathcal{B}_1\\) to the new green basis vectors in \\(\\mathcal{B}_2\\), we need to determine a new set of coordinates that direct us to the point using the green basis vectors as a frame of reference. In other words we need to determine \\((\\alpha_1,\\alpha_2)\\) such that travelling \\(\\alpha_1\\) units along the direction \\(\\v_1\\) and then \\(\\alpha_2\\) units along the direction \\(\\v_2\\) will lead us to the point in question. For the point \\((2,3)\\) that means \\[ \\pm 2\\\\3 \\mp = \\alpha_1\\v_1+\\alpha_2\\v_2 = \\alpha_1\\pm \\frac{\\sqrt{2}}{2}\\\\ \\frac{\\sqrt{2}}{2}\\mp+\\alpha_2\\pm \\frac{\\sqrt{2}}{2}\\\\ -\\frac{\\sqrt{2}}{2}\\mp . \\] This is merely a system of equations \\(\\bo{V}\\bo{a}=\\b\\): \\[ {\\textstyle\\frac{\\sqrt{2}}{2}}\\pm 1&amp;1 \\\\1&amp; -1\\mp \\pm \\alpha_1\\\\ \\alpha_2 \\mp = \\pm 2\\\\3 \\mp \\] The \\(2\\times 2\\) matrix \\(\\V\\) on the left-hand side has linearly independent columns and thus has an inverse. In fact, \\(\\V\\) is an orthonormal matrix which means its inverse is its transpose. Multiplying both sides of the equation by \\(\\V^{-1}=\\V^T\\) yields the solution \\[\\bo{a} =\\pm \\alpha_1 \\\\ \\alpha_2 \\mp = \\V^T\\b= \\pm \\frac{5\\sqrt{2}}{2} \\\\ -\\frac{\\sqrt{2}}{2} \\mp\\] This result tells us that in order to reach the red point (formerly known as (2,3) in our previous basis), we should travel \\(\\frac{5\\sqrt{2}}{2}\\) units along the direction of \\(\\v_1\\) and then \\(-\\frac{\\sqrt{2}}{2}\\) units along the direction \\(\\v_2\\) (Note that \\(\\v_2\\) points toward the southeast corner and we want to move northwest, hence the coordinate is negative). Another way (a more mathematical way) to say this is that the length of the orthogonal projection of \\(\\bo{a}\\) onto the span of \\(\\v_1\\) is \\(\\frac{5\\sqrt{2}}{2}\\), and the length of the orthogonal projection of \\(\\bo{a}\\) onto the span of \\(\\v_2\\) is \\(-\\frac{\\sqrt{2}}{2}\\). While it may seem that these are difficult distances to plot, they work out quite well if we examine our drawing in Figure 8.3, because the diagonal of each square is \\(\\sqrt{2}\\). In the same fashion, we can re-write all 3 of the red points on our graph in the new basis by solving the same system simultaneously for all the points. Let \\(\\B\\) be a matrix containing the original coordinates of the points and let \\(\\A\\) be a matrix containing the new coordinates: \\[\\B=\\pm -4 &amp; 2 &amp; 5 \\\\ -2 &amp; 3 &amp; 2 \\mp\\,\\, \\A=\\pm \\alpha_{11} &amp; \\alpha_{12} &amp; \\alpha_{13}\\\\ \\alpha_{21} &amp; \\alpha_{22} &amp; \\alpha_{23} \\mp\\] Then the new data coordinates on the rotated plane can be found by solving: \\[\\V\\A=\\B\\] And thus \\[\\A=\\V^T\\B =\\frac{\\sqrt{2}}{2} \\pm -6 &amp; 5 &amp;7\\\\ -2&amp;-1&amp;3\\mp\\] Using our new basis vectors, our alternative view of the data is that in Figure 8.4. Figure 8.4: Points plotted in the new basis, \\(\\mathcal{B}\\) In the above example, we changed our basis from the original elementary basis to a new orthogonal basis which provides a different view of the data. All of this amounts to a rotation of the data around the origin. No real information has been lost - the points maintain their distances from each other in nearly every distance metric. Our new variables, \\(\\v_1\\) and \\(\\v_2\\) are linear combinations of our original variables \\(\\e_1\\) and \\(\\e_2\\), thus we can transform the data back to its original coordinate system by again solving a linear system (in this example, we’d simply multiply the new coordinates again by \\(\\V\\)). In general, we can change bases using the procedure outlined in Theorem 8.1. Theorem 8.1 (Changing Bases) Given a matrix of coordinates (in columns), \\(\\A\\), in some basis, \\(\\mathcal{B}_1=\\{\\x_1,\\x_2,\\dots,\\x_n\\}\\), we can change the basis to \\(\\mathcal{B}_2=\\{\\v_1,\\v_2,\\dots,\\v_n\\}\\) with the new set of coordinates in a matrix \\(\\B\\) by solving the system \\[\\X\\A=\\V\\B\\] where \\(\\X\\) and \\(\\V\\) are matrices containing (as columns) the basis vectors from \\(\\mathcal{B}_1\\) and \\(\\mathcal{B}_2\\) respectively. Note that when our original basis is the elementary basis, \\(\\X=\\bo{I}\\), our system reduces to \\[\\A=\\V\\B.\\] When our new basis vectors are orthonormal, the solution to this system is simply \\[\\B=\\V^T\\A.\\] Definition 8.2 (Basis) A basis for an arbitrary vector space \\(\\mathcal{V}\\) is any set of linearly independent vectors \\(\\{\\mathcal{B}_1,\\dots, \\mathcal{B}_r\\}\\) such that \\[span(\\{\\mathcal{B}_1,\\dots, \\mathcal{B}_r\\}) = \\mathcal{\\mathbf{V}}.\\] \\(r\\) is said to be the dimension of the vector space \\(\\Re^n\\). A basis for the vector space \\(\\Re^n\\) is then any set of \\(n\\) linearly independent vectors in \\(\\Re^n\\); \\(n\\) is said to be the dimension of the vector space \\(\\Re^n\\). When the basis vectors are orthonormal (as in our prior examples), the set is called an orthonormal basis. The preceding discussion dealt entirely with bases for \\(\\Re^n\\) (our example was for points in \\(\\Re^2\\)). However, we will need to consider bases for subspaces of \\(\\Re^n\\). Recall that the span of two linearly independent vectors in \\(\\Re^3\\) is a plane. This plane is a 2-dimensional subspace of \\(\\Re^3\\). Its dimension is 2 because 2 basis vectors are required to represent this space. However, not all points from \\(\\Re^3\\) can be written in this basis - only those points which exist on the plane. Chapter, we will discuss how to proceed in a situation where the point we’d like to represent does not actually belong to the subspace we are interested in. This is the foundation for Least Squares. 8.1 Exercises What are the coordinates of the vector \\(\\x=\\pm 4\\\\3 \\mp\\) in the basis \\(\\left{\\pm -1\\\\-1 \\mp , \\pm 1\\\\-1 \\mp \\right}\\)? Draw a picture to make sure your answer lines up with intuition. In the following picture what would be the signs (+/-) of the coordinates of the green point in the basis \\(\\{\\v_1, \\v_2\\}\\)? Pick another point at random and answer the same question for that point. Write the orthonormal basis vectors from exercise 1 as linear combinations of the original elementary basis vectors. What is the length of the orthogonal projection of \\(\\a_1\\) onto \\(\\v_1\\)? List of Key Terms basis vectors orthonormal basis coordinates in different bases "],["orthog.html", "Chapter 9 Orthogonality 9.1 Orthonormal Basis 9.2 Orthogonal Projection 9.3 Why?? 9.4 Exercises", " Chapter 9 Orthogonality Orthogonal (or perpendicular) vectors have an angle between them of \\(90^{\\circ}\\), meaning that their cosine (and subsequently their inner product) is zero. Definition 9.1 (Orthogonality) Two vectors, \\(\\x\\) and \\(\\y\\), are orthogonal in \\(n\\)-space if their inner product is zero: \\[\\x^T\\y=0\\] Combining the notion of orthogonality and unit vectors we can define an orthonormal set of vectors, or an orthonormal matrix. Remember, for a unit vector, \\(\\x^T\\x = 1\\). Definition 9.2 (Orthonormal Set) The \\(n\\times 1\\) vectors \\(\\{\\x_1,\\x_2,\\x_3,\\dots,\\x_p\\}\\) form an orthonormal set if and only if \\(\\x_i^T\\x_j = 0\\,\\) when \\(i \\ne j\\) and \\(\\x_i^T\\x_i = 1\\,\\) (equivalently \\(\\|\\mathbf{x}_i\\|=1\\)) In other words, an orthonormal set is a collection of unit vectors which are mutually orthogonal. If we form a matrix, \\(\\X=(\\x_1|\\x_2|\\x_3|\\dots|\\x_p )\\), having an orthonormal set of vectors as columns, we will find that multiplying the matrix by its transpose provides a nice result: \\[\\begin{eqnarray*} \\X^T\\X = \\pm \\x_1^T \\\\ \\x_2^T \\\\ \\x_3^T \\\\ \\vdots \\\\ \\x_p^T \\mp \\pm \\x_1&amp;\\x_2&amp;\\x_3&amp;\\dots&amp;\\x_p \\mp &amp;=&amp; \\pm \\x_1^T\\x_1 &amp; \\x_1^T\\x_2 &amp; \\x_1^T\\x_3 &amp; \\dots &amp; \\x_1^T\\x_p \\\\ \\x_2^T\\x_1 &amp; \\x_2^T\\x_2 &amp; \\x_2^T\\x_3 &amp; \\dots &amp; \\x_2^T\\x_p \\\\ \\x_3^T\\x_1 &amp; \\x_3^T\\x_2 &amp; \\x_3^T\\x_3 &amp;\\dots &amp; \\x_3^T\\x_p \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\ddots &amp; \\vdots \\\\ \\x_p^T\\x_1 &amp; \\dots &amp; \\dots &amp; \\ddots &amp; \\x_p^T\\x_p \\mp \\\\ &amp;=&amp; \\pm 1 &amp; 0 &amp; 0 &amp; \\dots &amp; 0\\\\ 0 &amp; 1 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 1 \\mp = \\bo{I}_p \\end{eqnarray*}\\] We will be particularly interested in these types of matrices when they are square. If \\(\\X\\) is a square matrix with orthonormal columns, the arithmetic above means that the inverse of \\(\\X\\) is \\(\\X^T\\) (i.e. \\(\\X\\) also has orthonormal rows): \\[\\X^T\\X=\\X\\X^T = I.\\] Square matrices with orthonormal columns are called orthogonal matrices - this an unfortunate naming mismatch, we agree. Definition 9.3 (Orthogonal Matrix) A square matrix, \\(\\bo{U}\\) with orthonormal columns also has orthonormal rows and is called an orthogonal matrix. Such a matrix has an inverse which is equal to it’s transpose, \\[\\U^T\\U=\\U\\U^T = \\bo{I} \\] 9.1 Orthonormal Basis Our primary interest in orthonormal sets will be in the formulation of new bases for our data. An orthonormal basis has two advantages over other types of bases, with one advantage stemming from each of the two criteria for orthonormality: The basis vectors are mutually perpendicular. They are just rotations of the elementary basis vectors and thus when we examine our data in these new bases, we are merely observing a rotation of our data. This allows us to plot the new coordinates of our data on a plane without much regard for the basis vectors themselves. Basis vectors that are not orthogonal would create a distorted image of our data if we did this. We’d need to factor in the angle between the basis vectors to grasp our previously intuitive notions of distance and similarity. The basis vectors have length 1. So when we look at the coordinates associated with each basis vector, they tell us how many units to go in each direction. This way, again, we can ignore the basis vectors and focus on the coordinates alone. We truly just rotated the axes. These two advantages combine to mean that we can use our new coordinates in place of our original data without any loss of “signal.” We can rotate my multidimensional data cloud, put that rotated data into a regression or clustering model, and then draw conclusions that about my original data points (of course our “variables” (basis vectors) are no longer the same so we wouldn’t be able to draw any conclusions about our original variables from my regression analysis until we un-rotated the data, but the predictions would all match exactly). However, the real power of these advantages are the ease with which we can perform orthogonal projections and the ease with which we can transform from our original space to the new space and back again. 9.2 Orthogonal Projection Think about taking a giant flashlight and shining all of the data onto a subspace at a right angle. The resulting shadow would be the orthogonal projection of the data onto the subspace. Figure 9.1 brings this definition to life. Figure 9.1: Omitting a variable from an orthonormal basis amounts to orthogonal projection onto the span of the other basis vectors. Of course, data can exist below and all around the subspace in question, so it might be helpful to imagine two flashlights or many flashlights that project each data point down to the closest point on the subspace (an orthogonal projection onto a subspace of interest always gets you as close to your original data as possible, under the constraint that the projection be contained in the subspace). When we have a cloud of data and we “drop” one of our variables, geometrically this amounts to an orthogonal projection of the data onto the span of the other axes - reducing the dimensionality of the space in which it exists by 1. Figure 9.2 strives to make this clear. Figure 9.2: Omitting a variable from an orthonormal basis amounts to orthogonal projection onto the span of the other basis vectors. 9.3 Why?? My favorite question. “Whyyy do we have to learn this?!” It’s time to build some intuition toward that question. Consider the following 3-D scatter plot, which is interactive. Turn it around with your mouse and see what you notice. Figure 9.3: 3-Dimensional data cloud that suffers from severe multicollinearity. Does it look like this data is 3-dimensional in nature? It appears that it could be well-summarized if it existed on a plane. However, what is that plane? We can’t merely drop a variable in this instance, as doing so is quite likely to destroy a good proportion of the information from the data. Still, it seems clear that by rotating the data to the right set of axes, we could then squish it down and use 2 coordinates to describe the data without losing much of the information at all. What do we mean by “information?” In this context, using the word “variance” works well. We want to keep as much of the original variance as possible when we squish the cloud down to a plane with an orthogonal projection. Can you find the (approximate) rotation that gives you the best view of the data points? Figure 9.3 is really the jumping off point. Once we can intuitively see why we might benefit from a new basis and how one might be crafted, we’re ready to start digging in to PCA. Once we’ve exposed the basic terminology in Chapters 12 and 13, we can explore the magnificent world of dimension reduction and its many benefits in the case studies in Chapters @ref(PCA_apps_ukfood)-18. 9.4 Exercises Let \\[\\U=\\frac{1}{3}\\pm -1&amp;2&amp;0&amp;-2\\\\2&amp;2&amp;0&amp;1\\\\0&amp;0&amp;3&amp;0\\\\-2&amp;1&amp;0&amp;2\\mp\\] Show that \\(\\U\\) is an orthogonal matrix. Let \\(\\bo{b}=\\pm 1\\\\1\\\\1\\\\1\\mp\\). Solve the equation \\(\\U\\x=\\bo{b}\\). Draw the orthogonal projection of the points onto the subspace \\(span \\left{ \\pm 1\\\\-1\\mp \\right}\\) Are the vectors \\(\\v_1=\\pm 1\\\\-1\\\\1 \\mp\\) and \\(\\v_2=\\pm 0\\\\1\\\\1 \\mp\\) orthogonal? How do you know? What are the two conditions necessary for a collection of vectors to be orthonormal? Show that the vectors \\(\\v_1 = \\pm 3 \\\\ 1\\mp\\) and \\(\\v_2=\\pm -2 \\\\6\\mp\\) are orthogonal. Create an orthonormal basis for \\(\\Re^2\\) using these two direction vectors. Consider \\(\\a_1=(1,1)\\) and \\(\\a_2=(0,1)\\) as coordinates for points in the elementary basis. Write the coordinates of \\(\\a_1\\) and \\(\\a_2\\) in the orthonormal basis found in the previous exercise. Draw a picture which reflects the old and new basis vectors. Briefly explain why an orthonormal basis is important. Find two vectors which are orthogonal to \\(\\x=\\pm 1\\\\1\\\\1\\mp\\) Pythagorean Theorem. Show that \\(\\x\\) and \\(\\y\\) are orthogonal if and only if \\[\\|\\x+\\y\\|_2^2 = \\|\\x\\|_2^2 + \\|\\y\\|_2^2\\] (Hint: Recall that \\(\\|\\x\\|_2^2 = \\x^T\\x\\)) "],["leastsquares.html", "Chapter 10 Least Squares 10.1 Introducing Error 10.2 Why the normal equations?", " Chapter 10 Least Squares 10.1 Introducing Error The least squares problem arises in almost all areas of applied mathematics. In data science, the idea is generally to find an approximate mathematical relationship between predictor and target variables such that the sum of squared errors between the true target values and the predicted target values is minimized. In two dimensions, the goal would be to develop a line as depicted in Figure 10.1 such that the sum of squared vertical distances (the residuals, in green) between the true data (in red) and the mathematical prediction (in blue) is minimized. Figure 10.1: Least Squares Illustrated in 2 dimensions If we let \\(\\bo{r}\\) be a vector containing the residual values \\((r_1,r_2,\\dots,r_n)\\) then the sum of squared residuals can be written in linear algebraic notation as \\[\\sum_{i=1}^n r_i^2 = \\bo{r}^T\\bo{r}=(\\y-\\hat{\\y})^T(\\y-\\hat{\\y}) = \\|\\y-\\hat{\\y}\\|^2\\] Suppose we want to regress our target variable \\(\\y\\) on \\(p\\) predictor variables, \\(\\x_1,\\x_2,\\dots,\\x_p\\). If we have \\(n\\) observations, then the ideal situation would be to find a vector of parameters \\(\\boldsymbol\\beta\\) containing an intercept, \\(\\beta_0\\) along with \\(p\\) slope parameters, \\(\\beta_1,\\dots,\\beta_p\\) such that \\[\\begin{equation} \\tag{10.1} \\underbrace{\\bordermatrix{1&amp;\\x_1 &amp; \\x_2 &amp; \\dots &amp; \\x_p}{obs_1\\\\obs_2\\\\ \\vdots \\\\ obs_n}{\\left(\\begin{matrix} 1 &amp; x_{11} &amp; x_{12} &amp; \\dots &amp; x_{1p}\\\\ 1 &amp; x_{21} &amp; x_{22} &amp; \\dots &amp; x_{2p}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp;\\vdots\\\\ 1 &amp; x_{n1} &amp; x_{n2} &amp; \\dots &amp; x_{np} \\end{matrix}\\right) }}_{\\LARGE\\mathbf{X}} \\underbrace{\\left(\\begin{matrix} \\beta_0\\\\ \\beta_1\\\\ \\vdots \\\\ \\beta_p \\end{matrix}\\right)}_{\\LARGE \\boldsymbol\\beta} = \\underbrace{\\pm y_0\\\\ y_1 \\\\ \\vdots \\\\ y_n \\mp}_{\\LARGE \\y} \\end{equation}\\] With many more observations than variables, this system of equations will not, in practice, have a solution. Thus, our goal becomes finding a vector of parameters \\(\\hat{\\bbeta}\\) such that \\(\\X\\hat{\\bbeta}=\\hat{\\y}\\) comes as close to \\(\\y\\) as possible. Using the design matrix, \\(\\X\\), the least squares solution \\(\\hat{\\boldsymbol\\beta}\\) is the one for which \\[\\|\\y-\\X\\hat{\\boldsymbol\\beta} \\|^2=\\|\\y-\\hat{\\y}\\|^2\\] is minimized. Theorem 10.1 characterizes the solution to the least squares problem. Theorem 10.1 (Least Squares Problem and Solution) For an \\(n\\times m\\) matrix \\(\\X\\) and \\(n\\times 1\\) vector \\(\\y\\), let \\(\\bo{r} = \\X\\widehat{\\boldsymbol\\beta} - \\y\\). The least squares problem is to find a vector \\(\\widehat{\\boldsymbol\\beta}\\) that minimizes the quantity \\[\\sum_{i=1}^n r_i^2 = \\|\\y-\\X\\widehat{\\boldsymbol\\beta} \\|^2.\\] Any vector \\(\\widehat{\\bbeta}\\) which provides a minimum value for this expression is called a least-squares solution. The set of all least squares solutions is precisely the set of solutions to the so-called normal equations, \\[\\X^T\\X\\widehat{\\bbeta} = \\X^T\\y.\\] There is a unique least squares solution if and only if \\(rank(\\X)=m\\) (i.e. linear independence of variables or no perfect multicollinearity!), in which case \\(\\X^T\\X\\) is invertible and the solution is given by \\[\\widehat{\\bbeta} = (\\X^T\\X)^{-1}\\X^T\\y\\] Example 10.1 (Solving a Least Squares Problem) In 2014, data was collected regarding the percentage of linear algebra exercises done by students and the grade they received on their examination. Based on this data, what is the expected effect of completing an additional 10% of the exercises on a students exam grade?\\ To find the least squares regression line, we want to solve the equation \\(\\X\\bbeta = \\y\\): \\[ \\pm 1 &amp; 20\\\\ 1 &amp; 100\\\\ 1 &amp; 90\\\\ 1 &amp; 70\\\\ 1 &amp; 50\\\\ 1 &amp; 10\\\\ 1 &amp; 30\\mp \\pm \\beta_0 \\\\ \\beta_1 \\mp = \\pm 55\\\\100\\\\100\\\\70\\\\75\\\\25\\\\60 \\mp \\] This system is obviously inconsistent. Thus, we want to find the least squares solution \\(\\hat{\\bbeta}\\) by solving \\(\\X^T\\X\\hat{\\bbeta}=\\X^T\\y\\): \\[\\begin{eqnarray} \\small \\pm 1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1 \\\\20&amp;100&amp;90&amp;70&amp;50&amp;10&amp;30 \\mp\\pm 1 &amp; 20\\\\ 1 &amp; 100\\\\ 1 &amp; 90\\\\ 1 &amp; 70\\\\ 1 &amp; 50\\\\ 1 &amp; 10\\\\ 1 &amp; 30\\mp \\pm \\beta_0 \\\\ \\beta_1 \\mp &amp;=&amp;\\pm 1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1 \\\\20&amp;100&amp;90&amp;70&amp;50&amp;10&amp;30 \\mp \\pm 55\\\\100\\\\100\\\\70\\\\75\\\\25\\\\60 \\mp \\cr \\pm 7 &amp; 370\\\\370&amp;26900 \\mp \\pm \\beta_0 \\\\ \\beta_1 \\mp &amp;=&amp; \\pm 485 \\\\ 30800\\mp \\end{eqnarray}\\] Now, since multicollinearity was not a problem, we can simply find the inverse of \\(\\X^T\\X\\) and multiply it on both sides of the equation: \\[\\pm 7 &amp; 370\\\\370&amp;26900 \\mp^{-1}= \\pm 0.5233 &amp; -0.0072\\\\ -0.0072 &amp; 0.0001 \\mp\\] and so \\[\\pm \\beta_0 \\\\ \\beta_1 \\mp = \\pm 0.5233 &amp; -0.0072\\\\ -0.0072 &amp; 0.0001 \\mp \\pm 485 \\\\ 30800\\mp = \\pm 32.1109 \\\\0.7033\\mp\\] Thus, for each additional 10% of exercises completed, exam grades are expected to increase by about 7 points. The data along with the regression line \\[grade=32.1109+0.7033percent\\_exercises\\] is shown below. 10.2 Why the normal equations? The normal equations can be derived using matrix calculus (demonstrated at the end of this section) but the solution of the normal equations also has a nice geometrical interpretation. It involves the idea of orthogonal projection, a concept which will be useful for understanding future topics. 10.2.1 Geometrical Interpretation In order for a system of equations, \\(\\A\\x=\\b\\) to have a solution, \\(\\b\\) must be a linear combination of columns of \\(\\A\\). That is simply the definition of matrix multiplication and equality. If \\(\\A\\) is \\(m\\times n\\) then \\[\\A\\x=\\b \\Longrightarrow \\b = x_1\\A_1+x_2\\A_2+\\dots+x_n\\A_n.\\] As discussed in Chapter 7, another way to say this is that \\(\\b\\) is in the \\(span\\) of the columns of \\(\\A\\). The \\(span\\) of the columns of \\(\\A\\) is called the column space of \\(\\A\\). In Least-Squares applications, the problem is that \\(\\b\\) is not in the column space of \\(\\A\\). In essence, we want to find the vector \\(\\hat{\\b}\\) that is closest to \\(\\b\\) but exists in the column space of \\(\\A\\). Then we know that \\(\\A\\hat{\\x}=\\hat{\\b}\\) does have a unique solution, and that the right hand side of the equation comes as close to the original data as possible. By multiplying both sides of the original equation by \\(\\A^T\\) what we are really doing is projecting \\(\\b\\) orthogonally onto the column space of \\(\\A\\). We should think of the column space as a flat surface (perhaps a plane) in space, and \\(\\b\\) as a point that exists off of that flat surface. There are many ways to draw a line from a point to plane, but the shortest distance would always be travelled perpendicular (orthogonal) to the plane. You may recall from undergraduate calculus or physics that a normal vector to a plane is a vector that is orthogonal to that plane. The normal equations, \\(\\A^T\\A\\x=\\A^T\\b\\), help us find the closest point to \\(\\b\\) that belongs to the column space of \\(\\A\\) by means of an orthogonal projection. This geometrical vantage point is depicted in Figure 10.2. Figure 10.2: The normal equations yield the vector \\(\\hat{\\b}\\) in the column space of \\(\\A\\) which is closest to the original right hand side \\(\\b\\) vector. 10.2.2 Calculus Derivation If you’ve taken a course in undergraduate calculus, you recall that finding minima and maxima of functions typically involves taking their derivatives and setting them equal to zero. That approach to the derivation of the normal equations will be fruitful, but we first need to understand how to take derivatives of matrix equations. Without teaching vector calculus, we will simply provide the following required formulas for matrix derivatives. If you’ve taken some undergraduate calculus, perhaps you’ll see some parallels. While \\(\\frac{\\partial}{\\partial \\mathbf{x}}\\) would commonly be the formula reported, we’ve swapped out the \\(\\mathbf{x}\\) for \\(\\boldsymbol \\beta\\) in the table below in an effort to make our current problem more recognizable. Condition Formula \\(\\mathbf{a}\\) is not a function of \\(\\boldsymbol \\beta\\) \\(\\frac{\\partial \\mathbf{a}}{\\partial \\boldsymbol \\beta}= \\mathbf{0}\\) \\(\\mathbf{a}\\) is not a function of \\(\\boldsymbol \\beta\\) \\(\\frac{\\partial \\boldsymbol \\beta}{\\partial \\boldsymbol \\beta}= \\mathbf{I}\\) \\(\\A\\) is not a function of \\(\\boldsymbol \\beta\\) \\(\\frac{\\partial \\boldsymbol \\beta^T\\mathbf{A}}{\\partial \\boldsymbol \\beta}= \\mathbf{A}T\\) \\(\\A\\) is not a function of \\(\\boldsymbol \\beta\\) \\(\\frac{\\partial \\boldsymbol \\beta^T\\mathbf{A}\\boldsymbol \\beta}{\\partial \\boldsymbol \\beta}= (\\mathbf{A}+\\mathbf{A}^T)\\boldsymbol \\beta\\) \\(\\A\\) is not a function of \\(\\boldsymbol \\beta\\) \\(\\A\\) is symmetric \\(\\frac{\\partial \\boldsymbol \\beta^T\\mathbf{A}\\boldsymbol \\beta}{\\partial \\boldsymbol \\beta}= 2\\mathbf{A}\\boldsymbol \\beta\\) Now let’s start with our objective, which is to minimize sum of squared error, by writing it as the inner product of the vector of residuals with itself: \\[\\boldsymbol \\varepsilon^T \\boldsymbol \\varepsilon = (\\mathbf{y}-\\mathbf{X}\\boldsymbol \\beta)^T(\\mathbf{y}-\\mathbf{X}\\boldsymbol \\beta)\\] We’d like to minimize this function with respect to \\(\\boldsymbol \\beta\\), our vector of unknowns. Thus, our procedure will be to take the derivative with respect to \\(\\boldsymbol \\beta\\) and set it equal to 0. Note, on the second line of this proof, we take advantage of the fact that \\(\\mathbf{a}^T\\mathbf{b} = \\mathbf{b}^T\\mathbf{a}\\) for all \\(\\mathbf{a},\\mathbf{b} \\in \\Re^n\\) \\[\\begin{eqnarray*} \\frac{\\partial}{\\partial \\boldsymbol \\beta} \\left(\\boldsymbol \\varepsilon^T \\boldsymbol \\varepsilon\\right) &amp;=&amp; \\frac{\\partial}{\\partial \\boldsymbol \\beta} \\left(\\mathbf{y}^T\\mathbf{y} - \\mathbf{y}^T(\\mathbf{X}\\boldsymbol \\beta) - (\\mathbf{X}\\boldsymbol \\beta)^T\\mathbf{y} + (\\mathbf{X}\\boldsymbol \\beta)^T(\\mathbf{X}\\boldsymbol \\beta)\\right)\\\\ &amp;=&amp; \\frac{\\partial}{\\partial \\boldsymbol \\beta} \\left(\\mathbf{y}^T\\mathbf{y} -2\\mathbf{y}^T(\\mathbf{X}\\boldsymbol \\beta) + \\boldsymbol \\beta\\mathbf{X}^T\\mathbf{X} \\boldsymbol \\beta \\right)\\\\ &amp;=&amp; 0 - 2\\mathbf{X}^Ty + 2\\mathbf{X}^T\\mathbf{X}\\boldsymbol \\beta \\end{eqnarray*}\\] Setting the last line equal to zero and solving for \\(\\boldsymbol \\beta\\) completes the derivation. \\(\\Box\\) In Chapter 11, we’ll take a deeper dive into the utility of least squares for applied data science. "],["lsapp.html", "Chapter 11 Applications of Least Squares 11.1 Simple Linear Regression 11.2 Multiple Linear Regression", " Chapter 11 Applications of Least Squares 11.1 Simple Linear Regression 11.1.1 Cars Data The `cars’ dataset is included in the datasets package. This dataset contains observations of speed and stopping distance for 50 cars. We can take a look at the summary statistics by using the summary function. summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 We can plot these two variables as follows: plot(cars) 11.1.2 Setting up the Normal Equations Let’s set up a system of equations \\[\\mathbf{X}\\boldsymbol\\beta=\\mathbf{y}\\] to create the model \\[stopping\\_distance=\\beta_0+\\beta_1speed.\\] To do this, we need a design matrix \\(\\mathbf{X}\\) containing a column of ones for the intercept term and a column containing the speed variable. We also need a vector \\(\\mathbf{y}\\) containing the corresponding stopping distances. The model.matrix() function will be of use to us here. model.matrix() takes a formula and data matrix as input and exports the matrix that we represent as \\(\\mathbf{X}\\) in the normal equations. For datasets with categorical (factor) inputs, this function would also create dummy variables for each level, leaving out a reference level by default. You can override the default to leave out a reference level (when you override this default, you one-hot-encode your categorical variable) by including the following option as a third input to the function: contrasts.arg = lapply(starwars[,sapply(starwars,is.factor) ], contrasts, contrasts=FALSE # Create matrix X and label the columns X=model.matrix(dist~speed, data=cars) # Create vector y and label the column y=cars$dist Let’s print the first 10 rows of each to see what we did: # Show first 10 rows, all columns. To show only observations 2,4, and 7, for # example, the code would be X[c(2,4,7), ] X[1:10, ] ## (Intercept) speed ## 1 1 4 ## 2 1 4 ## 3 1 7 ## 4 1 7 ## 5 1 8 ## 6 1 9 ## 7 1 10 ## 8 1 10 ## 9 1 10 ## 10 1 11 y[1:10] ## [1] 2 10 4 22 16 10 18 26 34 17 11.1.3 Solving for Parameter Estimates and Statistics Now lets find our parameter estimates by solving the normal equations, \\[\\mathbf{X}^T\\mathbf{X}\\boldsymbol\\beta = \\mathbf{X}^T\\mathbf{y}\\] using the built in solve function. To solve the system \\(\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\) we’d use solve(A,b). (beta=solve(t(X) %*% X ,t(X)%*%y)) ## [,1] ## (Intercept) -17.579095 ## speed 3.932409 At the same time we can compute the residuals, \\[\\mathbf{r}=\\mathbf{y}-\\mathbf{\\hat{y}}\\] the total sum of squares (SST), \\[\\sum_{i=1}^n (y-\\bar{y})^2=(\\mathbf{y}-\\mathbf{\\bar{y}})^T(\\mathbf{y}-\\mathbf{\\bar{y}})=\\|\\mathbf{y}-\\mathbf{\\bar{y}}\\|^2\\] the regression sum of squares (SSR or SSM) \\[\\sum_{i=1}^n (\\hat{y}-\\bar{y})^2=(\\mathbf{\\hat{y}}-\\mathbf{\\bar{y}})^T(\\mathbf{\\hat{y}}-\\mathbf{\\bar{y}})=\\|\\mathbf{\\hat{y}}-\\mathbf{\\bar{y}}\\|^2\\] the residual sum of squares (SSE) \\[\\sum_{i=1}^n r_i =\\mathbf{r}^T\\mathbf{r}=\\|\\mathbf{r}\\|^2\\] and the unbiased estimator of the variance of the residuals, using the model degrees of freedom which is \\(n-2=48\\): \\[\\widehat{\\sigma_{\\varepsilon}}^2 =\\frac{SSE}{d.f.} = \\frac{\\|\\mathbf{r}\\|^2}{48}\\] Then \\(R^2\\): \\[R^2 = \\frac{SSR}{SST}\\] We can also compute the standard error of \\(\\widehat{\\boldsymbol\\beta}\\) since \\[\\begin{eqnarray*} \\widehat{\\boldsymbol\\beta} &amp;=&amp; (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\\\ var(\\widehat{\\boldsymbol\\beta})&amp;=&amp;var((\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y})\\\\ &amp;=&amp;(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T var(\\mathbf{y}) \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\ &amp;=&amp;(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T (\\widehat{\\sigma_{\\varepsilon}}^2) \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\ &amp;=&amp; \\widehat{\\sigma_{\\varepsilon}}^2 (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\ &amp;=&amp; \\widehat{\\sigma_{\\varepsilon}}^2(\\mathbf{X}^T\\mathbf{X})^{-1} \\end{eqnarray*}\\] The variances of each \\(\\widehat\\beta\\) are given by the diagonal elements of their covariance matrix (see Definition 6.3), and the standard errors of each \\(\\widehat\\beta\\) are thus obtained by taking the square roots of these diagonal elements: \\[s.e.(\\widehat{\\beta_i})=\\sqrt{\\widehat{\\sigma_{\\varepsilon}}[(\\mathbf{X}^T\\mathbf{X})^{-1}]_{ii}}\\] meany=mean(y) XXinv=solve(t(X)%*%X) yhat=X%*%XXinv%*%t(X)%*%y resid=y-yhat SStotal=norm(y-meany,type=&quot;2&quot;)^2 ### OR SStotal=t(y-meany)%*%(y-meany) SSreg=norm(yhat-meany,type=&quot;2&quot;)^2 ### OR SSreg=t(yhat-meany)%*%(yhat-meany) SSresid=norm(resid,type=&quot;2&quot;)^2 ### OR SSresid=t(resid)%*%resid Rsquared=SSreg/SStotal StdErrorResiduals=norm(resid/sqrt(48), type=&quot;2&quot;) #=sqrt(SSresid/48) CovBeta=SSresid*XXinv/48 StdErrorIntercept = sqrt(CovBeta[1,1]) StdErrorSlope = sqrt(CovBeta[2,2]) ## [1] &quot;Rsquared: 0.651079380758252&quot; ## [1] &quot;SSresid: 11353.5210510949&quot; ## [1] &quot;SSmodel: 21185.4589489052&quot; ## [1] &quot;StdErrorResiduals: 15.3795867488199&quot; ## [1] &quot;StdErrorIntercept: 6.75844016937923&quot; ## [1] &quot;StdErrorIntercept: 0.415512776657122&quot; Let’s plot our regression line over the original data: plot(cars) abline(beta[1],beta[2],col=&#39;blue&#39;) 11.1.4 OLS in R via lm() Finally, let’s compare our results to the built in linear model solver, lm(): fit = lm(dist ~ speed, data=cars) summary(fit) ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.069 -9.525 -2.272 9.215 43.201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.5791 6.7584 -2.601 0.0123 * ## speed 3.9324 0.4155 9.464 1.49e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.38 on 48 degrees of freedom ## Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 ## F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 anova(fit) ## Analysis of Variance Table ## ## Response: dist ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## speed 1 21186 21185.5 89.567 1.49e-12 *** ## Residuals 48 11354 236.5 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 11.2 Multiple Linear Regression 11.2.1 Bike Sharing Dataset "],["eigen.html", "Chapter 12 Eigenvalues and Eigenvectors 12.1 Diagonalization 12.2 Geometric Interpretation of Eigenvalues and Eigenvectors 12.3 Exercises", " Chapter 12 Eigenvalues and Eigenvectors Eigenvalues and eigenvectors are (scalar, vector)-pairs that form the “essence” of a matrix. The prefix eigen- is adopted from the German word eigen which means “characteristic, inherent, own” and was introduced by David Hilbert in 1904, but the study of these characteristic directions and magnitudes dates back to Euler’s study of the rotational motion of rigid bodies in the \\(18^{th}\\) century. Definition 12.1 (Eigenvalues and Eigenvectors) For a square matrix \\(\\A_{n\\times n}\\), a scalar \\(\\lambda\\) is called an eigenvalue of \\(\\A\\) if there is a nonzero vector \\(\\x\\) such that \\[\\A\\x=\\lambda\\x.\\] Such a vector, \\(\\x\\) is called an eigenvector of \\(\\A\\) corresponding to the eigenvalue \\(\\lambda\\). We sometimes refer to the pair \\((\\lambda,\\x)\\) as an eigenpair. Eigenvalues and eigenvectors have numerous applications from graphic design to quantum mechanics to geology to epidemiology. The main application of note for data scientists is Principal Component Analysis, but we will also see eigenvalue equations used in social network analysis to determine important players in a network and to detect communities in the network. Before we dive into those applications, let’s first get a handle on the definition by exploring some examples. Example 12.1 (Eigenvalues and Eigenvectors) Determine whether \\(\\x=\\pm 1\\\\1 \\mp\\) is an eigenvector of \\(\\A=\\pm 3 &amp; 1 \\\\1&amp;3 \\mp\\) and if so, find the corresponding eigenvalue.\\ To determine whether \\(\\x\\) is an eigenvector, we want to compute \\(\\A\\x\\) and observe whether the result is a multiple of \\(\\x\\). If this is the case, then the multiplication factor is the corresponding eigenvalue: \\[\\A\\x=\\pm 3 &amp; 1 \\\\1&amp;3 \\mp \\pm 1\\\\1 \\mp =\\pm 4\\\\4 \\mp=4\\pm 1\\\\1 \\mp\\] From this it follows that \\(\\x\\) is an eigenvector of \\(\\A\\) and the corresponding eigenvalue is \\(\\lambda = 4\\).\\ Is the vector \\(\\y=\\pm 2\\\\2 \\mp\\) an eigenvector? \\[\\A\\y=\\pm 3 &amp; 1 \\\\1&amp;3 \\mp \\pm 2\\\\2 \\mp =\\pm 8\\\\8 \\mp=4\\pm 2\\\\2 \\mp = 4\\y\\] Yes, it is and it corresponds to the same eigenvalue, \\(\\lambda=4\\) Example 12.1 shows a very important property of eigenvalue-eigenvector pairs. If \\((\\lambda,\\x)\\) is an eigenpair then any scalar multiple of \\(\\x\\) is also an eigenvector corresponding to \\(\\lambda\\). To see this, let \\((\\lambda,\\x)\\) be an eigenpair for a matrix \\(\\A\\) (which means that \\(\\A\\x=\\lambda\\x\\)) and let \\(\\y=\\alpha\\x\\) be any scalar multiple of \\(\\x\\). Then we have, \\[\\A\\y = \\A(\\alpha\\x)=\\alpha(\\A\\x) = \\alpha(\\lambda\\x) = \\lambda(\\alpha\\x)=\\lambda\\y\\] which shows that \\(\\y\\) (or any scalar multiple of \\(\\x\\)) is also an eigenvector associated with the eigenvalue \\(\\lambda\\). Thus, for each eigenvalue we have infinitely many eigenvectors. In the preceding example, the eigenvectors associated with \\(\\lambda = 4\\) will be scalar multiples of \\(\\x=\\pm 1\\\\1 \\mp\\). You may recall from Chapter 7 that the set of all scalar multiples of \\(\\x\\) is denoted \\(span(\\x)\\). The \\(span(\\x)\\) in this example represents the eigenspace of \\(\\lambda\\). Note: when using software to compute eigenvectors, it is standard practice for the software to provide the normalized/unit eigenvector. In some situations, an eigenvalue can have multiple eigenvectors which are linearly independent. The number of linearly independent eigenvectors associated with an eigenvalue is called the geometric multiplicity of the eigenvalue. Example 12.2 clarifies this concept. Example 12.2 (Geometric Multiplicity) Consider the matrix \\(\\A=\\pm 3 &amp; 0 \\\\0 &amp; 3 \\mp\\). It should be straightforward to see that \\(\\x_1 =\\pm 1 \\\\0 \\mp\\) and \\(\\x_2=\\pm 0\\\\1\\mp\\) are both eigenvectors corresponding to the eigenvalue \\(\\lambda = 3\\). \\(\\x_1\\) and \\(\\x_2\\) are linearly independent, therefore the geometric multiplicity of \\(\\lambda=3\\) is 2.\\ What happens if we take a linear combination of \\(\\x_1\\) and \\(\\x_2\\)? Is that also an eigenvector? Consider \\(\\y=\\pm 2 \\\\ 3 \\mp = 2\\x_1+3\\x_2\\). Then \\[\\A\\y = \\pm 3 &amp; 0 \\\\0 &amp; 3 \\mp \\pm 2 \\\\ 3 \\mp = \\pm 6 \\\\ 9 \\mp = 3 \\pm 2\\\\3\\mp = 3\\y\\] shows that \\(\\y\\) is also an eigenvector associated with \\(\\lambda=3\\). The eigenspace corresponding to \\(\\lambda=3\\) is the set of all linear combinations of \\(\\x_1\\) and \\(\\x_2\\), i.e. the \\(span(\\x_1,\\x_2)\\). We can generalize the result that we saw in Example 12.2 for any square matrix and any geometric multiplicity. Let \\(\\A_{n\\times n}\\) have an eigenvalue \\(\\lambda\\) with geometric multiplicity \\(k\\). This means there are \\(k\\) linearly independent eigenvectors, \\(\\x_1,\\x_2,\\dots,\\x_k\\) such that \\(\\A\\x_i=\\lambda\\x_i\\) for each eigenvector \\(\\x_i\\). Now if we let \\(\\y\\) be a vector in the \\(span(\\x_1,\\x_2,\\dots,\\x_k)\\) then \\(\\y\\) is some linear combination of the \\(\\x_i\\)’s: \\[\\y=\\alpha_1\\x_2+\\alpha_2\\x_2+\\dots+\\alpha_k\\x_k\\] Observe what happens when we multiply \\(\\y\\) by \\(\\A\\): \\[\\begin{eqnarray*} \\A\\y &amp;=&amp;\\A(\\alpha_1\\x_2+\\alpha_2\\x_2+\\dots+\\alpha_k\\x_k) \\\\ &amp;=&amp; \\alpha_1(\\A\\x_1)+\\alpha_2(\\A\\x_2)+\\dots +\\alpha_k(\\A\\x_k) \\\\ &amp;=&amp; \\alpha_1(\\lambda\\x_1)+\\alpha_2(\\lambda\\x_2)+\\dots +\\alpha_k(\\lambda\\x_k) \\\\ &amp;=&amp; \\lambda(\\alpha_1\\x_2+\\alpha_2\\x_2+\\dots+\\alpha_k\\x_k) \\\\ &amp;=&amp; \\lambda\\y \\end{eqnarray*}\\] which shows that \\(\\y\\) (or any vector in the \\(span(\\x_1,\\x_2,\\dots,\\x_k)\\)) is an eigenvector of \\(\\A\\) corresponding to \\(\\lambda\\). This proof allows us to formally define the concept of an eigenspace. Definition 12.2 (Eigenspace) Let \\(\\A\\) be a square matrix and let \\(\\lambda\\) be an eigenvalue of \\(\\A\\). The set of all eigenvectors corresponding to \\(\\lambda\\), together with the zero vector, is called the eigenspace of \\(\\lambda\\). The number of basis vectors required to form the eigenspace is called the geometric multiplicity of \\(\\lambda\\). Now, let’s attempt the eigenvalue problem from the other side. Given an eigenvalue, we will find the corresponding eigenspace in Example 12.3. Example 12.3 (Eigenvalues and Eigenvectors) Show that \\(\\lambda=5\\) is an eigenvalue of \\(\\A=\\pm 1 &amp; 2 \\\\4&amp;3 \\mp\\) and determine the eigenspace of \\(\\lambda=5\\).\\ Attempting the problem from this angle requires slightly more work. We want to find a vector \\(\\x\\) such that \\(\\A\\x=5\\x\\). Setting this up, we have: \\[\\A\\x = 5\\x.\\] What we want to do is move both terms to one side and factor out the vector \\(x\\). In order to do this, we must use an identity matrix, otherwise the equation wouldn’t make sense (we’d be subtracting a constant from a matrix). \\[\\begin{eqnarray*} \\A\\x-5\\x &amp;=&amp; \\bo{0}\\\\ (\\A-5\\bo{I})\\x &amp;=&amp; \\bo{0} \\\\ \\left( \\pm 1 &amp; 2 \\\\4&amp;3 \\mp - \\pm 5 &amp; 0 \\\\ 0 &amp; 5 \\mp \\right) \\pm x_1 \\\\ x_2 \\mp &amp;=&amp; \\pm 0 \\\\0 \\mp \\\\ \\pm -4 &amp; 2\\\\ 4 &amp; -2 \\mp \\pm x_1 \\\\ x_2 \\mp &amp;=&amp; \\pm 0 \\\\0 \\mp \\\\ \\end{eqnarray*}\\] Clearly, the matrix \\(\\A-\\lambda\\bo{I}\\) is singular (i.e. does not have linearly independent rows/columns). This will always be the case by the definition \\(\\A\\x=\\lambda\\x\\), and is often used as an alternative definition.\\ In order to solve this homogeneous system of equations, we use Gaussian elimination: \\[\\left(\\begin{array}{rr|r} -4 &amp; 2 &amp; 0 \\\\4 &amp; -2 &amp; 0 \\end{array}\\right)\\longrightarrow\\left(\\begin{array}{rr|r} 1 &amp; -\\frac{1}{2} &amp; 0 \\\\0 &amp; 0 &amp; 0 \\end{array}\\right)\\] This implies that any vector \\(\\x\\) for which \\(x_1-\\frac{1}{2}\\x_2=0\\) satisfies the eigenvector equation. We can pick any such vector, for example \\(\\x=\\pm 1\\\\2\\mp\\), and say that the eigenspace of \\(\\lambda=5\\) is \\[span\\left\\lbrace\\pm 1\\\\2 \\mp\\right\\rbrace\\] If we didn’t know either an eigenvalue or eigenvector of \\(\\A\\) and instead wanted to find both, we would first find eigenvalues by determining all possible \\(\\lambda\\) such that \\(\\A-\\lambda\\bo{I}\\) is singular and then find the associated eigenvectors. There are some tricks which allow us to do this by hand for \\(2\\times 2\\) and \\(3\\times 3\\) matrices, but after that the computation time is unworthy of the effort. Now that we have a good understanding of how to interpret eigenvalues and eigenvectors algebraically, let’s take a look at some of the things that they can do, starting with one important fact. Definition 12.3 (Eigenvalues and the Trace of a Matrix) Let \\(\\A\\) be an \\(n\\times n\\) matrix with eigenvalues \\(\\lambda_1,\\lambda_2,\\dots,\\lambda_n\\). Then the sum of the eigenvalues is equal to the trace of the matrix (recall that the trace of a matrix is the sum of its diagonal elements). \\[Trace(\\A)=\\sum_{i=1}^n \\lambda_i.\\] Example 12.4 (Trace of Covariance Matrix) Suppose that we had a collection of \\(n\\) observations on \\(p\\) variables, \\(\\x_1,\\x_2,\\dots,\\x_p\\). After centering the data to have zero mean, we can compute the sample variances as: \\[var(\\x_i)=\\frac{1}{n-1}\\x_i^T\\x_i =\\|\\x_i\\|^2\\] These variances form the diagonal elements of the sample covariance matrix, \\[\\ssigma = \\frac{1}{n-1}\\X^T\\X\\] Thus, the total variance of this data is \\[\\frac{1}{n-1}\\sum_{i=1}^n \\|\\x_i\\|^2 = Trace(\\ssigma) = \\sum_{i=1}^n \\lambda_i.\\] In other words, the sum of the eigenvalues of a covariance matrix provides the total variance in the variables \\(\\x_1,\\dots,\\x_p\\). 12.1 Diagonalization Let’s take another look at Example 12.3. We already showed that \\(\\lambda_1=5\\) and \\(\\v_1=\\pm 1\\\\2\\mp\\) is an eigenpair for the matrix \\(\\A=\\pm 1 &amp; 2 \\\\4&amp;3 \\mp\\). You may verify that \\(\\lambda_2=-1\\) and \\(\\v_2=\\pm 1\\\\-1 \\mp\\) is another eigenpair. Suppose we create a matrix of eigenvectors: \\[\\V=(\\v_1,\\v_2) = \\pm 1&amp;1\\\\2&amp;-1 \\mp\\] and a diagonal matrix containing the corresponding eigenvalues: \\[\\D=\\pm 5 &amp; 0 \\\\ 0 &amp; -1 \\mp\\] Then it is easy to verify that \\(\\A\\V=\\V\\D\\): \\[\\begin{eqnarray*} \\A\\V &amp;=&amp; \\pm 1 &amp; 2 \\\\4&amp;3 \\mp \\pm 1&amp;1\\\\2&amp;-1 \\mp\\\\ &amp;=&amp; \\pm 5&amp;-1\\\\10&amp;1 \\mp\\\\ &amp;=&amp; \\pm 1&amp;1\\\\2&amp;-1 \\mp\\pm 5 &amp; 0 \\\\ 0 &amp; -1 \\mp\\\\ &amp;=&amp;\\V\\D \\end{eqnarray*}\\] If the columns of \\(\\V\\) are linearly independent, which they are in this case, we can write: \\[\\V^{-1}\\A\\V = \\D\\] What we have just done is develop a way to transform a matrix \\(\\A\\) into a diagonal matrix \\(\\D\\). This is known as diagonalization. Definition 12.4 (Diagonalizable) An \\(n\\times n\\) matrix \\(\\A\\) is said to be diagonalizable if there exists an invertible matrix \\(\\bP\\) and a diagonal matrix \\(\\D\\) such that \\[\\bP^{-1}\\A\\bP=\\D\\] This is possible if and only if the matrix \\(\\A\\) has \\(n\\) linearly independent eigenvectors (known as a complete set of eigenvectors). The matrix \\(\\bP\\) is then the matrix of eigenvectors and the matrix \\(\\D\\) contains the corresponding eigenvalues on the diagonal. Determining whether or not a matrix \\(\\A_{n\\times n}\\) is diagonalizable is a little tricky. Having \\(rank(\\A)=n\\) is not a sufficient condition for having \\(n\\) linearly independent eigenvectors. The following matrix stands as a counter example: \\[\\A=\\pm -3&amp; 1 &amp; -3 \\\\20&amp; 3 &amp; 10 \\\\2&amp; -2 &amp; 4 \\mp\\] This matrix has full rank but only two linearly independent eigenvectors. Fortunately, for our primary application of diagonalization, we will be dealing with a symmetric matrix, which can always be diagonalized. In fact, symmetric matrices have an additional property which makes this diagonalization particularly nice, as we will see in Chapter 13. 12.2 Geometric Interpretation of Eigenvalues and Eigenvectors Since any scalar multiple of an eigenvector is still an eigenvector, let’s consider for the present discussion unit eigenvectors \\(\\x\\) of a square matrix \\(\\A\\) - those with length \\(\\|\\x\\|=1\\). By the definition, we know that \\[\\A\\x = \\lambda\\x\\] We know that geometrically, if we multiply \\(\\x\\) by \\(\\A\\), the resulting vector points in the same direction as \\(\\x\\). Geometrically, it turns out that multiplying the unit circle or unit sphere by a matrix \\(\\A\\) carves out an ellipse, or an ellipsoid. We can see eigenvectors visually by watching how multiplication by a matrix \\(\\A\\) changes the unit vectors. Figure 12.1 illustrates this. The blue arrows represent (a sampling of) the unit circle, all vectors \\(\\x\\) for which \\(\\|\\x\\|=1\\). The red arrows represent the image of the blue arrows after multiplication by \\(\\A\\), or \\(\\A\\x\\) for each vector \\(\\x\\). We can see how almost every vector changes direction when multiplied by \\(\\A\\), except the eigenvector directions which are marked in black. Such a picture provides a nice geometrical interpretation of eigenvectors for a general matrix, but we will see in Chapter 13 just how powerful these eigenvector directions are when we look at symmetric matrix. Figure 12.1: Visualizing eigenvectors (in black) using the image (in red) of the unit sphere (in blue) after multiplication by \\(\\A\\). 12.3 Exercises Show that \\(\\v\\) is an eigenvector of \\(\\A\\) and find the corresponding eigenvalue: $= &amp; 2 \\2 &amp; 1 \\-3 $ $= &amp; 1 \\6 &amp; 0 \\-2 $ $= &amp; -2 \\5 &amp; -7 \\2 $ Show that \\(\\lambda\\) is an eigenvalue of \\(\\A\\) and list two eigenvectors corresponding to this eigenvalue: \\(\\A=\\pm 0&amp; 4\\\\-1&amp;5\\mp \\quad \\lambda = 4\\) \\(\\A=\\pm 0&amp; 4\\\\-1&amp;5\\mp \\quad \\lambda = 1\\) Based on the eigenvectors you found in exercises 2, can the matrix \\(\\A\\) be diagonalized? Why or why not? If diagonalization is possible, explain how it would be done. Can a rectangular matrix have eigenvalues/eigenvectors? "],["pca.html", "Chapter 13 Principal Components Analysis 13.1 Geometrical comparison with Least Squares 13.2 Covariance or Correlation Matrix? 13.3 PCA in R 13.4 Variable Clustering with PCA", " Chapter 13 Principal Components Analysis We now have the tools necessary to discuss one of the most important concepts in mathematical statistics: Principal Components Analysis (PCA). PCA involves the analysis of eigenvalues and eigenvectors of the covariance or correlation matrix. Its development relies on the following important facts: Theorem 13.1 (Diagonalization of Symmetric Matrices) All \\(n\\times n\\) real valued symmetric matrices (like the covariance and correlation matrix) have two very important properties: They have a complete set of \\(n\\) linearly independent eigenvectors, \\(\\{\\v_1,\\dots,\\v_n\\}\\), corresponding to eigenvalues \\[\\lambda_1 \\geq \\lambda_2 \\geq\\dots\\geq \\lambda_n.\\] Furthermore, these eigenvectors can be always be chosen to be orthonormal so that if \\(\\V=[\\v_1|\\dots|\\v_n]\\) then \\[\\V^{T}\\V=\\bo{I}\\] or equivalently, \\(\\V^{-1}=\\V^{T}\\). Letting \\(\\D\\) be a diagonal matrix with \\(D_{ii}=\\lambda_i\\), by the definition of eigenvalues and eigenvectors we have for any symmetric matrix \\(\\bo{S}\\), \\[\\bo{S}\\V=\\V\\D\\] Thus, any symmetric matrix \\(\\bo{S}\\) can be diagonalized in the following way: \\[\\V^{T}\\bo{S}\\V=\\D\\] Covariance and Correlation matrices (when there is no perfect multicollinearity in variables) have the additional property that all of their eigenvalues are positive (nonzero). They are positive definite matrices. Now that we know we have a complete set of eigenvectors, it is common to order them according to the magnitude of their corresponding eigenvalues. From here on out, we will use \\((\\lambda_1,\\v_1)\\) to represent the largest eigenvalue of a matrix and its corresponding eigenvector. When working with a covariance or correlation matrix, this eigenvector associated with the largest eigenvalue is called the first principal component and points in the direction for which the variance of the data is maximal. Example 13.1 illustrates this point. Example 13.1 (Eigenvectors of the Covariance Matrix) Suppose we have a matrix of data for 10 individuals on 2 variables, \\(\\x_1\\) and \\(\\x_2\\). Plotted on a plane, the data appears as follows: Our data matrix for these points is: \\[\\X=\\pm 1 &amp; 1\\\\2&amp;1\\\\2&amp;4\\\\3&amp;1\\\\4&amp;4\\\\5&amp;2\\\\6&amp;4\\\\6&amp;6\\\\7&amp;6\\\\8&amp;8 \\mp\\] the means of the variables in \\(\\X\\) are: \\[\\bar{\\x}=\\pm 4.4 \\\\ 3.7 \\mp. \\] When thinking about variance directions, our first step should be to center the data so that it has mean zero. Eigenvectors measure the spread of data around the origin. Variance measures spread of data around the mean. Thus, we need to equate the mean with the origin. To center the data, we simply compute \\[\\X_c=\\X-\\e\\bar{\\x}^T = \\pm 1 &amp; 1\\\\2&amp;1\\\\2&amp;4\\\\3&amp;1\\\\4&amp;4\\\\5&amp;2\\\\6&amp;4\\\\6&amp;6\\\\7&amp;6\\\\8&amp;8 \\mp - \\pm 4.4 &amp; 3.7 \\\\4.4 &amp; 3.7 \\\\4.4 &amp; 3.7 \\\\4.4 &amp; 3.7 \\\\4.4 &amp; 3.7 \\\\4.4 &amp; 3.7 \\\\4.4 &amp; 3.7 \\\\4.4 &amp; 3.7 \\\\4.4 &amp; 3.7 \\\\4.4 &amp; 3.7 \\mp = \\pm -3.4&amp;-2.7\\\\-2.4&amp;-2.7\\\\-2.4&amp; 0.3\\\\-1.4&amp;-2.7\\\\ -0.4&amp; 0.3\\\\0.6&amp;-1.7\\\\1.6&amp; 0.3\\\\1.6&amp; 2.3\\\\2.6&amp; 2.3\\\\3.6&amp; 4.3\\mp.\\] Examining the new centered data, we find that we’ve only translated our data in the plane - we haven’t distorted it in any fashion. Thus the covariance matrix is: \\[\\ssigma=\\frac{1}{9}(\\X_c^T\\X_c)= \\pm 5.6 &amp; 4.8\\\\4.8&amp;6.0111 \\mp \\] The eigenvalue and eigenvector pairs of \\(\\ssigma\\) are (rounded to 2 decimal places) as follows: \\[(\\lambda_1,\\v_1)=\\left( 10.6100 , \\begin{bmatrix} 0.69 \\\\ 0.72 \\end{bmatrix}\\right) \\mbox{ and } (\\lambda_2,\\v_2)= \\left( 1.0012,\\begin{bmatrix}-0.72\\\\0.69 \\end{bmatrix}\\right)\\] Let’s plot the eigenvector directions on the same graph: The eigenvector \\(\\v_1\\) is called the first principal component. It is the direction along which the variance of the data is maximal. The eigenvector \\(\\v_2\\) is the second principal component. In general, the second principal component is the direction, orthogonal to the first, along which the variance of the data is maximal (in two dimensions, there is only one direction possible.) Why is this important? Let’s consider what we’ve just done. We started with two variables, \\(\\x_1\\) and \\(\\x_2\\), which appeared to be correlated. We then derived new variables, \\(\\v_1\\) and \\(\\v_2\\), which are linear combinations of the original variables: \\[\\begin{eqnarray} \\v_1 &amp;=&amp; 0.69\\x_1 + 0.72\\x_2 \\\\ \\tag{13.1} \\v_2 &amp;=&amp; -0.72\\x_1 + 0.69\\x_2 \\end{eqnarray}\\] These new variables are completely uncorrelated. To see this, let’s represent our data according to the new variables - i.e. let’s change the basis from \\(\\mathcal{B}_1=[\\x_1,\\x_2]\\) to \\(\\mathcal{B}_2=[\\v_1,\\v_2]\\). Example 13.2 (The Principal Component Basis) Let’s express our data in the basis defined by the principal components. We want to find coordinates (in a \\(2\\times 10\\) matrix \\(\\A\\)) such that our original (centered) data can be expressed in terms of principal components. This is done by solving for \\(\\A\\) in the following equation (see Chapter 8 and note that the rows of \\(\\X\\) define the points rather than the columns): \\[\\begin{eqnarray} \\X_c &amp;=&amp; \\A \\V^T \\\\ \\pm -3.4&amp;-2.7\\\\-2.4&amp;-2.7\\\\-2.4&amp; 0.3\\\\-1.4&amp;-2.7\\\\ -0.4&amp; 0.3\\\\0.6&amp;-1.7\\\\1.6&amp; 0.3\\\\1.6&amp; 2.3\\\\2.6&amp; 2.3\\\\3.6&amp; 4.3 \\mp &amp;=&amp; \\pm a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\\\ a_{31} &amp; a_{32}\\\\ a_{41} &amp; a_{42}\\\\ a_{51} &amp; a_{52}\\\\ a_{61} &amp; a_{62}\\\\ a_{71} &amp; a_{72}\\\\ a_{81} &amp; a_{82}\\\\ a_{91} &amp; a_{92}\\\\ a_{10,1} &amp; a_{10,2} \\mp \\pm \\v_1^T \\\\ \\v_2^T \\mp \\end{eqnarray}\\] Conveniently, our new basis is orthonormal meaning that \\(\\V\\) is an orthogonal matrix, so \\[\\A=\\X\\V .\\] The new data coordinates reflect a simple rotation of the data around the origin: Visually, we can see that the new variables are uncorrelated. You may wish to confirm this by calculating the covariance. In fact, we can do this in a general sense. If \\(\\A=\\X_c\\V\\) is our new data, then the covariance matrix is diagonal: \\[\\begin{eqnarray*} \\ssigma_A &amp;=&amp; \\frac{1}{n-1}\\A^T\\A \\\\ &amp;=&amp; \\frac{1}{n-1}(\\X_c\\V)^T(\\X_c\\V) \\\\ &amp;=&amp; \\frac{1}{n-1}\\V^T((\\X_c^T\\X_c)\\V\\\\ &amp;=&amp;\\frac{1}{n-1}\\V^T((n-1)\\ssigma_X)\\V\\\\ &amp;=&amp;\\V^T(\\ssigma_X)\\V\\\\ &amp;=&amp;\\V^T(\\V\\D\\V^T)\\V\\\\ &amp;=&amp; \\D \\end{eqnarray*}\\] Where \\(\\ssigma_X=\\V\\D\\V^T\\) comes from the diagonalization in Theorem 13.1. By changing our variables to principal components, we have managed to “hide” the correlation between \\(\\x_1\\) and \\(\\x_2\\) while keeping the spacial relationships between data points in tact. Transformation back to variables \\(\\x_1\\) and \\(\\x_2\\) is easily done by using the linear relationships in from Equation (13.1). 13.1 Geometrical comparison with Least Squares In least squares regression, our objective is to maximize the amount of variance explained in our target variable. It may look as though the first principal component from Example 13.1 points in the direction of the regression line. This is not the case however. The first principal component points in the direction of a line which minimizes the sum of squared orthogonal distances between the points and the line. Regressing \\(\\x_2\\) on \\(\\x_1\\), on the other hand, provides a line which minimizes the sum of squared vertical distances between points and the line. This is illustrated in Figure 13.1. Figure 13.1: Principal Components vs. Regression Lines The first principal component about the mean of a set of points can be represented by that line which most closely approaches the data points. Let this not conjure up images of linear regression in your head, though. In contrast, linear least squares tries to minimize the distance in a single direction only (the direction of your target variable axes). Thus, although the two use a similar error metric, linear least squares is a method that treats one dimension of the data preferentially, while PCA treats all dimensions equally. You might be tempted to conclude from Figure 13.1 that the first principal component and the regression line “ought to be similar.” This is a terrible conclusion if you consider a large multivariate dataset and the various regression lines that would predict each variable in that dataset. In PCA, there is no target variable and thus no single regression line that we’d be comparing to. 13.2 Covariance or Correlation Matrix? Principal components analysis can involve eigenvectors of either the covariance matrix or the correlation matrix. When we perform this analysis on the covariance matrix, the geometric interpretation is simply centering the data and then determining the direction of maximal variance. When we perform this analysis on the correlation matrix, the interpretation is standardizing the data and then determining the direction of maximal variance. The correlation matrix is simply a scaled form of the covariance matrix. In general, these two methods give different results, especially when the scales of the variables are different. The covariance matrix is the default for (most) \\(\\textsf{R}\\) PCA functions. The correlation matrix is the default in SAS and the covariance matrix method is invoked by the option: proc princomp data=X cov; var x1--x10; run; Choosing between the covariance and correlation matrix can sometimes pose problems. The rule of thumb is that the correlation matrix should be used when the scales of the variables vary greatly. In this case, the variables with the highest variance will dominate the first principal component. The argument against automatically using correlation matrices is that it turns out to be quite a brutal way of standardizing your data - forcing all variables to contain the same amount of information (after all, don’t we equate variance to information?) seems naive and counterintuitive when it is not absolutely necessary for differences in scale. We hope that the case studies outlined in Chapter 14 will give those who always use the correlation option reason for pause, and we hope that, in the future, they will consider multiple presentations of the data and their corresponding low-rank representations of the data. 13.3 PCA in R Let’s find Principal Components using the iris dataset. This is a well-known dataset, often used to demonstrate the effect of clustering algorithms. It contains numeric measurements for 150 iris flowers along 4 dimensions. The fifth column in the dataset tells us what species of Iris the flower is. There are 3 species. Sepal.Length Sepal.Width Petal.Length Petal.Width Species Setosa Versicolor Virginica Let’s first take a look at the scatterplot matrix: pairs(~Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,data=iris,col=c(&quot;red&quot;,&quot;green3&quot;,&quot;blue&quot;)[iris$Species]) It is apparent that some of our variables are correlated. We can confirm this by computing the correlation matrix with the cor() function. We can also check out the individual variances of the variables and the covariances between variables by examining the covariance matrix (cov() function). Remember - when looking at covariances, we can really only interpret the sign of the number and not the magnitude as we can with the correlations. cor(iris[1:4]) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length 1.0000000 -0.1175698 0.8717538 0.8179411 ## Sepal.Width -0.1175698 1.0000000 -0.4284401 -0.3661259 ## Petal.Length 0.8717538 -0.4284401 1.0000000 0.9628654 ## Petal.Width 0.8179411 -0.3661259 0.9628654 1.0000000 cov(iris[1:4]) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length 0.6856935 -0.0424340 1.2743154 0.5162707 ## Sepal.Width -0.0424340 0.1899794 -0.3296564 -0.1216394 ## Petal.Length 1.2743154 -0.3296564 3.1162779 1.2956094 ## Petal.Width 0.5162707 -0.1216394 1.2956094 0.5810063 We have relatively strong positive correlation between Petal Length, Petal Width and Sepal Length. It is also clear that Petal Length has more than 3 times the variance of the other 3 variables. How will this effect our analysis? The scatter plots and correlation matrix provide useful information, but they don’t give us a true sense for how the data looks when all 4 attributes are considered simultaneously. In the next section we will compute the principal components directly from eigenvalues and eigenvectors of the covariance or correlation matrix. It’s important to note that this method of computing principal components is not actually recommended - the answer provided is the same, but the numerical stability and efficiency of this method may be dubious for large datasets. The Singular Value Decomposition (SVD), which will be discussed in Chapter 15, is generally a preferred route to computing principal components. In fact, the numerical computation of principal components is outside of the scope of this book. We recommend cite3? for a treatment of the topic. using both the covariance matrix and the correlation matrix, and see what we can learn about the data. Let’s start with the covariance matrix which is the default setting for the prcomp function in R. 13.3.1 Covariance PCA Let’s start with the covariance matrix which is the default setting for the prcomp function in R. It’s worth repeating that a dedicated principal component function like prcomp() is superior in numerical stability and efficiency to the lines of code in the next section. The only reason for directly computing the covariance matrix and its eigenvalues and eigenvectors (as opposed to prcomp()) is for edification. Computing a PCA in this manner, just this once, will help us grasp the exact mathematics of the situation and discover the nuances of them. 13.3.2 Principal Components, Loadings, and Variance Explained covM = cov(iris[1:4]) eig=eigen(covM,symmetric=TRUE,only.values=FALSE) c=colnames(iris[1:4]) eig$values ## [1] 4.22824171 0.24267075 0.07820950 0.02383509 rownames(eig$vectors)=c(colnames(iris[1:4])) eig$vectors ## [,1] [,2] [,3] [,4] ## Sepal.Length 0.36138659 -0.65658877 -0.58202985 0.3154872 ## Sepal.Width -0.08452251 -0.73016143 0.59791083 -0.3197231 ## Petal.Length 0.85667061 0.17337266 0.07623608 -0.4798390 ## Petal.Width 0.35828920 0.07548102 0.54583143 0.7536574 The eigenvalues tell us how much of the total variance in the data is directed along each eigenvector. Thus, the amount of variance along \\(\\mathbf{v}_1\\) is \\(\\lambda_1\\) and the proportion of variance explained by the first principal component is \\[\\frac{\\lambda_1}{\\lambda_1+\\lambda_2+\\lambda_3+\\lambda_4}\\] eig$values[1]/sum(eig$values) ## [1] 0.9246187 Thus 92% of the variation in the Iris data is explained by the first component alone. What if we consider the first and second principal component directions? Using this two dimensional representation (approximation/projection) we can capture the following proportion of variance: \\[\\frac{\\lambda_1+\\lambda_2}{\\lambda_1+\\lambda_2+\\lambda_3+\\lambda_4}\\] sum(eig$values[1:2])/sum(eig$values) ## [1] 0.9776852 With two dimensions, we explain 97.8% of the variance in these 4 variables! The entries in each eigenvector are called the loadings of the variables on the component. The loadings give us an idea how important each variable is to each component. For example, it seems that the third variable in our dataset (Petal Length) is dominating the first principal component. This should not come as too much of a shock - that variable had (by far) the largest amount of variation of the four. In order to capture the most amount of variance in a single dimension, we should certainly be considering this variable strongly. The variable with the next largest variance, Sepal Length, dominates the second principal component. Note: Had Petal Length and Sepal Length been correlated, they would not have dominated separate principal components, they would have shared one. These two variables are not correlated and thus their variation cannot be captured along the same direction. 13.3.3 Scores and PCA Projection Lets plot the projection of the four-dimensional iris data onto the two dimensional space spanned by the first 2 principal components. To do this, we need coordinates. These coordinates are commonly called scores in statistical texts. We can find the coordinates of the data on the principal components by solving the system \\[\\mathbf{X}=\\mathbf{A}\\mathbf{V}^T\\] where \\(\\mathbf{X}\\) is our original iris data (centered to have mean = 0) and \\(\\mathbf{A}\\) is a matrix of coordinates in the new principal component space, spanned by the eigenvectors in \\(\\mathbf{V}\\). Solving this system is simple enough - since \\(\\mathbf{V}\\) is an orthogonal matrix. Let’s confirm this: eig$vectors %*% t(eig$vectors) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length 1.000000e+00 4.163336e-17 -2.775558e-17 -2.775558e-17 ## Sepal.Width 4.163336e-17 1.000000e+00 1.665335e-16 1.942890e-16 ## Petal.Length -2.775558e-17 1.665335e-16 1.000000e+00 -2.220446e-16 ## Petal.Width -2.775558e-17 1.942890e-16 -2.220446e-16 1.000000e+00 t(eig$vectors) %*% eig$vectors ## [,1] [,2] [,3] [,4] ## [1,] 1.000000e+00 -2.289835e-16 0.000000e+00 -1.110223e-16 ## [2,] -2.289835e-16 1.000000e+00 2.775558e-17 -1.318390e-16 ## [3,] 0.000000e+00 2.775558e-17 1.000000e+00 1.110223e-16 ## [4,] -1.110223e-16 -1.318390e-16 1.110223e-16 1.000000e+00 We’ll have to settle for precision at 15 decimal places. Close enough! So to find the scores, we simply subtract the means from our original variables to create the data matrix \\(\\mathbf{X}\\) and compute \\[\\mathbf{A}=\\mathbf{X}\\mathbf{V}\\] # The scale function centers and scales by default X=scale(iris[1:4],center=TRUE,scale=FALSE) # Create data.frame from matrix for plotting purposes. scores=data.frame(X %*% eig$vectors) # Change default variable names colnames(scores)=c(&quot;Prin1&quot;,&quot;Prin2&quot;,&quot;Prin3&quot;,&quot;Prin4&quot;) # Print coordinates/scores of first 10 observations scores[1:10, ] ## Prin1 Prin2 Prin3 Prin4 ## 1 -2.684126 -0.31939725 -0.02791483 0.002262437 ## 2 -2.714142 0.17700123 -0.21046427 0.099026550 ## 3 -2.888991 0.14494943 0.01790026 0.019968390 ## 4 -2.745343 0.31829898 0.03155937 -0.075575817 ## 5 -2.728717 -0.32675451 0.09007924 -0.061258593 ## 6 -2.280860 -0.74133045 0.16867766 -0.024200858 ## 7 -2.820538 0.08946138 0.25789216 -0.048143106 ## 8 -2.626145 -0.16338496 -0.02187932 -0.045297871 ## 9 -2.886383 0.57831175 0.02075957 -0.026744736 ## 10 -2.672756 0.11377425 -0.19763272 -0.056295401 To this point, we have simply computed coordinates (scores) on a new set of axis (principal components, eigenvectors, loadings). These axis are orthogonal and are aligned with the directions of maximal variance in the data. When we consider only a subset of principal components (like 2 components accounting for 97% of the variance), then we are projecting the data onto a lower dimensional space. Generally, this is one of the primary goals of PCA: Project the data down into a lower dimensional space (onto the span of the principal components) while keeping the maximum amount of information (i.e. variance). Thus, we know that almost 98% of the data’s variance can be seen in two-dimensions using the first two principal components. Let’s go ahead and see what this looks like: plot(scores$Prin1, scores$Prin2, main=&quot;Data Projected on First 2 Principal Components&quot;, xlab=&quot;First Principal Component&quot;, ylab=&quot;Second Principal Component&quot;, col=c(&quot;red&quot;,&quot;green3&quot;,&quot;blue&quot;)[iris$Species]) 13.3.4 PCA functions in R irispca=prcomp(iris[1:4]) # Variance Explained summary(irispca) ## Importance of components: ## PC1 PC2 PC3 PC4 ## Standard deviation 2.0563 0.49262 0.2797 0.15439 ## Proportion of Variance 0.9246 0.05307 0.0171 0.00521 ## Cumulative Proportion 0.9246 0.97769 0.9948 1.00000 # Eigenvectors: irispca$rotation ## PC1 PC2 PC3 PC4 ## Sepal.Length 0.36138659 -0.65658877 0.58202985 0.3154872 ## Sepal.Width -0.08452251 -0.73016143 -0.59791083 -0.3197231 ## Petal.Length 0.85667061 0.17337266 -0.07623608 -0.4798390 ## Petal.Width 0.35828920 0.07548102 -0.54583143 0.7536574 # Coordinates of first 10 observations along PCs: irispca$x[1:10, ] ## PC1 PC2 PC3 PC4 ## [1,] -2.684126 -0.31939725 0.02791483 0.002262437 ## [2,] -2.714142 0.17700123 0.21046427 0.099026550 ## [3,] -2.888991 0.14494943 -0.01790026 0.019968390 ## [4,] -2.745343 0.31829898 -0.03155937 -0.075575817 ## [5,] -2.728717 -0.32675451 -0.09007924 -0.061258593 ## [6,] -2.280860 -0.74133045 -0.16867766 -0.024200858 ## [7,] -2.820538 0.08946138 -0.25789216 -0.048143106 ## [8,] -2.626145 -0.16338496 0.02187932 -0.045297871 ## [9,] -2.886383 0.57831175 -0.02075957 -0.026744736 ## [10,] -2.672756 0.11377425 0.19763272 -0.056295401 All of the information we computed using eigenvectors aligns with what we see here, except that the coordinates/scores and the loadings of Principal Component 3 are of the opposite sign. In light of what we know about eigenvectors representing directions, this should be no cause for alarm. The prcomp function arrived at the unit basis vector pointing in the negative direction of the one we found directly from the eig function - which should negate all the coordinates and leave us with an equivalent mirror image in all of our projections. 13.3.5 The Biplot One additional feature that R users have created is the biplot. The PCA biplot allows us to see where our original variables fall in the space of the principal components. Highly correlated variables will fall along the same direction (or exactly opposite directions) as a change in one of these variables correlates to a change in the other. Uncorrelated variables will appear further apart. The length of the variable vectors on the biplot tell us the degree to which variability in variable is explained in that direction. Shorter vectors have less variability than longer vectors. So in the biplot below, petal width and petal length point in the same direction indicating that these variables share a relatively high degree of correlation. However, the vector for petal width is much shorter than that of petal length, which means you can expect a higher degree of change in petal length as you proceed to the right along PC1. PC1 explains more of the variance in petal length than it does petal width. If we were to imagine a third PC orthogonal to the plane shown, petal width is likely to exist at much larger angle off the plane - here, it is being projected down from that 3-dimensional picture. biplot(irispca, col = c(&quot;gray&quot;, &quot;blue&quot;)) We can examine some of the outlying observations to see how they align with these projected variable directions. It helps to compare them to the quartiles of the data. Also keep in mind the direction of the arrows in the plot. If the arrow points down then the positive direction is down - indicating observations which are greater than the mean. Let’s pick out observations 42 and 132 and see what the actual data points look like in comparison to the rest of the sample population. summary(iris[1:4]) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 # Consider orientation of outlying observations: iris[42, ] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 42 4.5 2.3 1.3 0.3 setosa iris[132, ] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 132 7.9 3.8 6.4 2 virginica 13.4 Variable Clustering with PCA The direction arrows on the biplot are merely the coefficients of the original variables when combined to make principal components. Don’t forget that principal components are simply linear combinations of the original variables. For example, here we have the first principal component (the first column of \\(\\V\\)), \\(\\mathbf{v}_1\\) as: eig$vectors[,1] ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 0.36138659 -0.08452251 0.85667061 0.35828920 This means that the coordinates of the data along the first principal component, which we’ll denote here as \\(PC_1\\) are given by a simple linear combination of our original variables after centering (for covariance PCA) or standardization (for correlation PCA) \\[PC_1 = 0.36Sepal.Length-0.08Sepal.Width+0.85Petal.Length +0.35Petal.Width\\] the same equation could be written for each of the vectors of coordinates along principal components, \\(PC_1,\\dots, PC_4\\). Essentially, we have a system of equations telling us that the rows of \\(\\V^T\\) (i.e. the columns of \\(\\V\\)) give us the weights of each variable for each principal component: \\[\\begin{equation} \\tag{13.2} \\begin{bmatrix} PC_1\\\\PC_2\\\\PC_3\\\\PC_4\\end{bmatrix} = \\mathbf{V}^T\\begin{bmatrix}Sepal.Length\\\\Sepal.Width\\\\Petal.Length\\\\Petal.Width\\end{bmatrix} \\end{equation}\\] Thus, if want the coordinates of our original variables in terms of Principal Components (so that we can plot them as we do in the biplot) we need to look no further than the rows of the matrix \\(\\mathbf{V}\\) as \\[\\begin{equation} \\tag{13.3} \\begin{bmatrix}Sepal.Length\\\\Sepal.Width\\\\Petal.Length\\\\Petal.Width\\end{bmatrix} =\\mathbf{V}\\begin{bmatrix} PC_1\\\\PC_2\\\\PC_3\\\\PC_4\\end{bmatrix} \\end{equation}\\] means that the rows of \\(\\mathbf{V}\\) give us the coordinates of our original variables in the PCA space. The transition from Equation (13.2) to Equation (13.3) is provided by the orthogonality of the eigenvectors per Theorem 13.1. #First entry in each eigenvectors give coefficients for Variable 1: eig$vectors[1,] ## [1] 0.3613866 -0.6565888 -0.5820299 0.3154872 \\[Sepal.Length = 0.361 PC_1 - 0.657 PC_2 - 0.582 PC_3 + 0.315 PC_4\\] You can see this on the biplot. The vector shown for Sepal.Length is (0.361, -0.656), which is the two dimensional projection formed by throwing out components 3 and 4. Variables which lie upon similar directions in the PCA space tend to change together in a similar fashion. We might consider Petal.Width and Petal.Length as a cluster of variables because they share a direction on the biplot, which means they represent much of the same information (the underlying construct being the “size of the petal” in this case). 13.4.1 Correlation PCA We can complete the same analysis using the correlation matrix. I’ll leave it as an exercise to compute the Principal Component loadings and scores and variance explained directly from eigenvectors and eigenvalues. You should do this and compare your results to the R output. (Beware: you must transform your data before solving for the scores. With the covariance version, this meant centering - for the correlation version, this means standardization as well) irispca2=prcomp(iris[1:4], cor=TRUE) ## Warning: In prcomp.default(iris[1:4], cor = TRUE) : ## extra argument &#39;cor&#39; will be disregarded summary(irispca2) ## Importance of components: ## PC1 PC2 PC3 PC4 ## Standard deviation 2.0563 0.49262 0.2797 0.15439 ## Proportion of Variance 0.9246 0.05307 0.0171 0.00521 ## Cumulative Proportion 0.9246 0.97769 0.9948 1.00000 irispca2$rotation ## PC1 PC2 PC3 PC4 ## Sepal.Length 0.36138659 -0.65658877 0.58202985 0.3154872 ## Sepal.Width -0.08452251 -0.73016143 -0.59791083 -0.3197231 ## Petal.Length 0.85667061 0.17337266 -0.07623608 -0.4798390 ## Petal.Width 0.35828920 0.07548102 -0.54583143 0.7536574 irispca2$x[1:10,] ## PC1 PC2 PC3 PC4 ## [1,] -2.684126 -0.31939725 0.02791483 0.002262437 ## [2,] -2.714142 0.17700123 0.21046427 0.099026550 ## [3,] -2.888991 0.14494943 -0.01790026 0.019968390 ## [4,] -2.745343 0.31829898 -0.03155937 -0.075575817 ## [5,] -2.728717 -0.32675451 -0.09007924 -0.061258593 ## [6,] -2.280860 -0.74133045 -0.16867766 -0.024200858 ## [7,] -2.820538 0.08946138 -0.25789216 -0.048143106 ## [8,] -2.626145 -0.16338496 0.02187932 -0.045297871 ## [9,] -2.886383 0.57831175 -0.02075957 -0.026744736 ## [10,] -2.672756 0.11377425 0.19763272 -0.056295401 plot(irispca2$x[,1],irispca2$x[,2], main=&quot;Data Projected on First 2 Principal Components&quot;, xlab=&quot;First Principal Component&quot;, ylab=&quot;Second Principal Component&quot;, col=c(&quot;red&quot;,&quot;green3&quot;,&quot;blue&quot;)[iris$Species]) biplot(irispca2) Here you can see the direction vectors of the original variables are relatively uniform in length in the PCA space. This is due to the standardization in the correlation matrix. However, the general message is the same: Petal.Width and Petal.Length Cluster together, and many of the same observations appear “on the fray” on the PCA space - although not all of them! 13.4.2 Which Projection is Better? What do you think? It depends on the task, and it depends on the data. One flavor of PCA is not “better” than the other. Correlation PCA is appropriate when the scales of your attributes differ wildly, and covariance PCA would be inappropriate in that situation. But in all other scenarios, when the scales of our attributes are roughly the same, we should always consider both dimension reductions and make a decision based upon the resulting output (variance explained, projection plots, loadings). For the iris data, The results in terms of variable clustering are pretty much the same. For clustering/classifying the 3 species of flowers, we can see better separation in the covariance version. 13.4.3 Beware of biplots Be careful not to draw improper conclusions from biplots. Particularly, be careful about situations where the first two principal components do not summarize the majority of the variance. If a large amount of variance is captured by the 3rd or 4th (or higher) principal components, then we must keep in mind that the variable projections on the first two principal components are flattened out versions of a higher dimensional picture. If a variable vector appears short in the 2-dimensional projection, it means one of two things: That variable has small variance That variable appears to have small variance when depicted in the space of the first two principal components, but truly has a larger variance which is represented by 3rd or higher principal components. Let’s take a look at an example of this. We’ll generate 500 rows of data on 4 nearly independent normal random variables. Since these variables are uncorrelated, we might expect that the 4 orthogonal principal components will line up relatively close to the original variables. If this doesn’t happen, then at the very least we can expect the biplot to show little to no correlation between the variables. We’ll give variables \\(2\\) and \\(3\\) the largest variance. Multiple runs of this code will generate different results with similar implications. means=c(2,4,1,3) sigmas=c(7,9,10,8) sample.size=500 data=mapply(function(mu,sig){rnorm(mu,sig, n=sample.size)},mu=means,sig=sigmas) cor(data) ## [,1] [,2] [,3] [,4] ## [1,] 1.00000000 -0.049023673 0.05625622 -0.027725186 ## [2,] -0.04902367 1.000000000 0.06038850 -0.009804244 ## [3,] 0.05625622 0.060388497 1.00000000 -0.016110819 ## [4,] -0.02772519 -0.009804244 -0.01611082 1.000000000 pc=prcomp(data,scale=TRUE) summary(pc) ## Importance of components: ## PC1 PC2 PC3 PC4 ## Standard deviation 1.0360 1.0254 0.9936 0.9424 ## Proportion of Variance 0.2683 0.2628 0.2468 0.2220 ## Cumulative Proportion 0.2683 0.5312 0.7780 1.0000 pc$rotation ## PC1 PC2 PC3 PC4 ## [1,] -0.4821201 -0.6230137 0.23475726 0.5694762 ## [2,] -0.3213893 0.7526628 -0.05102459 0.5723671 ## [3,] -0.7191799 0.1576449 0.34850788 -0.5800609 ## [4,] 0.3834702 0.1431804 0.90599546 0.1078062 biplot(pc) Figure 13.2: BiPlot of Iris Data Obviously, the wrong conclusion to make from this biplot is that Variables 1 and 4 are correlated. Variables 1 and 4 do not load highly on the first two principal components - in the whole 4-dimensional principal component space they are nearly orthogonal to each other and to variables 1 and 2. Thus, their orthogonal projections appear near the origin of this 2-dimensional subspace. The morals of the story: - Always corroborate your results using the variable loadings and the amount of variation explained by each variable. - When a variable shows up near the origin in a biplot, it is generally not well represented by your two-dimensional approximation of the data. "],["pcaapp.html", "Chapter 14 Applications of Principal Components 14.1 Dimension reduction 14.2 Exploratory Analysis 14.3 FIFA Soccer Players 14.4 Cancer Genetics", " Chapter 14 Applications of Principal Components Principal components have a number of applications across many areas of statistics. In the next sections, we will explore their usefulness in the context of dimension reduction. In Chapter we will look at how PCA is used to solve the issue of multicollinearity in biased regression. 14.1 Dimension reduction It is quite common for an analyst to have too many variables. There are two different solutions to this problem: Feature Selection: Choose a subset of existing variables to be used in a model. Feature Extraction: Create a new set of features which are combinations of original variables. 14.1.1 Feature Selection Let’s think for a minute about feature selection. What are we really doing when we consider a subset of our existing variables? Take the two dimensional data in Example (while two-dimensions rarely necessitate dimension reduction, the geometrical interpretation extends to higher dimensions as usual!). The centered data appears as follows: Now say we perform some kind of feature selection (there are a number of ways to do this, chi-square tests for instances) and we determine that the variable \\(\\x_2\\) is more important than \\(\\x_1\\). So we throw out \\(\\x_2\\) and we’ve reduced the dimensions from \\(p=2\\) to \\(k=1\\). Geometrically, what does our new data look like? By dropping \\(\\x_1\\) we set all of those horizontal coordinates to zero. In other words, we project the data orthogonally onto the \\(\\x_2\\) axis: Now, how much information (variance) did we lose with this projection? The total variance in the original data is \\[\\|\\x_1\\|^2+\\|\\x_2\\|^2.\\] The variance of our data reduction is \\[\\|\\x_2\\|^2.\\] Thus, the proportion of the total information (variance) we’ve kept is \\[\\frac{\\|\\x_2\\|^2}{\\|\\x_1\\|^2+\\|\\x_2\\|^2}=\\frac{6.01}{5.6+6.01} = 51.7\\%.\\] Our reduced dimensional data contains only 51.7% of the variance of the original data. We’ve lost a lot of information! The fact that feature selection omits variance in our predictor variables does not make it a bad thing! Obviously, getting rid of variables which have no relationship to a target variable (in the case of supervised modeling like prediction and classification) is a good thing. But, in the case of unsupervised learning techniques, where there is no target variable involved, we must be extra careful when it comes to feature selection. In summary, Feature Selection is important. Examples include: Removing variables which have little to no impact on a target variable in supervised modeling (forward/backward/stepwise selection). Removing variables which have obvious strong correlation with other predictors. Removing variables that are not interesting in unsupervised learning (For example, you may not want to use the words th'' andof’’ when clustering text). Feature Selection is an orthogonal projection of the original data onto the span of the variables you choose to keep. Feature selection should always be done with care and justification. In regression, could create problems of endogeneity (errors correlated with predictors - omitted variable bias). For unsupervised modelling, could lose important information. 14.1.2 Feature Extraction PCA is the most common form of feature extraction. The rotation of the space shown in Example represents the creation of new features which are linear combinations of the original features. If we have \\(p\\) potential variables for a model and want to reduce that number to \\(k\\), then the first \\(k\\) principal components combine the individual variables in such a way that is guaranteed to capture as much ``information’’ (variance) as possible. Again, take our two-dimensional data as an example. When we reduce our data down to one-dimension using principal components, we essentially do the same orthogonal projection that we did in Feature Selection, only in this case we conduct that projection in the new basis of principal components. Recall that for this data, our first principal component \\(\\v_1\\) was \\[\\v_1 = \\pm 0.69 \\\\0.73 \\mp.\\] Projecting the data onto the first principal component is illustrated in Figure How much variance do we keep with \\(k\\) principal components? The proportion of variance explained by each principal component is the ratio of the corresponding eigenvalue to the sum of the eigenvalues (which gives the total amount of variance in the data). Theorem @ref().1: Proportion of Variance Explained The proportion of variance explained by the projection of the data onto principal component \\(\\v_i\\) is \\[\\frac{\\lambda_i}{\\sum_{j=1}^p \\lambda_j}.\\] Similarly, the proportion of variance explained by the projection of the data onto the first \\(k\\) principal components (\\(k&lt;j\\)) is \\[ \\frac{\\sum_{i=1}^k\\lambda_i}{\\sum_{j=1}^p \\lambda_j}\\] In our simple 2 dimensional example we were able to keep \\[\\frac{\\lambda_1}{\\lambda_1+\\lambda_2}=\\frac{10.61}{10.61+1.00} = 91.38\\%\\] of our variance in one dimension. 14.2 Exploratory Analysis 14.2.1 UK Food Consumption 14.2.1.1 Explore the Data The data for this example can be read directly from our course webpage. When we first examine the data, we will see that the rows correspond to different types of food/drink and the columns correspond to the 4 countries within the UK. Our first matter of business is transposing this data so that the 4 countries become our observations (i.e. rows). food=read.csv(&quot;http://birch.iaa.ncsu.edu/~slrace/LinearAlgebra2021/Code/ukfood.csv&quot;, header=TRUE,row.names=1) library(reshape2) #melt data matrix into 3 columns library(ggplot2) #heatmap head(food) ## England Wales Scotland N.Ireland ## Cheese 105 103 103 66 ## Carcass meat 245 227 242 267 ## Other meat 685 803 750 586 ## Fish 147 160 122 93 ## Fats and oils 193 235 184 209 ## Sugars 156 175 147 139 food=as.data.frame(t(food)) head(food) ## Cheese Carcass meat Other meat Fish Fats and oils Sugars ## England 105 245 685 147 193 156 ## Wales 103 227 803 160 235 175 ## Scotland 103 242 750 122 184 147 ## N.Ireland 66 267 586 93 209 139 ## Fresh potatoes Fresh Veg Other Veg Processed potatoes Processed Veg ## England 720 253 488 198 360 ## Wales 874 265 570 203 365 ## Scotland 566 171 418 220 337 ## N.Ireland 1033 143 355 187 334 ## Fresh fruit Cereals Beverages Soft drinks Alcoholic drinks ## England 1102 1472 57 1374 375 ## Wales 1137 1582 73 1256 475 ## Scotland 957 1462 53 1572 458 ## N.Ireland 674 1494 47 1506 135 ## Confectionery ## England 54 ## Wales 64 ## Scotland 62 ## N.Ireland 41 Next we will visualize the information in this data using a simple heat map. To do this we will standardize and then melt the data using the package, and then use a heatmap. food.std = scale(food, center=T, scale = T) food.melt = melt(food.std, id.vars = row.names(food.std), measure.vars = 1:17) ggplot(data = food.melt, aes(x=Var1, y=Var2, fill=value)) + geom_tile(color = &quot;white&quot;)+ scale_fill_gradient2(low = &quot;blue&quot;, high = &quot;red&quot;, mid = &quot;white&quot;, midpoint = 0, limit = c(-2,2), space = &quot;Lab&quot; ) + theme_minimal()+ theme(axis.title.x = element_blank(),axis.title.y = element_blank(), axis.text.y = element_text(face = &#39;bold&#39;, size = 12, colour = &#39;black&#39;), axis.text.x = element_text(angle = 45, vjust = 1, face = &#39;bold&#39;, size = 12, colour = &#39;black&#39;, hjust = 1))+coord_fixed() 14.2.1.2 prcomp() function for PCA The prcomp() function is the one I most often recommend for reasonably sized principal component calculations in R. This function returns a list with class “prcomp” containing the following components (from help prcomp): The option scale = TRUE inside the prcomp() function instructs the program to use orrelation PCA. The default is covariance PCA. pca=prcomp(food, scale = T) This first plot just looks at magnitudes of eigenvalues - it is essentially the screeplot in barchart form. summary(pca) ## Importance of components: ## PC1 PC2 PC3 PC4 ## Standard deviation 3.4082 2.0562 1.07524 6.344e-16 ## Proportion of Variance 0.6833 0.2487 0.06801 0.000e+00 ## Cumulative Proportion 0.6833 0.9320 1.00000 1.000e+00 plot(pca, main = &quot;Bar-style Screeplot&quot;) The next plot views our four datapoints (locations) projected onto the 2-dimensional subspace (from 17 dimensions) that captures as much information (i.e. variance) as possible. plot(pca$x, xlab = &quot;Principal Component 1&quot;, ylab = &quot;Principal Component 2&quot;, main = &#39;The four observations projected into 2-dimensional space&#39;) text(pca$x[,1], pca$x[,2],row.names(food)) 14.2.1.3 The BiPlot Now we can also view our original variable axes projected down onto that same space! biplot(pca$x,pca$rotation, cex = c(1.5, 1), col = c(&#39;black&#39;,&#39;red&#39;))#, Figure 14.1: BiPlot: The observations and variables projected onto the same plane. # xlim = c(-0.8,0.8), ylim = c(-0.6,0.7)) 14.2.1.4 Formatting the biplot for readability I will soon introduce the autoplot() function from the `ggfortify package, but for now I just want to show you that you can specify which variables (and observations) to include in the biplot by directly specifying the loadings matrix and scores matrix of interest in the biplot function: desired.variables = c(2,4,6,8,10) biplot(pca$x, pca$rotation[desired.variables,1:2], cex = c(1.5, 1), col = c(&#39;black&#39;,&#39;red&#39;), xlim = c(-6,5), ylim = c(-4,4)) 14.2.1.5 What are all these axes? Those numbers relate to the scores on PC1 and PC2 (sometimes normalized so that each new variable has variance 1 - and sometimes not) and the loadings on PC1 and PC2 (sometimes normalized so that each variable vector is a unit vector - and sometimes scaled by the eigenvalues or square roots of the eigenvalues in some fashion). Generally, I’ve never found it useful to hunt down how each package is rendering the biplot, as they should be providing the same information regardless of the numbers on the axes. We don’t actually use those numbers to help us draw conclusions. We use the directions of the arrows and the layout of the points in reference to those direction arrows. vmax = varimax(pca$rotation[,1:2]) new.scores = pca$x[,1:2] %*% vmax$rotmat biplot(new.scores, vmax$loadings[,1:2], # xlim=c(-60,60), # ylim=c(-60,60), cex = c(1.5, 1), xlab = &#39;Rotated Axis 1&#39;, ylab = &#39;Rotated Axis 2&#39;) vmax$loadings[,1:2] ## PC1 PC2 ## Cheese 0.02571143 0.34751491 ## Carcass meat -0.16660468 -0.24450375 ## Other meat 0.11243721 0.27569481 ## Fish 0.22437069 0.17788300 ## Fats and oils 0.35728064 -0.22128124 ## Sugars 0.30247003 0.07908986 ## Fresh potatoes 0.22174898 -0.40880955 ## Fresh Veg 0.26432097 0.09953752 ## Other Veg 0.27836185 0.11640174 ## Processed potatoes -0.17545152 0.39011648 ## Processed Veg 0.29583164 0.05084727 ## Fresh fruit 0.15852128 0.24360131 ## Cereals 0.34963293 -0.13363398 ## Beverages 0.30030152 0.07604823 ## Soft drinks -0.36374762 0.07438738 ## Alcoholic drinks 0.04243636 0.34240944 ## Confectionery 0.05450175 0.32474821 14.3 FIFA Soccer Players 14.3.0.1 Explore the Data We begin by loading in the data and taking a quick look at the variables that we’ll be using in our PCA for this exercise. You may need to install the packages from the following library() statements. library(reshape2) #melt correlation matrix into 3 columns library(ggplot2) #correlation heatmap library(ggfortify) #autoplot bi-plot library(viridis) # magma palette ## Loading required package: viridisLite library(plotrix) # color.legend Now we’ll read the data directly from the web, take a peek at the first 5 rows, and explore some summary statistics. ## Name Age Photo ## 1 Cristiano Ronaldo 32 https://cdn.sofifa.org/48/18/players/20801.png ## 2 L. Messi 30 https://cdn.sofifa.org/48/18/players/158023.png ## 3 Neymar 25 https://cdn.sofifa.org/48/18/players/190871.png ## 4 L. Suárez 30 https://cdn.sofifa.org/48/18/players/176580.png ## 5 M. Neuer 31 https://cdn.sofifa.org/48/18/players/167495.png ## 6 R. Lewandowski 28 https://cdn.sofifa.org/48/18/players/188545.png ## Nationality Flag Overall Potential ## 1 Portugal https://cdn.sofifa.org/flags/38.png 94 94 ## 2 Argentina https://cdn.sofifa.org/flags/52.png 93 93 ## 3 Brazil https://cdn.sofifa.org/flags/54.png 92 94 ## 4 Uruguay https://cdn.sofifa.org/flags/60.png 92 92 ## 5 Germany https://cdn.sofifa.org/flags/21.png 92 92 ## 6 Poland https://cdn.sofifa.org/flags/37.png 91 91 ## Club Club.Logo Value Wage ## 1 Real Madrid CF https://cdn.sofifa.org/24/18/teams/243.png €95.5M €565K ## 2 FC Barcelona https://cdn.sofifa.org/24/18/teams/241.png €105M €565K ## 3 Paris Saint-Germain https://cdn.sofifa.org/24/18/teams/73.png €123M €280K ## 4 FC Barcelona https://cdn.sofifa.org/24/18/teams/241.png €97M €510K ## 5 FC Bayern Munich https://cdn.sofifa.org/24/18/teams/21.png €61M €230K ## 6 FC Bayern Munich https://cdn.sofifa.org/24/18/teams/21.png €92M €355K ## Special Acceleration Aggression Agility Balance Ball.control Composure ## 1 2228 89 63 89 63 93 95 ## 2 2154 92 48 90 95 95 96 ## 3 2100 94 56 96 82 95 92 ## 4 2291 88 78 86 60 91 83 ## 5 1493 58 29 52 35 48 70 ## 6 2143 79 80 78 80 89 87 ## Crossing Curve Dribbling Finishing Free.kick.accuracy GK.diving GK.handling ## 1 85 81 91 94 76 7 11 ## 2 77 89 97 95 90 6 11 ## 3 75 81 96 89 84 9 9 ## 4 77 86 86 94 84 27 25 ## 5 15 14 30 13 11 91 90 ## 6 62 77 85 91 84 15 6 ## GK.kicking GK.positioning GK.reflexes Heading.accuracy Interceptions Jumping ## 1 15 14 11 88 29 95 ## 2 15 14 8 71 22 68 ## 3 15 15 11 62 36 61 ## 4 31 33 37 77 41 69 ## 5 95 91 89 25 30 78 ## 6 12 8 10 85 39 84 ## Long.passing Long.shots Marking Penalties Positioning Reactions Short.passing ## 1 77 92 22 85 95 96 83 ## 2 87 88 13 74 93 95 88 ## 3 75 77 21 81 90 88 81 ## 4 64 86 30 85 92 93 83 ## 5 59 16 10 47 12 85 55 ## 6 65 83 25 81 91 91 83 ## Shot.power Sliding.tackle Sprint.speed Stamina Standing.tackle Strength ## 1 94 23 91 92 31 80 ## 2 85 26 87 73 28 59 ## 3 80 33 90 78 24 53 ## 4 87 38 77 89 45 80 ## 5 25 11 61 44 10 83 ## 6 88 19 83 79 42 84 ## Vision Volleys position ## 1 85 88 1 ## 2 90 85 1 ## 3 80 83 1 ## 4 84 88 1 ## 5 70 11 4 ## 6 78 87 1 ## Acceleration Aggression Agility Balance Ball.control ## Min. :11.00 Min. :11.00 Min. :14.00 Min. :11.00 Min. : 8 ## 1st Qu.:56.00 1st Qu.:43.00 1st Qu.:55.00 1st Qu.:56.00 1st Qu.:53 ## Median :67.00 Median :58.00 Median :65.00 Median :66.00 Median :62 ## Mean :64.48 Mean :55.74 Mean :63.25 Mean :63.76 Mean :58 ## 3rd Qu.:75.00 3rd Qu.:69.00 3rd Qu.:74.00 3rd Qu.:74.00 3rd Qu.:69 ## Max. :96.00 Max. :96.00 Max. :96.00 Max. :96.00 Max. :95 ## Composure Crossing Curve Dribbling Finishing ## Min. : 5.00 Min. : 5.0 Min. : 6.0 Min. : 2.00 Min. : 2.00 ## 1st Qu.:51.00 1st Qu.:37.0 1st Qu.:34.0 1st Qu.:48.00 1st Qu.:29.00 ## Median :60.00 Median :54.0 Median :48.0 Median :60.00 Median :48.00 ## Mean :57.82 Mean :49.7 Mean :47.2 Mean :54.94 Mean :45.18 ## 3rd Qu.:67.00 3rd Qu.:64.0 3rd Qu.:62.0 3rd Qu.:68.00 3rd Qu.:61.00 ## Max. :96.00 Max. :91.0 Max. :92.0 Max. :97.00 Max. :95.00 ## Free.kick.accuracy GK.diving GK.handling GK.kicking ## Min. : 4.00 Min. : 1.00 Min. : 1.00 Min. : 1.00 ## 1st Qu.:31.00 1st Qu.: 8.00 1st Qu.: 8.00 1st Qu.: 8.00 ## Median :42.00 Median :11.00 Median :11.00 Median :11.00 ## Mean :43.08 Mean :16.78 Mean :16.55 Mean :16.42 ## 3rd Qu.:57.00 3rd Qu.:14.00 3rd Qu.:14.00 3rd Qu.:14.00 ## Max. :93.00 Max. :91.00 Max. :91.00 Max. :95.00 ## GK.positioning GK.reflexes Heading.accuracy Interceptions ## Min. : 1.00 Min. : 1.00 Min. : 4.00 Min. : 4.00 ## 1st Qu.: 8.00 1st Qu.: 8.00 1st Qu.:44.00 1st Qu.:26.00 ## Median :11.00 Median :11.00 Median :55.00 Median :52.00 ## Mean :16.54 Mean :16.91 Mean :52.26 Mean :46.53 ## 3rd Qu.:14.00 3rd Qu.:14.00 3rd Qu.:64.00 3rd Qu.:64.00 ## Max. :91.00 Max. :90.00 Max. :94.00 Max. :92.00 ## Jumping Long.passing Long.shots Marking ## Min. :15.00 Min. : 7.00 Min. : 3.00 Min. : 4.00 ## 1st Qu.:58.00 1st Qu.:42.00 1st Qu.:32.00 1st Qu.:22.00 ## Median :66.00 Median :56.00 Median :51.00 Median :48.00 ## Mean :64.84 Mean :52.37 Mean :47.11 Mean :44.09 ## 3rd Qu.:73.00 3rd Qu.:64.00 3rd Qu.:62.00 3rd Qu.:63.00 ## Max. :95.00 Max. :93.00 Max. :92.00 Max. :92.00 ## Penalties Positioning Reactions Short.passing ## Min. : 5.00 Min. : 2.00 Min. :28.00 Min. :10.00 ## 1st Qu.:39.00 1st Qu.:38.00 1st Qu.:55.00 1st Qu.:53.00 ## Median :50.00 Median :54.00 Median :62.00 Median :62.00 ## Mean :48.92 Mean :49.53 Mean :61.85 Mean :58.22 ## 3rd Qu.:61.00 3rd Qu.:64.00 3rd Qu.:68.00 3rd Qu.:68.00 ## Max. :92.00 Max. :95.00 Max. :96.00 Max. :92.00 ## Shot.power Sliding.tackle Sprint.speed Stamina ## Min. : 3.00 Min. : 4.00 Min. :11.00 Min. :12.00 ## 1st Qu.:46.00 1st Qu.:24.00 1st Qu.:57.00 1st Qu.:56.00 ## Median :59.00 Median :52.00 Median :67.00 Median :66.00 ## Mean :55.57 Mean :45.56 Mean :64.72 Mean :63.13 ## 3rd Qu.:68.00 3rd Qu.:64.00 3rd Qu.:75.00 3rd Qu.:74.00 ## Max. :94.00 Max. :91.00 Max. :96.00 Max. :95.00 ## Standing.tackle Strength Vision Volleys ## Min. : 4.00 Min. :20.00 Min. :10.00 Min. : 4.00 ## 1st Qu.:26.00 1st Qu.:58.00 1st Qu.:43.00 1st Qu.:30.00 ## Median :54.00 Median :66.00 Median :54.00 Median :44.00 ## Mean :47.41 Mean :65.24 Mean :52.93 Mean :43.13 ## 3rd Qu.:66.00 3rd Qu.:74.00 3rd Qu.:64.00 3rd Qu.:57.00 ## Max. :92.00 Max. :98.00 Max. :94.00 Max. :91.00 These variables are scores on the scale of [0,100] that measure 34 key abilities of soccer players. No player has ever earned a score of 100 on any of these attributes - no player is perfect! It would be natural to assume some correlation between these variables and indeed, we see lots of it in the following heatmap visualization of the correlation matrix. cor.matrix = cor(fifa[,13:46]) cor.matrix = melt(cor.matrix) ggplot(data = cor.matrix, aes(x=Var1, y=Var2, fill=value)) + geom_tile(color = &quot;white&quot;)+ scale_fill_gradient2(low = &quot;blue&quot;, high = &quot;red&quot;, mid = &quot;white&quot;, midpoint = 0, limit = c(-1,1), space = &quot;Lab&quot;, name=&quot;Correlation&quot;) + theme_minimal()+ theme(axis.title.x = element_blank(),axis.title.y = element_blank(), axis.text.x = element_text(angle = 45, vjust = 1, size = 9, hjust = 1))+coord_fixed() Figure 14.2: Heatmap of correlation matrix for 34 variables of interest What jumps out right away are the “GK” (Goal Keeping) abilities - these attributes have very strong positive correlation with one another and negative correlation with the other abilities. After all, goal keepers are not traditionally well known for their dribbling, passing, and finishing abilities! Outside of that, we see a lot of red in this correlation matrix – many attributes share a lot of information. This is the type of situation where PCA shines. 14.3.0.2 Principal Components Analysis Let’s take a look at the principal components analysis. Since the variables are on the same scale, I’ll start with covariance PCA (the default in R’s prcomp() function). fifa.pca = prcomp(fifa[,13:46] ) We can then print the summary of variance explained and the loadings on the first 3 components: summary(fifa.pca) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 ## Standard deviation 74.8371 43.5787 23.28767 20.58146 16.12477 10.71539 ## Proportion of Variance 0.5647 0.1915 0.05468 0.04271 0.02621 0.01158 ## Cumulative Proportion 0.5647 0.7561 0.81081 0.85352 0.87973 0.89131 ## PC7 PC8 PC9 PC10 PC11 PC12 PC13 ## Standard deviation 10.17785 9.11852 8.98065 8.5082 8.41550 7.93741 7.15935 ## Proportion of Variance 0.01044 0.00838 0.00813 0.0073 0.00714 0.00635 0.00517 ## Cumulative Proportion 0.90175 0.91013 0.91827 0.9256 0.93270 0.93906 0.94422 ## PC14 PC15 PC16 PC17 PC18 PC19 PC20 ## Standard deviation 7.06502 6.68497 6.56406 6.50459 6.22369 6.08812 6.00578 ## Proportion of Variance 0.00503 0.00451 0.00434 0.00427 0.00391 0.00374 0.00364 ## Cumulative Proportion 0.94926 0.95376 0.95811 0.96237 0.96628 0.97001 0.97365 ## PC21 PC22 PC23 PC24 PC25 PC26 PC27 ## Standard deviation 5.91320 5.66946 5.45018 5.15051 4.86761 4.34786 4.1098 ## Proportion of Variance 0.00353 0.00324 0.00299 0.00267 0.00239 0.00191 0.0017 ## Cumulative Proportion 0.97718 0.98042 0.98341 0.98609 0.98848 0.99038 0.9921 ## PC28 PC29 PC30 PC31 PC32 PC33 PC34 ## Standard deviation 4.05716 3.46035 3.37936 3.31179 3.1429 3.01667 2.95098 ## Proportion of Variance 0.00166 0.00121 0.00115 0.00111 0.0010 0.00092 0.00088 ## Cumulative Proportion 0.99374 0.99495 0.99610 0.99721 0.9982 0.99912 1.00000 fifa.pca$rotation[,1:3] ## PC1 PC2 PC3 ## Acceleration -0.13674335 0.0944478107 -0.141193842 ## Aggression -0.15322857 -0.2030537953 0.105372978 ## Agility -0.13598896 0.1196301737 -0.017763073 ## Balance -0.11474980 0.0865672989 -0.072629834 ## Ball.control -0.21256812 0.0585990154 0.038243802 ## Composure -0.13288575 -0.0005635262 0.163887637 ## Crossing -0.21347202 0.0458210228 0.124741235 ## Curve -0.20656129 0.1254947094 0.180634730 ## Dribbling -0.23090613 0.1259819707 -0.002905379 ## Finishing -0.19431248 0.2534086437 0.006524693 ## Free.kick.accuracy -0.18528508 0.0960404650 0.219976709 ## GK.diving 0.20757999 0.0480952942 0.326161934 ## GK.handling 0.19811125 0.0464542553 0.314165622 ## GK.kicking 0.19261876 0.0456942190 0.304722126 ## GK.positioning 0.19889113 0.0456384196 0.317850121 ## GK.reflexes 0.21081755 0.0489895700 0.332751195 ## Heading.accuracy -0.17218607 -0.1115416097 -0.125135161 ## Interceptions -0.15038835 -0.3669025376 0.162064432 ## Jumping -0.03805419 -0.0579221746 0.012263523 ## Long.passing -0.16849827 -0.0435009943 0.224584171 ## Long.shots -0.21415526 0.1677851237 0.157466462 ## Marking -0.14863254 -0.4076616902 0.078298039 ## Penalties -0.16328049 0.1407803994 0.024403976 ## Positioning -0.22053959 0.1797895382 0.020734699 ## Reactions -0.04780774 0.0001844959 0.250247098 ## Short.passing -0.18176636 -0.0033124240 0.118611543 ## Shot.power -0.19592137 0.0989340925 0.101707386 ## Sliding.tackle -0.14977558 -0.4024030355 0.069945935 ## Sprint.speed -0.13387287 0.0804847541 -0.146049405 ## Stamina -0.17231648 -0.0634639786 -0.016509650 ## Standing.tackle -0.15992073 -0.4039763876 0.086418583 ## Strength -0.02186264 -0.1151018222 0.096053864 ## Vision -0.13027169 0.1152237536 0.260985686 ## Volleys -0.18465028 0.1888480712 0.076974579 It’s clear we can capture a large amount of the variance in this data with just a few components. In fact just 2 components yield 76% of the variance! Now let’s look at some projections of the players onto those 2 principal components. The scores are located in the fifa.pca$x matrix. plot(fifa.pca$x[,1],fifa.pca$x[,2], col=alpha(c(&#39;red&#39;,&#39;blue&#39;,&#39;green&#39;,&#39;black&#39;)[as.factor(fifa$position)],0.4), pch=16, xlab = &#39;Principal Component 1&#39;, ylab=&#39;Principal Component 2&#39;, main = &#39;Projection of Players onto 2 PCs, Colored by Position&#39;) legend(125,-45, c(&#39;Forward&#39;,&#39;Defense&#39;,&#39;Midfield&#39;,&#39;GoalKeeper&#39;), c(&#39;red&#39;,&#39;blue&#39;,&#39;green&#39;,&#39;black&#39;), bty = &#39;n&#39;, cex=1.1) Figure 14.3: Projection of the FIFA players’ skill data into 2 dimensions. Player positions are evident. The plot easily separates the field players from the goal keepers, and the forwards from the defenders. As one might expect, midfielders are sandwiched by the forwards and defenders, as they play both roles on the field. The labeling of player position was imperfect and done using a list of the players’ preferred positions, and it’s likely we are seeing that in some of the players labeled as midfielders that appear above the cloud of red points. We can also attempt a 3-dimensional projection of this data: library(plotly) library(processx) colors=alpha(c(&#39;red&#39;,&#39;blue&#39;,&#39;green&#39;,&#39;black&#39;)[as.factor(fifa$position)],0.4) graph = plot_ly(x = fifa.pca$x[,1], y = fifa.pca$x[,2], z= fifa.pca$x[,3], type=&#39;scatter3d&#39;, mode=&quot;markers&quot;, marker = list(color=colors)) graph Figure 14.4: Projection of the FIFA players’ skill data into 3 dimensions. Player positions are evident. 14.3.0.3 The BiPlot BiPlots can be tricky when we have so much data and so many variables. As you will see, the default image leaves much to be desired, and will motivate our move to the ggfortify library to use the autoplot() function. The image takes too long to render and is practically unreadable with the whole dataset, so I demonstrate the default biplot() function with a sample of the observations. biplot(fifa.pca$x[sample(1:16501,2000),],fifa.pca$rotation[,1:2], cex=0.5, arrow.len = 0.1) Figure 14.5: The default biplot function leaves much to be desired here The autoplot function uses the `ggplot2``` package and is superior when we have more data. autoplot(fifa.pca, data = fifa, colour = alpha(c(&#39;red&#39;,&#39;blue&#39;,&#39;green&#39;,&#39;orange&#39;)[as.factor(fifa$pos)],0.4), loadings = TRUE, loadings.colour = &#39;black&#39;, loadings.label = TRUE, loadings.label.size = 3.5, loadings.label.alpha = 1, loadings.label.fontface=&#39;bold&#39;, loadings.label.colour = &#39;black&#39;, loadings.label.repel=T) ## Warning: `select_()` was deprecated in dplyr 0.7.0. ## Please use `select()` instead. ## Warning in if (value %in% columns) {: the condition has length &gt; 1 and only the ## first element will be used Figure 14.6: The autoplot() biplot has many more options for readability. Many expected conclusions can be drawn from this biplot. The defenders tend to have stronger skills of interception, slide tackling, standing tackling, and marking, while forwards are generally stronger when it comes to finishing, long.shots, volleys, agility etc. Midfielders are likely to be stronger with crossing, passing, ball.control, and stamina. 14.3.0.4 Further Exploration Let’s see what happens if we color by the variable ‘overall’ which is designed to rank a player’s overall quality of play. palette(alpha(magma(100),0.6)) plot(fifa.pca$x[,1],fifa.pca$x[,2], col=fifa$Overall,pch=16, xlab = &#39;Principal Component 1&#39;, ylab=&#39;Principal Component 2&#39;) color.legend(130,-100,220,-90,seq(0,100,50),alpha(magma(100),0.6),gradient=&quot;x&quot;) Figure 14.7: Projection of Players onto 2 PCs, Colored by “Overall” Ability We can attempt to label some of the outliers, too. First, we’ll look at the 0.001 and 0.999 quantiles to get a sense of what coordinates we want to highlight. Then we’ll label any players outside of those bounds and surely find some familiar names. # This first chunk is identical to the chunk above. I have to reproduce the plot to label it. palette(alpha(magma(100),0.6)) plot(fifa.pca$x[,1], fifa.pca$x[,2], col=fifa$Overall,pch=16, xlab = &#39;Principal Component 1&#39;, ylab=&#39;Principal Component 2&#39;, xlim=c(-175,250), ylim = c(-150,150)) color.legend(130,-100,220,-90,seq(0,100,50),alpha(magma(100),0.6),gradient=&quot;x&quot;) # Identify quantiles (high/low) for each PC (quant1h = quantile(fifa.pca$x[,1],0.9997)) ## 99.97% ## 215.4003 (quant1l = quantile(fifa.pca$x[,1],0.0003)) ## 0.03% ## -130.1493 (quant2h = quantile(fifa.pca$x[,2],0.9997)) ## 99.97% ## 100.208 (quant2l = quantile(fifa.pca$x[,2],0.0003)) ## 0.03% ## -101.8846 # Next I create a logical vector which identifies the outliers # (i.e. TRUE = outlier, FALSE = not outlier) outliers = fifa.pca$x[,1] &gt; quant1h | fifa.pca$x[,1] &lt; quant1l | fifa.pca$x[,2] &gt; quant2h | fifa.pca$x[,2] &lt; quant2l # Here I label them by name, jittering the coordinates of the text so it&#39;s more readable text(jitter(fifa.pca$x[outliers,1],factor=1), jitter(fifa.pca$x[outliers,2],factor=600), fifa$Name[outliers], cex=0.7) What about by wage? First we need to convert their salary, denominated in Euros, to a numeric variable. # First, observe the problem with the Wage column as it stands head(fifa$Wage) ## [1] &quot;€565K&quot; &quot;€565K&quot; &quot;€280K&quot; &quot;€510K&quot; &quot;€230K&quot; &quot;€355K&quot; # Use regular expressions to remove the Euro sign and K from the wage column # then covert to numeric fifa$Wage = as.numeric(gsub(&#39;[€K]&#39;, &#39;&#39;, fifa$Wage)) # new data: head(fifa$Wage) ## [1] 565 565 280 510 230 355 palette(alpha(magma(100),0.6)) plot(fifa.pca$x[,1], fifa.pca$x[,2], col=fifa$Wage,pch=16, xlab = &#39;Principal Component 1&#39;, ylab=&#39;Principal Component 2&#39;) color.legend(130,-100,220,-90,c(min(fifa$Wage),max(fifa$Wage)),alpha(magma(100),0.6),gradient=&quot;x&quot;) Figure 14.8: Projection of Players onto 2 Principal Components, Colored by Wage 14.3.0.5 Rotations of Principal Components We might be able to align our axes more squarely with groups of original variables that are strongly correlated and tell a story. Perhaps we might be able to find latent variables that indicate the position specific ability of players. Let’s see what falls out after varimax and quartimax rotation. Recall that in order to employ rotations, we have to first decide on a number of components. A quick look at a screeplot or cumulative proportion variance explained should help to that aim. plot(cumsum(fifa.pca$sdev^2)/sum(fifa.pca$sdev^2), type = &#39;b&#39;, cex=.75, xlab = &quot;# of components&quot;, ylab = &quot;% variance explained&quot;) Figure 14.9: Cumulative proportion of variance explained by rank of the decomposition (i.e. the number of components) Let’s use 3 components, since the marginal benefit of using additional components seems small. Once we rotate the loadings, we can try to use a heatmap to visualize what they might represent. vmax = varimax(fifa.pca$rotation[,1:3]) loadings = fifa.pca$rotation[,1:3]%*%vmax$rotmat melt.loadings = melt(loadings) ggplot(data = melt.loadings, aes(x=Var2, y=Var1, fill=value)) + geom_tile(color = &quot;white&quot;)+ scale_fill_gradient2(low = &quot;blue&quot;, high = &quot;red&quot;, mid = &quot;white&quot;, midpoint = 0, limit = c(-1,1)) 14.4 Cancer Genetics Read in the data. The load() function reads in a dataset that has 20532 columns and may take some time. You may want to save and clear your environment (or open a new RStudio window) if you have other work open. load(&#39;LAdata/geneCancerUCI.RData&#39;) table(cancerlabels$Class) ## ## BRCA COAD KIRC LUAD PRAD ## 300 78 146 141 136 Original Source: The cancer genome atlas pan-cancer analysis project BRCA = Breast Invasive Carcinoma COAD = Colon Adenocarcinoma KIRC = Kidney Renal clear cell Carcinoma LUAD = Lung Adenocarcinoma PRAD = Prostate Adenocarcinoma We are going to want to plot the data points according to their different classification labels. We should pick out a nice color palette for categorical attributes. We chose to assign palette Dark2 but feel free to choose any categorical palette that attracts you in the code below! library(RColorBrewer) display.brewer.all() palette(brewer.pal(n = 8, name = &quot;Dark2&quot;)) The first step is typically to explore the data. Obviously we can’t look at ALL the scatter plots of input variables. For the fun of it, let’s look at a few of these scatter plots which we’ll pick at random. First pick two column numbers at random, then draw the plot, coloring by the label. You could repeat this chunk several times to explore different combinations. Can you find one that does a good job of separating any of the types of cancer? par(mfrow=c(2,3)) for(i in 1:6){ randomColumns = sample(2:20532,2) plot(cancer[,randomColumns],col = cancerlabels$Class) } Figure 14.10: Random 2-Dimensional Projections of Cancer Data To restore our plot window from that 3-by-2 grid, we run dev.off() dev.off() ## null device ## 1 14.4.1 Computing the PCA The function is the one I most often recommend for reasonably sized principal component calculations in R. This function returns a list with class “prcomp” containing the following components (from help prcomp): : the standard deviations of the principal components (i.e., the square roots of the eigenvalues of the covariance/correlation matrix, though the calculation is actually done with the singular values of the data matrix). : the matrix of variable loadings (i.e., a matrix whose columns contain the eigenvectors). The function princomp returns this in the element loadings. : if retx is true the value of the rotated data (i.e. the scores) (the centred (and scaled if requested) data multiplied by the rotation matrix) is returned. Hence, cov(x) is the diagonal matrix \\(diag(sdev^2)\\). For the formula method, napredict() is applied to handle the treatment of values omitted by the na.action. : the centering and scaling used, or FALSE. The option inside the function instructs the program to use correlation PCA. The default is covariance PCA. Now let’s compute the first three principal components and examine the data projected onto the first 2 axes. We can then look in 3 dimensions. pcaOut = prcomp(cancer,rank = 3, scale = F) plot(pcaOut$x[,1], pcaOut$x[,2], col = cancerlabels$Class, xlab = &quot;Principal Component 1&quot;, ylab = &quot;Principal Component 2&quot;, main = &#39;Genetic Samples Projected into 2-dimensions \\n using COVARIANCE PCA&#39;) Figure 14.11: Covariance PCA of genetic data 14.4.2 3D plot with package Make sure the plotly package is installed for the 3d plot. To get the plot points colored by group, we need to execute the following command that creates a vector of colors (specifying a color for each observation). colors = factor(palette()) colors = colors[cancerlabels$Class] table(colors, cancerlabels$Class) ## ## colors BRCA COAD KIRC LUAD PRAD ## #00000499 300 0 0 0 0 ## #01010799 0 78 0 0 0 ## #02020B99 0 0 146 0 0 ## #03031199 0 0 0 141 0 ## #05041799 0 0 0 0 136 ## #07061C99 0 0 0 0 0 ## #09072199 0 0 0 0 0 ## #0C092699 0 0 0 0 0 ## #0F0B2C99 0 0 0 0 0 ## #120D3299 0 0 0 0 0 ## #150E3799 0 0 0 0 0 ## #180F3E99 0 0 0 0 0 ## #1C104499 0 0 0 0 0 ## #1F114A99 0 0 0 0 0 ## #22115099 0 0 0 0 0 ## #26125799 0 0 0 0 0 ## #2A115D99 0 0 0 0 0 ## #2F116399 0 0 0 0 0 ## #33106899 0 0 0 0 0 ## #38106C99 0 0 0 0 0 ## #3C0F7199 0 0 0 0 0 ## #400F7499 0 0 0 0 0 ## #45107799 0 0 0 0 0 ## #49107899 0 0 0 0 0 ## #4E117B99 0 0 0 0 0 ## #51127C99 0 0 0 0 0 ## #56147D99 0 0 0 0 0 ## #5A167E99 0 0 0 0 0 ## #5D177F99 0 0 0 0 0 ## #61198099 0 0 0 0 0 ## #661A8099 0 0 0 0 0 ## #6A1C8199 0 0 0 0 0 ## #6D1D8199 0 0 0 0 0 ## #721F8199 0 0 0 0 0 ## #76218199 0 0 0 0 0 ## #79228299 0 0 0 0 0 ## #7D248299 0 0 0 0 0 ## #82258199 0 0 0 0 0 ## #86278199 0 0 0 0 0 ## #8A298199 0 0 0 0 0 ## #8E2A8199 0 0 0 0 0 ## #922B8099 0 0 0 0 0 ## #962C8099 0 0 0 0 0 ## #9B2E7F99 0 0 0 0 0 ## #9F2F7F99 0 0 0 0 0 ## #A3307E99 0 0 0 0 0 ## #A7317D99 0 0 0 0 0 ## #AB337C99 0 0 0 0 0 ## #AF357B99 0 0 0 0 0 ## #B3367A99 0 0 0 0 0 ## #B8377999 0 0 0 0 0 ## #BC397899 0 0 0 0 0 ## #C03A7699 0 0 0 0 0 ## #C43C7599 0 0 0 0 0 ## #C83E7399 0 0 0 0 0 ## #CD407199 0 0 0 0 0 ## #D0416F99 0 0 0 0 0 ## #D5446D99 0 0 0 0 0 ## #D8456C99 0 0 0 0 0 ## #DC486999 0 0 0 0 0 ## #DF4B6899 0 0 0 0 0 ## #E34E6599 0 0 0 0 0 ## #E6516399 0 0 0 0 0 ## #E9556299 0 0 0 0 0 ## #EC586099 0 0 0 0 0 ## #EE5C5E99 0 0 0 0 0 ## #F1605D99 0 0 0 0 0 ## #F2655C99 0 0 0 0 0 ## #F4695C99 0 0 0 0 0 ## #F66D5C99 0 0 0 0 0 ## #F7735C99 0 0 0 0 0 ## #F9785D99 0 0 0 0 0 ## #F97C5D99 0 0 0 0 0 ## #FA815F99 0 0 0 0 0 ## #FB866199 0 0 0 0 0 ## #FC8A6299 0 0 0 0 0 ## #FC906599 0 0 0 0 0 ## #FCEFB199 0 0 0 0 0 ## #FCF4B699 0 0 0 0 0 ## #FCF8BA99 0 0 0 0 0 ## #FCFDBF99 0 0 0 0 0 ## #FD956799 0 0 0 0 0 ## #FD9A6A99 0 0 0 0 0 ## #FDDC9E99 0 0 0 0 0 ## #FDE1A299 0 0 0 0 0 ## #FDE5A799 0 0 0 0 0 ## #FDEBAB99 0 0 0 0 0 ## #FE9E6C99 0 0 0 0 0 ## #FEA36F99 0 0 0 0 0 ## #FEA87399 0 0 0 0 0 ## #FEAC7699 0 0 0 0 0 ## #FEB27A99 0 0 0 0 0 ## #FEB67D99 0 0 0 0 0 ## #FEBB8199 0 0 0 0 0 ## #FEC08599 0 0 0 0 0 ## #FEC48899 0 0 0 0 0 ## #FEC98D99 0 0 0 0 0 ## #FECD9099 0 0 0 0 0 ## #FED39599 0 0 0 0 0 ## #FED79999 0 0 0 0 0 library(plotly) graph = plot_ly(x = pcaOut$x[,1], y = pcaOut$x[,2], z= pcaOut$x[,3], type=&#39;scatter3d&#39;, mode=&quot;markers&quot;, marker = list(color=colors)) graph 14.4.3 3D plot with package library(rgl) ## ## Attaching package: &#39;rgl&#39; ## The following object is masked from &#39;package:plotrix&#39;: ## ## mtext3d knitr::knit_hooks$set(webgl = hook_webgl) Make sure the rgl package is installed for the 3d plot. plot3d(x = pcaOut$x[,1], y = pcaOut$x[,2], z= pcaOut$x[,3], col = colors, xlab = &quot;Principal Component 1&quot;, ylab = &quot;Principal Component 2&quot;, zlab = &quot;Principal Component 3&quot;) You must enable Javascript to view this page properly. 14.4.4 Variance explained Proportion of Variance explained by 2,3 components: summary(pcaOut) ## Importance of first k=3 (out of 801) components: ## PC1 PC2 PC3 ## Standard deviation 75.7407 61.6805 58.57297 ## Proportion of Variance 0.1584 0.1050 0.09472 ## Cumulative Proportion 0.1584 0.2634 0.35815 # Alternatively, if you had computed the ALL the principal components (omitted the rank=3 option) then # you could directly compute the proportions of variance explained using what we know about the # eigenvalues: # sum(pcaOut$sdev[1:2]^2)/sum(pcaOut$sdev^2) # sum(pcaOut$sdev[1:3]^2)/sum(pcaOut$sdev^2) 14.4.5 Using Correlation PCA The data involved in this exercise are actually on the same scale, and normalizing them may not be in your best interest because of this. However, it’s always a good idea to explore both decompositions if you have time. pca.cor = prcomp(cancer, rank=3, scale =T) An error message! Cannot rescale a constant/zero column to unit variance. Solution: check for columns with zero variance and remove them. Then, re-check dimensions of the matrix to see how many columns we lost. cancer = cancer[,apply(cancer, 2, sd)&gt;0 ] dim(cancer) ## [1] 801 20264 Once we’ve taken care of those zero-variance columns, we can proceed to compute the correlation PCA: pca.cor = prcomp(cancer, rank=3, scale =T) plot(pca.cor$x[,1], pca.cor$x[,2], col = cancerlabels$Class, xlab = &quot;Principal Component 1&quot;, ylab = &quot;Principal Component 2&quot;, main = &#39;Genetic Samples Projected into 2-dimensions \\n using CORRELATION PCA&#39;) Figure 14.12: Correlation PCA of genetic data And it’s clear just from the 2-dimensional projection that correlation PCA does not seem to work as well as covariance PCA when it comes to separating the 4 different types of cancer. Indeed, we can confirm this from the proportion of variance explained, which is substantially lower than that of covariance PCA: summary(pca.cor) ## Importance of first k=3 (out of 801) components: ## PC1 PC2 PC3 ## Standard deviation 46.2145 42.11838 39.7823 ## Proportion of Variance 0.1054 0.08754 0.0781 ## Cumulative Proportion 0.1054 0.19294 0.2710 14.4.6 Range standardization as an alternative to covariance PCA We can also put all the variables on a scale of 0 to 1 if we’re concerned about issues with scale (in this case, scale wasn’t an issue - but the following approach still might be provide interesting projections in some datasets). This transformation would be as follows for each variable \\(\\mathbf{x}\\): \\[\\frac{\\mathbf{x} - \\min(\\mathbf{x})}{\\max(\\mathbf{x})-\\min(\\mathbf{x})}\\] cancer = cancer[,apply(cancer,2,sd)&gt;0] min = apply(cancer,2,min) range = apply(cancer,2, function(x){max(x)-min(x)}) minmax.cancer=scale(cancer,center=min,scale=range) Then we can compute the covariance PCA of that range-standardized data without concern: minmax.pca = prcomp(minmax.cancer, rank=3, scale=F ) plot(minmax.pca$x[,1],minmax.pca$x[,2],col = cancerlabels$Class, xlab = &quot;Principal Component 1&quot;, ylab = &quot;Principal Component 2&quot;) Figure 14.13: Covariance PCA of range standardized genetic data "],["svd.html", "Chapter 15 The Singular Value Decomposition (SVD) 15.1 Resolving a Matrix into Components 15.2 Data Compression 15.3 Noise Reduction", " Chapter 15 The Singular Value Decomposition (SVD) The Singular Value Decomposition (SVD) is one of the most important concepts in applied mathematics. It is used for a number of application including dimension reduction and data analysis. Principal Components Analysis (PCA) is a special case of the SVD. Let’s start with the formal definition, and then see how PCA relates to that definition. Definition 15.1 (Singular Value Decomposition) For any \\(m\\times n\\) matrix \\(\\A\\) with \\(rank(\\A)=r\\), there are orthogonal matrices \\(\\U_{m\\times m}\\) and \\(\\V_{n\\times n}\\) and a diagonal matrix \\(\\D_{r\\times r}=diag(\\sigma_1,\\sigma_2,\\dots,\\sigma_r)\\) such that \\[\\begin{equation} \\tag{15.1} \\A = \\U \\underbrace{\\pm \\D &amp; \\bo{0} \\\\\\bo{0}&amp;\\bo{0} \\mp}_{\\text{$m\\times n$}} \\V^T \\quad \\mbox{with}\\quad \\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_r\\geq 0 \\end{equation}\\] The \\(\\sigma_i\\)’s are called the nonzero singular values of \\(\\A\\). (When \\(r&lt;p=\\min\\{m,n\\}\\) (i.e. when \\(\\A\\) is not full-rank), \\(\\A\\) is said to have an additional \\(p-r\\) zero singular values). This factorization is called a singular value decomposition of \\(\\A\\), and the columns of \\(\\U\\) and \\(\\V\\) are called the left- and right-hand singular vectors for \\(\\A\\), respectively. Properties of the SVD 1. The left-hand singular vectors are a set of orthonormal eigenvectors for \\(\\A\\A^T\\). 2. The right-hand singular vectors are a set of orthonormal eigenvectors for \\(\\A^T\\A\\). 3. The singular values are the square roots of the eigenvalues for \\(\\A^T\\A \\mbox{ and } \\A\\A^T\\), as these matrices have the same eigenvalues. 4. The first singular value is equal to the matrix two-norm: \\[\\sigma_1 = \\max_{\\|\\x\\|=1} \\|\\A\\x\\|_2 = \\|\\A\\|_2\\] 5. The Frobenius norm of the matrix is also related to the singular values: \\[\\|\\A\\|_F = \\sqrt{\\sum_{i,j} A_{ij}^2} = \\sqrt{\\sum_{i=1}^r \\sigma_i^2}\\] 6. Singular values represent distances to lower rank matrices. \\[\\sigma_{k+1}=\\min_{rank(\\bo{B})=k} \\|\\A-\\bo{B}\\|_2\\] 7. The truncated SVD (Equation (15.3)) provides the closest rank k approximation to our original matrix in the Euclidean sense. When we studied PCA, one of the goals was to find the new coordinates, or scores, of the data in the principal components basis. If our original (centered or standardized) data was contained in the matrix \\(\\X\\) and the eigenvectors of the covariance/correlation matrix (\\(\\X^T\\X\\)) were columns of a matrix \\(\\V\\), then to find the scores (call these \\(\\mathbf{S}\\)) of the observations on the eigenvectors we used the following equation (which is the transpose of Equation (13.3)): \\[\\X=\\mathbf{S}\\V^T.\\] This equation mimics Equation (15.1) because the matrix \\(\\V^T\\) in Equation (eq:svd) is also a matrix of eigenvectors for \\(\\A^T\\A\\). This means that the principal component scores \\(\\mathbf{S}\\) are actually a set of unit eigenvectors for \\(\\A\\A^T\\) scaled by the singular values in \\(\\D\\): \\[\\mathbf{S}=\\U \\pm \\D &amp; \\bo{0} \\\\ \\bo{0}&amp;\\bo{0} \\mp .\\] 15.1 Resolving a Matrix into Components One of the primary goals of the singular value decomposition is to resolve the data in \\(\\A\\) into \\(r\\) mutually orthogonal components by writing the matrix factorization as a sum of outer products using the corresponding columns of \\(\\U\\) and rows of \\(\\V^T\\): \\[\\A = \\U \\pm \\D &amp; \\bo{0} \\\\\\bo{0}&amp;\\bo{0} \\mp\\V^T = \\pm \\u_1 &amp; \\u_2 &amp; \\dots &amp;\\u_m \\mp \\pm \\sigma_1 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 \\\\ 0 &amp; \\ddots &amp; 0 &amp; \\vdots &amp; 0 \\\\ \\vdots &amp; 0&amp; \\sigma_r &amp; 0 &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp;0\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp;0 \\mp \\pm \\v_1^T \\\\ \\v_2^T \\\\ \\vdots \\\\ \\v_n^T \\mp\\] \\[= \\sigma_1\\u_1\\v_1^T+\\sigma_2\\u_2\\v_2^T+\\dots+\\sigma_r\\u_r\\v_r^T.\\] \\[\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\sigma_r\\] For simplicity, let \\(\\Z_i=\\u_i\\v_i^T\\) act as basis matrices for this expansion, so we have \\[\\begin{equation} \\tag{15.2} \\A=\\sum_{i=1}^r \\sigma_i \\Z_i. \\end{equation}\\] This representation can be regarded as a Fourier expansion. The coefficient (singular value) \\(\\sigma_i\\) can be interpreted as the proportion of \\(\\A\\) lying in the “direction” of \\(\\Z_i\\). When \\(\\sigma_i\\) is small, omitting that term from the expansion will cause only a small amount of the information in \\(\\A\\) to be lost. This fact has important consequences for compression and noise reduction. 15.2 Data Compression We’ve already seen how PCA can be used to reduce the dimensions of our data while keeping the most amount of variance. The way this is done is by simply ignoring those components for which the proportion of variance is small. Supposing we keep \\(k\\) principal components, this amounts to truncating the sum in Equation (15.2) after \\(k\\) terms: \\[\\begin{equation} \\tag{15.3} \\A \\approx \\sum_{i=1}^{k} \\sigma_i \\Z_i. \\end{equation}\\] As it turns out, this truncation has important consequences in many applications. One example is that of image compression. An image is simply an array of pixels. Supposing the image size is \\(m\\) pixels tall by \\(n\\) pixels wide, we can capture this information in an \\(m\\times n\\) matrix if the image is in grayscale, or an \\(m\\times 3n\\) matrix for a [r,g,b] color image (we’d need 3 values for each pixel to recreate the pixel’s color). These matrices can get very large (a 6 megapixel photo is 6 million pixels). Rather than store the entire matrix, we can store an approximation to the matrix using only a few (well, more than a few) singular values and singular vectors. This is the basis of image compression. An approximated photo will not be as crisp as the original - some information will be lost - but most of the time we can store much less than the original matrix and still get a good depiction of the image. 15.3 Noise Reduction Many applications arise where the relevant information contained in a matrix is contaminated by a certain level of noise. This is particularly common with video and audio signals, but also arises in text data and other types of (usually high dimensional) data. The truncated SVD (Equation (15.3)) can actually reduce the amount of noise in data and increase the overall signal-to-noise ratio under certain conditions. Let’s suppose, for instance, that our matrix \\(\\A_{m\\times n}\\) contains data which is contaminated by noise. If that noise is assumed to be random (or nondirectional) in the sense that the noise is distributed more or less uniformly across the components \\(\\Z_i\\), then there is just as much noise “in the direction” of one \\(\\Z_i\\) as there is in the other. If the amount of noise along each direction is approximately the same, and the \\(\\sigma_i\\)’s tell us how much (relevant) information in \\(\\A\\) is directed along each component \\(\\Z_i\\), then it must be that the ratio of “signal” (relevant information) to noise is decreasing across the ordered components, since \\[\\sigma_1 \\geq \\sigma_2\\geq \\dots \\geq \\sigma_r\\] implies that the signal is greater in earlier components. So letting \\(SNR(\\sigma_i\\Z_i)\\) denote the signal-to-noise ratio of each component, we have \\[SNR(\\sigma_1\\Z_1) \\geq SNR(\\sigma_2\\Z_2)\\geq \\dots \\geq SNR(\\sigma_r\\Z_r)\\] This explains why the truncated SVD, \\[\\A \\approx \\sum_{i=1}^{k} \\sigma_i \\Z_i \\quad \\mbox{where}\\quad k&lt;r\\] can, in many scenarios, filter out some of the noise without losing much of the significant information in \\(\\A\\). "],["svdapp.html", "Chapter 16 Applications of SVD 16.1 Text Mining 16.2 Image Compression", " Chapter 16 Applications of SVD 16.1 Text Mining Text mining is another area where the SVD is used heavily. In text mining, our data structure is generally known as a Term-Document Matrix. The documents are any individual pieces of text that we wish to analyze, cluster, summarize or discover topics from. They could be sentences, abstracts, webpages, or social media updates. The terms are the words contained in these documents. The term-document matrix represents what’s called the “bag-of-words” approach - the order of the words is removed and the data becomes unstructured in the sense that each document is represented by the words it contains, not the order or context in which they appear. The \\((i,j)\\) entry in this matrix is the number of times term \\(j\\) appears in document \\(i\\). Definition 16.1 (Term-Document Matrix) Let \\(m\\) be the number of documents in a collection and \\(n\\) be the number of terms appearing in that collection, then we create our term-document matrix \\(\\A\\) as follows: \\[\\begin{equation} \\begin{array}{ccc} &amp; &amp; \\text{term 1} \\quad \\text{term $j$} \\,\\, \\text{term $n$} \\\\ \\A_{m\\times n} = &amp; \\begin{array}{c} \\hbox{Doc 1} \\\\ \\\\ \\\\ \\hbox{Doc $i$} \\\\ \\\\ \\hbox{Doc $m$} \\\\ \\end{array} &amp; \\left( \\begin{array}{ccccccc} &amp; &amp; &amp; |&amp; &amp; &amp; \\\\ &amp; &amp; &amp; |&amp; &amp; &amp; \\\\ &amp; &amp; &amp; |&amp; &amp; &amp; \\\\ &amp; - &amp; - &amp;f_{ij} &amp; &amp; &amp; \\\\ &amp; &amp; &amp; &amp; &amp; &amp; \\\\ &amp; &amp; &amp; &amp; &amp; &amp; \\\\ \\end{array} \\right) \\end{array} \\nonumber \\end{equation}\\] where \\(f_{ij}\\) is the frequency of term \\(j\\) in document \\(i\\). A binary term-document matrix will simply have \\(\\A_{ij}=1\\) if term \\(j\\) is contained in document \\(i\\). 16.1.1 Note About Rows vs. Columns You might be asking yourself, “Hey, wait a minute. Why do we have documents as columns in this matrix? Aren’t the documents like our observations?” Sure! Many data scientists insist on having the documents on the rows of this matrix. But, before you do that, you should realize something. Many SVD and PCA routines are created in a way that is more efficient when your data is long vs. wide, and text data commonly has more terms than documents. The equivalence of the two presentations should be easy to see in all matrix factorization applications. If we have \\[\\A = \\U\\mathbf{D}\\V^T\\] then, \\[\\A^T = \\V\\mathbf{D}\\U^T\\] so we merely need to switch our interpretations of the left- and right-singular vectors to switch from document columns to document rows. Beyond any computational efficiency argument, we prefer to keep our documents on the columns here because of the emphasis placed earlier in this text regarding matrix multiplication viewed as a linear combination of columns. The animation in Figure 2.6 is a good thing to be clear on before proceeding here. 16.1.2 Term Weighting Term-document matrices tend to be large and sparse. Term-weighting schemes are often used to downplay the effect of commonly used words and bolster the effect of rare but semantically important words . The most popular weighting method is known as Term Frequency-Inverse Document Frequency (TF-IDF). For this method, the raw term-frequencies \\(f_{ij}\\) in the matrix \\(\\A\\) are multiplied by global weights called inverse document frequencies, \\(w_i\\), for each term. These weights reflect the commonality of each term across the entire collection and ultimately quantify a term’s ability to narrow one’s search results (the foundations of text analysis were, after all, dominated by search technology). The inverse document frequency of term \\(i\\) is: \\[w_i = \\log \\left( \\frac{\\mbox{total # of documents}}{\\mbox{# documents containing term } i} \\right)\\] To put this weight in perspective, for a collection of \\(n=10,000\\) documents we have \\(0\\leq w_j \\leq 9.2\\), where \\(w_j=0\\) means the word is contained in every document (rendering it useless for search) and \\(w_j=9.2\\) means the word is contained in only 1 document (making it very useful for search). The document vectors are often normalized to have unit 2-norm, since their directions (not their lengths) in the term-space is what characterizes them semantically. 16.1.3 Other Considerations In dealing with text, we want to do as much as we can do minimize the size of the dictionary (the collection of terms which enumerate the rows of our term-document matrix) for both computational and practical reasons. The first effort we’ll make toward this goal is to remove so-called stop words, or very common words that appear in a great many sentences like articles (“a,” “an,” “the”) and prepositions (“about,” “for,” “at”) among others. Many projects also contain domain-specific stop words. For example, one might remove the word “Reuters” from a corpus of Reuters’ newswires. The second effort we’ll often make is to apply a stemming algorithm which reduces words to their stem. For example, the words “swimmer” and “swimming” would both be reduced to their stem, “swim.” Stemming and stop word removal can greatly reduce the size of the dictionary and also help draw meaningful connections between documents. 16.1.4 Latent Semantic Indexing The noise-reduction property of the SVD was extended to text processing in 1990 by Susan Dumais et al, who named the effect Latent Semantic Indexing (LSI). LSI involves the singular value decomposition of the term-document matrix defined in Definition 16.1. In other words, it is like a principal components analysis using the unscaled, uncentered inner-product matrix \\(\\A^T\\A\\). If the documents are normalized to have unit length, this is a matrix of cosine similarities (see Chapter 6). Cosine similarity is the most common measure of similarity between documents for text mining. If the term-document matrix is binary, this is often called the co-occurrence matrix because each entry gives the number of times two words occur in the same document. It certainly seems logical to view text data in this context as it contains both an informative signal and semantic noise. LSI quickly grew roots in the information retrieval community, where it is often used for query processing. The idea is to remove semantic noise, due to variation and ambiguity in vocabulary and presentation style, without losing significant amounts of information. For example, a human may not differentiate between the words “car” and “automobile,” but indeed the words will become two separate entities in the raw term-document matrix. The main idea in LSI is that the realignment of the data into fewer directions should force related documents (like those containing “car” and “automobile”) closer together in an angular sense, thus revealing latent semantic connections. Purveyors of LSI suggest that the use of the Singular Value Decomposition to project the documents into a lower-dimensional space results in a representation which reflects the major associative patterns of the data while ignoring less important influences. This projection is done with the simple truncation of the SVD shown in Equation (15.3). As we have seen with other types of data, the very nature of dimension reduction makes possible for two documents with similar semantic properties to be mapped closer together. Unfortunately, the mixture of signs (positive and negative) in the singular vectors (think principal components) makes the decomposition difficult to interpret. While the major claims of LSI are legitimate, this lack of interpretability is still conceptually problematic for some folks. In order to make this point as clear as possible, consider the original “term basis” representation for the data, where each document (from a collection containing \\(m\\) total terms in the dictionary) could be written as: \\[\\A_j = \\sum_{i=1}^{m} f_{ij}\\e_i\\] where \\(f_{ij}\\) is the frequency of term \\(i\\) in the document, and \\(\\e_i\\) is the \\(i^{th}\\) column of the \\(m\\times m\\) identity matrix. The truncated SVD gives us a new set of coordinates (scores) and basis vectors (principal component features): \\[\\A_j \\approx \\sum_{i=1}^r \\alpha_i \\u_i\\] but the features \\(\\u_i\\) live in the term space, and thus ought to be interpretable as a linear combinations of the original “term basis.” However the linear combinations, having both positive and negative coefficients, tends to be semantically obscure in practice - These new features do not often form meaningful topics for the text, although they often do organize in a meaningful way as we will demonstrate in the next section. 16.1.5 Example Let’s consider a corpus of short documents, perhaps status updates from social media sites. We’ll keep this corpus as minimal as possible to demonstrate the utility of the SVD for text. Figure 16.1: A corpus of 6 documents. Words occurring in more than one document appear in bold. Stop words removed, stemming utilized. Document numbers correspond to term-document matrix below. \\[\\begin{equation*} \\begin{array}{cc} &amp; \\begin{array}{cccccc} \\;doc_1\\; &amp; \\;doc_2\\;&amp; \\;doc_3\\;&amp; \\;doc_4\\;&amp; \\;doc_5\\;&amp; \\;doc_6\\; \\end{array}\\\\ \\begin{array}{c} \\hbox{cat} \\\\ \\hbox{dog}\\\\ \\hbox{eat}\\\\ \\hbox{tired} \\\\ \\hbox{toy}\\\\ \\hbox{injured} \\\\ \\hbox{ankle} \\\\ \\hbox{broken} \\\\ \\hbox{swollen} \\\\ \\hbox{sprained} \\\\ \\end{array} &amp; \\left( \\begin{array}{cccccc} \\quad 1\\quad &amp; \\quad 2\\quad &amp; \\quad 2\\quad &amp; \\quad 0\\quad &amp; \\quad 0\\quad &amp; \\quad 0\\quad \\\\ \\quad 2\\quad &amp; \\quad 3\\quad &amp; \\quad 2\\quad &amp; \\quad 0\\quad &amp; \\quad 0\\quad &amp; \\quad 0\\quad \\\\ \\quad 2\\quad &amp; \\quad 0\\quad &amp; \\quad 1\\quad &amp; \\quad 0\\quad &amp; \\quad 0\\quad &amp; \\quad 0\\quad \\\\ \\quad 0\\quad &amp; \\quad 1\\quad &amp; \\quad 0\\quad &amp; \\quad 0\\quad &amp; \\quad 1\\quad &amp; \\quad 0\\quad \\\\ \\quad 0\\quad &amp; \\quad 1\\quad &amp; \\quad 1\\quad &amp; \\quad 0\\quad &amp; \\quad 0\\quad &amp; \\quad 0\\quad \\\\ \\quad 0\\quad &amp; \\quad 0\\quad &amp; \\quad 0\\quad &amp; \\quad 1\\quad &amp; \\quad 1\\quad &amp; \\quad 0\\quad \\\\ \\quad 0\\quad &amp; \\quad 0\\quad &amp; \\quad 0\\quad &amp; \\quad 1\\quad &amp; \\quad 1\\quad &amp; \\quad 1\\quad \\\\ \\quad 0\\quad &amp; \\quad 0\\quad &amp; \\quad 0\\quad &amp; \\quad 1\\quad &amp; \\quad 0\\quad &amp; \\quad 1\\quad \\\\ \\quad 0\\quad &amp; \\quad 0\\quad &amp; \\quad 0\\quad &amp; \\quad 1\\quad &amp; \\quad 0\\quad &amp; \\quad 1\\quad \\\\ \\quad 0\\quad &amp; \\quad 0\\quad &amp; \\quad 0\\quad &amp; \\quad 1\\quad &amp; \\quad 1\\quad &amp; \\quad 0\\quad \\\\ \\end{array}\\right) \\end{array} \\end{equation*}\\] We’ll start by entering this matrix into R. Of course the process of parsing a collection of documents and creating a term-document matrix is generally more automatic. The tm text mining library is recommended for creating a term-document matrix in practice. A=matrix(c(1,2,2,0,0,0, 2,3,2,0,0,0, 2,0,1,0,0,0, 0,1,0,0,1,0, 0,1,1,0,0,0, 0,0,0,1,1,0, 0,0,0,1,1,1, 0,0,0,1,0,1, 0,0,0,1,0,1, 0,0,0,1,1,0), nrow=10, byrow=T) A ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 2 2 0 0 0 ## [2,] 2 3 2 0 0 0 ## [3,] 2 0 1 0 0 0 ## [4,] 0 1 0 0 1 0 ## [5,] 0 1 1 0 0 0 ## [6,] 0 0 0 1 1 0 ## [7,] 0 0 0 1 1 1 ## [8,] 0 0 0 1 0 1 ## [9,] 0 0 0 1 0 1 ## [10,] 0 0 0 1 1 0 Because our corpus is so small, we’ll skip the step of term-weighting, but we will normalize the documents to have equal length. In other words, we’ll divide each document vector by its two-norm so that it becomes a unit vector: A_norm = apply(A, 2, function(x){x/c(sqrt(t(x)%*%x))}) A_norm ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.3333333 0.5163978 0.6324555 0.0000000 0.0 0.0000000 ## [2,] 0.6666667 0.7745967 0.6324555 0.0000000 0.0 0.0000000 ## [3,] 0.6666667 0.0000000 0.3162278 0.0000000 0.0 0.0000000 ## [4,] 0.0000000 0.2581989 0.0000000 0.0000000 0.5 0.0000000 ## [5,] 0.0000000 0.2581989 0.3162278 0.0000000 0.0 0.0000000 ## [6,] 0.0000000 0.0000000 0.0000000 0.4472136 0.5 0.0000000 ## [7,] 0.0000000 0.0000000 0.0000000 0.4472136 0.5 0.5773503 ## [8,] 0.0000000 0.0000000 0.0000000 0.4472136 0.0 0.5773503 ## [9,] 0.0000000 0.0000000 0.0000000 0.4472136 0.0 0.5773503 ## [10,] 0.0000000 0.0000000 0.0000000 0.4472136 0.5 0.0000000 We then compute the SVD of A_norm and observe the left- and right-singular vectors. Since the matrix \\(\\A\\) is term-by-document, you might consider the terms as being the “units” of the rows of \\(\\A\\) and the documents as being the “units” of the columns. For example, \\(\\A_{23}=2\\) could logically be interpreted as “there are 2 units of the word dog per document number 3.” In this mentality, any factorization of the matrix should preserve those units. Similar to any “Change of Units Railroad”, matrix factorization can be considered in terms of units assigned to both rows and columns: \\[\\A_{\\text{term} \\times \\text{doc}} = \\U_{\\text{term} \\times \\text{factor}}\\mathbf{D}_{\\text{factor} \\times \\text{factor}}\\V^T_{\\text{factor} \\times \\text{doc}}\\] Thus, when we examine the rows of the matrix \\(\\U\\), we’re looking at information about each term and how it contributes to each factor (i.e. the “factors” are just linear combinations of our elementary term vectors); When we examine the columns of the matrix \\(\\V^T\\), we’re looking at information about how each document is related to each factor (i.e. the documents are linear combinations of these factors with weights corresponding to the elements of \\(\\V^T\\)). And what about \\(\\mathbf{D}?\\) Well, in classical factor analysis the matrix \\(\\mathbf{D}\\) is often combined with either \\(\\U\\) or \\(\\V^T\\) to obtain a two-matrix factorization. \\(\\mathbf{D}\\) describes how much information or signal from our original matrix exists along each of the singular components. It is common to use a screeplot, a simple line plot of the singular values in \\(\\mathbf{D}\\), to determine an appropriate rank for the truncation in Equation (15.3). out = svd(A_norm) plot(out$d, ylab = &#39;Singular Values of A_norm&#39;) Figure 16.2: Screeplot for the Toy Text Dataset Noticing the gap, or “elbow” in the screeplot at an index of 2 lets us know that the first two singular components contain notably more information than the components to follow - A major proportion of pattern or signal in this matrix lies long 2 components, i.e. there are 2 major topics that might provide a reasonable approximation to the data. What’s a “topic” in a vector space model? A linear combination of terms! It’s just a column vector in the term space! Let’s first examine the left-singular vectors in \\(\\U\\). Remember, the rows of this matrix describe how the terms load onto factors, and the columns are those mysterious “factors” themselves. out$u ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] -0.52980742 -0.04803212 0.01606507 -0.24737747 0.23870207 0.45722153 ## [2,] -0.73429739 -0.06558224 0.02165167 -0.08821632 -0.09484667 -0.56183983 ## [3,] -0.34442976 -0.03939120 0.10670326 0.83459702 -0.14778574 0.25277609 ## [4,] -0.11234648 0.16724740 -0.47798864 -0.22995963 -0.59187851 -0.07506297 ## [5,] -0.20810051 -0.01743101 -0.01281893 -0.34717811 0.23948814 0.42758997 ## [6,] -0.03377822 0.36991575 -0.41154158 0.15837732 0.39526231 -0.10648584 ## [7,] -0.04573569 0.58708873 0.01651849 -0.01514815 -0.42604773 0.38615891 ## [8,] -0.02427277 0.41546131 0.45839081 -0.07300613 0.07255625 -0.15988106 ## [9,] -0.02427277 0.41546131 0.45839081 -0.07300613 0.07255625 -0.15988106 ## [10,] -0.03377822 0.36991575 -0.41154158 0.15837732 0.39526231 -0.10648584 So the first “factor” of SVD is as follows: \\[\\text{factor}_1 = -0.530 \\text{cat} -0.734 \\text{dog}-0.344 \\text{eat}-0.112 \\text{tired} -0.208 \\text{toy}-0.034 \\text{injured} -0.046 \\text{ankle}-0.024 \\text{broken} -0.024 \\text{swollen} -0.034 \\text{sprained} \\] We can immediately see why people had trouble with LSI as a topic model – it’s hard to intuit how you might treat a mix of positive and negative coefficients in the output. If we ignore the signs and only investigate the absolute values, we can certainly see some meaningful topic information in this first factor: the largest magnitude weights all go to the words from the documents about pets. You might like to say that negative entries mean a topic is anticorrelated with that word, and to some extent this is correct. That logic works nicely, in fact, for factor 2: \\[\\text{factor}_2 = -0.048\\text{cat}-0.066\\text{dog}-0.039\\text{eat}+ 0.167\\text{tired} -0.017\\text{toy} 0.370\\text{injured}+ 0.587\\text{ankle} +0.415\\text{broken} + 0.415\\text{swollen} + 0.370\\text{sprained}\\] However, circling back to factor 1 then leaves us wanting to see different signs for the two groups of words. Nevertheless, the information separating the words is most certainly present. Take a look at the plot of the words’ loadings along the first two factors in Figure 16.3. Figure 16.3: Projection of the Terms onto First two Singular Dimensions Moving on to the documents, we can see a similar clustering pattern in the columns of \\(\\V^T\\) which are the rows of \\(\\V\\), shown below: out$v ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] -0.55253068 -0.05828903 0.10665606 0.74609663 -0.2433982 -0.2530492 ## [2,] -0.57064141 -0.02502636 -0.11924683 -0.62022594 -0.1219825 -0.5098650 ## [3,] -0.60092838 -0.06088635 0.06280655 -0.10444424 0.3553232 0.7029012 ## [4,] -0.04464392 0.65412158 0.05781835 0.12506090 0.6749109 -0.3092635 ## [5,] -0.06959068 0.50639918 -0.75339800 0.06438433 -0.3367244 0.2314730 ## [6,] -0.03357626 0.55493581 0.63206685 -0.16722869 -0.4803488 0.1808591 In fact, the ability to separate the documents with the first two singular vectors is rather magical here, as shown visually in Figure 16.4. Figure 16.4: Projection of the Docuemnts onto First two Singular Dimensions Figure 16.4 demonstrates how documents that live in a 10-dimensional term space can be compressed down to 2-dimensions in a way that captures the major information of interest. If we were to take that 2-truncated SVD of our term-document matrix and multiply it back together, we’d see an approximation of our original term-document matrix, and we could calculate the error involved in that approximation. We could equivalently calculate that error by using the singular values. A_approx = out$u[,1:2]%*% diag(out$d[1:2])%*%t(out$v[,1:2]) # Sum of element-wise squared error (norm(A-A_approx,&#39;F&#39;))^2 ## [1] 24.44893 # Sum of squared singular values truncated (sum(out$d[3:6]^2)) ## [1] 1.195292 However, multiplying back to the original data is not generally an action of interest to data scientists. What we are after in the SVD is the dimensionality reduced data contained in the columns of \\(\\V^T\\) (or, if you’ve created a document-term matrix, the rows of \\(\\U\\). 16.2 Image Compression While multiplying back to the original data is not generally something we’d like to do, it does provide a nice illustration of noise-reduction and signal-compression when working with images. The following example is not designed to teach you how to work with images for the purposes of data science. It is merely a nice visual way to see what’s happening when we truncate the SVD and omit these directions that have “minimal signal.” 16.2.1 Image data in R Let’s take an image of a leader that we all know and respect: Figure 16.5: Michael Rappa, PhD, Founding Director of the Institute for Advanced Analytics and Distinguished Professor at NC State This image can be downloaded from the IAA website, after clicking on the link on the left hand side “Michael Rappa / Founding Director.” Let’s read this image into R. You’ll need to install the pixmap package: #install.packages(&quot;pixmap&quot;) library(pixmap) Download the image to your computer and then set your working directory in R as the same place you have saved the image: setwd(&quot;/Users/shaina/Desktop/lin-alg&quot;) The first thing we will do is examine the image as an [R,G,B] (extension .ppm) and as a grayscale (extension .pgm). Let’s start with the [R,G,B] image and see what the data looks like in R: rappa = read.pnm(&quot;LAdata/rappa.ppm&quot;) ## Warning in rep(cellres, length = 2): &#39;x&#39; is NULL so the result will be NULL #Show the type of the information contained in our data: str(rappa) ## Formal class &#39;pixmapRGB&#39; [package &quot;pixmap&quot;] with 8 slots ## ..@ red : num [1:160, 1:250] 1 1 1 1 1 1 1 1 1 1 ... ## ..@ green : num [1:160, 1:250] 1 1 1 1 1 1 1 1 1 1 ... ## ..@ blue : num [1:160, 1:250] 1 1 1 1 1 1 1 1 1 1 ... ## ..@ channels: chr [1:3] &quot;red&quot; &quot;green&quot; &quot;blue&quot; ## ..@ size : int [1:2] 160 250 ## ..@ cellres : num [1:2] 1 1 ## ..@ bbox : num [1:4] 0 0 250 160 ## ..@ bbcent : logi FALSE You can see we have 3 matrices - one for each of the colors: red, green, and blue. Rather than a traditional data frame, when working with an image, we have to refer to the elements in this data set with @ rather than with $. rappa@size ## [1] 160 250 We can then display a heat map showing the intensity of each individual color in each pixel: rappa.red=rappa@red rappa.green=rappa@green rappa.blue=rappa@blue image(rappa.green) Figure 16.6: Intensity of green in each pixel of the original image Oops! Dr. Rappa is sideways. To rotate the graphic, we actually have to rotate our coordinate system. There is an easy way to do this (with a little bit of matrix experience), we simply transpose the matrix and then reorder the columns so the last one is first: (note that nrow(rappa.green) gives the number of columns in the transposed matrix) rappa.green=t(rappa.green)[,nrow(rappa.green):1] image(rappa.green) Rather than compressing the colors individually, let’s work with the grayscale image: greyrappa = read.pnm(&quot;LAdata/rappa.pgm&quot;) ## Warning in rep(cellres, length = 2): &#39;x&#39; is NULL so the result will be NULL str(greyrappa) ## Formal class &#39;pixmapGrey&#39; [package &quot;pixmap&quot;] with 6 slots ## ..@ grey : num [1:160, 1:250] 1 1 1 1 1 1 1 1 1 1 ... ## ..@ channels: chr &quot;grey&quot; ## ..@ size : int [1:2] 160 250 ## ..@ cellres : num [1:2] 1 1 ## ..@ bbox : num [1:4] 0 0 250 160 ## ..@ bbcent : logi FALSE rappa.grey=greyrappa@grey #again, rotate 90 degrees rappa.grey=t(rappa.grey)[,nrow(rappa.grey):1] image(rappa.grey, col=grey((0:1000)/1000)) Figure 16.7: Greyscale representation of original image 16.2.2 Computing the SVD of Dr. Rappa Now, let’s use what we know about the SVD to compress this image. First, let’s compute the SVD and save the individual components. Remember that the rows of \\(\\mathbf{v}^T\\) are the right singular vectors. R outputs the matrix \\(\\mathbf{v}\\) which has the singular vectors in columns. rappasvd=svd(rappa.grey) U=rappasvd$u d=rappasvd$d Vt=t(rappasvd$v) Now let’s compute some approximations of rank 3, 10 and 50: rappaR3=U[ ,1:3]%*%diag(d[1:3])%*%Vt[1:3, ] image(rappaR3, col=grey((0:1000)/1000)) Figure 16.8: Rank 3 approximation of the image data rappaR10=U[ ,1:10]%*%diag(d[1:10])%*%Vt[1:10, ] image(rappaR10, col=grey((0:1000)/1000)) Figure 16.9: Rank 10 approximation of the image data rappaR25=U[ ,1:25]%*%diag(d[1:25])%*%Vt[1:25, ] image(rappaR25, col=grey((0:1000)/1000)) Figure 16.10: Rank 50 approximation of the image data How many singular vectors does it take to recognize Dr. Rappa? Certainly 25 is sufficient. Can you recognize him with even fewer? You can play around with this and see how the image changes. 16.2.3 The Noise One of the main benefits of the SVD is that the signal-to-noise ratio of each component decreases as we move towards the right end of the SVD sum. If \\(\\mathbf{x}\\) is our data matrix (in this example, it is a matrix of pixel data to create an image) then, \\[\\begin{equation} \\mathbf{X}= \\sigma_1\\mathbf{u}_1\\mathbf{v}_1^T + \\sigma_2\\mathbf{u}_2\\mathbf{v}_2^T + \\sigma_3\\mathbf{u}_3\\mathbf{v}_3^T + \\dots + \\sigma_r\\mathbf{u}_r\\mathbf{v}_r^T \\tag{15.2} \\end{equation}\\] where \\(r\\) is the rank of the matrix. Our image matrix is full rank, \\(r=160\\). This is the number of nonzero singular values, \\(\\sigma_i\\). But, upon examinination, we see many of the singular values are nearly 0. Let’s examine the last 20 singular values: d[140:160] ## [1] 0.035731961 0.033644986 0.033030189 0.028704912 0.027428124 0.025370919 ## [7] 0.024289497 0.022991926 0.020876657 0.020060538 0.018651373 0.018011032 ## [13] 0.016299834 0.015668836 0.013928107 0.013046327 0.011403096 0.010763141 ## [19] 0.009210187 0.008421977 0.004167310 We can think of these values as the amount of “information” directed along those last 20 singular components. If we assume the noise in the image or data is uniformly distributed along each orthogonal component \\(\\mathbf{u}_i\\mathbf{v}_i^T\\), then there is just as much noise in the component \\(\\sigma_1\\mathbf{u}_1\\mathbf{v}_1^T\\) as there is in the component \\(\\sigma_{160}\\mathbf{u}_{160}\\mathbf{v}_{160}^T\\). But, as we’ve just shown, there is far less information in the component \\(\\sigma_{160}\\mathbf{u}_{160}\\mathbf{v}_{160}^T\\) than there is in the component \\(\\sigma_1\\mathbf{u}_1\\mathbf{v}_1^T\\). This means that the later components are primarily noise. Let’s see if we can illustrate this using our image. We’ll construct the parts of the image that are represented on the last few singular components # Using the last 25 components: rappa_bad25=U[ ,135:160]%*%diag(d[135:160])%*%Vt[135:160, ] image(rappa_bad25, col=grey((0:1000)/1000)) Figure 16.11: The last 25 components, or the sum of the last 25 terms in equation (15.2) # Using the last 50 components: rappa_bad50=U[ ,110:160]%*%diag(d[110:160])%*%Vt[110:160, ] image(rappa_bad50, col=grey((0:1000)/1000)) Figure 16.12: The last 50 components, or the sum of the last 50 terms in equation (15.2) # Using the last 100 components: (4 times as many components as it took us to recognize the face on the front end) rappa_bad100=U[ ,61:160]%*%diag(d[61:160])%*%Vt[61:160, ] image(rappa_bad100, col=grey((0:1000)/1000)) Figure 16.13: The last 100 components, or the sum of the last 100 terms in equation (15.2) Mostly noise. In the last of these images, we see the outline of Dr. Rappa. One of the first things to go when images are compressed are the crisp outlines of objects. This is something you may have witnessed in your own experience, particularly when changing the format of a picture to one that compresses the size. "],["fa.html", "Chapter 17 Factor Analysis 17.1 Assumptions of Factor Analysis 17.2 Determining Factorability 17.3 Communalities 17.4 Number of Factors 17.5 Rotation of Factors 17.6 Methods of Factor Analysis 17.7 Case Study: Personality Tests", " Chapter 17 Factor Analysis Factor Analysis is about looking for underlying relationships or associations. In that way, factor analysis is a correlational study of variables, aiming to group or cluster variables along dimensions. It may also be used to provide an estimate (factor score) of a latent construct which is a linear combination of variables. For example, a standardized test might ask hundreds of questions on a variety of quantitative and verbal subjects. Each of these questions could be viewed as a variable. However, the quantitative questions collectively are meant to measure some latent factor, that is the individual’s quantitative reasoning. A Factor Analysis might be able to reveal these two latent factors (quantitative reasoning and verbal ability) and then also provide an estimate (score) for each individual on each factor. Any attempt to use factor analysis to summarize or reduce a set to data should be based on a conceptual foundation or hypothesis. It should be remembered that factor analysis will produce factors for most sets of data. Thus, if you simply analyze a large number of variables in the hopes that the technique will ``figure it out\", your results may look as though they are grasping at straws. The quality or meaning/interpretation of the derived factors is best when related to a conceptual foundation that existed prior to the analysis. 17.1 Assumptions of Factor Analysis No outliers in the data set Adequate sample size As a rule of thumb, maintain a ratio of variables to factors of at least 3 (some say 5). This depends on the application. You should have at least 10 observations for each variable (some say 20). This often depends on what value of factor loading you want to declare as significant. See Table for the details on this. No perfect multicollinearity Homoskedasticity not required between variables (all variances not required to be equal) Linearity of variables desired - only models linear correlation between variables Interval data (as opposed to nominal) Measurement error on the variables/observations has constant variance and is, on average, 0 Normality is not required Factor loadings are the correlation of each variable and the factor. This table is a guide for the sample sizes necessary to consider a factor loading significant. For example, in a sample of 100, factor loadings of 0.55 are considered significant. In a sample size of 70, however, factor loadings must reach 0.65 to be considered significant. Significance based on 0.05 level, a power level of 80 percent. Source: Computations made with SOLO Power Analysis, BMDP Statistical Software, Inc., 1993 Sample Size Needed for Significance Factor Loading 350 .30 250 .35 200 .40 150 .45 120 .50 100 .55 85 .60 70 .65 60 .70 50 .75 17.2 Determining Factorability Before we even begin the process of factor analysis, we have to do some preliminary work to determine whether or not the data even lends itself to this technique. If none of our variables are correlated, then we cannot group them together in any meaningful way! Bartlett’s Sphericity Test and the KMO index are two statistical tests for whether or not a set of variables can be factored. These tests do not provide information about the appropriate number of factors, only whether or not such factors even exist. 17.2.1 Visual Examination of Correlation Matrix Depending on how many variables you are working with, you may be able to determine whether or not to proceed with factor analysis by simply examining the correlation matrix. With this examination, we are looking for two things: Correlations that are significant at the 0.01 level of significance. At least half of the correlations should be significant in order to proceed to the next step. Correlations are “sufficient” to justify applying factor analysis. As a rule of thumb, at least half of the correlations should be greater than 0.30. 17.2.2 Barlett’s Sphericity Test Barlett’s sphericity test checks if the observed correlation matrix is significantly different from the identity matrix. Recall that the correlation of two variables is equal to 0 if and only if they are orthogonal (and thus completely uncorrelated). When this is the case, we cannot reduce the number of variables any further, neither PCA nor Factor Analysis will be able to compress the information reliably into fewer dimensions. For Barlett’s test, \\[H_0 = \\mbox{ The variables are orthogonal} \\] Which implies that there are no underlying factors to be uncovered. Obviously, we must be able to reject this hypothesis for a meaningful result in PCA. 17.2.3 Kaiser-Meyer-Olkin (KMO) Measure of Sampling Adequacy The goal of the Kaiser-Meyer-Olkin (KMO) measure of sampling adequacy is similar to that of Bartlett’s test in that it checks if we can factorize efficiently the original variables. However, the KMO measure is based on the idea of partial correlation. The correlation matrix is always the starting point. We know that the variables are more or less correlated, but the correlation between two variables can be influenced by the others. So, we use the partial correlation in order to measure the relation between two variables by removing the effect of the remaining variables. The KMO index compares the raw values of correlations between variables and those of the partial correlations. If the KMO index is high (\\(\\approx 1\\)), then PCA can act efficiently; if the KMO index is low (\\(\\approx 0\\)), then PCA is not relevant. Generally a KMO index greater than 0.5 is considered acceptable to proceed with factor analysis. Table 17.1 contains the information about interpretting KMO results that was provided in the original 1974 paper. KMO value Degree of Common Variance 0.90 to 1.00 Marvelous 0.80 to 0.89 Middling 0.60 to 0.69 Mediocre 0.50 to 0.59 Miserable 0.00 to 0.49 Don’t Factor Table: (#tab:KMO) Interpretting the KMO value. \\end{center} \\end{table} So, for example, if you have a survey with 100 questions/variables and you obtained a KMO index of 0.61, this tells you that the degree of common variance between your variables is mediocre, on the border of being miserable. While factor analysis may still be appropriate in this case, you will find that such an analysis will not account for a substantial amount of variance in your data. It may still account for enough to draw some meaningful conclusions, however. 17.3 Communalities You can think of communalities as multiple \\(R^2\\) values for regression models predicting the variables of interest from the factors (the reduced number of factors that your model uses). The communality for a given variable can be interpreted as the proportion of variation in that variable explained by the chosen factors. Take for example the SAS output for factor analysis on the Iris dataset shown in Figure . The factor model (which settles on only one single factor) explains 98% of the variability in petal length. In other words, if you were to use this factor in a simple linear regression model to predict petal length, the associated \\(R^2\\) value should be 0.98. Indeed you can verify that this is true. The results indicate that this single factor model will do the best job explaining variability in petal length, petal width, and sepal length. Figure 17.1: SAS output for PROC FACTOR using Iris Dataset One assessment of how well a factor model is doing can be obtained from the communalities. What you want to see is values that are close to one. This would indicate that the model explains most of the variation for those variables. In this case, the model does better for some variables than it does for others. If you take all of the communality values, \\(c_i\\) and add them up you can get a total communality value: \\[\\sum_{i=1}^p \\widehat{c_i} = \\sum_{i=1}^k \\widehat{\\lambda_i}\\] Here, the total communality is 2.918. The proportion of the total variation explained by the three factors is \\[\\frac{2.918}{4}\\approx 0.75.\\] The denominator in that fraction comes from the fact that the correlation matrix is used by default and our dataset has 4 variables. Standardized variables have variance of 1 so the total variance is 4. This gives us the percentage of variation explained in our model. This might be looked at as an overall assessment of the performance of the model. The individual communalities tell how well the model is working for the individual variables, and the total communality gives an overall assessment of performance. 17.4 Number of Factors A good rule of thumb for determining the number of factors is to only choose factors with associated eigenvalue (or variance) greater than 1. Since the correlation matrix is used for factor analysis, we want our factors to explain more variance than any individual variable from our dataset. If this rule of thumb produces too many factors, it is reasonable to raise that limiting condition only if the number of factors still explains a reasonable amount of the total variance. 17.5 Rotation of Factors The purpose of rotating factors is to make them more interpretable. If factor loadings are relatively constant across variables, they don’t help us find latent structure or clusters of variables. This will often happen in PCA when the goal is only to find directions of maximal variance. Thus, once the number of components/factors is fixed and a projection of the data onto a lower-dimensional subspace is done, we are free to rotate the axes of the result without losing any variance. The axes will no longer be principal components! The amount of variance explained by each factor will change, but the total amount of variance in the reduced data will stay the same because all we have done is rotate the basis. The goal is to rotate the factors in such a way that the loading matrix develops a more sparse structure. A sparse loading matrix (one with lots of very small entries and few large entries) is far easier to interpret in terms of finding latent variable groups. The two most common rotations are varimax and quartimax. The goal of varimax rotation is to maximize the squared factor loadings in each factor, i.e. to simplify the columns of the factor matrix. In each factor, the large loadings are increased and the small loadings are decreased so that each factor has only a few variables with large loadings. In contrast, the goal of quartimax rotation is to simply the rows of the factor matrix. In each variable the large loadings are increased and the small loadings are decreased so that each variable will only load on a few factors. Which of these factor rotations is appropriate 17.6 Methods of Factor Analysis Factor Analysis is much like PCA in that it attempts to find some latent variables (linear combinations of original variables) which can describe large portions of the total variance in data. There are numerous ways to compute factors for factor analysis, the two most common methods are: The principal axis method (i.e. PCA) and Maximum Likelihood Estimation. In fact, the default method for PROC FACTOR with no additional options is merely PCA. For some reason, the scores and factors may be scaled differently, involving the standard deviations of each factor, but nonetheless, there is absolutely nothing different between PROC FACTOR defaults and PROC PRINCOMP. The difference between Factor Analysis and PCA is two-fold: In factor analysis, the factors are usually rotated to obtain a more sparse (i.e. interprettable) structure varimax rotation is the most common rotation. Others include promax, and quartimax.) The factors try to only explain the “common variance” between variables. In other words, Factor Analysis tries to estimate how much of each variable’s variance is specific to that variable and not “covarying” (for lack of a better word) with any other variables. This specific variance is then subtracted from the diagonal of the covariance matrix before factors or components are found. We’ll talk more about the first difference than the second because it generally carries more advantages. 17.6.1 PCA Rotations Let’s first talk about the motivation behind principal component rotations. Compare the following sets of (fabricated) factors, both using the variables from the iris dataset. Listed below are the loadings of each variable on two factors. Which set of factors is more easily interpretted? The difference between these factors might be described as ``sparsity\". Factor Set 2 has more zero loadings than Factor Set 1. It also has entries which are comparitively larger in magnitude. This makes Factor Set 2 much easier to interpret! Clearly F1 is dominated by the variables Sepal.Width (positively correlated) and Petal.Length (negatively correlated), whereas F2 is dominated by the variables Sepal.Length (positively) and Petal.Width (negatively). Factor interpretation doesn’t get much easier than that! With the first set of factors, the story is not so clear. This is the whole purpose of factor rotation, to increase the interpretability of factors by encouraging sparsity. Geometrically, factor rotation tries to rotate a given set of factors (like those derived from PCA) to be more closely aligned with the original variables once the dimensions of the space have been reduced and the variables have been pushed closer together in the factor space. Let’s take a look at the actual principal components from the iris data and then rotate them using a varimax rotation. In order to rotate the factors, we have to decide on some number of factors to use. If we rotated all 4 orthogonal components to find sparsity, we’d just end up with our original variables again! irispca = princomp(iris[,1:4],scale=T) ## Warning: In princomp.default(iris[, 1:4], scale = T) : ## extra argument &#39;scale&#39; will be disregarded summary(irispca) ## Importance of components: ## Comp.1 Comp.2 Comp.3 Comp.4 ## Standard deviation 2.0494032 0.49097143 0.27872586 0.153870700 ## Proportion of Variance 0.9246187 0.05306648 0.01710261 0.005212184 ## Cumulative Proportion 0.9246187 0.97768521 0.99478782 1.000000000 irispca$loadings ## ## Loadings: ## Comp.1 Comp.2 Comp.3 Comp.4 ## Sepal.Length 0.361 0.657 0.582 0.315 ## Sepal.Width 0.730 -0.598 -0.320 ## Petal.Length 0.857 -0.173 -0.480 ## Petal.Width 0.358 -0.546 0.754 ## ## Comp.1 Comp.2 Comp.3 Comp.4 ## SS loadings 1.00 1.00 1.00 1.00 ## Proportion Var 0.25 0.25 0.25 0.25 ## Cumulative Var 0.25 0.50 0.75 1.00 # Since 2 components explain a large proportion of the variation, lets settle on those two: rotatedpca = varimax(irispca$loadings[,1:2]) rotatedpca$loadings ## ## Loadings: ## Comp.1 Comp.2 ## Sepal.Length 0.223 0.716 ## Sepal.Width -0.229 0.699 ## Petal.Length 0.874 ## Petal.Width 0.366 ## ## Comp.1 Comp.2 ## SS loadings 1.00 1.00 ## Proportion Var 0.25 0.25 ## Cumulative Var 0.25 0.50 # Not a drastic amount of difference, but clearly an attempt has been made to encourage # sparsity in the vectors of loadings. # NOTE: THE ROTATED FACTORS EXPLAIN THE SAME AMOUNT OF VARIANCE AS THE FIRST TWO PCS # AFTER PROJECTING THE DATA INTO TWO DIMENSIONS (THE BIPLOT) ALL WE DID WAS ROTATE THOSE # ORTHOGONAL AXIS. THIS CHANGES THE PROPORTION EXPLAINED BY *EACH* AXIS, BUT NOT THE TOTAL # AMOUNT EXPLAINED BY THE TWO TOGETHER. # The output from varimax can&#39;t tell you about proportion of variance in the original data # because you didn&#39;t even tell it what the original data was! 17.7 Case Study: Personality Tests In this example, we’ll use a publicly available dataset that describes personality traits of nearly Read in the Big5 Personality test dataset, which contains likert scale responses (five point scale where 1=Disagree, 3=Neutral, 5=Agree. 0 = missing) on 50 different questions in columns 8 through 57. The questions, labeled E1-E10 (E=extroversion), N1-N10 (N=neuroticism), A1-A10 (A=agreeableness), C1-C10 (C=conscientiousness), and O1-O10 (O=openness) all attempt to measure 5 key angles of human personality. The first 7 columns contain demographic information coded as follows: Race Chosen from a drop down menu. 1=Mixed Race 2=Arctic (Siberian, Eskimo) 3=Caucasian (European) 4=Caucasian (Indian) 5=Caucasian (Middle East) 6=Caucasian (North African, Other) 7=Indigenous Australian 8=Native American 9=North East Asian (Mongol, Tibetan, Korean Japanese, etc) 10=Pacific (Polynesian, Micronesian, etc) 11=South East Asian (Chinese, Thai, Malay, Filipino, etc) 12=West African, Bushmen, Ethiopian 13=Other (0=missed) Age Entered as text (individuals reporting age &lt; 13 were not recorded) Engnat Response to “is English your native language?” 1=yes 2=no 0=missing Gender Chosen from a drop down menu 1=Male 2=Female 3=Other 0=missing Hand “What hand do you use to write with?” 1=Right 2=Left 3=Both 0=missing options(digits=2) big5 = read.csv(&#39;http://birch.iaa.ncsu.edu/~slrace/LinearAlgebra2021/Code/big5.csv&#39;) To perform the same analysis we did in SAS, we want to use Correlation PCA and rotate the axes with a varimax transformation. We will start by performing the PCA. We need to set the option ```scale=T} to perform PCA on the correlation matrix rather than the default covariance matrix. We will only compute the first 5 principal components because we have 5 personality traits we are trying to measure. We could also compute more than 5 and take the number of components with eigenvalues &gt;1 to match the default output in SAS (without n=5 option). 17.7.1 Raw PCA Factors options(digits=5) pca.out = prcomp(big5[,8:57], rank = 5, scale = T) Remember the only difference between the default PROC PRINCOMP output and the default PROC FACTOR output in SAS was the fact that the eigenvectors in PROC PRINCOMP were normalized to be unit vectors and the factor vectors in PROC FACTOR were those same eigenvectors scaled by the square roots of the eigenvalues. So we want to multiply each eigenvector column output in pca.out$rotation (recall this is the loading matrix or matrix of eigenvectors) by the square root of the corresponding eigenvalue given in pca.out$sdev. You’ll recall that multiplying a matrix by a diagonal matrix on the right has the effect of scaling the columns of the matrix. So we’ll just make a diagonal matrix, \\(\\textbf{S}\\) with diagonal elements from the pca.out$sdev vector and scale the columns of the pca.out$rotation matrix. Similarly, the coordinates of the data along each component then need to be divided by the standard deviation to cancel out this effect of lengthening the axis. So again we will multiply by a diagonal matrix to perform this scaling, but this time, we use the diagonal matrix \\(\\textbf{S}^{-1}=\\) diag(1/(pca.out$sdev)). \\ Matrix multiplication in R is performed with the \\%\\*\\% operator. fact.loadings = pca.out$rotation[,1:5] %*% diag(pca.out$sdev[1:5]) fact.scores = pca.out$x[,1:5] %*%diag(1/pca.out$sdev[1:5]) # PRINT OUT THE FIRST 5 ROWS OF EACH MATRIX FOR CONFIRMATION. fact.loadings[1:5,1:5] ## [,1] [,2] [,3] [,4] [,5] ## E1 -0.52057 0.27735 -0.29183 0.13456 -0.25072 ## E2 0.51025 -0.35942 0.26959 -0.14223 0.21649 ## E3 -0.70998 0.15791 -0.11623 0.21768 -0.11303 ## E4 0.58361 -0.20341 0.31433 -0.17833 0.22788 ## E5 -0.65751 0.31924 -0.16404 0.12496 -0.21810 fact.scores[1:5,1:5] ## [,1] [,2] [,3] [,4] [,5] ## [1,] -2.53286 -1.16617 0.276244 0.043229 -0.069518 ## [2,] 0.70216 -1.22761 1.095383 1.615919 -0.562371 ## [3,] -0.12575 1.33180 1.525208 -1.163062 -2.949501 ## [4,] 1.29926 1.17736 0.044168 -0.784411 0.148903 ## [5,] -0.37359 0.47716 0.292680 1.233652 0.406582 This should match the output from SAS and it does. Remember these columns are unique up to a sign, so you’ll see factor 4 does not have the same sign in both software outputs. This is not cause for concern. Figure 17.2: Default (Unrotated) Factor Loadings Output by SAS Figure 17.3: Default (Unrotated) Factor Scores Output by SAS 17.7.2 Rotated Principal Components The next task we may want to undertake is a rotation of the factor axes according to the varimax procedure. The most simple way to go about this is to use the varimax() function to find the optimal rotation of the eigenvectors in the matrix pca.out$rotation. The varimax() function outputs both the new set of axes in the matrix called loadings and the rotation matrix (rotmat) which performs the rotation from the original principal component axes to the new axes. (i.e. if \\(\\textbf{V}\\) contains the old axes as columns and \\(\\hat{\\textbf{V}}\\) contains the new axes and \\(\\textbf{R}\\) is the rotation matrix then \\(\\hat{\\textbf{V}} = \\textbf{V}\\textbf{R}\\).) That rotation matrix can be used to perform the same rotation on the scores of the observations. If the matrix \\(\\textbf{U}\\) contains the scores for each observation, then the rotated scores \\(\\hat{\\textbf{U}}\\) are found by \\(\\hat{\\textbf{U}} = \\textbf{U}\\textbf{R}\\) varimax.out = varimax(fact.loadings) rotated.fact.loadings = fact.loadings %*% varimax.out$rotmat rotated.fact.scores = fact.scores %*% varimax.out$rotmat # PRINT OUT THE FIRST 5 ROWS OF EACH MATRIX FOR CONFIRMATION. rotated.fact.loadings[1:5,] ## [,1] [,2] [,3] [,4] [,5] ## E1 -0.71232 -0.0489043 0.010596 -0.03206926 0.055858 ## E2 0.71592 -0.0031185 0.028946 0.03504236 -0.121241 ## E3 -0.66912 -0.2604049 0.131609 0.01704690 0.263679 ## E4 0.73332 0.1528552 -0.023367 0.00094685 -0.053219 ## E5 -0.74534 -0.0757539 0.100875 -0.07140722 0.218602 rotated.fact.scores[1:5,] ## [,1] [,2] [,3] [,4] [,5] ## [1,] -1.09083 -2.04516 1.40699 -0.38254 0.5998386 ## [2,] 0.85718 -0.19268 1.07708 2.03665 -0.2178616 ## [3,] -0.92344 2.58761 2.43566 -0.80840 -0.1833138 ## [4,] 0.61935 1.53087 -0.79225 -0.59901 -0.0064665 ## [5,] -0.39495 -0.10893 -0.24892 0.99744 0.9567712 And again we can see that these line up with our SAS Rotated output, however the order does not have to be the same! SAS conveniently reorders the columns according to the variance of the data along that new direction. Since we have not done that in R, the order of the columns is not the same! Factors 1 and 2 are the same in both outputs, but SAS Factor 3 = R Factor 4 and SAS Factor 5 = (-1)* R Factor 4. The coordinates are switched too so nothing changes in our interpretation. Remember, when you rotate factors, you no longer keep the notion that the “first vector” explains the most variance unless you reorder them so that is true (like SAS does). Figure 17.4: Rotated Factor Loadings Output by SAS knitr::include_graphics(&#39;RotatedScores.png&#39;) Figure 17.5: Rotated Factor Scores Output by SAS 17.7.3 Visualizing Rotation via BiPlots Let’s start with a peek at BiPlots of the first 2 of principal component loadings, prior to rotation. Notice that here I’m not going to bother with any scaling of the factor loadings as I’m not interested in forcing my output to look like SAS’s output. I’m also downsampling the observations because 20,000 is far to many to plot. biplot(pca.out$x[sample(1:19719,1000),1:2], pca.out$rotation[,1:2], cex=c(0.2,1)) Figure 17.6: BiPlot of Projection onto PC1 and PC2 biplot(pca.out$x[sample(1:19719,1000),3:4], pca.out$rotation[,3:4], cex=c(0.2,1)) Figure 17.7: BiPlot of Projection onto PC3 and PC4 Let’s see what happens to these biplots after rotation: vmax = varimax(pca.out$rotation) newscores = pca.out$x%*%vmax$rotmat biplot(newscores[sample(1:19719,1000),1:2], vmax$loadings[,1:2], cex=c(0.2,1), xlab = &#39;Rotated Axis 1&#39;, ylab = &#39;Rotated Axis 2&#39;) Figure 17.8: BiPlot of Projection onto Rotated Axes 1,2. Extroversion questions align with axis 1, Neuroticism with Axis 2 biplot(newscores[sample(1:19719,1000),3:4], vmax$loadings[,3:4], cex=c(0.2,1), xlab = &#39;Rotated Axis 3&#39;, ylab = &#39;Rotated Axis 4&#39;) Figure 17.9: BiPlot of Projection onto Rotated Axes 3,4. Agreeableness questions align with axis 3, Openness with Axis 4. After the rotation, we can see the BiPlots tell a more distinct story. The extroversion questions line up along rotated axes 1, neuroticism along rotated axes 2, and agreeableness and openness are reflected in rotated axes 3 and 4 respectively. The fifth rotated component can be confirmed to represent the last remaining category which is conscientiousness. "],["otherdimred.html", "Chapter 18 Dimension Reduction for Visualization 18.1 Multidimensional Scaling", " Chapter 18 Dimension Reduction for Visualization 18.1 Multidimensional Scaling Multidimensional scaling is a technique which aims to represent higher-dimensional data in a lower-dimensional space while keeping the pairwise distances between points as close to their original distances as possible. It takes as input a distance matrix, \\(\\D\\) where \\(\\D_{ij}\\) is some measure of distance between observation \\(i\\) and observation \\(j\\) (most often Euclidean distance). The original observations may involve many variables and thus exist in a high-dimensional space. The output of MDS is a set of coordinates, usually in 2-dimensions for the purposes of visualization, such that the Euclidean distance between observation \\(i\\) and observation \\(j\\) in the new lower-dimensional representation is an approximation to \\(\\D_{ij}\\). One of the outputs in R will be a measure that is akin to the “percentage of variation explained” by PCs. The difference is that the matrix we are representing is not a covariance matrix, so this ratio is not exactly a percentage of variation. It can, however, be used to judge how much information is retained by the lower dimensional representation. This is output in the GOF vector (presumably standing for Goodness of Fit). The first entry in GOF gives the ratio of the sum of the \\(k\\) largest eigenvalues to the sum of the absolute values of all the eigenvalues, and the second entry in GOF gives the ratio of the sum of the \\(k\\) largest eigenvalues to the sum of only the positive eigenvalues. \\[GOF[1] = \\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{i=1}^n |\\lambda_i|}\\] and \\[GOF[2] = \\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{i=1}^n \\max(\\lambda_i,0)}\\] ### MDS of Iris Data Let’s take a dataset we’ve already worked with, like the iris dataset, and see how this is done. Recall that the iris data contains measurements of 150 flowers (50 each from 3 different species) on 4 variables: Sepal.Length, Sepal.Width, Petal.Length, and Petal.Width. To examine a 2-dimensional representation of this data via Multidimensional Scaling, we simply compute a distance matrix and run the MDS procedure: D = dist(iris[,1:4]) fit = cmdscale(D, eig=TRUE, k=2) # k is the number of dimensions desired fit$eig[1:12] # view first dozen eigenvalues ## [1] 6.3001e+02 3.6158e+01 1.1653e+01 3.5514e+00 3.4866e-13 3.1863e-13 ## [7] 2.0112e-13 1.3770e-13 7.7470e-14 3.2881e-14 3.0740e-14 2.1786e-14 fit$GOF # view the Goodness of Fit measures ## [1] 0.97769 0.97769 # plot the solution, colored by iris species: x = fit$points[,1] y = fit$points[,2] # The pch= option controls the symbol output. 16=filled circles. plot(x,y,col=c(&quot;red&quot;,&quot;green3&quot;,&quot;blue&quot;)[iris$Species], pch=16, xlab=&#39;Coordinate 1&#39;, ylab=&#39;Coordinate 2&#39;) Figure 18.1: Multidimensional Scaling of the Iris Data We can tell from the eigenvalues alone that two dimensions should be relatively sufficient to summarize this data. After two large eigenvalues, the remainder drop off and become small, signifying a lack of further information. Indeed, the Goodness of Fit measurements back up this intuition: values close to 1 indicate a good fit with minimal error. 18.1.1 MDS of Leukemia dataset Let’s take a look at another example, this time using the Leukemia dataset which has 5000 variables. It is unreasonable to expect that we can get as good of a fit of this data using only two dimensions! There will obviously be much more error. However, we can still get a visualization that should at least show us which observations are close to each other and which are far away. leuk=read.csv(&#39;http://birch.iaa.ncsu.edu/~slrace/Code/leukemia.csv&#39;) As you may recall, this data has some variables with 0 variance; those entire columns are constant. To determine which ones, we first remove the last column which is a character vector that identifies the type of leukemia. type = leuk[ , 5001] leuk = leuk[,1:5000] # If desired, could supply names of columns that have 0 variance with # names(leuk[, sapply(leuk, function(v) var(v, na.rm=TRUE)==0)]) # The na.rm=T would allow us to keep any missing information and still compute # the variance using the non-missing values. In this instance, it is not necessary # because we have no missing values. # We can remove these columns from the data with: leuk=leuk[,apply(leuk, 2, var, na.rm=TRUE) != 0] # compute distances matrix t=dist(leuk) fit=cmdscale(t,eig=TRUE, k=2) fit$GOF ## [1] 0.35822 0.35822 x = fit$points[,1] y = fit$points[,2] #The cex= controls the size of the circles in the plot function. plot(x,y,col=c(&quot;red&quot;,&quot;green&quot;,&quot;blue&quot;)[factor(type)], cex=3, xlab=&#39;Coordinate 1&#39;, ylab=&#39;Coordinate 2&#39;, main = &#39;Multidimensional Scaling of Raw Leukemia Data&#39;) text(x,y,labels=row.names(leuk)) Figure 18.2: Multidimensional Scaling of the Leukemia Data What if we standardize our data before running the MDS procedure? Will that effect our results? Let’s see how it looks on the standardized version of the leukemia data. # We can experiment with standardization to see how it # effects our results: leuk2=scale(leuk,center=TRUE, scale=TRUE) t2=dist(leuk2) fit2=cmdscale(t2,eig=TRUE,k=2) fit2$GOF ## [1] 0.21287 0.21287 x2 = fit2$points[,1] y2 = fit2$points[,2] #The cex= controls the size of the circles in the plot function. plot(x2,y2,col=c(&quot;red&quot;,&quot;green&quot;,&quot;blue&quot;)[factor(type)], cex=3, xlab=&#39;Coordinate 1&#39;, ylab=&#39;Coordinate 2&#39;, main = &#39;Multidimensional Scaling of Standardized Leukemia Data&#39;) text(x2,y2,labels=row.names(leuk)) A note on standardization Clearly, things have changed substantially. We shouldn’t give to much creedence to the decreased Goodness of Fit statistics. I don’t necessarily believe that we are explaining less information just because we scaled our data, the fact that this number has changed should likely be attributed to the fact that we have significantly decreased all of the eigenvalues of the matrix, and not in any predictable or meaningful way. It’s more important to focus on what we are trying to represent and that is differences between samples. Perhaps if there are some genes for which values vary wildly between the different leukemia types, and other genes which don’t show much variation, then we should keep this information in the data. By standardizing the data, we’re making the variation of every gene equal to 1 - which stands to wash out some of the bigger, more discriminating factors in the distance calculations. This consideration is something that will need to be made for each dataset on a case-by-case basis. If our dataset had variables with wide scale variations (like income and number of cars) then standardization is a much more reasonable approach! There are several things to keep in mind when studying an MDS map. The axis are, by themselves, meaningless. The orientation of the picture is completely arbitrary. All that matters is the relative proximity of the points in the map. Are they close? Are they far apart? "],["sna.html", "Chapter 19 Social Network Analysis 19.1 Working with Network Data 19.2 Network Visualization - igraph package", " Chapter 19 Social Network Analysis 19.1 Working with Network Data We’ll use the popular igraph package to explore the student slack network in R. The data has been anonymized for use in this text. First, we load the two data frames that contain the information for our network: - SlackNetwork contains the interactions between pairs of students. An interaction between students was defined as either an emoji-reaction or threaded reply to a post. The source of the interaction is the individual reacting or replying and the target of the interaction is the user who originated the post. This data frame also contains the channel in which the interaction takes place, and 9 binary flags indicating the presence or absence of certain keywords or phrases of interest. - users contains user-level attributes like the cohort to which a student belongs (‘blue’ or ‘orange’). library(igraph) ## ## Attaching package: &#39;igraph&#39; ## The following object is masked from &#39;package:plotly&#39;: ## ## groups ## The following objects are masked from &#39;package:stats&#39;: ## ## decompose, spectrum ## The following object is masked from &#39;package:base&#39;: ## ## union load(&#39;LAdata/slackanon2021.RData&#39;) head(SlackNetwork) ## source target channel notes study howdoyou python R SAS beer ## 1 U0130T4056Y U0130T30B36 random FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## 2 U012UMSH7FC U0130T30B36 random FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## 3 U012N097BUN U0130T30B36 random FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## 4 U012E0B3YBZ U0130T30B36 random FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## 5 U0130T1GKMJ U0130T30B36 random FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## 6 U0130T486SY U0130T30B36 random FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## food snack edgeID ## 1 FALSE FALSE 1 ## 2 FALSE FALSE 2 ## 3 FALSE FALSE 3 ## 4 FALSE FALSE 4 ## 5 FALSE FALSE 5 ## 6 FALSE FALSE 6 head(users) ## userID Cohort ## 1 U012E07QP1D o ## 2 U012E08EWTZ b ## 3 U012E08R0CX o ## 4 U012E08TYMV b ## 5 U012E096VF1 o ## 6 U012E09JRAB o Using this information, we can create an igraph network object using the graph_from_data_frame() function. We can then apply some functions from the igraph package to discover the underlying data as we’ve already seen it. Because this network has almost 42,000 edges overall, we’ll subset the data and only look at interactions from the general channel. 19.2 Network Visualization - igraph package SlackNetworkSubset = SlackNetwork[SlackNetwork$channel==&#39;general&#39;,] slack = graph_from_data_frame(SlackNetworkSubset, directed = TRUE, vertices = users) plot(slack) The default plots certainly leave room for improvement. We notice that one user is not connected to the rest of the network in the general channel, signifying that this user has not reacted or replied in a threaded fashion to any posts in this channel, nor have they created a post that received any interaction. We can delete this vertex from the network by taking advantage of the delete.vertices() function specifying that we want to remove all vertices with degree equal to zero. You’ll recall that the degree of a vertex is the number of edges that connect to it. slack=delete.vertices(slack,degree(slack)==0) There are various ways that we can improve the network visualization, but we will soon see that layout is, by far, the most important. First, let’s explore how we can use the plot options to change the line weight, size, and color of the nodes and edges to improve the visualization in the following chunk. plot(slack, edge.arrow.size = .3, vertex.label=NA,vertex.size=10, vertex.color=&#39;gray&#39;,edge.color=&#39;blue&#39;) ### Layout algorithms for igraph package The igraph package has many different layout algorithms available; type ?igraph::layout for a list of them. By clicking on each layout in the help menu, you’ll be able to distinguish which of the layouts are force-directed and which are not. Force-directed layouts generally provide the highest quality network visualizations. The Davidson-Harel (layout_with_dh), Fruchterman-Reingold (layout_with_fr), DrL (layout_with_drl) and multidimensional scaling algorithms (layout_with_mds) are probably the most well-known algorithms available in this package. We recommend that you compute the layout outside of the plot function so that you may use it again without re-computing it. After all, a layout is just a two dimensional array of coordinates that specifies where each node should be placed. If you compute the layout inside the plot function then every time you make a small adjustment like color or edge arrow size, you will have to your computer will have to re-compute the layout algorithm. The following code chunk computes 4 different layouts and then plots the resulting networks on a 2x2 grid for comparison. We encourage you to substitute four different layouts (listed in the help document at the bottom) in place of the ones chosen here as part of your exploration. #?igraph::layout l = layout_with_lgl(slack) l2 = layout_with_fr(slack) l3 = layout_with_drl(slack) l4 = layout_with_mds(slack) par(mfrow=c(2,2),mar=c(1,1,1,1)) # Above tells the graphic window to use the # following plots to fill out a 2x2 grid with margins of 1 unit # on each side. Must reset these options with dev.off() when done! plot(slack, edge.arrow.size = .3, vertex.label=NA,vertex.size=10, vertex.color=&#39;lightblue&#39;, layout=l,main=&quot;Large Graph Layout&quot;) plot(slack, edge.arrow.size = .3, vertex.label=NA,vertex.size=10, vertex.color=&#39;lightblue&#39;, layout=l2,main=&quot;Fruchterman-Reingold&quot;) plot(slack, edge.arrow.size = .3, vertex.label=NA,vertex.size=10, vertex.color=&#39;lightblue&#39;, layout=l3,main=&quot;DrL&quot;) plot(slack, edge.arrow.size = .3, vertex.label=NA,vertex.size=10, vertex.color=&#39;lightblue&#39;, layout=l4,main = &quot;MDS&quot;) To reset your plot window, you should run dev.off() or else your future plots will continue to display in a 2x2 grid. dev.off() 19.2.1 Adding attribute information to your visualization We commonly want to represent information about our nodes using color or size. This is easily done by passing a vector of colors into the plot function that maintains the order in the users data frame. We can then create a legend and locate it in our plot window as desired. plot(slack, edge.arrow.size = .2, vertex.label=V(slack)$name, vertex.size=10, vertex.label.cex = 0.3, vertex.color=c(&quot;blue&quot;,&quot;orange&quot;)[as.factor(V(slack)$Cohort)], layout=l3, main = &quot;Slack Network Colored by Cohort&quot;) legend(x=-1.5,y=0,unique(V(slack)$Cohort),pch=21, pt.bg=c(&quot;blue&quot;,&quot;orange&quot;),pt.cex=2,bty=&quot;n&quot;,ncol=1) A (nearly) complete list of plot option parameters is given below: vertex.color: Node color vertex.frame.color: Node border color vertex.shape: Vector containing shape of vertices, like “circle,” “square,” “csquare,” “rectangle” etc vertex.size: Size of the node (default is 15) vertex.size2: The second size of the node (e.g. for a rectangle) vertex.label: Character vector used to label the nodes vertex.label.color: Character vector specifying color the nodes vertex.label.family: Font family of the label (e.g.“Times,” “Helvetica”) vertex.label.font: Font: 1 plain, 2 bold, 3, italic, 4 bold italic, 5 symbol vertex.label.cex: Font size (multiplication factor, device-dependent) vertex.label.dist: Distance between the label and the vertex vertex.label.degree: The position of the label in relation to the vertex (use pi) edge.color: Edge color edge.width: Edge width, defaults to 1 edge.arrow.size: Arrow size, defaults to 1 edge.arrow.width: Arrow width, defaults to 1 edge.lty: Line type, 0 =“blank,” 1 =“solid,” 2 =“dashed,” 3 =“dotted,” etc edge.curved: Edge curvature, range 0-1 (FALSE sets it to 0, TRUE to 0.5) and if you’d like to try a dark-mode style visualization, consider the global graphical parameter to change the background color of your visual: par(bg=\"black\"). Any one of these option parameters can be set according to a variable in your dataset, or a metric about your graph. For example, let’s define degree as the number of edges that are adjacent to a given vertex. We can size the vertices according to their degree by including that information in the plot function as follows, using the degree() function. We just have to keep in mind that the vertex.size plot attribute is expecting the same range of sizes that you would provide for any points on a plot, and since the degree of a vertex can be very high in this case, we should put it on a scale that seems more reasonable. In this example. we divide the degree by the maximum degree to create a number between 0 and 1 and then multiply it by 10 to create vertex.size values between zero and 10. plot(slack, edge.arrow.size = .2, vertex.label=V(slack)$name, vertex.size=10*degree(slack, v=V(slack), mode=&#39;all&#39;)/max(degree(slack, v=V(slack), mode=&#39;all&#39;)), vertex.label.cex = 0.3, vertex.color=c(&quot;blue&quot;,&quot;orange&quot;)[as.factor(V(slack)$Cohort)], layout=l3, main = &quot;Slack Network Colored by Cohort&quot;) legend(x=-1.5,y=0,c(&quot;Orange&quot;,&quot;Blue&quot;),pch=21, pt.bg=c(&quot;Orange&quot;,&quot;Blue&quot;),pt.cex=2,bty=&quot;n&quot;,ncol=1) ## Package networkD3 The network D3 package creates the same type of visualizations that you would see in the JavaScript library D3. These visualizations are highly interactive and quite beautiful. library(networkD3) 19.2.2 Preparing the data for networkD3 The one thing that you’ll have to keep in mind when creating this visualization is the insistence of this package that your label names (indices) of your nodes start from zero. To use this package, you need a data frame containing the edge list and a data frame containing the node data. While we already have these data frames prepared, the following chunk of code shows you how to extract them from an igraph object and easily transform your ID or label column into a counter that starts from 0. You can see the first few rows of the resulting data frames below. nodes=data.frame(vertex_attr(slack)) nodes$ID=0:(vcount(slack)-1) #data frame with edge list edges=data.frame(get.edgelist(slack)) colnames(edges)=c(&quot;source&quot;,&quot;target&quot;) edges=merge(edges, nodes[,c(&quot;name&quot;,&quot;ID&quot;)],by.x=&quot;source&quot;,by.y=&quot;name&quot;) edges=merge(edges, nodes[,c(&quot;name&quot;,&quot;ID&quot;)],by.x=&quot;target&quot;,by.y=&quot;name&quot;) edges=edges[,3:4] colnames(edges)=c(&quot;source&quot;,&quot;target&quot;) head(edges) ## source target ## 1 80 0 ## 2 77 0 ## 3 99 0 ## 4 22 0 ## 5 101 1 ## 6 71 1 head(nodes) ## name Cohort ID ## 1 U012E07QP1D o 0 ## 2 U012E08EWTZ b 1 ## 3 U012E08R0CX o 2 ## 4 U012E08TYMV b 3 ## 5 U012E096VF1 o 4 ## 6 U012E09JRAB o 5 Once we have our data in the right format it’s easy to create the force-directed network a la D3 with the forceNetwork() function, and to save it as an .html file with the saveNetwork() function. 19.2.3 Creating an Interactive Visualization with networkD3 The following visualization is interactive! Try it by hovering on or dragging a node. colors = JS(&#39;d3.scaleOrdinal().domain([&quot;b&quot;, &quot;o&quot;]).range([&quot;#0000ff&quot;, &quot;#ffa500&quot;])&#39;) forceNetwork(Links=edges, Nodes=nodes, Source = &quot;source&quot;, Target = &quot;target&quot;, NodeID=&quot;name&quot;, Group=&quot;Cohort&quot;, colourScale=colors, charge=-100,fontSize=12, opacity = 0.8, zoom=F, legend=T) 19.2.4 Saving your Interactive Visualization to .html Exploration of the resulting visualization is likely to be smoother in .html, so let’s export this visualization to a file with saveNetwork(). j=forceNetwork(Links=edges, Nodes=nodes, Source = &quot;source&quot;, Target = &quot;target&quot;, NodeID=&quot;name&quot;, Group=&quot;Cohort&quot;, fontSize=12, opacity = 0.8, zoom=T, legend=T) saveNetwork(j, file = &#39;Slack2021.html&#39;) You can find the resulting file in your working directory (or you can specify a path rather than just a file name) and open it with any web browser. "],["clusintro.html", "Chapter 20 Introduction 20.1 Mathematical Setup 20.2 The Number of Clusters, \\(k\\) 20.3 Partitioning of Graphs and Networks 20.4 History of Data Clustering", " Chapter 20 Introduction Clustering is the task of partitioning a set of objects into subsets (clusters) so that objects in the same cluster are similar in some sense. Thus, a “cluster” is a generally a subjective entity determined by how an observer defines the similarity of objects. Take for example Figure 20.1 where we depict eight objects that differ by shape and color. Depending on the notion of similarity used (color, shape, or both) these objects might be clustered in one of the three ways shown. Figure 20.1: Three Meaningful Clusterings of One Set of Objects 20.1 Mathematical Setup In order to define this problem in a mathematical sense, it is necessary to quantify the attributes of objects using data so that we can mathematically or statistically determine notions of similarity. Data clustering refers to the process of grouping data points naturally based on information found in the data which describes its characteristics and relationships. It is not an exact science and, as we will discuss at length, there is no best method for partitioning data into clusters. 20.1.1 Data We will use the terms “observation,” “object,” and “data point” interchangeably to refer to the entities we aim to partition into clusters. These data points could represent any population of interest, be it a collection of documents or a group of Iris flowers. Each data point will be considered as a column vector, containing measurements of features, attributes, or variables (again, used interchangeably) which characterize it. For example, a column vector characterizing a document could have as many rows as there are words in the dictionary, and each entry in the vector could indicate the number of times each word occurred in the document. An Iris flower, on the other hand, may be characterized by far fewer attributes, perhaps measurements on the size of its petal and sepal. It is assumed we have \\(n\\) such objects, each represented by a column vector \\(\\x_j\\) containing measurements on \\(m\\) variables. All of this information is collected in an \\(m \\times n\\) data matrix, \\(\\X\\), which will serve as input to the various clustering methods. \\[\\X=[\\x_1,\\x_2,\\dots,\\x_n]\\] The aim of data clustering is to automatically determine clusters in these populations based upon the information contained in those vectors. In the document collection, the goal may be to identify clusters of documents which discuss similar subject matter whereas in the Iris data, the goal may be to learn about different species of Iris flowers. In applied data mining, variables fall into the following four categories: Nominal/Categorical, Ordinal, Interval, or Ratio. Nominal/Categorical: Variables which have no ordering, for example ethnicity, color or shape. Ordinal: Variables which can be rank-ordered but for which distances have no meaning. For example, scores assigned to levels education (0=no high school, 1=some high school, 2=high school diploma). The distance between 0 and 1 is not necessarily the same as the distance between 1 and 2, but the numbers have some meaning by order. Interval: Variables for which differences can be interpreted but for with ratios make no sense. For example if temperature is measured in degrees Fahrenheit the distance from 20 to 30 degrees is the same as the distance from 70 to 80 degrees, however 80 degrees is not ``twice as hot’’ as 40 degrees. Ratio: variables for which a meaningful ratio can be constructed. For example height or weight. An absolute zero is meaningful for a ratio variable. For the algorithms contained herein, it is assumed that the attributes used are ratio variables, although many of the methods can be extended to include other types of data (or one can simply use dummy variables and/or ordinal encoding as long as one explores the potential impacts of such decisions). 20.2 The Number of Clusters, \\(k\\) One of the important problems in cluster analysis is the determination of the number of clusters, \\(k\\). The number of clusters can also be a matter of subjectivity. Take for instance the 2-dimensional scatter plot of points in Figure 20.2. Using the points’ proximity in Euclidean space as a measure of their similarity, one could argue that there are any number of clusters in this simple illustration. However, most people would agree that there is indeed cluster-like structure in this data. Figure 20.2: How Many Clusters do You See? Figure 20.2 also motivates a discussion of subcluster structure. Figure 20.3 shows two possible clusterings with different numbers of clusters. The clustering on the right depicts a logical subdivison of the clusters on the left. Figure 20.3: Two Reasonable Answers with Different Numbers of Clusters It is easy to imagine data in which the number of clusters to specify is a matter of debate only because groups of related objects can be meaningfully divided into subcategories or subclusters. For example a collection of webpages may clearly fall into 3 categories: sports, investment banking, and astronomy. If the webpages about sports further divide into 2 categories like baseball and basketball then we’d refer to that as subclustering. 20.3 Partitioning of Graphs and Networks Another popular research problem in clustering is the partitioning of graphs. In network applications this problem has become known to many as community detection [60], although the underlying problem of partitioning graphs has been studied extensively for years [25]–[27], [30], [55], [58], [68]. A graph is a set of vertices (or equivalently nodes) \\(V=\\{1,2,\\dots, n\\}\\) together with a set of edges \\(E=\\{(i,j) : i,j \\in V\\}\\) which connect vertices together. A weighted graph is one in which the edges are assigned some weight \\(w_{ij}\\) whereas an unweighted graph has binary weights for edges: \\(w_{ij}=1\\) if \\((i,j) \\in E\\), \\(w_{ij}=0\\) otherwise. Our focus will be on undirected graphs in which \\(w_{ij}=w_{ji}\\). All algorithms for graph partitioning (or network community detection) will rely on an adjacency matrix. Definition 20.1 (Adjacency Matrix) An adjacency matrix \\(\\A\\) for an undirected graph, \\(\\mathcal{G}=(V,E)\\), is an \\(n \\times n\\) symmetric matrix defined as follows: \\[\\A_{ij}= \\begin{cases} w_{ij}, &amp; \\text{if } (i,j) \\in E \\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] Figure 20.4 is an example of a graph exhibiting cluster or community structure. The weights of the edges are depicted by their thickness. It is expected that edges within the clusters occur more frequently and with higher weight than edges between the clusters. Thus, once the rows and columns of the matrix are reordered according to their cluster membership, we expect to see a matrix that is block-diagonally dominant - that is, one in which values in square diagonal blocks are relatively large compared to those in the off-diagonal blocks. Figure 20.4: A Graph with Clusters and its Block Diagonally Dominant Adjacency Matrix In much of the literature on graph partitioning, it is suggested that the data clustering problem can be transformed into a graph partitioning problem by means of a similarity matrix [68]. A similarity matrix \\(\\bo{S}\\) is an \\(n \\times n\\) symmetric matrix where \\(\\bo{S}_{ij}\\) measures some notion of similarity between data points \\(\\x_i\\) and \\(\\x_j\\). There are a wealth of metrics available to gauge similarity or dissimilarity, see for example [15]. Common measures of similarity rely on Euclidean or angular distance measures. Two of the most popular similarity functions in the literature are the following: Any similarity matrix can be considered an adjacency matrix for a graph, thus transforming the original data clustering problem into a graph partitioning problem. Several algorithms for data clustering and graph partitioning are provided in Section 21. Regardless of the method chosen for clustering, similarity matrices can be a useful tool for visualizing cluster results in light of the block diagonal structure shown in Figure 20.4. This block diagonal structure can be visualized using a heat map of a similarity matrix. A matrix heat map represents each value in a matrix by a colored pixel indicating the magnitude of the value. Figure 20.5 is an example of a heat map using real data. The data are a collection of news articles (documents) from the web containing seven different topics of discussion. The rows and columns of the cosine similarity matrix for these documents have been reordered according to some cluster solution. White pixels represent negligible values of similarity. Some of these topics are closely related, which can be seen by the heightened level of similarities between blocks. Figure 20.5: Heat Map of Similarity Matrix Exhibiting Cluster Structure While a heat-map visualization allows the user to get a sense of the quality of a specific clustering, it does not always make it easy to determine which is a better solution given two different clusterings. Since clustering is an unsupervised process, quantitative measures of cluster quality are often used to compare different clusterings. A short survery of these metrics is given in Section 23. First, we will take a brief historical look at the roots of cluster analysis and how it emerged into a major field of research in the late twentieth century. 20.4 History of Data Clustering According to Anil Jain in [49], data clustering first appeared in an article written by Forrest Clements in 1954 about anthropological data [48]. However, a Google Scholar search provides several earlier publications whose titles also contain the phrase “cluster analysis” [46]. In fact, the discussion of data clustering dates back to the 1930’s when anthropologists Driver and Kroeber [45] and psychologists Zubin [44] and Tryon [43] realized the utility of such analysis in determining cultural or psychological classifications. While the usefulness of such techniques was clear to researchers in many social and biological disciplines at that time, the lack of computational tools made the analysis time consuming and practically impossible for large sets of data. Cluster analysis exploded into the limelight in the 1960’s and `70’s after Sokal and Sneath’s 1963 book Principles of Numerical Taxonomy [50]. Although the text is primarily geared toward biologists faced with the task of classifying organisms, it motivated researchers from many different disciplines to consider the problem of data clustering from other angles like computing, statistics, and domain specific applications. [24]. Sokal and Sneath’s book presents a detailed discussion of the simple, intuitive, and still popular hierarchical clustering (see Section 21.1) techniques for biological taxonomy. These authors also provided perhaps the earliest mention of the matrix heat map visualizations of clustering that are still popular today. Figure 20.6 shows an example of one of these heat maps, then drawn by hand, from their book. Figure 20.6: Hand Drawn Matrix Heat Map Visualization from 1963 Book by Sokal and Sneath Prior to the development of modern computational resources, programs for numerical taxonomy were written in machine language and not easily transferred from one computer to another [50]. However, by the mid 1960s, it was clear that the advancement in technology would probably keep pace with advancements in algorithm design and many researchers from various disciplines began to contribute to the clustering literature. The following sections present an in-depth view of the most popular developments in data clustering since that time. Chapter ?? explores a common problem associated with the massive datasets of modern day. References "],["clusteralgos.html", "Chapter 21 Algorithms for Data Clustering 21.1 Hierarchical Algorithms 21.2 Iterative Partitional Algorithms 21.3 Density Search Algorithms 21.4 Conclusion", " Chapter 21 Algorithms for Data Clustering There have been countless algorithms proposed for data clustering. While a complete survey and discussion of clustering algorithms would be nearly impossible, this chapter provides an introduction to some of the most popular algorithms to date. For the purposes of organization, the algorithms are divided into 3 groups: Hierarchical, Iterative Partitional, and Density-based. 21.1 Hierarchical Algorithms As discussed in Chapter 20, data clustering became popular in the biological fields of phylogeny and taxonomy. Even prior to the advancement of numerical taxonomy, it was common for scientists in this field to communicate relationships by way of a dendrogram or tree diagram as illustrated in Figure 21.1 [50]. Dendrograms provide a nested hierarchy of similarity that allow the researcher to see different levels of clustering that may exist in data, particularly in phylogenic data. Agglomerative hierarchical clustering has its roots in this domain. 21.1.1 Agglomerative Hierarchical Clustering The idea behind agglomerative heirarchical clustering is to link similar objects or similar clusters of objects together in a hierarchy where the highest levels of similarity is represented by the lowest level connections. These methods are called agglomerative because they begin with each data point in a separate cluster and at each step they merge clusters together according to some decision rule until eventually all of the points end up in a single cluster. For example, in Figure 21.1, objects 1 and 2 exhibit the highest level of similarity as indicated by the height of the branch that connects them. Also illustrated in the dendrogram is the fact that the blue cluster and green cluster are more similar to each other than they are to the red cluster. One of the advantages to these hierarchical structures is that branches can be cut to achieve any number of clusters desired by the user. For example, in Figure 21.1 if only the highest branch of the dendrogram is cut, the result is two clusters: {{1,2,3},{4,5,6,7,8,9}}. When the next highest branch is cut, we are left with 3 clusters: {{1,2,3},{4,5,6},{7,8,9}}. Figure 21.1: A Dendrogram exhibiting linkage/similarity between 9 objects in 3 clusters. There are a number of different systems for determining linkage in hierarchical clustering dendrograms. For a complete discussion, we suggests the classic books by Anderberg [42] or Jain and Dubes [41]. The basic scheme for hierarchical clustering algorithms is outlined in the algorithm below. Agglomerative Hierarchical Clustering Input: n objects to be clustered. Begin by assigning each object to its own cluster. Compute the pairwise similarities between each cluster. Find the most similar pair of clusters and merge them into a single cluster. There is now one less cluster. Compute pairwise similarities between the new cluster and each of the old clusters. Repeat steps 3-4 until all objects belong to a single cluster of size n. Output: Dendrogram depicting each merge step. What differentiates the numerous hierarchical clustering algorithms is the choice of similarity metric used and the way the chosen similarity metric is used to compare clusters in step 4. For example, suppose Euclidean distance is chosen to compute the similarity (or dissimilarity) between objects in step 2. In step 4, the same notion of similarity must be extended to compare clusters of objects. Several methods of computing pairwise distances between clusters have been proposed over the years. The most common approaches are as follows: Single-Linkage: The distance between two clusters is equal to the shortest distance from any member of one cluster to any member of the other cluster. Complete-Linkage: The distance between two clusters is equal to the greatest distance from any member of one cluster to any member of the other cluster. Average-Linkage: The distance between two clusters is equal to the average distance from any member of one cluster to any member of the other cluster. While many people have been given credit for the methods listed above, it appears that numerical taxonomers Sneath, Sokal and Michener were the first to describe the Single- and Average-linkage protocols, while ecologist Sorenson had previously pioneered Complete-linkage in his ecological studies. These early researchers used correlation coefficients to measure similarity between objects, but they suggest in 1963 that other correlation-like or distance-like measures could also be useful [50]. The paper by Stephen Johnson in 1967 [40] formalized the single- and complete-linkage algorithms in a more general data clustering setting. Other linkage techniques for hierarchical clustering, such as centroid and median linkage, have been proposed as well. We refer interested readers to Anderberg [42] for more on these variants. The main drawback of agglomerative hierarchical schemes is their computational complexity. In recent years, variations like BIRCH [36] and CURE [35] have been developed in an effort to combat this problem. Another feature which causes problems in some applications is that once a connection between points or clusters is made, it cannot be undone. For this reason, hierarchical algorithms often suffer in the presence of noise and outliers. 21.1.2 Principal Direction Divisive Partitioning (PDDP) While the hierarchical algorithms discussed above were agglomerative, it is also possible to create a cluster hierarchy or dendrogram by iteratively {dividing points into groups until a desired number of groups is reached. Principal Direction Divisive Partitioning (PDDP) is one example of a {divisive hierarchical algorithm. Other partitional methods which will be discussed in Section 22.1 can also be placed in this hierarchical framework. PDDP was proposed in [57] by Daniel Boley at the University of Minnesota. PDDP has become popular due to its computational efficiency and ability to handle large data sets. We will explain this algorithm in a different, but equivalent context than is done in the original paper. At each step of this method, data are projected onto the first principal component and split into two groups based upon which side of the mean their projections fall. The first principal component, as discussed in Chapter 13, creates the total least squares line, \\(\\mathcal{L}\\), which is the line which minimizes the total sum of squares of orthogonal deviations between the data and \\(\\mathcal{L}\\) among all lines in \\(\\Re^m\\). Let \\(\\X=[\\mathbf{x}_1,\\mathbf{x}_2,\\dots,\\mathbf{x}_n]\\) be the data points and \\(\\mathcal{L}(\\mathbf{u},\\bo{p})\\) be a line in \\(\\Re^m\\) where \\(\\bo{p}\\) is a point on a line and \\(\\mathbf{u}\\) is the direction of the line. The projection of \\(\\mathbf{x}_j\\) onto \\(\\mathcal{L}(\\mathbf{u},\\bo{p})\\) is given by \\[\\widehat{\\mathbf{x}_j} = \\mathbf{u}\\mathbf{u}^T(\\mathbf{x}_j-\\bo{p})+\\bo{p},\\] and therefore the orthogonal distance between \\(\\mathbf{x}_j\\) and \\(\\mathcal{L}(\\mathbf{u},p)\\) is \\[\\mathbf{x}_j - \\widehat{\\mathbf{x}_j} = (\\bo{I}-\\mathbf{u}\\mathbf{u}^T)(\\mathbf{x}_j-\\bo{p}).\\] Consequently, the total least squares line is the line \\(\\mathcal{L}\\) which minimizes (over directions \\(\\mathbf{u}\\) and points \\(\\bo{p}\\)) \\[\\begin{equation*} \\begin{split} f(\\mathbf{u},\\bo{p}) &amp;= \\sum_{j=1}^{n} \\|\\mathbf{x}_j - \\widehat{\\mathbf{x}_j}\\|_2^2\\\\ &amp;=\\sum_{j=1}^{n} \\|(\\bo{I}-\\mathbf{u}\\mathbf{u}^T)(\\mathbf{x}_j-\\bo{p})\\|_2^2\\\\ &amp;= \\|(\\bo{I}-\\mathbf{u}\\mathbf{u}^T)(\\X-\\bo{p}\\e^T)\\|_F^2. \\end{split} \\end{equation*}\\] The following definition precisely characterizes the first principal component as the total least squares line. Definition 1.3 (First Principal Component (Total Least Squares Line)) The First Principal Component (total least squares line) for the column data in \\(\\X=[\\mathbf{x}_1,\\mathbf{x}_2,\\dots,\\mathbf{x}_n]\\) is given by \\[\\mathcal{L} = \\{\\alpha \\mathbf{u}_1(\\X_c) + \\boldsymbol\\mu | \\alpha \\in \\Re\\},\\] where \\(\\boldsymbol\\mu= \\X\\e/n\\) is the mean (centroid) of the column data, and \\(\\mathbf{u}_1(\\bo{X}_c)\\) is the principal left-hand singular vector of the centered matrix \\[\\bo{X}_c=\\X-\\boldsymbol\\mu\\e^T = \\X(\\bo{I}-\\e\\e^T/n).\\] The orthogonal projection of the data onto the total least squares line will capture the maximum amount of directional variance over all possible one dimensional orthogonal projections. This fact is treated in greater detail in Chapter 13. Boley’s PDDP algorithm partitions the data into two clusters at each step based upon whether their projections onto the total least squares line fall to the left or to the right of \\(\\boldsymbol \\mu\\). This is equivalent to examining the signs of the projections of the centered data, \\(\\X_c\\), onto the direction \\(\\mathbf{u}_1(\\bo{X}_c)\\). Conveniently, the signs of the projections are determined by the signs of the entries in the principal right-hand singular vector, \\(\\vv_1(\\X_c)\\). A simple example motivating this method is illustrated in Figure 21.2. Figure 21.2: Illustration of Principal Direction Divisive Partitioning: Two Clusters and their Corresponding Projections on the First Principal Component Once the data are divided, the two clusters are examined to find the one with the greatest variance (scatter). This subset of data is then extracted from the original data matrix, centered and projected onto the span of its own first principal component. The split at zero is made again and the algorithm proceeds iteratively until the desired number of clusters has been produced. It is necessary to note, however, that the example in Figure 21.2 is truly an ideal geometric configuration of data. Figure 21.3 illustrates two configurations in which PDDP would fail. In the configuration on the left, both clusters would be split down the middle, and in the configuration on the right, the middle cluster would be split in the first iteration. Unfortunately, once data points are separated in an iteration of PDDP, there is no chance for them to be rejoined later. Table 21.1 provides the PDDP Algorithm. Figure 21.3: Failures of Principal Direction Divisive Partitioning: Two Configurations of Data that would be Poorly Clustered by PDDP Input: \\(n\\) data points \\(\\X=[\\mathbf{x}_1,\\mathbf{x}_2,\\dots,\\mathbf{x}_n]\\) and number of clusters \\(k\\) Center the data to have mean zero: \\(\\X_c = \\X-\\boldsymbol \\mu\\e^T\\). Compute the first right singular vector of \\(\\X_c\\), \\(\\vv_1\\). Partition the data into two clusters based upon the signs of the entries in \\(\\vv_1\\). Compute the variance of each existing cluster and choose the cluster with largest variance to partition next. Repeat steps 1-4 using only the data in the cluster with largest variance until eventually \\(k\\) clusters are formed. Output: Resulting \\(k\\)-clusters Table 21.1: Principal Direction Divisive Partitioning (PDDP) Since its initial publication, variations of the PDDP algorithm have been proposed, most notably PDDP(\\(\\ell\\)) [56] and KPDDP [3], both developed by Dimitrios Zeimpekis and Efstratios Gallopoulos from the University of Patras in Greece. PDDP(\\(\\ell\\)) uses the sign patterns in the first \\(\\ell\\) principal components to partition the data into at most \\(2^\\ell\\) clusters at each step of the algorithm, whereas KPDDP is a kernel variant which uses \\(k\\)-means to steer the cluster assignments at each step. 21.2 Iterative Partitional Algorithms Iterative partitional algorithms begin with an initial partition of the data into \\(k\\) clusters and iteratively update the cluster memberships according to some notion of what constitutes a ``better\" partition [41], [42]. The \\(k\\)-means algorithm is one example of a partitional algorithm. Before we get into the details of the modern day \\(k\\)-means algorithms, we’ll take a look back at the history that fostered its development as one of the best-known and most widely used clustering algorithms in the world. 21.2.1 Early Partitional Algorithms Although the name ``\\(k\\)-means\" was first used by MacQueen in 1967 [38], the partitional method generally referred to by this name today was proposed by Forgy in 1965 [39]. Forgy’s algorithm involves iteratively updating \\(k\\) seed points which, at each pass of the algorithm, define a partitioning of the data by associating each data point with its nearest seed point. The seeds are then updated to represent the centroids (means) of the resulting clusters and the process is repeated. Euclidean distance is the most common metric for measuring the nearness of points in these algorithms, but other metrics, such as Mahalanobis distance and angular distance, can and have been used as well. \\(K\\)-means can also handle binary or categorical variables by using simple matching coefficients found in the data mining literature, for example [64]. Forgy’s method is outlined in Table 21.2. Input: Data points and an initial cluster configuration of the data, defined by \\(k\\) seed points (start in step 1) or an initial clustering (start in step 2). Assign each data point to the cluster associated with the nearest seed point. Compute new seed points to be the centroids of the clusters. Repeat steps 1 and 2 until no data points change cluster membership in step 2. Output: Final Clusters Table 21.2: Forgy’s \\(k\\)-means Algorithm [42] In 1966, Jancey suggested a variation of this method where the new seeds points in step 2 were computed by reflecting the old seed point across the new centroid, as depicted in Figure 21.4. Jancey argued that the data’s nearness to point 1 grouped them into a cluster initially, and thus using a seed point which exaggerates this movement toward the new centroid ought to help speed up convergence, and possibly lead to a better solution by avoiding local minima [37]. Figure 21.4: Jancey’s method of reflecting old seed point across the centroid to determine new seed point MacQueen’s 1967 partitional process, which he called ``\\(k\\)-means\", differs from Forgy’s formulation in that it a) specifies initial seed points and b) assigns data points to clusters one-by-one, updating the seed to be the centroid of the cluster each time a new point is added. The algorithm only makes one pass through the data. MacQueen’s method is presented in Table 21.3. Input: \\(n\\) data points Choose the first \\(k\\) data points as clusters with one member each. Set i=1. Assign the \\((k+i)^{th}\\) data point to the cluster with the closest centroid. Recompute the cetroid of the updated cluster. Set \\(i=i+1\\). Repeat step 2 until \\(i=n-k\\) and all the data points have been assigned. Use final cluster centroids to determine a final clustering by re-assigning each data point to the cluster associated with its nearest centroid. Output: Final Clusters Table 21.3: MacQueens \\(k\\)-means Algorithm As you can see, MacQueen’s algorithm, while similar in spirit, is quite different from that proposed by Forgy. The set of clusters found is likely to be dependent upon the order of the data, a property generally undesirable in cluster analysis. MacQueen stated that in his experience, these discrepancies in final solution based upon the order of the data were generally minor, and thus not unlike those caused by the choice of initialization in Forgy’s method. An advantage of MacQueen’s algorithm is the reduced computation load achieved by avoiding the continual processing of the data to convergence. It has also been suggested that MacQueen’s method may be useful to initialize the seeds for Forgy’s algorithm [42] and in fact this option is available in many data mining software packages like SAS’s Enterprise Miner. Discussion of some additional partitional methods, including Dubes and Jain’s FORGY implementation and Ball and Hall’s ISODATA algorithm, is deferred to Chapter ?? because they involve procedures aimed at determining the number of clusters in the data. 21.2.2 \\(k\\)-means We will finish our discussion of \\(k\\)-means with what has become the classical presentation. We begin with a matrix of column data, \\(\\X=[\\mathbf{x}_1,\\mathbf{x}_2,\\dots,\\mathbf{x}_n]\\) where \\(\\mathbf{x}_i \\in \\Re^m, 1 \\leq i \\leq n\\). The objective of \\(k\\)-means is to determine a partitioning of the data into \\(k\\) sets, \\(C=\\{C_1, C_2, \\dots, C_k\\}\\), such that an intra-cluster sum of squares cost function is minimized: \\[ \\mbox{arg}\\min_C \\sum_{i=1}^{k} \\sum_{\\mathbf{x}_j \\in C_i} \\|\\bo{x}_j-\\boldsymbol \\mu_i \\|^2 \\] Any desired distance metric can be used, according to the applications and whims of the user. Euclidean distance is standard, and leads to the specification Euclidean \\(k\\)-means. In document clustering, it is common to use the cosine of the angle between two data vectors (documents) to measure their distance from each other. This variant is commonly referred to as spherical \\(k\\)-means and will be discussed briefly in Section 21.2.2.1. The \\(k\\)-means algorithm, which is essentially the same as Forgy’s algorithm in Section 21.2, is presented in Table 21.4. Input: Data points \\(\\{\\mathbf{x}_1,\\mathbf{x}_2,\\dots,\\mathbf{x}_n\\}\\) and set of initial centroids \\(\\{\\boldsymbol \\mu_1^{(0)},\\boldsymbol \\mu_2^{(0)},\\dots, \\boldsymbol \\mu_k^{(0)}\\}\\). Assign each data point to cluster associated with the nearest centroid. \\[ C_j^{(t)} = \\{\\mathbf{x}_i : \\|\\mathbf{x}_i-\\boldsymbol \\mu_j^{(t)} \\| \\leq \\|\\mathbf{x}_i-\\boldsymbol \\mu_l^{(t)} \\| \\,\\, \\forall 1 \\leq l \\leq k\\}\\] If two centroids are equally close, the tie is broken arbitrarily. The new centroid for each cluster is calculated by setting \\[\\boldsymbol \\mu_j^{(t+1)}=\\frac{1}{|C_j^{(t)}|} \\sum_{\\mathbf{x}_i \\in C_j^{(t)}} \\mathbf{x}_i\\] Repeat steps 2 and 3 until the centroids remain stationary. Output: \\(k\\) clusters \\(C_1,C_2,\\dots,C_k\\) Table 21.4: Euclidean \\(k\\)-means This algorithm is guaranteed to converge because there are a finite number of partitions possible and at each pass of the algorithm the intra-cluster sum of squares cost function is decreased due to the fact that points are reassigned to a new cluster only if they are closer to the existing centroid of the new cluster than they were to the old one. The cost function is further reduced as the new centroids are calculated and the process repeats, lowering the cost function at each step. However, it is quite common for the algorithm to converge to local minima, particularly with large datasets. The output of \\(k\\)-means is sensitive to the initialization of the centroids and the choice of distance metric used in step 2. Randomly initialized centroids tend to be the most popular, but one can also seed the algorithm with centroids of clusters determined by another clustering algorithm. 21.2.2.1 Spherical \\(k\\)-means In some applications, such as document clustering, similarity is often measured by the cosine of the angle \\(\\theta\\) between two objects \\(\\mathbf{x}_i\\) and \\(\\mathbf{x}_j\\) (each normalized to have unit norm), \\[\\cos(\\theta)=\\mathbf{x}_i^T\\mathbf{x}_j.\\] This similarity is often transformed into a distance by computing the quantity \\(d(\\mathbf{x}_i,\\mathbf{x}_j)=1-\\cos(\\theta)\\) to formulate the spherical \\(k\\)-means objective function as follows: \\[\\min_C \\sum_{i=1}^k \\sum_{\\mathbf{x} \\in C_i} 1- \\mathbf{x}^T \\bo{c}_i.\\] Where $_i = _i $ is the normalized centroid of the cluster. The spherical \\(k\\)-means algorithm is the same as the euclidean \\(k\\)-means algorithm aside from the definition of nearness in step 2. 21.2.2.2 \\(k\\)-mediods: Partitioning around Mediods (PAM) and Clustering Large Applications (CLARA) In 1987, Kaufman and Rousseeuw devised another partitional method which searched through data in order to find \\(k\\) representative points (or mediods) belonging to the dataset which would serve as cluster centers in the same way the centroids do in \\(k\\)-means. They called these points ``representative\" because it was thought the points would give some interpretability to the groups by exhibiting some defining characteristics of their associated clusters and distinguishing characteristics from other clusters. The authors’ original algorithm, Partitioning around Mediods (PAM), was not suitable for large datasets because of the computation time necessary to search through the data points to build the set of \\(k\\) representative points. The same authors developed a second algorithm, Clustering Large Applications (CLARA), to combat this problem. The central idea of CLARA was to use PAM on large datasets by sampling the data and applying the algorithm on the smaller sample. Once \\(k\\) representative points were found in the sample, the remaining data were associated with the mediod to which they were closest. The quality of the clustering is measured by the average distance of every object to its representative point. Five such samples are drawn, and the clustering that results in the lowest average distance is retained [34]. 21.2.3 The Expectation-Maximization (EM) Clustering Algorithm The Expectation-Maximization (EM) Algorithm, originally proposed by Dempster, Laird, and Rubin in 1977 [33], is one that has been used to solve many types of statistical problems over the years. It is generally used to determine parameters of a statistical model used to describe observations in a dataset. Here we will show how the algorithm is used for clustering, as in [32]. Our discussion is limited to the variant of the algorithm which uses Gaussian mixtures to model the data. Supposing that our data points, \\(\\mathbf{x}_1,\\mathbf{x}_2,\\dots,\\mathbf{x}_n\\), each belonging to one of \\(k\\) clusters (or classes), \\(C_1,C_2,\\dots, C_k\\). Then there exists some latent variables \\(y_i, \\,\\, 1\\leq i\\leq n\\), which identify the class membership of each \\(\\mathbf{x}_i\\). It is assumed that each class label \\(C_i\\) determines the probability distribution of the data in that class. Here, we assume that this distribution is multivariate Gaussian. The parameters of this model include the a priori probabilities of each of the \\(k\\) classes, \\(P(C_i)\\), and the parameters of the corresponding normal distributions \\(\\boldsymbol \\mu_i\\) and \\(\\mathbf{\\Sigma_i}\\), which are the mean and covariance matrix respectively. The objective of the EM algorithm is to determine the parameters which maximize the likelihood of the data: \\[\\log L = \\sum_i (\\log P(y_i) + \\log P(\\mathbf{x}_i|y_i))\\] The EM algorithm takes as input a set of \\(m\\)-dimensional data points, \\(\\{\\mathbf{x}_i\\}_{i=1}^n\\), the desired number of clusters \\(k\\), and an initial set of parameters \\(\\theta_j\\) for each cluster \\(C_j\\) \\(1\\leq j\\leq k\\). For Guassian mixtures, \\(\\theta_j\\) consists of mean \\(\\boldsymbol \\mu_j\\) and an \\(m\\times m\\) covariance matrix \\(\\mathbf{\\Sigma_j}\\). The a priori probability of each cluster, \\(\\alpha_j = P(C_j)\\) must also be initialized and updated throughout the algorithm. If no information is known about the underlying clusters, then we suggest initialization \\(\\alpha_j = 1/k\\) for all clusters \\(C_j\\). EM then operates by iteratively executing an expectation step, where the probability that each data point belongs to each of the \\(k\\) classes is computed, followed by a maximization step, where the parameters for each class are updated to maximize the likelihood of the data [32]. These steps are summarized in Table 21.5. Input: \\(n\\) data points, \\(\\{\\mathbf{x}_i\\}_{i=1}^n\\), number of clusters \\(k\\), and initial set of parameters for each cluster \\(C_j\\): \\(\\alpha_j\\) and \\(\\theta_j = \\{\\boldsymbol \\mu_j, \\Sigma_j\\}\\,\\,1\\leq j\\leq k\\) Expectation Step: Compute the probability of each data point \\(\\mathbf{x}_i\\) being drawn from each class distribution, \\(C_j\\): \\[p_{ij} = P(\\mathbf{x}_i|\\alpha_j,\\boldsymbol \\mu_j,\\Sigma_j) \\propto \\alpha_j P(\\mathbf{x}_i|\\boldsymbol \\mu_j,\\Sigma_j)\\] Maximization Step: Update the parameters to maximize the likelihood of the data: \\[\\alpha_j = \\frac{1}{n} \\sum_{i=1}^{n} p_{ij}\\] \\[\\boldsymbol \\mu_j = \\frac{\\sum_{i=1}^{n} p_{ij}\\mathbf{x}_i}{\\sum_{i=1}^{n} p_{ij}}\\] \\[\\Sigma_j = \\frac{\\sum_{i=1}^n p_{ij}(\\mathbf{x}_i-\\boldsymbol \\mu_j)(\\mathbf{x}_i-\\boldsymbol \\mu_j)^T}{\\sum_{i=1}^{n} p_{ij}}\\] Repeat steps 1-2 until convergence. Output: Class label \\(j\\) for each \\(\\mathbf{x}_i\\) such that \\(p_{ij} \\geq p_{il}\\,\\,1\\leq l \\leq k\\) Table 21.5: Expectation-Maximization Algorithm for Clustering [32]. The EM Algorithm with Gaussian mixtures works well for clustering when the normality assumption of the underlying clusters holds true. Unfortunately, it is difficult to know if this is the case prior to the identification of the clusters. The algorithm suffers considerable computational drawbacks, particularly with regards to storage of the \\(k\\) covariance matrices \\(\\mathbf{\\Sigma_j}\\in \\Re^{m\\times m}\\), and is not easily run in parallel. For this reason, the EM algorithm is generally limited in its ability to be used on large datasets, particularly when the number of attributes \\(m\\) is very large, as it is in document clustering. 21.3 Density Search Algorithms If objects are depicted as data points in a metric space, then one may interpret the problem of clustering as an attempt to find areas of the space that are densely populated by points, separated by less populated areas. A natural approach to the problem is then to search through the space seeking these dense regions. Such algorithms have been referred to as density search algorithms [22]. While these algorithms tend to suffer on real data in both accuracy efficiency, their ability to identify noise and to estimate the number of clusters \\(k\\) makes them worthy of discussion. Many density search algorithms have their roots in the single-linkage hierarchical algorithms described in Section 21.1. Individual points are joined together in clusters one-by-one based upon their similarity (or nearness in space). However in this case there exists some criteria for which objects are rejected from joining an existing cluster and instead are set out to form their own cluster. For example, suppose we had two distinct well separated dense regions of points. Beginning with a single point in the first region, we form a cluster and search through the remaining points one by one adding them to the cluster in they satisfy some specified criterion of nearness to the points already in the cluster. Once all the points in the first region are combined into a single cluster, the purpose of the criterion is to reject points from the second region from joining the first cluster, causing them to create a new cluster. The conception of density search algorithms dates to the late `60s with the taxmap method of Carmichael et al. in [20], [21] and the mode analysis method of Wishart [19]. In taxmap the authors suggested criterion like the drop in average similarity upon adding a new point to a cluster. In mode analysis the criterion was simply containment in a specified radius of points in a cluster. The problem with this approach was that it had trouble finding both large and small clusters simultaneously [22]. All density search algorithms suffer from the inability to find clusters of varying density, no matter how the term is defined in application, because the density of points is used to define the notion of a cluster. High dimensional data adds to this problem as demonstrated in Chapter ?? because as the size of the space grows, the points naturally become less and less dense inside of it. Another problem with density search algorithm is the necessity to search through data again and again, making their implementation difficult if not irrelevant for large data sets. Among the benefits to these methods are the inherent estimation of the number of clusters and their ability to find irregularly shaped (non-convex) clusters. Several algorithms in this category, like Density Based Spacial Clustering of Applications with Noise (DBSCAN) also make an effort to determine outliers or noise in the data. Because of the computational workload of these methods, we will abandon them after the present discussion in favor of more efficient methods. For an in-depth analysis of other density search algorithms and their variants, see [18]. 21.3.1 Density Based Spacial Clustering of Applications with Noise (DBSCAN) Density Based Spacial Clustering of Applications with Noise (DBSCAN) is an algorithm proposed by Ester, Kriegel, Sander, and Xu in 1996 [17], which uses the Euclidean nearness of a group of points in \\(m\\)-space to define density. The algorithm uses the terminology in Definition 21.1. Definition 21.1 (DBSCAN Terms) The following definitions will aid our discussion of the DBSCAN algorithm: Dense Point and \\(\\rho_{min}\\): A point \\(\\mathbf{x}_j\\) is called dense if there are at least \\(\\rho_{min}\\) other points contained in its \\(\\epsilon\\)-neighborhood. Direct Density Reachability: A point \\(\\mathbf{x}_i\\) is called directly density reachable from a point \\(\\mathbf{x}_j\\) if it is in the \\(\\epsilon\\)-neighborhood surrounding \\(\\mathbf{x}_j\\), i.e. if \\(\\mathbf{x}_i \\in \\mathscr{N}(\\mathbf{x}_j,\\epsilon)\\), and \\(\\mathbf{x}_j\\) is a dense point. Density Reachability: A point \\(\\mathbf{x}_i\\) is called density reachable from a point \\(\\mathbf{x}_j\\) if there is a sequence of points \\(\\mathbf{x}_{1},\\mathbf{x}_{2},\\dots, \\mathbf{x}_{p}\\) with \\(\\mathbf{x}_{1}=\\mathbf{x}_j\\) and \\(\\mathbf{x}_{p}=\\mathbf{x}_i\\) where each \\(\\mathbf{x}_{{k+1}}\\) is directly density reachable from \\(\\mathbf{x}_{k}.\\) Noise Point: A point \\(\\mathbf{x}_l\\) is called a noise point or outlier if it contains 0 points in its \\(\\epsilon\\)-neighborhood. The relationship of density reachability is not symmetric. This fact is illustrated in Figure 21.5. A point in this illustration is dense if its \\(\\epsilon\\)-neighborhood contains at least \\(\\rho_{min} = 2\\) other points. The green point \\(a\\) is density reachable from the blue point \\(b\\), however the reverse is not true because \\(a\\) is not a dense point. Because of this, we introduce the notion of density connectedness. Figure 21.5: DBSCAN Illustration Definition 21.2 (Density Connectedness) Two points \\(\\mathbf{x}_i\\) and \\(\\mathbf{x}_j\\) are density-connected if there exists some point \\(\\mathbf{x}_k\\) such that both \\(\\mathbf{x}_i\\) and \\(\\mathbf{x}_j\\) are density reachable from \\(x_k\\). In Figure 21.5, it is clear that we can say points \\(a\\) and \\(b\\) are density-connected since they are each density reachable from any of the 4 points in between them. The point \\(c\\) in this illustration is a noise point or outlier because there are no points contained in its \\(\\epsilon\\)-neighborhood. Using these definitions, we can formalize the properties that define a cluster in DBSCAN. Definition 21.3 (DBSCAN Cluster) Given the parameters \\(\\rho_{min}\\) and \\(\\epsilon\\), a DBSCAN cluster is a set of points that satisfy the two following conditions: All points within the cluster are mutually density-connected. If a point is density-connected to any point in the cluster, it is part of the cluster as well. Table 21.6 describes how DBSCAN finds such clusters. Input: Set of points \\(\\mathbf{X}=[\\mathbf{x}_1,\\mathbf{x}_2,\\dots,\\mathbf{x}_n]\\) to be clustered and parameters \\(\\epsilon\\) and \\(\\rho_{min}\\) For each unvisited point \\(p=\\mathbf{x}_i\\), do: Mark \\(p\\) as visited. Let \\(\\mathcal{N}\\) be the set of points contained in the \\(\\epsilon\\)-neighborhood around \\(p\\). If \\(|\\mathcal{N}| &lt; \\rho_{min}\\) mark \\(p\\) as noise. Else let \\(C\\) be the next cluster. Do: Add \\(p\\) to cluster \\(C\\). For each point \\(p&#39;\\) in \\(\\mathcal{N}\\), do: If \\(p&#39;\\) is not visited, mark \\(p&#39;\\) as visited, let \\(\\mathscr{N}&#39;\\) be the set of points contained in the \\(\\epsilon\\)-neighborhood around \\(p&#39;\\). If \\(|\\mathcal{N}&#39;| \\geq \\rho_{min}\\) let \\(\\mathcal{N}=\\mathcal{N} \\cup \\mathcal{N}&#39;\\) If \\(p&#39;\\) is not yet a member of any cluster, add \\(p&#39;\\) to cluster \\(C\\). Output: Clusters found \\(C_1,\\dots,C_k\\) Table 21.6: Density Based Spacial Clustering of Applications with Noise (DBSCAN) [64] 21.4 Conclusion The purpose of this chapter was to give the reader a basic understanding of hierarchical, iterative partitional, and density search approaches to data clustering. One of the main concerns addressed in this paper is that all of these algorithms have merit, but in application rarely do the algorithms completely agree on a solution. In fact, algorithms with random inputs like \\(k\\)-means are not even likely to agree with themselves over a number of different trials. It can be extremely difficult to qualitatively measure the goodness of your clustering when the data cannot be visualized in 2 or 3 dimensions. While there are a number of metrics to help the user get a sense of the compactness of the clusters (see Chapter 23), the effect of noise and outliers can often blur the true picture. It is also common for such metrics to take nearly equivalent values for vastly different cluster solutions, forcing the user to choose a solution using domain knowledge and utility. First we will look at another class of clustering methods which aim to solve the graph partitioning problem described in Chapter ??. The difference between the problems of data clustering and graph partitioning is merely the structure of the input objects to be clustered. In data clustering, the input objects are composed of measurements on \\(m\\) variables or features. If we interpret the graph partitioning problem in such a way that input objects are vertices on a graph and the variables describing them are the weights of the edges by which they are connected to other vertices, then it becomes clear we can use any of the methods in this chapter to cluster the columns of an adjacency matrix as described in Chapter ??. Similarly if one creates a similarity matrix for objects from a data clustering problem, we can cluster that matrix using the theory and algorithms from graph partitioning. While each problem can be transformed into the other, the design of the algorithms for the two cases is generally quite different. In the next chapter, we provide a thorough overview of some popular graph clustering algorithms. References "],["chap1-5.html", "Chapter 22 Algorithms for Graph Partitioning 22.1 Spectral Clustering 22.2 Fiedler Partitioning 22.3 Stochastic Clustering", " Chapter 22 Algorithms for Graph Partitioning 22.1 Spectral Clustering Spectral clustering is a term that data-miners have given to the partitioning problem as it arose in graph theory. The theoretical framework for spectral clustering was laid in 1973 by Miroslav Fiedler [30], [31]. We will begin with a discussion of this early work, and then take a look at how others have adapted the framework to meet the needs of data clustering. In this setting, we have a graph \\(G\\) on a set of vertices \\(N=\\{1,2,\\dots,n\\}\\) with edge set \\(E=\\{(i,j) : i,j \\in N \\mbox{and} i \\leftrightarrow j\\}\\). Edges between the vertices are recorded in an adjacency matrix \\(\\A = (a_{ij})\\), where \\(a_{ij}\\) is equal to the weight of the edge connecting vertex (object) \\(i\\) and vertex \\(j\\) and \\(a_{ij}=0\\) if \\((i,j) \\notin E\\). For the immediate discussion, we will assume the graph has no “self-loops,” i.e. \\(a_{ii}=0 \\forall i\\). Spectral clustering algorithms typically involve the Laplacian matrix associated with a graph. A Laplacian matrix is defined as follows: Definition 22.1 (The Laplacian Matrix) The Laplacian Matrix, \\(\\mathbf{L}\\), of an undirected, weighted graph with adjacency matrix \\(\\A=(a_{ij})\\) and diagonal degree matrix \\(\\mathbf{D}=\\mbox{diag}(\\A\\e)\\) is: \\[\\mathbf{L}=\\mathbf{D}-\\A\\] The Laplacian matrix is symmetric, singular, and positive semi-definite. To see this third property, construct an \\(n \\times |E|\\) “vertex-edge incidence” matrix \\(\\U\\) with rows corresponding to vertices and columns corresponding to edges. Allow the edges of the original graph to be directed arbitrarily, and set \\[ \\U_{v,e} = \\left\\{ \\begin{array}{lr} +\\sqrt{a_{ij}} : &amp;\\mbox{if } v \\mbox{ is the head of } e\\\\ -\\sqrt{a_{ij}} : &amp;\\mbox{if } v \\mbox{ is the tail of } e\\\\ 0 : &amp;\\mbox{otherwise} \\end{array} \\right. \\] Then \\(\\mathbf{L}=\\U\\U^T\\) is positive semi-definite [31]. \\(\\mathbf{L}\\) gives rise to a nice quadratic form: \\[\\begin{equation} \\tag{22.1} \\mathbf{y}^T \\mathbf{L} \\mathbf{y} = \\sum_{(i,j) \\in E}a_{ij} (y_i - y_j)^2. \\end{equation}\\] Let \\(\\sigma(\\mathbf{L})=\\{\\lambda_1 \\leq \\lambda_2 \\leq \\dots \\leq \\lambda_n\\}\\) be the spectrum of \\(\\mathbf{L}\\). Since \\(\\mathbf{L}\\) is positive semi-definite, \\(\\lambda_i \\geq 0 \\forall i\\). Also, since the row sums of \\(\\mathbf{L}\\) are zero, \\(\\lambda_1=0\\). Furthermore if the graph, \\(G\\), is composed of \\(k\\) connected components, each disconnected from each other, then \\(\\lambda_1=\\lambda_2=\\dots=\\lambda_k = 0\\) and \\(\\lambda_j \\geq 0 \\mbox{ for } j\\geq k+1\\). In [31] Fiedler defined the algebraic connectivity of the graph as the second smallest eigenvalue, \\(\\lambda_2\\), because its magnitude provides information about how easily the graph is to be disconnected into two components. Later, in [30], he alluded to the utility of the eigenvector associated with \\(\\lambda_2\\) in determining this two-component decomposition of a graph. 22.2 Fiedler Partitioning Suppose we wish to decompose our graph into two components (or clusters of vertices) \\(C_1\\) and \\(C_2\\) where the edges exist more frequently and with higher weight inside the clusters than between the two clusters. In other words, we intend to make an edge-cut disconnecting the graph into two clusters. It is desired that the resulting partition satisfies the following objectives: minimize the total weight of edges cut (edges in between components) maximize the total weight of edges inside the two components. To begin with, lets take the quadratic form in Equation ?? and let \\(\\y\\) be a vector that determines the cluster membership of each vertex as follows: \\[ \\y_i=\\left\\{ \\begin{array}{lr} +1 &amp; : \\mbox{if vertex } i \\mbox{ belongs in } C_1\\\\ -1 &amp; : \\mbox{if vertex } i \\mbox{ belongs in } C_2\\\\ \\end{array} \\right. \\] Our first goal is then to minimize Equation (22.1) over all such vectors \\(\\y\\): \\[\\begin{equation} \\tag{22.2} \\min_{\\y} \\y^T \\mathbf{L} \\y = \\sum_{(i,j) \\in E} a_{ij} (\\y_i-\\y_j)^2 = 2 \\sum_{\\substack{(i,j) \\in E \\\\i \\in C_1, j \\in C_2}} 4 a_{ij} \\end{equation}\\] Note that the final sum is doubled to reflect the fact that each edge connecting \\(C_1\\) and \\(C_2\\) will be counted twice. However, the above formulation is incomplete because it does not take into account the second objective, which is to maximize the total weight of edges inside the two components. Indeed it seems the minimum solution to Equation ?? would often involve cutting all of the edges adjacent to a single vertex of minimal degree, disconnecting the graph into components of size \\(1\\) and \\(n-1\\), which is generally undesirable. In addition, the above optimization problem is NP-hard. To solve the latter problem, the objective function is relaxed from discrete to continuous. By the Rayleigh theorem, \\[\\min_{\\|\\y\\|_2=1} \\y^T\\mathbf{L} \\y = \\lambda_1\\] with \\(\\y^*\\) being the eigenvector corresponding to the smallest eigenvalue. However, for the Laplacian matrix, \\(y^*=\\e\\). In context, this makes sense - in order to minimize the weight of edges cut, we should simply assign all vertices to one cluster, leaving the second empty. In order to divide the vertices into two clusters we need an additional constraint on \\(\\y\\). Since clusters of relatively balanced size are desirable, a natural constraint is \\(\\y^T\\e=0\\). By the Courant-Fischer theorem, \\[\\begin{equation} \\tag{22.3} \\min_{\\substack{\\| \\y \\|_2=1 \\\\ \\y^T \\e=0}} \\y^T \\mathbf{L} \\y = \\lambda_2 \\end{equation}\\] with \\(\\y^*=\\textbf{v}_2\\) being the eigenvector corresponding to the second smallest eigenvalue, \\(\\lambda_2\\). This vector is often referred to as the Fiedler vector after the man who identified its usefulness in graph partitioning. We define the Fiedler graph partition as follows: Definition 22.2 (Fiedler Graph Partition) Let \\(G=(N,E)\\) be a connected graph on vertex set \\(N=\\{1,2,\\dots,n\\}\\) with adjacency matrix \\(\\A\\). Let \\(\\mathbf{L}=\\mathbf{D}-\\A\\) be the Laplacian matrix of \\(G\\). Let \\(\\textbf{v}_2\\) be an eigenvector corresponding to the second smallest eigenvalue of \\(\\mathbf{L}\\). The Fiedler partition is: \\[\\begin{eqnarray*} C_1 &amp;=&amp; \\{i \\in N : \\textbf{v}_2(i) &lt;0\\}\\\\ C_2 &amp;=&amp; \\{i \\in N : \\textbf{v}_2(i) &gt;0\\} \\end{eqnarray*}\\] Vertices \\(j\\), for which \\(\\textbf{v}_2(j)=0\\), can be arbitrarily placed into either cluster. There is no uniform agreement on how to determine the cluster membership of vertices for which \\(\\textbf{v}_2(j)=0\\). The decision to make the assignment arbitrarily comes from experimental results that indicate in some scenarios these zero valuated vertices are equally drawn to either cluster. Situations where there are a large proportion of zero valuated vertices may be indicative of a graph which does not conform well to Fiedler’s partition, and we suggest the user tread lightly in these cases. Figure 22.1 shows the experimental motivation for our arbitrary assignment of zero valuated vertices. The vertices in these graphs are labelled according to the sign of the corresponding entry in \\(\\textbf{v}_2\\). We highlight the red vertex and watch how its sign in \\(\\textbf{v}_2\\) changes as nodes and edges are added to the graph. Figure 22.1: Fiedler Partitions and Zero Valuated Vertices In order to create more than two clusters, the Fiedler graph partition can be performed iteratively, by examining the subgraphs induced by the vertices in \\(C_1\\) and \\(C_2\\) and partitioning each based upon their own Fiedler vector, or in an extended fashion using multiple eigenvectors. This iterative method requires a cluster to be chosen for further division, perhaps based upon the algebraic connectivity of the cluster. It is also possible to use the sign patterns in subsequent eigenvectors to further partition the graph. This approach is called Extended Fiedler Clustering and is discussed in Section 22.2.1.1. First, let’s take a more rigorous look at why the sign patterns of the Fiedler vector provide us with a partition. 22.2.1 Linear Algebraic Motivation for the Fiedler vector The relaxation proposed in Equation (22.3) does not give us a general understanding of why the sign patterns of the Fiedler vector are useful in determining cluster membership information. We will use the following facts: Lemma 22.1 (Fiedler Lemma 1) Let \\(\\mathbf{L}=\\mathbf{D}-\\A\\) be a Laplacian matrix for a graph \\(G=(V,E)\\) with \\(|V|=n\\). Let \\(\\sigma(L)=\\lambda_1 \\leq \\lambda_2 \\leq \\dots \\leq \\lambda_n\\) Then \\(\\mathbf{L}\\) is symmetric and positive semi-definite with \\(\\lambda_1=0\\). Lemma 22.2 (Fiedler Lemma 2) \\(\\lambda_2(L)=0\\) if and only if the graph \\(G\\) has 2 components, \\(C_1\\) and \\(C_2\\) which are completely disconnected from each other (i.e. there are no edges connecting the vertices in \\(C_1\\) to the vertices in \\(C_2\\).) Lemma 22.3 (Fiedler Lemma 3) Let \\(\\mathbf{M}\\) be a symmetric matrix of rank \\(r\\). Let \\[\\mathbf{M} = \\mathbf{V} \\mathbf{D} \\mathbf{V}^T = (\\textbf{v}_1, \\textbf{v}_2, \\dots, \\textbf{v}_r) \\left( \\begin{matrix} \\sigma_1 &amp; 0 &amp; \\ldots &amp; 0\\\\ 0 &amp; \\sigma_2 &amp; \\ldots &amp; 0\\\\ \\vdots &amp; \\vdots &amp; \\ddots&amp; \\vdots\\\\ 0 &amp; 0 &amp;\\ldots &amp; \\sigma_r\\\\ \\end{matrix} \\right) \\left( \\begin{matrix} \\textbf{v}_1^T\\\\ \\textbf{v}_2^T\\\\ \\vdots \\\\ \\textbf{v}_r^T\\\\ \\end{matrix} \\right) =\\sum_{i=1}^r \\sigma_i \\textbf{v}_i \\textbf{v}_i^T\\] be the singular value decomposition of \\(\\mathbf{M}\\), with \\(\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\sigma_r\\). Let \\(\\widetilde{\\mathbf{M}}\\) be the closest (in the euclidean sense) rank \\(k\\) approximation to \\(\\mathbf{M}\\): \\[\\widetilde{\\mathbf{M}}=\\mbox{arg}\\min_{\\mathbf{B} , rank(\\mathbf{B})=k}\\|\\mathbf{M} - \\bf B\\|.\\] Then \\(\\widetilde{\\mathbf{M}}\\) is given by the truncated singular value decomposition: \\[B=\\sum_{i=1}^k \\sigma_i \\textbf{v}_i \\textbf{v}_i^T\\] From these simple tools, we can get a sense for why the signs of the Fiedler vector will determine a natural partition of a graph into two components. In the simplest case, \\(\\lambda_1=\\lambda_2=0\\), the graph contains two disjoint components, \\(C_1\\) and \\(C_2\\). Thus, there exists some permutation matrix \\(\\mathbf{P}\\) such that \\(\\mathbf{P} \\mathbf{L} \\mathbf{P}\\) is block diagonal: \\[ \\mathbf{P} \\mathbf{L} \\mathbf{P} =\\left( \\begin{array}{cc} \\mathbf{L}_1 &amp; 0\\\\ 0 &amp; \\mathbf{L}_2\\\\ \\end{array} \\right) \\] and \\(\\mathbf{L}_1\\) is the Laplacian matrix for the graph of \\(C_1\\) and \\(\\mathbf{L}_2\\) is the Laplacian for \\(C_2\\). Let \\(n_1\\) and \\(n_2\\) be the number of of vertices in each component. Clearly the eigenvectors \\(\\textbf{v}_2\\) and \\(\\textbf{v}_2\\) associated with \\(\\lambda_1=0\\) and \\(\\lambda_2=0\\) are contained in the span of \\(\\mathbf{u}_1\\) and \\(\\mathbf{u}_2\\) where: \\[\\begin{equation} \\mathbf{u}_1 = \\begin{array}{cc}\\begin{array}{c} 1\\\\ \\vdots \\\\ n_1 \\\\ n_1+1 \\\\ \\vdots \\\\ n \\end{array} &amp;\\left( \\begin{array}{c} 1 \\\\ \\vdots \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right) \\end{array} \\qquad\\mbox{and}\\qquad \\mathbf{u}_2 =\\begin{array}{cc}\\begin{array}{c} 1\\\\ \\vdots \\\\ n_1 \\\\ n_1+1 \\\\ \\vdots \\\\ n \\end{array} &amp;\\left( \\begin{array}{c}0 \\\\ \\vdots \\\\ 0 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array} \\right)\\end{array} \\end{equation}\\] For all Laplacian matrices, it is convention to consider \\(\\textbf{v}_1=\\e\\), the vector of all ones. Under this convention, the two conditions \\(\\textbf{v}_2 \\perp \\textbf{v}_1\\) and \\(\\textbf{v}_2 \\in span \\{ \\mathbf{u}_1,\\mathbf{u}_2\\}\\) necessarily force \\(\\textbf{v}_2\\) to have a form such that the component membership of each vertex is discernible from the sign of the corresponding entry in the vector: \\[\\textbf{v}_2 = \\alpha(\\mathbf{u}_1-\\frac{n_1}{n_2} \\mathbf{u}_2).\\] The case when \\(\\mathbf{L}\\) is not disconnected into two components is far more interesting, as this is generally the problem encountered in practice. We will start with a connected graph, so that our Laplacian matrix has rank \\(n-1\\). Let the spectral decomposition of our Laplacian matrix \\(\\mathbf{L}\\) (which is equivalent to its singular value decomposition because \\(\\mathbf{L}\\) is symmetric) be: \\[\\mathbf{L}=\\sum_{i=2}^n \\lambda_i \\textbf{v}_i \\textbf{v}_i^T.\\] Let \\(\\widetilde{\\mathbf{L}}\\) be the closest rank \\(n-2\\) approximation to \\(\\mathbf{L}\\). Then, by Lemmas 22.2 and 22.3, \\[\\widetilde{\\mathbf{L}} = \\sum_{i=3}^n \\lambda_i \\textbf{v}_i \\textbf{v}_i^T\\] and there exists a permutation matrix \\(\\mathbf{P}\\) such that \\[ \\mathbf{P} \\widetilde{\\mathbf{L}} \\mathbf{P} =\\left( \\begin{matrix} \\widetilde{\\mathbf{L}}_1 &amp; 0\\\\ 0 &amp; \\widetilde{\\mathbf{L}}_2 \\end{matrix} \\right). \\] Suppose we permute the rows of \\(\\mathbf{L}\\) accordingly so that \\[ \\mathbf{P} \\mathbf{L} \\mathbf{P} =\\left( \\begin{matrix} \\mathbf{L}_1 &amp; -\\mathbf{E}\\\\ -\\mathbf{E}^T &amp; \\mathbf{L}_2\\\\ \\end{matrix} \\right) \\] where \\(\\mathbf{E}_{ij} \\geq 0 \\forall i,j\\) because \\(\\mathbf{L}\\) is a Laplacian matrix. Consider the difference between \\(\\mathbf{L}\\) and \\(\\widetilde{\\mathbf{L}}\\): \\[\\mathbf{L}-\\widetilde{\\mathbf{L}} = \\lambda_2 \\textbf{v}_2 \\textbf{v}_2^T\\] which entails \\(\\mathbf{L} - \\lambda_2 \\textbf{v}_2 \\textbf{v}_2^T = \\widetilde{\\mathbf{L}}\\). If we permute the vector \\(\\textbf{v}_2\\) in the same manner as the matrices \\(\\mathbf{L}\\) and \\(\\widetilde{\\mathbf{L}}\\), then one thing is clear: \\[\\begin{equation*} \\lambda_2\\mathbf{P}\\textbf{v}\\textbf{v}_2^T\\mathbf{P} = \\left(\\begin{matrix} \\A &amp; -\\mathbf{E} \\\\ -\\mathbf{E}^T &amp; B \\end{matrix}\\right) \\end{equation*}\\] Thus, if \\[\\begin{equation*}\\mathbf{P}\\textbf{v} =\\left(\\begin{matrix} \\mathbf{a} \\\\ \\mathbf{b}\\\\ \\end{matrix} \\right) \\end{equation*}\\] where \\(\\mathbf{a} \\in \\Re^{n_1}\\) and \\(\\mathbf{b} \\in \\Re^{n_2}\\) 22.2.1.1 Extended Fiedler Clustering In the extended Fiedler algorithm, we use the sign patterns of entries in the first \\(l\\) eigenvectors of \\(\\mathbf{L}\\) to create up to \\(k=2^l\\) clusters. For instance, suppose we had 10 vertices, and used the \\(l=3\\) eigenvectors \\(\\textbf{v}_2,\\textbf{v}_3,\\mbox{ and }\\textbf{v}_4\\). Suppose the sign of the entries in these eigenvectors are recorded as follows: \\[ \\begin{array}{cc} &amp; \\begin{array}{ccc} \\mathbf{v}_2 &amp; \\mathbf{v}_3&amp;\\mathbf{v}_4 \\end{array}\\cr \\begin{array}{c} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\\\ 6 \\\\ 7 \\\\ 8 \\\\ 9 \\\\ 10 \\end{array} &amp; \\left( \\begin{array}{ccc} +&amp;+&amp;-\\cr -&amp;+&amp;+\\cr +&amp;+&amp;+\\cr -&amp;-&amp;-\\cr -&amp;-&amp;-\\cr +&amp;+&amp;-\\cr -&amp;-&amp;-\\cr -&amp;+&amp;+\\cr +&amp;-&amp;+\\cr +&amp;+&amp;+ \\end{array} \\right) \\end{array} \\] Then the 10 vertices are clustered as follows: \\[ \\{1,6\\},\\quad \\{2,8\\},\\quad \\{3,10\\},\\quad \\{4,5,7\\},\\quad \\{9\\}. \\] Extended Fiedler makes clustering the data into a specified number of clusters \\(k\\) difficult, but may be able determine a natural choice for \\(k\\) as it partitions the data along several eigenvectors. In a 1990 paper by Pothen, Simon and Liou, an alternative formulation of the Fiedler partition is proposed [25]. Rather than partition the vertices based upon the sign of their corresponding entries in \\(\\mathbf{v}_2\\), the vector \\(\\mathbf{v}_2\\) is instead divided at its median value. The main motivation for this approach was to split the vertices into sets of equal size. In 2003, Ding et al. derived an objective function for determining an ideal split point for similar partitions using the second eigenvector of the normalized Laplacian, defined in Section 22.2.2.2 [26]. The basic idea outlined above has been adapted and altered hundreds if not thousands of times in the past 20 years. The present discussion is meant merely as an introduction to the literature. 22.2.2 Graph Cuts The majority of spectral algorithms are derived as alterations of the objective function in Equation (22.3).The idea is the same: partition the graph into two components by means of a minimized edge cut, while requiring that the two components remain somewhat balanced in size (i.e. do not simply isolate a small number of vertices). Two common objective functions which embody this idea are the ratio cut (RatioCut) [23], the normalized cut (Ncut) [54]. 22.2.2.1 Ratio Cut The ratio cut objective function was first introduced by Hagen and Kahng in 1992 [23]. Given a graph \\(G(V,E)\\) with vertex set \\(V\\) partitioned into \\(k\\) disjoint clusters, \\(V_1,V_2,\\dots V_k\\), the ratio cut of the given partition is defined as \\[\\mbox{RatioCut}(V_1,V_2,\\dots,V_k) = \\sum_{i=1}^k \\frac{w(V_i,\\bar{V_i})}{|V_i|}\\] where \\(|V_i|\\) is the number of vertices in \\(V_i\\), \\(\\bar{V_i}\\) is the complement of the set \\(V_i\\) and, given two vertex sets \\(A\\) and \\(B\\), \\(w(A,B)\\) is the sum of the weights of the edges between vertices in \\(A\\) and vertices in \\(B\\). Let \\(\\mathbf{H}\\) be an \\(n\\times k\\) matrix indicating cluster membership of vertices by its entries: \\[\\begin{equation} \\tag{22.4} \\mathbf{H}_{ij}= \\begin{cases} \\frac{1}{\\sqrt{|V_j|}}, &amp; \\mbox{if the } i^{th} \\mbox{ vertex is in cluster } V_j \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\end{equation}\\] Then \\(\\mathbf{H}^T\\mathbf{H}=\\mathbf{I}\\) and minimizing the ratio cut over all possible partitionings is equivalent to minimizing \\[f(\\mathbf{H}) = \\mbox{Trace}(\\mathbf{H}^T\\mathbf{L}\\mathbf{H})\\] over all matrices \\(\\mathbf{H}\\) described by Equation (22.4), where \\(\\mathbf{L}\\) is the Laplacian matrix from Definition 22.1. The exact minimization of this objective function is again NP-hard, but relaxing the conditions on \\(\\mathbf{H}\\) to \\(\\mathbf{H}^T\\mathbf{H}=\\mathbf{I}\\) yields a solution \\(\\mathbf{H}^*\\) with columns containing the eigenvectors of \\(\\mathbf{L}\\) corresponding to the \\(k\\) smallest eigenvalues. Unfortunately, after this relaxation it is not necessarily possible to automatically determine from \\(\\mathbf{H}^*\\) which vertices belong to each cluster. Instead, it is necessary to look for clustering patterns in the rows of \\(\\mathbf{H}^*\\). This is a common conceptual drawback of the relaxation of objective functions in spectral clustering. The best way to procede after the relaxation is to cluster the rows of \\(\\mathbf{H}^*\\) with an algorithm like \\(k\\)-means to determine a final clustering. The ratio cut minimization method is generally referred to as unnormalized spectral clustering [69]. The algorithm is as follows: Input: \\(n \\times n\\) adjacency (or similarity) matrix \\(\\mathbf{A}\\) for a graph on vertices (or objects) \\(\\{1,\\dots,n\\}\\) and desired number of clusters \\(k\\) Compute the Laplacian \\(\\mathbf{L}=\\mathbf{D}-\\mathbf{A}\\). Compute the first \\(k\\) eigenvectors \\(\\mathbf{V}=\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_k\\) of \\(\\mathbf{L}\\) corresponding to the \\(k\\) smallest eigenvalues. Let \\(\\mathbf{y}_{i}\\) be the \\(i^{th}\\) row of \\(\\mathbf{V}\\) Cluster the points \\(\\mathbf{y}_i \\in \\Re^k\\) with the \\(k\\)-means algorithm into clusters \\(\\bar{C}_1,\\dots \\bar{C}_k\\). Output: Clusters \\(C_1,\\dots,C_k\\) such that \\(C_j = \\{i : \\mathbf{y}_i \\in \\bar{C}_j\\|\\) Table 22.1: Unnormalized Spectral Clustering (RatioCut) [69] 22.2.2.2 Normalized Cut (Ncut) The normalized cut objective function was introduced by Shi and Malik in 2000 [54]. Given a graph \\(G(V,E)\\) with vertex set \\(V\\) partitioned into \\(k\\) disjoint clusters, \\(V_1,V_2,\\dots V_k\\), the normalized cut of the given partition is defined as \\[\\mbox{Ncut}(V_1,V_2,\\dots,V_k)= \\sum_{i=1}^k \\frac{w(V_i,\\bar{V_i})}{\\mbox{vol}(V_i)},\\] where \\(\\mbox{vol}(V_i)\\) is the sum of the weights of the edges connecting the vertices in \\(V_i\\). Whereas the size of a subgraph \\(V_i\\) in the ratio cut formulation is measured by the number of vertices \\(|V_i|\\), in the normalized cut formulation it is measured by the total weight of the edges in the subgraph. Thus, minimizing the normalized cut is equivalent to minimizing \\[f(\\mathbf{H}) = \\mbox{Trace}(\\mathbf{H}^T\\mathbf{L}\\mathbf{H})\\] over all matrices \\(\\mathbf{H}\\) with the following form: \\[\\begin{equation} \\mathbf{H}_{ij}= \\begin{cases} \\frac{1}{\\sqrt{\\mbox{vol}(V_j)}}, &amp; \\mbox{if the } i^{th} \\mbox{ vertex is in cluster } V_j \\\\ 0 &amp; \\text{otherwise.} \\end{cases} \\tag{22.5} \\end{equation}\\] With \\(\\mathbf{H}^T \\mathbf{D} \\mathbf{H} = \\mathbf{I}\\) where \\(\\mathbf{D}\\) is the diagonal degree matrix from Definition 22.1. Thus, to relax the problem, we substitute \\(\\mathbf{G}=\\mathbf{D}^{1/2}\\mathbf{H}\\) and minimize \\[f(\\mathbf{G})=\\mathbf{G}^T \\mathscr{L} \\mathbf{G}\\] subject to \\(\\mathbf{G}^T\\mathbf{G}=\\mathbf{I}\\), where \\(\\mathscr{L}=\\mathbf{D}^{-1/2}\\mathbf{L} \\mathbf{D}^{-1/2}\\) is called the normalized Laplacian. Similarly, the solution to the relaxed problem is the matrix \\(\\mathbf{G}^*\\) with columns containing eigenvectors associated with the \\(k\\) smallest eigenvalues of \\(\\mathscr{L}\\). Again, the immediate interpretation of the entries in \\(\\mathbf{G}^*\\) is lost in the relaxation and so a clustering algorithm like \\(k\\)-means is used to determine the patterns. Input: \\(n \\times n\\) adjacency (or similarity) matrix \\(\\mathbf{A}\\) for a graph on vertices (or objects) \\(\\{1,\\dots,n\\}\\) and desired number of clusters \\(k\\) Compute the normalized Laplacian \\(\\mathscr{L}=\\mathbf{D}^{-1/2}\\mathbf{L} \\mathbf{D}^{-1/2}\\). Compute the first \\(k\\) eigenvectors \\(\\mathbf{V}=[\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_k]\\) of \\(\\mathscr{L}\\) corresponding to the \\(k\\) smallest eigenvalues. Let \\(\\mathbf{y}_{i}\\) be the \\(i^{th}\\) row of \\(\\mathbf{V}\\) Cluster the points \\(\\mathbf{y}_i \\in \\Re^k\\) with the \\(k\\)-means algorithm into clusters \\(\\bar{C}_1,\\dots \\bar{C}_k\\). Output: Clusters \\(C_1,\\dots,C_k\\) such that \\(C_j = \\{i : \\mathbf{y}_i \\in \\bar{C}_j\\|\\) Table 22.2: Normalized Spectral Clustering (Ncut) [69] 22.2.2.3 Other Normalized Cuts While the algorithm in Table 22.2 carries “normalized cut” in its title, other researchers have suggested alternative ways to consider normalized cuts in a graph. In a popular 2001 paper, Ng, Jordan, and Weiss made a slight alteration of the previous algorithm which simply normalized the rows of the eigenvector matrix computed in step 2 to have unit length before proceeding to step 3 [55]. This algorithm is presented in Table 22.3. Input: \\(n \\times n\\) adjacency (or similarity) matrix \\(\\mathbf{A}\\) for a graph on vertices (or objects) \\(\\{1,\\dots,n\\}\\) and desired number of clusters \\(k\\) Compute the normalized Laplacian \\(\\mathscr{L}=\\mathbf{D}^{-1/2}\\mathbf{L} \\mathbf{D}^{-1/2}\\). Compute the first \\(k\\) eigenvectors \\(\\mathbf{V}=[\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_k]\\) of \\(\\mathscr{L}\\) corresponding to the \\(k\\) smallest eigenvalues. Normalize the rows of \\(\\mathbf{V}\\) to have unit 2-norm. Let \\(\\mathbf{y}_{i}\\) be the \\(i^{th}\\) row of \\(\\mathbf{V}\\) Cluster the points \\(\\mathbf{y}_i \\in \\Re^k\\) with the \\(k\\)-means algorithm into clusters \\(\\bar{C}_1,\\dots \\bar{C}_k\\). Output: Clusters \\(C_1,\\dots,C_k\\) such that \\(C_j = \\{i : \\mathbf{y}_i \\in \\bar{C}_j\\|\\) Table 22.3: Normalized Spectral Clustering according to Ng, Jordan and Weiss (NJW) [69] In 2001, Meila and Shi altered the objective function once again, and derived yet another spectral algorithm using the normalized random walk Laplacian, \\(\\mathscr{L}_{rw} = \\mathbf{D}^{-1}\\mathbf{L} = \\mathbf{I} - \\mathbf{D}^{-1} \\A\\) [61]. As shown in [51], if \\(\\lambda\\) is an eigenvalue for \\(\\mathscr{L}\\) with corresponding eigenvector \\(\\mathbf{v}\\) then \\(\\lambda\\) is also an eigenvalue for \\(\\mathscr{L}_{rw}\\) with corresponding eigenvector \\(\\mathbf{D}^{1/2}\\mathbf{v}\\). This formulation amounts to a different scaling of the eigenvectors in step 3 of Table 22.3. This normalized random walk Laplacian will present itself again in Section 22.2.3. Meila and Shi’s spectral clustering method is outlined in Table 22.4. Input: \\(n \\times n\\) adjacency (or similarity) matrix \\(\\mathbf{A}\\) for a graph on vertices (or objects) \\(\\{1,\\dots,n\\}\\) and desired number of clusters \\(k\\) Compute the normalized random walk Laplacian $_{rw}=^{-1} $. Compute the first \\(k\\) eigenvectors \\(\\mathbf{V}=[\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_k]\\) of \\(\\mathscr{L}_{rw}\\) corresponding to the \\(k\\) smallest eigenvalues. Normalize the rows of \\(\\mathbf{V}\\) to have unit 2-norm. Let \\(\\mathbf{y}_{i}\\) be the \\(i^{th}\\) row of \\(\\mathbf{V}\\) Cluster the points \\(\\mathbf{y}_i \\in \\Re^k\\) with the \\(k\\)-means algorithm into clusters \\(\\bar{C}_1,\\dots \\bar{C}_k\\). Output: Clusters \\(C_1,\\dots,C_k\\) such that \\(C_j = \\{i : \\mathbf{y}_i \\in \\bar{C}_j\\|\\) Table 22.4: Normalized Spectral Clustering according to Meila and Shi All of the spectral algorithms outlined thus far seem very similar in their formulation, yet in practice they tend to produce quite different results. This presents a problem because while each method has merit in its own right, it is impossible to predict which one will work best on any particular graph. We will discuss this problem further in . 22.2.3 Power Iteration Clustering In a 2010 paper, Frank Lin and William Cohen propose a fast, scalable algorithm for clustering graphs using the power method (or power iteration) [68]. Let \\(\\mathbf{W}=\\mathbf{D}^{-1}\\A\\) be the \\(n\\times n\\) row-normalized (row stochastic) adjacency matrix for a graph, and let \\(\\mathbf{v}_0 \\neq 0\\) be a vector in \\(\\Re^n\\). A simple method for computing the eigenvector corresponding to the largest eigenvalue of \\(\\mathbf{W}\\) is the power method, which repeatedly computes the power iteration \\[\\mathbf{v}_{t+1}=c\\mathbf{W}\\mathbf{v}_t\\] where \\(c=1/\\|\\mathbf{W}\\mathbf{v}_t\\|_1\\) is a normalizing constant to prevent \\(\\mathbf{v}_t\\) from growing too large. Applying the power method to convergence on \\(\\mathbf{W}\\) would result in the uniform vector \\(\\alpha \\e\\) where \\(\\alpha = 1/n.\\) However, stepping through a small number of power iterations, will result in a vector that contains combined information from the eigenvectors associated with the largest eigenvalues. The formulation of Meila and Shi’s spectral algorithm in [61] warranted the use of the eigenvectors corresponding to the \\(k\\) smallest eigenvalues of the normalized random walk Laplacian \\(\\mathscr{L}_{rw} = \\mathbf{I}-\\mathbf{W}\\) which is equivalent to the consideration of the eigenvectors of the largest eigenvalues of \\(\\mathbf{W}\\). Thus, the idea behind Power Iteration Clustering (PIC) is to detect and stop the power method at some number of iterations \\(t\\) such that \\(\\mathbf{v}_t\\) is a useful linear combination of the first \\(k\\) eigenvectors. The analysis in [68] motivates the idea that the power method should pass through some initial stage of local convergence at the cluster level before going on to the stage of global convergence toward the uniform vector. At this stopping point, it is expected that \\(\\mathbf{v}_t\\) will be an approximately piecewise constant vector, nearly uniform on each of the clusters. Thus, the clusters at this stage will be revealed by the closeness of their corresponding entries in \\(\\mathbf{v}_t\\). See [68] for the complete analysis. The PIC procedure is given in Table 22.5. Applying the power method to \\(\\mathbf{P}^T\\) would equate to watching the probability distribution of a random walker evolve through time steps of the Markov Chain, where \\(\\mathbf{v}_t\\) is the distribution at time \\(t\\), and eventually would converge to the stationary distribution in Equation (22.10). However, according to Lin and Cohen, stepping through a limited number of power iterations on \\(\\mathbf{P}\\) is equivalent to observing the same chain backwards, so that \\(\\mathbf{v}_t(i)\\) gives the observer a sense of the most likely distribution of the chain \\(t\\) steps in the past, given that the walk ended with distribution \\(\\mathbf{v}_0\\). On a graph with cluster structure as described above, a random walker that ends up on a particular vertex \\(j \\in C_1\\) is more or less equally likely to have come from any other node in \\(C_1\\) (but relatively unlikely to have come from \\(C_i, i\\neq1\\)), making the distribution close to uniform on the vertices in \\(C_1\\). The same argument is true for any cluster \\(C_j\\) and thus, by stepping backwards through time we expect to find these distribution vectors which are nearly uniform on each cluster \\(C_j\\). For a complete discussion of the algorithm, including a more detailed mathematical analysis, consult [68]. Input: A row-stochastic matrix \\(\\mathbf{P}=\\mathbf{D}^{-1}\\A\\) where \\(\\A\\) is an adjacency or similarity matrix and the number of clusters \\(k\\). Pick an initial vector \\(\\mathbf{v}_0\\). [68] suggests the degree vector \\(\\mathbf{v}_0 = \\A\\e.\\) Set \\(\\mathbf{v}_{t+1} = \\frac{\\mathbf{P}\\mathbf{v}_t}{\\|\\mathbf{P}\\mathbf{v}_t\\|_1}\\) and \\(\\delta_{t+1} = |\\mathbf{v}_{t+1}-\\mathbf{v}_t|.\\) Increment \\(t\\) and repeat step 2 until \\(|\\delta_t-\\delta_{t+1}| \\simeq \\mathbf{0}.\\) Use \\(k\\)-means to cluster points on \\(\\mathbf{v}_t\\) and return clusters \\(C_1, C_2,\\dots,C_k.\\) Output: Clusters \\(C_1, C_2,\\dots,C_k\\). Table 22.5: Power Iteration Clustering (PIC) [68] 22.2.4 Clustering via Modularity Maximization Another technique proposed in the network community detection literature compares the structure of a given graph to what one may expect from a random graph on the same vertices [28], [29]. The motivation for this method was that simply counting edges between clusters as was done in previous spectral methods may not be the best way to define clusters in graph. A better approach may be to somehow measure whether they are fewer edges than expected between communities. Let \\(\\A\\) be the adjacency matrix of the graph (or network) and let \\(\\mathbf{P}\\) be the adjacency matrix of a random graph on the same vertices containing the expected value of weights on that graph. Then the matrix \\(\\mathbf{B}=\\A-\\mathbf{P}\\) would contain information about how the structure of \\(\\A\\) deviates from what is expected. Obviously this formulation relies on some underlying probability distribution of the weights in the random graph, known as the null model. The most common null model uses the degree sequence of the vertices in the given graph, \\(\\{d_1,d_2,\\dots,d_n\\}\\), where \\(d_i\\) is the degree of vertex \\(i\\) (i.e. \\(d_i\\) is the sum of the weights of the edges connected to vertex \\(i\\)), to create the probabilities [29] \\[\\begin{equation} \\tag{22.6} p(\\mbox{edge}(i,j)) = \\frac{d_j}{\\sum_{k=1}^n d_k}. \\end{equation}\\] Thus, the expected value of the weight of the edge from \\(i\\) to \\(j\\) is \\[\\mathbf{P}_{ij} = E(w(i,j)) = d_i \\left(\\frac{d_j}{\\sum_{k=1}^n d_k}\\right).\\] One may recognize that the probabilities in Equation (22.6) are precisely the stationary probabilities of the random walk on the graph defined by \\(\\A\\), and thus seem a reasonable choice for a null model. This formulation gives us \\(E(w(i,j)) = E(w(j,i))\\) as desired for an undirected graph. Using this null model, a modularity matrix \\(\\mathbf{B}\\) is formed as \\[\\mathbf{B} = \\A - \\mathbf{P}.\\] For a division of the data into two clusters, let \\(\\mathbf{s}\\) be an \\(n\\times 1\\) vector indicating cluster membership by \\[\\mathbf{s}_{i} = \\begin{cases} -1 &amp;: \\mbox{vertex } i \\mbox{ belongs in cluster 1}\\\\ \\,\\,\\,\\,1 &amp;: \\mbox{vertex } i \\mbox{ belongs in cluster 2} \\end{cases}. \\] Let \\(d=\\sum_{k=1}^n d_k\\). The modularity of a given partition is defined by \\[\\begin{equation} \\tag{22.7} Q= \\frac{1}{2d} \\sum_{i,j} \\mathbf{B}_{ij} \\mathbf{s}_i\\mathbf{s}_j = \\frac{1}{2d}\\mathbf{s}^T\\mathbf{B}\\mathbf{s}. \\end{equation}\\] The goal of the algorithm proposed in [29] is to maximize this quantity, thus we can drop the constant \\(1/2d\\) and write the objective as \\[\\begin{equation} \\tag{22.8} \\max_{\\substack{\\mathbf{s} \\\\ \\mathbf{s}_i = \\pm 1}} Q = \\mathbf{s}^T \\mathbf{B} \\mathbf{s} \\end{equation}\\] Illustrative Example To get an idea of why this is true, consider the case where we have two relatively obvious clusters \\(C_1\\) and \\(C_2\\) in a graph and reorder the rows and columns of the adjacency matrix to reflect this structure, \\[ \\A=\\left[ \\begin{array}{cc} \\A_{C_1} &amp; \\mathbf{E} \\\\ \\mathbf{E}^T &amp; \\A_{C_2} \\end{array}\\right] \\] Where \\(\\A_{C_1}\\) and \\(\\A_{C_2}\\) are relatively dense matrices with larger entries representing the weight of edges within the clusters \\(C_1\\) and \\(C_2\\) respectively and \\(\\mathbf{E}\\) is a sparse matrix with smaller entries representing the weight of edges which connect the two clusters. In a random graph with no community or cluster structure, we’d be likely to find just as many edges between the clusters as within clusters. Thus, after subtracting \\(\\mathbf{P}\\) our modularity matrix may look something like \\[ \\mathbf{B}=\\left[ \\begin{array}{cc} \\mathbf{B}_{11} &amp; \\mathbf{B}_{12} \\\\ \\mathbf{B}_{21} &amp; \\mathbf{B}_{22} \\end{array}\\right] \\approx\\left[ \\begin{array}{cc} + &amp; - \\\\ - &amp; + \\end{array}\\right] \\] Where the indicated signs reflect the sign tendancy of values in \\(\\mathbf{B}\\). In other words, the entries in the diagonal blocks \\(\\mathbf{B}_{11}\\) and \\(\\mathbf{B}_{22}\\) tend to be positive because the edges within clusters had larger weights than one would expect at random and the entries in the off diagonal blocks \\(\\mathbf{B}_{12}\\) and \\(\\mathbf{B}_{21}\\) tend to be negative because the edges between clusters had smaller weights than one would expect at random. Thus, the modularity of this graph, \\(\\mathbf{Q} = \\mathbf{s}^T \\mathbf{B} \\mathbf{s}\\), will be maximized by the appropriate partition \\(\\mathbf{s}^T=[\\mathbf{s}^T_1,\\mathbf{s}^T_2]=[\\e^T_{C_1}, -\\e^T_{C_2}]\\). In order to maximize the modularity objective function given in Equation (22.8), let \\(\\uu_1,\\uu_2,\\dots,\\uu_n\\) be an orthonormal set of eigenvectors for \\(\\mathbf{B}\\) corresponding respectively to the eigenvalues \\(\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_n\\). Write the vector \\(\\mathbf{s}\\) as a linear combination of eigenvectors, \\[\\mathbf{s} = \\sum_{i=1}^n \\alpha_i \\uu_i\\] where \\[\\alpha_i =\\uu_i^T \\mathbf{s}.\\] Then, the objective function from Equation (22.8) becomes \\[\\max_{\\substack{\\mathbf{s} \\\\ \\mathbf{s}_i = \\pm 1}} \\left(\\sum_{i=1}^n \\alpha_i \\uu_i^T \\mathbf{B}\\right)\\left(\\sum_{i=1}^n \\alpha_i \\uu_i\\right) = \\sum_{i=1}^n \\lambda_i (\\uu_i^T \\mathbf{S})^2.\\] This optimization is NP-hard due to the constraint that \\(\\mathbf{s}_i =\\pm 1\\). It is clear that without this constraint one would choose \\(\\mathbf{s}\\) proportional to \\(\\uu_1\\), maximizing the first term in the summation (associated with the largest eigenvalue) and terminating the others. A reasonable way to proceed in light of this information is to maximize the leading term and ignore the remaining terms. To accomplish this, it is quite clear that we should choose \\(\\mathbf{s}\\) so that its entries match the signs of the entries in \\(\\uu_1\\). The placement of vertices corresponding to zero entries in \\(\\uu_1\\) will be decided arbitrarily. However, if the leading eigenvalue of the modularity matrix is negative then the corresponding eigenvector is \\(\\e\\), leading us to no partition. According to [29] the ``no partition’’ solution in this scenario is in fact the correct result, i.e. a negative leading eigenvalue indicates there is no community or cluster structure in the graph. This gives a clear stopping point for the procedure, which allows it to automatically determine an appropriate number of clusters or communities to create. Unfortunately, the arbitrary placement of vertices corresponding to zero entries in \\(\\uu_1\\) may in some cases affect the determined number of clusters. To create more than 2 clusters, the above procedure can be repeated on each of the subgraphs induced by the vertices in each cluster found. This leads us to an iterative divisive (hierarchical) algorithm like the iterative Fiedler method in Section 22.2.1.1 and PDDP in Section ??. The modularity clustering procedure is formalized in Table . Input: \\(n \\times n\\) adjacency matrix \\(\\A\\) for an undirected graph to be partitioned Let \\(d_i\\) be the \\(i^{th}\\) row sum of \\(\\A\\). Let \\(d=\\sum_{i=1}^n d_i\\) Form the matrix \\(\\mathbf{P}\\) with \\(\\mathbf{P}_{ij}=d_i d_j / d\\). Form the modularity matrix \\(\\mathbf{B}=\\A-\\mathbf{P}\\). Compute the largest eigenvalue \\(\\lambda_1\\) and corresponding eigenvector \\(\\uu_1\\) of \\(\\mathbf{B}\\). If \\(\\lambda_1 &lt; 0\\), stop. There is no partition of this graph. Otherwise partition the vertices of the graph into 2 clusters as follows \\[\\begin{equation} \\tag{22.9} \\begin{split} C_1 &amp;= \\{i : \\uu_1(i) &lt;0\\} \\cr C_2 &amp;= \\{i : \\uu_1(i) \\geq 0\\} \\end{split} \\end{equation}\\] Determine further partitions by extracting the rows and columns of the original adjacency matrix corresponding to the vertices in each cluster to form \\(\\A&#39;\\) and repeat the algorithm with \\(\\A&#39;\\) until each created cluster fails to partition in step 5. Output: Final clusters. Table 22.6: Modularity Procedure for Network Community Detection (Newman) [29] 22.3 Stochastic Clustering An alternative way to interpret a graph is by considering a random walk along the edges. For an undirected graph with adjacency matrix \\(\\A\\), we can create a transition probability matrix \\(\\mathbf{P}\\) by dividing each row by the corresponding row sum. Using the degree matrix from Definition 22.1 we have \\[\\mathbf{P}=\\mathbf{D}^{-1}\\A.\\] If our graph does indeed have some cluster structure, i.e. sets of vertices \\(C_1,C_2, \\dots,C_k\\) for which the total weight of edges within each set are substantially higher than the total weight of edges between the different sets, then a random walker in a given cluster \\(C_i\\) is more likely to stay in \\(C_i\\) for several steps than he is to transition to another cluster \\(C_j\\). It is well known that for a connected and undirected graph, the long term probability distribution is given by \\[\\begin{equation} \\tag{22.10} \\pi^T = \\frac{\\e^T\\mathbf{D}}{\\e^T\\mathbf{D}\\e^T} \\end{equation}\\] Which is not likely to give any cluster information. However, the short-term evolution of this walk can tell us something about the cluster structure because a random walker is far more likely, in the short-run, to remain inside a cluster than he is to transition between clusters. The Stochastic Clustering Algorithm (SCA) of Wessell and Meyer [62] takes advantage of this fact. 22.3.1 Stochastic Clustering Algorithm (SCA) In a 2012 paper, Chuck Wessell and Carl Meyer formulated a clustering model by creating a symmetric (doubly stochastic) transition matrix \\(\\mathbf{P}\\) [62], [63] from the adjacency matrix of a graph. The method in this paper is quite similar to that in PIC except that here the mathematics of the ``backward’’ Markov Chain intuition given in [68] works out in this context because the probability transition matrix is symmetric. One added feature in this algorithm is the automatic determination of the number of clusters in the data, using eigenvalues of the transition matrix \\(\\mathbf{P}\\). Wessell and Meyer’s formulation is based on theory that was developed by Nobel Laureate economist Herbert Simon and his student Albert Ando. This theory surrounds the mixing rates of resources or wealth in local economies (composed of states in a Markov chain) as part of a global economy (which links together some states from each local economy). It is assumed that the adjacency matrix for the graph is irreducible, or equivalently that the graph is connected. The basic idea is that resources will be exchanged more frequently at a local level than they will at the global level. Suppose individual companies from a global economy are represented as nodes in a graph with edges between them signifying the amount of trade between each pair of companies. Natural clusters would form in this graph at a local level, represented by the strong and frequent trade relationships of proximal companies. Let \\(k\\) be the number of local economies (clusters), each containing \\(n_i\\) states \\(i=1,\\dots,k\\), and define the distribution of resources at time \\(t\\) as \\(\\mathbf{\\pi}_t\\), given a starting distribution \\(\\mathbf{\\pi}_0\\). Then \\[\\mathbf{\\pi}_t^T = \\mathbf{\\pi}_0^T\\mathbf{P}^t\\] The heavily localized trade in this global economy leads to a so-called short-term stabilization of the system characterized by a distribution vector at some time \\(t\\) which is nearly constant across each local economy: \\[\\mathbf{\\pi}_t^T \\approx \\left( \\frac{\\alpha_1}{n_1} \\frac{\\alpha_1}{n_1} \\dots \\frac{\\alpha_1}{n_1} | \\frac{\\alpha_2}{n_2} \\frac{\\alpha_2}{n_2} \\dots \\frac{\\alpha_2}{n_2} | \\dots | \\frac{\\alpha_k}{n_k} \\frac{\\alpha_k}{n_k} \\dots \\frac{\\alpha_k}{n_k} \\right)\\] After this short-term stabilization, the distribution of goods in the Markov Chain is eventually expected to converge to a constant level across every state. However, in the period following the short-run stabilization, the distribution vector retains its approximately piecewise constant structure for a some time before settling down into its final uniform equilibrium. Wessell and Meyer’s derivation requires the creation of a symmetric probability transition matrix \\(\\mathbf{P}\\) from the adjacency matrix \\(\\A\\) by means of a simultaneous row and column scaling. In other words, a diagonal matrix \\(\\mathbf{S}\\) is determined for which \\[\\mathbf{S}\\A\\mathbf{S}= \\mathbf{P}\\] is a doubly stochastic transition probability matrix. This task turns out to be quite simple, \\(\\mathbf{S}\\) is found by iterating a single step until convergence. Letting \\(\\mathbf{S}_{ii}=\\mathbf{s}(i)\\), the diagonal scaling procedure put forth by Ruiz [[16]} is simply: \\[\\begin{equation} \\tag{22.11} \\begin{split} \\mathbf{s}_0 &amp;= \\e \\\\ \\mathbf{s}_{t+1}(i) &amp;=\\sqrt{\\frac{\\mathbf{s}_t(i)}{\\A_{i*}^T\\mathbf{s}_t}} \\end{split} \\end{equation}\\] In [63], it is convincingly argued that the diagonal scaling procedure does not change the underlying cluster structure of the data in \\(\\A\\), and thus that the desired information is not damaged by this transformation. The clusters in this method are found in a similar manner to PIC, where \\(k\\)-means is employed to find the nearly piecewise constant segments of the distribution vector \\(\\mathbf{\\pi}_t\\) after a short number of steps. The Stochastic Clustering Algorithm automatically determines the number of clusters in the data by counting the number of eigenvalues whose value is close to \\(\\lambda_1=1\\). This group of eigenvalues near 1 is referred to as the Perron cluster. We postpone discussion of this matter to where it will be analyzed in detail. For now, we present the Stochastic Clustering method in Table 22.7. The eigenvector iterations in SCA are quite similar to those put forth in PIC, and users commonly create visualizations of the iterations that look quite similar to those in Figure ??. Input: Adjacency matrix \\(\\A\\) for some graph to be partitioned Convert \\(\\A\\) to a symmetric probability transition matrix \\(\\mathbf{P}\\) using the diagonal scaling procedure given in (22.11). Calculate the eigenvalues of \\(\\mathbf{P}\\) and determine \\(k\\) to be the number of eigenvalues in the Perron cluster. Create a random initial probability distribution \\(\\mathbf{\\pi}_0^T\\). Track the evolution of \\(\\mathbf{\\pi}_{t+1}^T = \\mathbf{\\pi}_t^T \\mathbf{P}\\). After each multiplication, cluster the entries of \\(\\mathbf{\\pi}_t\\) using \\(k\\)-means. When this clustering has remained the same for a user-preferred number of iterations stop. __ Output:__ \\(k\\) clusters found \\(C_1, C_2,\\dots, C_k\\). Table 22.7: Stochastic Clustering Algorithm (SCA) [62] References "],["validation.html", "Chapter 23 Cluster Validation 23.1 Internal Validity Metrics 23.2 External Validity Metrics", " Chapter 23 Cluster Validation The algorithms in Chapters ?? and 22.1, with the exception of DBSCAN, all suffer from the same drawback: they require the user to input the number of clusters for the algorithm to create. This is problematic because in practice it is unlikely that the user knows exactly how many clusters they are looking for. In fact, this may be precisely the information that the researcher is after. In the next chapter, we will discuss some popular approaches to determining a suitable value for \\(k\\). Many of these approaches seek to choose the “best” clustering from a set of clusterings containing different numbers of clusters. In order to present these approaches, we must first look at ways one might quantitatively describe the “goodness” of a clustering. Such measures fall into the realm of cluster validation In Chapter ?? it was made clear that the concept of a cluster (and hence the “optimal clustering” of a group of data) is a subjective one. Thus, it is impossible to truly quantify the quality or accuracy of a clustering, without first being given a set of categorical labels assumed to be the optimal clustering. Cluster validation metrics which use such labels (i.e. the “answers”) are called external metrics because they use additional information that was not contained in the data input to the clustering algorithm. Clearly such labels are non-existent in practice, or we would not need to do clustering at all. Thus, it is necessary to establish some internal metrics, which use only the information contained in the data, in order to get a sense for the quality or validity of a given clustering. In addition, relative metrics are established with the aim of comparing two different clusterings. The goal of this chapter is to provide but a brief introduction to internal, external and relative metrics to fit our needs. For a more comprehensive survey of cluster validation see for example [15], [22], [34], [41], [42], [53]. 23.1 Internal Validity Metrics Most internal metrics aim to describe the cohesion of each cluster and the separation between clusters. The cohesion of each cluster is some measure of its compactness, i.e how proximal or similar the objects in that cluster are to each other. The separation between clusters is some measure of the distance between them or how dissimilar the objects in different clusters are. Some metrics aim to quantify cohesion and separation separately, while others take both ideas into account in one measure. ### General Cluster Cohesion and Separation: Graphs vs. Data #### Cohesion Generally, cluster cohesion measures the similarity or proximity of the points within a cluster. The definitions of cohesion for graph partitioning and data partitioning problems differ slightly depending on the similarity measure used. In graph partitioning, the goal is to measure how similar, or close, vertices in a cluster are to one another, whereas in data clustering cohesion is generally measured by the similarity of the points in a cluster to some representative point (usually the mean or centroid) of that cluster [53]. This difference is illustrated in Figure 23.1. The red lines represent the similarity/distance quantities of interest in either scenario, and the red point in Figure ?? is a representative point which is not necessarily a data point. In our analysis, representative points will be defined as centroids and thus may be referred to as such. Figure 23.1: Cluster Cohesion in Data (left) compared to Graph-Based Cluster Cohesion (right) Depending on the proximity or similarity function used, the two quantities in Figure 23.1 may or may not be the same. Often times for graphs and networks, there is no simple way to define a centroid or representative point for a cluster. The particular representation for a cohesion metric will always be dependent on a choice of distance or similarity function. Thus, for graphs we merely define the general concept of cohesion as follows: Definition 23.1 (General Cluster Cohesion in Graphs) For a graph \\(G(V,E)\\) with edge weights \\(w_{ij}\\), and a partition of the vertices into \\(k\\) disjoint sets \\(C=\\{C_1,C_2,\\dots, C_k\\}\\), the cohesion of cluster \\(C_p\\) is \\[\\mbox{cohesion}(C_p) = \\sum_{i,j \\in C_p} w_{ij}.\\] Given this definition, it should be clear that if \\(w_{ij}\\) is a measure of similarity between vertices \\(i\\) and \\(j\\) then higher values of cohesion are desired, whereas if \\(w_{ij}\\) measures distance or dissimilarity then lower values of cohesion are desired. For data clustering problems, cluster cohesion is similarly defined, only now the similarities or proximities are measured between each point in the cluster and the cluster’s representative point. Definition 23.2 (General Cluster Cohesion for Data) Let \\(\\X=[\\x_1,\\x_2,\\dots, \\x_n]\\) be an \\(m\\times n\\) matrix of column data, and let \\(C=\\{C_1,C_2,\\dots,C_k\\}\\) be a set of disjoint clusters partitioning the data with corresponding representative points \\(\\{\\cc_1,\\cc_2,\\dots,\\cc_k\\}\\). Then the cohesion of cluster \\(C_p\\) is \\[\\mbox{cohesion}(C_p) = \\sum_{\\x_i \\in C_p} d(\\x_i,\\cc_p)\\] Where \\(d\\) is any distance or similarity function. Again, the given definitions are not associated with any particular distance or similarity function and thus define a broad classes of metrics for measuring cluster cohesion. 23.1.0.1 Separation The goal in clustering is not only to form groups of points which are similar or proximal, but also to assure some level of separation or dissimilarity between these groups. Thus, in addition to measuring cluster cohesion, it is also wise to consider cluster separation. Again this concept is a little different for graphs, where the separation is measured pairwise between points in different clusters, than it is for data, where separation is generally measured between the representative points of different clusters. This difference is presented with the following 2 definitions. Definition 23.3 (General Cluster Separation for Graphs) For a graph \\(G(V,E)\\) with edge weights \\(w_{ij}\\), and a partition of the vertices into \\(k\\) disjoint sets \\(C=\\{C_1,C_2,\\dots, C_k\\}\\). The separation between clusters \\(C_p\\) and \\(C_q\\) is \\[\\mbox{separation}(C_p,C_q) = \\sum_{\\substack{i \\in C_p \\\\ j \\in C_q}} w_{i,j}.\\] Definition 23.4 (General Cluster Separation for Data) Let \\(\\X=[\\x_1,\\x_2,\\dots, \\x_n]\\) be an \\(m\\times n\\) matrix of column data, and let \\(C=\\{C_1,C_2,\\dots,C_k\\}\\) be a set of disjoint clusters in the data with corresponding representative points \\(\\{\\cc_1,\\cc_2,\\dots,\\cc_k\\}\\). Then the separation between clusters \\(C_p\\) and \\(C_q\\) is \\[\\mbox{separation}(C_p,C_q) = d(\\cc_p,\\cc_q)\\] where \\(d\\) is any distance or similarity function. 23.1.0.2 Averaging Measures of Cohesion and Separation for a Set of Clusters Definitions 23.1, 23.2, 23.3 and 23.4 provide simple, well-defined metrics (given a proximity or similarity measure) for individual clusters \\(C_p\\) or pairs of clusters \\((C_p,C_q)\\) that can be combined into overall measures for a clustering \\(C = \\{C_1,C_2,\\dots,C_k\\}\\) by some weighted average [53]. The weights for such an average vary according to applications and user-preference, but they typically reflect the size of the clusters in some way. At the end of this chapter, in Table 23.2, we provide a few examples of these overall metrics. 23.1.1 Common Measures of Cohesion and Separation As stated earlier, the previous definitions were considered “general” in that they did not specify particular functions of similarity or distance. Here we discuss some specific measures which have become established as foundations of cluster validation in the literature. 23.1.1.1 Sum of Squared Error (SSE) The sum of squared error (SSE) metric incorporates the squared euclidean distances from each point in a given cluster to the centroid of the cluster, defined as \\[\\mean_j = \\frac{1}{n_j} \\sum_{\\x_i \\in C_j} \\x_i.\\] This is equivalent to measuring the average pairwise distance between points in a cluster, as one would do in a graph having Euclidean distance as a measure of proximity. The SSE of a single cluster is then \\[\\begin{eqnarray*} \\text{SSE}(C_j)&amp;=&amp;\\sum_{\\x_i \\in C_j} \\| \\x_i - \\mean_j \\|_2^2 \\\\ &amp;=&amp;\\frac{1}{2n_j}\\sum_{\\x_i,\\x_l \\in C_j} \\|\\x_i - \\x_l\\|_2^2 \\label{SSE} \\end{eqnarray*}\\] where \\(n_j = |C_j|\\) . The SSE of an entire clustering \\(C\\) is simply the sum of the SSE for each cluster \\(C_j \\in C\\) \\[\\mbox{SSE}(C)=\\sum_{j=1}^k \\sum_{\\x_i \\in C_j} \\|\\x_i - \\mean_j\\|_2^2.\\] Smaller values of SSE indicate more cohesive or compact clusters. One may recognize Equation (??) as the objective function from Section 21.2.2 because minimizing the SSE is the goal of the Euclidean algorithm. We can use the same idea to measure cluster separation by computing the Between Group Sums of Squares (SSB), which is a weighted average of the squared distances from the cluster centroids \\(\\{\\mean_1, \\mean_2,\\dots,\\mean_k\\}\\) to the over all centroid of the dataset \\(\\mean_* = \\frac{1}{n} \\sum_{i=1}^n \\x_i\\): \\[ \\mbox{SSB}(C) =\\sum_{j=1}^k n_j\\|\\mean_j-\\mean_*\\|_2^2. \\] It is straightforward to show that the total sum of squares (TSS) of the data \\[\\mbox{TSS}(\\X)=\\sum_{i=1}^n \\|\\x_i-\\mean_*\\|_2^2,\\] which is constant, is equal to the sum of the SSE and SSB for every clustering \\(C\\), i.e. \\[\\mbox{TSS}(\\X)=\\mbox{SSE}(C) + \\mbox{SSB}(C),\\] thus minimizing the SSE (attaining more cohesion) is equivalent to maximizing the SSB (attaining more separation). Sum of Squared Error is used as a tool in the calculation of the gap statistic, outlined in the next chapter, a popular parameter used to determine the number of clusters in data. 23.1.1.2 Ray and Turi’s Validity Measure In [14] a measure of cluster validity is chosen as the ratio of intracluster distance to intercluster distance. The authors define these distances as \\[M_{intra} = \\frac{1}{n}\\mbox{SSE}(C) = \\frac{1}{n}\\sum_{j=1}^k \\sum_{\\x_i \\in C_j} \\|\\x_i - \\mean_j\\|^2.\\] and \\[M_{inter} = \\min_{1\\leq i \\leq j \\leq k} \\|\\mean_i - \\mean_j\\|^2.\\] Clearly a good clustering should have small \\(M_{intra}\\) and large \\(M_{inter}\\). Ray and Turi’s validity measure, \\[V=\\frac{M_{intra}}{M_{inter}}\\] is expected to take on smaller values for a better clustering [15]. 23.1.1.3 Silhouette Coefficients Silhouette coefficients are popular indices that combine the concepts of cohesion and separation [64]. These indices are defined for each object or observation \\(\\x_i,\\, i=1,\\dots, n\\) in the data set using two parameters \\(a_i\\) and \\(b_i\\), measuring cohesion and separation respectively. These parameters and the silhouette coefficient for an object \\(\\x_i\\) are computed as follows: Suppose, for a given clustering \\(C=\\{C_1,\\dots, \\C_k\\}\\) with \\(|C_j|=n_j\\), that the point \\(\\x_i\\) belongs to cluster \\(C_p\\) Then \\(a_i\\) is the average distance (or similarity) of point \\(\\x_i\\) from the other points in \\(C_p\\), \\[ a_i = \\frac{1}{n_p} \\sum_{\\x_j \\in C_p} d(\\x_i,\\x_j)\\] Define the distance (or similarity) between \\(\\x_i\\) and the remaining clusters \\(C_q, 1\\leq\\,q\\leq\\,k, q\\neq p\\) to be the average distance (or similarity) between \\(\\x_i\\) and the points in each cluster, \\[d(\\x_i,C_q) = \\frac{1}{n_q} \\sum_{\\x_j \\in C_q} d(\\x_i,\\x_j).\\] Then \\(b_i\\) is defined to be the minimum of these distances (or maximum for similarity): \\[b_i = \\min_{q \\neq p} d(\\x_i,C_q).\\] The silhouette coefficient for \\(\\x_i\\) is then \\[s_i = \\frac{(b_i-a_i)}{\\max(a_i,b_i)} \\mbox{ (for distance metrics)}\\] \\[s_i = \\frac{(a_i-b_i)}{\\max(a_i,b_i)} \\mbox{ (for similarity metrics)}\\] The silhouette coefficient takes on values \\(-1 \\leq s_i \\leq 1\\), where negative values undesirably indicate that \\(\\x_i\\) is closer (or more similar) on the average to points in another cluster than to points in its own cluster, and values close to \\(1\\) indicate a good clustering.\\ Silhouette coefficients are commonly averaged for all points in a cluster to get an overall sense for the validity of that cluster. 23.2 External Validity Metrics Many of the results presented in Chapter ?? will use data sets for which the class labels of each object are known. Using this information, one can generally create validity metrics that are easier to understand and compare across clusterings. Such metrics are known as external metrics because of their dependence on the external class labels. We will show that most external metrics can be transformed into relative metrics which compute the similarity between two clusterings. Using the information from external class labels, one can create a so-called confusion matrix (also called a matching matrix). The confusion matrix is simply a table that shows correspondence between predicted cluster labels (determined by an algorithm) and the actual or “true” cluster labels of the data. A simple example is given in Figure 23.2, where the actual class labels (science',math’, and `french’) are shown across the columns of the matrix and the clusters determined by an algorithm (\\(C_1, C_2,\\) and \\(C_3\\)) are shown along the rows. The \\((i,j)^{th}\\) entry in the confusion matrix is then the number of objects from the dataset that had class label \\(j\\) and were assigned to cluster \\(i\\). Figure 23.2: Example of a Confusion Matrix For this simple example, one may assume that cluster 1 (\\(C_1\\)) corresponds to the class Science', cluster 2 corresponds to the classMath’, and likewise that cluster 3 represents the class `French’, even though the clustering algorithm did not split these classes apart perfectly. Most external metrics will rely on the values in the confusion matrix. 23.2.1 Accuracy Accuracy is a measure between 0 and 1 that simply measures the proportion of objects that were labelled correctly by an algorithm. This is not always a straightforward task, given that the labels assigned by a clustering algorithm are done so arbitrarily in that it does not matter if one refers to the same group of points as “cluster 1” or “cluster 2.” In the confusion matrix in Figure 23.2, it is easy to identify which cluster labels corresponds to which class. In this case it is easy to see that out of a total of 153 objects, only 13 were classified incorrectly, leading to an accuracy of 140/153 \\(\\approx\\) 91.5%. However with a more confusing confusion matrix, like that shown in Figure 23.3, the answer is not quite as clear and thus it is left to determine exactly how to match predicted cluster labels with assigned class labels in an appropriate way. Figure 23.3: A More Confusing Confusion Matrix This turns out to be a well studied matching problem from graph theory, known as a maximum matching for a bipartite graph. If we transform our confusion matrix from Figure 23.3 into an undirected bipartite graph with edge weights corresponding to edges in the confusion matrix, the result would be the graph in Figure ??. The task is then to find a set of 3 edges, each beginning at distinct vertices on the left and ending at distinct vertices on the right such that the sum of the edge weights is maximal. The solution to this problem is shown in Figure 23.4 and it is clear that the matching of predicted labels to actual labels did not actually change from the simpler version of this confusion matrix in Figure 23.2, it just became less obvious because of the errors made by the algorithm. Figure 23.4: Bipartite Graph of Confusion Matrix (left) and Matching Predicted Class Labels to Actual Class Labels (right) Once the predicted class labels are matched to the actual labels, the accuracy of a clustering is straightforward to compute by \\[\\mbox{Accuracy}(C)=\\frac{\\mbox{# of objects labelled correctly}}{n}.\\] The accuracy of the second clustering given in Figure 23.3 is 118/153 \\(\\approx\\) 77%, which is sharply lower than the 91.5% achieved by the clustering in Figure 23.2. The nice thing about accuracy as a metric is it provides a contextual interpretation and thus allows us to quantify an answer to the question “how much better is this clustering?” This is not necessarily true of other external metrics, as you will see in the next sections. The aspect of this metric that requires some computation is the determination of the maximum matching as shown in Figure @ref(fig:bipartitematching}. Fortunately, this problem is one that was solved by graph theorist H.W. Kuhn in 1955 [59]. Kuhn’s algorithm was adapted by James Munkres in 1957 and the resulting method was dubbed the Kuhn-Munkres Algorithm, or sometimes the Hungarian Algorithm in honor of the mathematicians who pioneered the work upon which Kuhn’s method was based [52]. This algorithm is fast and computationally inexpensive. The details of the process are not pertinent to the present discussion, but can be found in any handbook of graph theory algorithms. 23.2.1.1 Comparing Two Clusterings: Agreement The accuracy metric, along with other external metrics, can be used to compute the similarity between two different cluster solutions. Since, in practice, class labels are not available for the data, the user may run two different clustering algorithms (or even the same algorithm with different representations of the data as input or different initializations) and get two different clusterings as a result. The natural question is then “how similar are these two clusterings?” Treating one clustering as class labels and computing the accuracy of the second compared to the first will provide the percentage of data points for which the two clusterings agree on cluster assignment. Thus, when comparing two clusterings, the accuracy metric becomes a measure of agreement between the two clusterings. As such, value of 90% agreement indicates that 90% of the data points were clustered the same way in both clusterings. 23.2.2 Entropy The notion of entropy is associated with randomness. As a clustering metric, entropy measures the degree to which the predicted clusters consist of objects belonging to a single class, as opposed to many classes. Suppose a cluster (as predicted by an algorithm) contains objects belonging to multiple classes (as given by the class labels). Define the quantities $n_i = $ number of objects in cluster \\(C_i\\) $n_{ij} = $ number of objects in cluster \\(C_i\\) having class label \\(j\\) $p_{ij} = = $ probability that a member of cluster \\(C_i\\) belongs to class \\(j\\) Then the entropy of each cluster \\(C_i\\) is \\[\\mbox{entropy}(C_i) = -\\sum_{j=1}^L p_{ij} \\log_2 p_{ij}\\] where \\(L\\) is the number of classes, and the total entropy for a set of clusters, \\(C\\), is the sum of the entropies for each cluster weighted by the proportion of points in that cluster: \\[\\mbox{entropy}(C)=\\sum_{i=1}^k \\frac{n_i}{n} \\mbox{entropy}(C_i).\\] Smaller values of entropy indicate a less random distribution of class labels within clusters [64]. One benefit of using entropy rather than accuracy is that it can be calculated for any number of clusters \\(k\\), whereas accuracy is restricted to the case where \\(k=L\\). Sample Calculations for Entropy Comparing the two clusterings represented by the confusion matrices in Figures 23.2 and 23.3, we’d see that for the first example, \\[\\begin{eqnarray*} p_{11}=\\frac{45}{50} &amp; p_{12}=\\frac{5}{50} &amp; p_{13}=0 \\\\ p_{21}=\\frac{8}{48} &amp; p_{22}=\\frac{40}{48} &amp; p_{23}=0 \\\\ p_{31}=0 &amp; p_{32}=0 &amp; p_{33}=1 \\end{eqnarray*}\\] so that \\[\\begin{eqnarray*} \\mbox{entropy}(C_1) &amp;=&amp; - ( \\frac{45}{50} \\log_2 \\frac{45}{50} + \\frac{5}{50} \\log_2 \\frac{5}{50}) = 0.469\\\\ \\mbox{entropy}(C_2) &amp;=&amp; - (\\frac{8}{48} \\log_2 \\frac{8}{48} + \\frac{40}{48} \\log_2 \\frac{40}{48}) = 0.65\\\\ \\mbox{entropy}(C_3) &amp;=&amp; - (\\log_2 1) = 0 \\end{eqnarray*}\\] and thus the total entropy of the first clustering is \\[\\mbox{entropy}(C) = \\frac{50}{153} (0.469) + \\frac{48}{153}(0.65) = \\framebox{0.357}.\\] And for the second example, we have \\[\\begin{eqnarray*} p_{11}=\\frac{25}{30} &amp; p_{12}=\\frac{5}{30} &amp; p_{13}=0 \\\\ p_{21}=\\frac{30}{68} &amp; p_{22}=\\frac{38}{68} &amp; p_{23}=0 \\\\ p_{31}=0 &amp; p_{32}=0 &amp; p_{33}=1 \\end{eqnarray*}\\] yielding \\[\\begin{eqnarray*} \\mbox{entropy}(C_1) &amp;=&amp; - ( \\frac{25}{30} \\log_2 \\frac{25}{30} + \\frac{5}{30} \\log_2 \\frac{5}{30} ) = 0.65 \\\\ \\mbox{entropy}(C_2) &amp;=&amp; - (\\frac{30}{68} \\log_2 \\frac{30}{68} + \\frac{38}{68} \\log_2 \\frac{38}{68}) = 0.99 \\\\ \\mbox{entropy}(C_3) &amp;=&amp; - (\\log_2 1) = 0 \\end{eqnarray*}\\] and finally the total entropy of the second clustering is \\[\\mbox{entropy}(C) = \\frac{30}{153} (0.469) + \\frac{68}{153}(0.65) = \\framebox{0.568}\\] revealing a higher-overall entropy and thus a worse partition of the data compared to the first clustering. 23.2.3 Purity Purity is a simple measure of the extent to which a predicted cluster contains objects of a single class [64]. Using the quantities defined in the previous section, the purity of a cluster is defined as \\[\\mbox{purity}(C_i) = \\max_j p_{ij}\\] and the purity of a clustering \\(C\\) is the weighted average \\[\\mbox{purity}(C) = \\sum_{i=1}^k \\frac{n_i}{n} \\mbox{purity}(C_i).\\] The purity metric takes on positive values less than 1, where values of 1 reflect the desirable situation where each cluster only contains objects from a single class. Like entropy, purity can be computed for any number of clusters, \\(k\\). Purity and accuracy are often confused and used interchangeably but they are not the same. Purity takes no matching of class labels to cluster labels into account, and thus it is possible for the purity of two clusters to count the proportion of objects having the same class label. For example, suppose we had only two class labels given, A and B, for a set of 10 objects and set our clustering algorithm to seek 2 clusters in the data and the following confusion matrix resulted: \\[\\begin{array}{c | c c} &amp;A &amp; B \\\\ \\hline C_1 &amp; 3 &amp; 2\\\\ C_2 &amp; 3 &amp; 2 \\end{array}\\] Then the purity of each cluster would be \\(\\frac{3}{5}\\) referring in both cases to the proportion of objects having class label A. High values of purity are easy to achieve when the number of clusters is large. For example, by assigning each object to its own cluster we’d achieve perfect purity. One metric that accounts for such a tradeoff is Normalized Mutual Information, presented next. Sample Purity Calculations Again, we’ll compare the two clusterings represented by the confusion matrices in Figures 23.2 and 23.3. For the first clustering, \\[\\begin{eqnarray*} \\mbox{purity}(C_1) &amp;=&amp; \\max(\\frac{45}{50} ,\\frac{5}{50} ,0) = \\frac{45}{50}= 0.9 \\\\ \\mbox{purity}(C_2) &amp;=&amp; \\max(\\frac{8}{48},\\frac{40}{48},0) = \\frac{40}{48} = 0.83 \\\\ \\mbox{purity}(C_2) &amp;=&amp; \\max(0,0,1) = 1 \\end{eqnarray*}\\] so the overall purity is \\[\\mbox{purity}(C) = \\frac{50}{153} (0.9) + \\frac{48}{153}(0.83) + \\frac{55}{153}(1) = \\framebox{0.914}.\\] Similarly for the second clustering we have, \\[\\begin{eqnarray*} \\mbox{purity}(C_1) &amp;=&amp; \\max(\\frac{25}{30},\\frac{5}{30},0) = \\frac{25}{30}) = 0.83 \\\\ \\mbox{purity}(C_2) &amp;=&amp; \\max(\\frac{30}{68},\\frac{38}{68} ,0) = \\frac{38}{68}) = 0.56 \\\\ \\mbox{purity}(C_2) &amp;=&amp; \\max(0,0,1) = 1 \\end{eqnarray*}\\] And thus the overall purity is \\[\\mbox{purity}(C) = \\frac{30}{153} (0.83) + \\frac{68}{153}(0.56) + \\frac{55}{153}(1) = \\framebox{0.771}.\\] 23.2.4 Mutual Information (MI) and Normalized Mutual Information (NMI) Mutual Information (MI) is a measure that has been used in various data applications [64]. The objective of this metric is to measure the amount information about the class labels revealed by a clustering. Adopting the previous notation, $n_i = $ number of objects in cluster \\(C_i\\) $n_{ij} = $ number of objects in cluster \\(C_i\\) having class label \\(j\\) $p_{ij} = n_{ij}/n_i = $ probability that a member of cluster \\(C_i\\) belongs to class \\(j\\) also let \\(l_j =\\) the number of objects having class label \\(j\\) \\(L =\\) the number of classes \\(\\mathcal{L} =\\{\\mathcal{L}_1,\\dots,\\mathcal{L}_L\\}\\) the “proper” clustering according to class labels and, as always, let \\(n=\\) the number of objects in the data \\(k=\\) the number of clusters in the clustering. The Mutual Information of a clustering \\(C\\) is then \\[\\mbox{MI}(C)=\\sum_{i=1}^k \\sum_{j=1}^L p_{ij} \\log \\frac{n n_{ij}}{n_i l_j}\\] and the Normalized Mutual Information of \\(C\\) is \\[\\mbox{NMI}(C) = \\frac{\\mbox{MI}(C)}{[\\mbox{entropy}(C) + \\mbox{entropy}(\\mathcal{L})]/2}\\] Clearly, when \\(\\mathcal{L}\\) corresponds the class labels we have \\(\\mbox{entropy}(\\mathcal{L})=0\\) but if user’s objective is instead to compare two different clusterings, this piece of the equation is necessary. Thus, using the same treatment used for agreement between two clusterings, one can compute the mutual information between two clusterings. 23.2.5 Other External Measures of Validity There are a number of other measures that can either be used to validate a clustering in the presence of class labels or to compare the similarity between two clusterings \\(C=\\{C_1,C_2,\\dots,C_k\\}\\) and \\(\\hat{C}=\\{\\hat{C}_1,\\hat{C}_2,\\dots,\\hat{C_k}\\}\\). In our presentation we will consider the second clustering to correspond to the class labels, but in the same way that the accuracy metric can be used to compute agreement, these measures are often used to compare different clusterings. To begin we define the following parameters [15]: \\(a\\) is the number of pairs of data points which are in the same cluster in \\(C\\) and have the same class labels (i.e. are in the same cluster in \\(\\hat{C}\\)). \\(b\\) is the number of pairs of data points which are in the same cluster in \\(C\\) and have different class labels. \\(c\\) is the number of pairs of data points which are in different clusters in \\(C\\) and have the same class labels. \\(d\\) is the number of pairs of data points which are in different clusters in \\(C\\) and have different class labels. These four parameters add up to the total number of pairs of points in the data set, \\(N\\), \\[a+b+c+d = N = \\frac{n(n-1)}{2}.\\] From these values we can compute a number of different similarity coefficients, a few of which are provided in Table 23.1 [15]. Name Formula Jaccard Coefficient \\(\\displaystyle J = \\frac{a}{a+b+c}\\) Rand Statistic \\(\\displaystyle R = \\frac{a+b}{N}\\) Folkes and Mallows Index \\(\\displaystyle \\sqrt{\\frac{a}{a+b} \\frac{a}{a+c}}\\) Odds Ratio \\(\\displaystyle \\frac{ad}{bc}\\) Table 23.1: Some Common Similarity Coefficients 23.2.5.1 Hubert’s \\(\\Gamma\\) Statistic Another measure popular in the clustering literature is Hubert’s \\(\\Gamma\\) statistic, which aims to measure the correlation between two clusterings, or between one clustering and the class label solution [15], [64]. Here we define an \\(n\\times n\\) adjacency matrix for a clustering \\(C\\), denoted \\(\\bo{Y}\\) such that \\[\\begin{equation} \\label{clusteradjacency} \\bo{Y}_{ij} = \\begin{cases} 1 &amp; \\text{object } i \\text{ and object } j \\text{ are in the same cluster in } C \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\end{equation}\\] Similarly, let \\(\\bo{H}\\) be an adjacency matrix pertaining to the class label partition (or a different clustering) as follows: \\[\\begin{equation} \\bo{H}_{ij} = \\begin{cases} 1 &amp; \\mbox{object } i \\mbox{ and object } j \\mbox{ have the same class label } \\\\ 0 &amp; \\mbox{otherwise} \\end{cases} \\end{equation}\\] Then Hubert’s \\(\\Gamma\\) statistic, defined as \\[\\Gamma=\\frac{1}{N} \\sum_{i=1}^{n-1} \\sum_{i+1}^n \\bo{Y}_{ij} H_{ij},\\] is a way of measuring the correlation between the clustering and the class label partition [15]. 23.2.6 Summary Table Name Overall Measure Cluster Weight Type Overall Cohesion (Graphs) ${p=1}^k p {i,j C_p} w{ij} $ \\(\\displaystyle \\alpha_p= \\frac{1}{n_i}\\) Graph Cohesion \\(\\mathcal{G}_{CS}\\) (Graph C-S Measure) \\(\\displaystyle\\sum_{p=1}^k \\alpha_p \\sum_{\\substack{q=1 \\\\ q\\neq p}}^k \\sum_{\\substack{i \\in C_p \\\\ j \\in C_q}} w_{ij}\\) \\(\\displaystyle\\alpha_p = \\frac{1}{\\sum_{i,j \\in C_p} w_{ij}}\\) Graph Cohesion &amp; Separation Sum Squared Error (SSE) (Data) \\(\\displaystyle \\sum_{p=1}^k \\alpha_p \\sum_{\\x_i\\in C_p} \\|\\x_i - \\mean_p\\|^2\\) \\(\\displaystyle\\alpha_p = 1\\) Data Cohesion Ray and Turi’s \\(M_{intra}\\) \\(\\displaystyle \\sum_{p=1}^k \\alpha_p \\sum_{\\x_i\\in C_p} \\|\\x_i - \\mean_p\\|^2\\) \\(\\displaystyle\\alpha_p = \\frac{1}{n}\\) Data Cohesion Ray and Turi’s \\(M_{inter}\\) $_{1i j k} |_i - _j|^2 $ N/A Data Separation Ray and Turi’s Validity Measure \\(\\displaystyle\\frac{M_{intra}}{M_{inter}}\\) N/A Data Cohesion &amp; Separation Table 23.2: Some Common Measures of Overall Cohesion and Separation [64] References "],["findk.html", "Chapter 24 Determining the Number of Clusters \\(k\\) 24.1 Methods based on Cluster Validity (Stopping Rules) 24.2 Sum Squared Error (SSE) Cohesion Plots 24.3 Graph Methods Based on Eigenvalues (Perron Cluster Analysis)", " Chapter 24 Determining the Number of Clusters \\(k\\) As previously discussed, one of the major dilemmas faced when using the clustering algorithms from Chapters ?? and 22.1 is that these algorithms take the number of clusters, \\(k\\), as input. Therefore, it is necessary to somehow determine a reasonable estimate for the number of clusters present. Many methods have been proposed for this task, for a more in-depth summary we suggest the 1999 book by Gordon [4] or the 1985 paper by Milligan and Cooper [13]. The purpose of this chapter is to survey some established methodologies for this task, and to motivate our novel method discussed in Chapter ??. 24.1 Methods based on Cluster Validity (Stopping Rules) The most popular methods for determining the number of clusters involve observing some internal measure of cluster validity (like those outlined in the previous chapter) as the number, \\(k\\), of clusters increases. Cohesion scores like SSE are expected to be monotonically decreasing as \\(k\\) increases. At some value, \\(k^*\\), the marginal drop in SSE is expected to flatten drastically, indicating that further division of the clusters does not provide a significant improvement in terms of cohesion [41]. Methods based on cluster validity can be implemented with any clustering algorithm the user desires. Unfortunately, if the algorithm used is not working well with the dataset then the resulting determination of the number of clusters will be flawed. Furthermore, it is possible to get different results with different algorithms or different cohesion metrics, which may instil the user with little confidence in a given solution. In the hierarchical algorithms from Section 21.1, a series of solutions ranging from \\(k=1\\) to \\(k=n\\) clusters are output, and thus the methods for determining an appropriate value for \\(k\\) in these procedures are often referred to as stopping rules. Since hierarchical algorithms tend to be slow and computationally expensive for large datasets, the stopping rules which cannot be extended to include general partitions of the data will be omitted from the discussion. 24.2 Sum Squared Error (SSE) Cohesion Plots For a simple example for this stopping rule methodology, consider the so-called Ruspini dataset in Figure 24.1, which has been used to demonstrate clustering algorithms in the literature. This dataset consists of 75 two dimensional points in the first Cartesian quadrant, and visually it seems clear that these points fall into \\(k=4\\) different clusters, using Euclidean distance as a measure of proximity. (Some individuals may argue that these 4 clusters could be meaningfully broken down into smaller clusters. These arguments are certainly valid, but we base our decision to specify 4 on the following assumptions: a) If asked to choose 4 clusters, most human beings would choose the same 4 - this may not be the case with 5 or 6; b) If we consider these points as a sample from a population, then it is reasonable to suspect that the collection of more data may destroy the subcluster appearance - that is, there is more observed evidence of 4 clusters than any other number.) We ought to be able to uncover this “true” number of clusters by observing the level of decrease in the SSE metric as the number of clusters increase, and determining an “elbow” in the curve at \\(k^*=4\\) where the SSE flattens out for \\(k\\geq 4\\). Figure 24.1: The Two-Dimensional Ruspini Dataset Figure 24.2 shows some examples of clusters found in the data using \\(k\\)-means and \\(k=2, 3, 4, 5, 6\\) clusters. The initialization of seed points was done randomly in each case. Figure 24.2 shows the SSE (as described in Section ?? for the 6 different clusterings. We wish to point out that these 5 clusterings are “good” or reasonable clusterings upon visual inspection. Indeed, this first SSE plot properly depicts \\(k^*=4\\) as the “elbow” of the curve, where the marginal decrease in SSE for adding additional clusters flattens out. Figure 24.2: 5 “Good” \\(k\\)-means Clusterings of the Ruspini Dataset and the Corresponding SSE Plot User Beware As always, with \\(k\\)-means, it is of the utmost importance that the user pay close attention to the output from the algorithm. In our creation of the SSE plot in Figure 24.2, we came by the two solutions, associated with \\(k=4\\) and \\(k=5\\) clusters respectively, that are shown in Figure 24.3. Because we are able to visualize the data in 2 dimensions (which, practically speaking, means we could have identified \\(k^*=4\\) by visualizing the original data anyhow), we were able to throw away these two solutions upon inspection. If we did not do this, the resulting SSE plot shown in Figure 24.3 would have clearly misled us to choose \\(k^*=3\\) clusters. Without being able to visually inspect the solutions, it is wise to run several iterations of the \\(k\\)-means algorithm for each \\(k\\) and use some criteria (like lowest SSE, or most frequent SSE [68]) to choose an appropriate clustering for inclusion in the SSE plot. While this is not guaranteed to circumvent problematic SSE plots like that shown in Figure 24.3, it can help in many situations and certainly won’t hurt in others. This dependence on good clusterings is a glaring drawback of stopping rule methodology, because not all algorithms can produce multiple results for a single value of \\(k\\) to choose from. Figure 24.3: Example of ‘Poor’ Clusterings and their Effect on SSE Plot 24.2.1 Cosine-Cohesion Plots for Text Data Further complicating the method of cohesion plots is the curse of dimensionality discussed in Chapter ??. For high dimensional data, it is unusual to witness such drastic “elbows” in these plots. To illustrate this effect, we consider a combination of 3 text datasets used frequently in the information retrieval literature: ‘Medlars,’ ‘Cranfield,’ ‘CISI’ [67]. The Medlars-Cranfield-CISI (MCC) collection consists of nearly 4,000 scientific abstracts from 3 different disciplines. These 3 disciplines (Medlars = medicine, Cranfield = aerodynamics, CISI = information science) form 3 relatively distinct clusters in the data, which are not particularly difficult to uncover (For example, spherical \\(k\\)-means frequently achieves 98% accuracy on the full-dimensional data). For this experiment, we ran 25 trials of the spherical \\(k\\)-means algorithm for each value of \\(k=2,3,\\dots,20\\) and from each set of trials chose the solution with the lowest objective value. The resulting SSE plot is shown in Figure 24.4. It is difficult to identify a distinct “elbow” in this curve. Figure 24.4: Spherical \\(k\\)-means Objective Function Values for \\(2\\leq k \\leq 20\\) Because of the behavior of distance metrics in high dimensional space, it is often easier (and always faster) to find clusters after reducing the dimensions of a dataset by one of the methods discussed in Chapter ??. Because the singular value decomposition generally works well for text data, we conduct this same experiment on the Medlars-Cranfield-CISI dataset using projections onto the first \\(r=8,12, \\mbox{ and } 20\\) singular vectors. Using the correct number of clusters \\(k^*=3\\), the \\(k\\)-means algorithm is able to achieve the same accuracy of 98% on each of these dimension reductions, indicating that the clustering information is by no means lost in the lower dimensional representations. However, the SSE plots for these lower dimensional representations, shown in Figure 24.5, do no better at clearly indicating an appropriate number of clusters. In fact, these graphs seem to flatten out at \\(k=r\\). Again, 25 trials of the \\(k\\)-means algorithm were run for each value of \\(k\\) and the solution with the lowest SSE was chosen to represent that value of \\(k\\) in the plots. Figure 24.5: SSE Plots for Medlars-Cranfield-CISI Clusterings using SVD Reduction to \\(r=\\{8,12,20\\}\\) dimensions 24.2.2 Ray and Turi’s Method In [14], Ray and Turi suggested the use of their validity metric for determining the number of clusters. Unlike the SSE plots investigated previously, this method does not rely on the subjectivity of the user. Instead, the goal is simply to find the minimum value of their validity metric over the clusterings produced for various values of \\(k\\). Recalling the definition from Chapter 23 Section 23.1.1.2, we have the validity of a clustering defined as \\[\\begin{equation} v=\\frac{M_{intra}}{M_{inter}} (\\#rayturivalidity) \\end{equation}\\] where \\[\\begin{eqnarray} M_{intra} &amp;=&amp; \\frac{1}{n} \\sum_{i=1}^{k} \\sum_{\\x in C_i} \\| \\x - \\mean_i\\|_2^2 \\\\ M_{inter} &amp;=&amp; \\min_{1\\leq i &lt;j \\leq k} \\|\\mean_i - \\mean_j\\|^2 \\end{eqnarray}\\] and \\(\\mean_i\\) is the centroid of cluster \\(i\\). In their original work in [14], the authors’ goal was to cluster images. They noticed for these datasets that the minimum value for the validity metric frequently occurred for small numbers of clusters in the range of 2, 3, or 4 because of the large inter-cluster distances occurring when the number of clusters is small. This was undesirable in their application to image processing because the number of clusters was not expected to be small. To account for this fact, they proposed the following procedural adjustment for determining the number of clusters: Specify the maximum number of clusters to be considered, \\(k_{max}\\). For \\(k=2,\\dots,k_{max}\\) use \\(k\\)-means to cluster the data into \\(k\\) clusters. For each clustering \\(C(k)\\) compute the validity metric, \\(v(k)\\) from Equation (??). Locate the first local maximum in the validity measure, \\(\\tilde{k}\\) such that \\[v(\\tilde{k}-1) &lt; v(\\tilde{k}) &gt; v(\\tilde{k}+1)\\] Choose the optimal number of clusters, \\(k^*\\), to be the modified minimum such that \\(\\tilde{k} &lt; k^* \\leq k_{max}\\) is the number which minimizes the validity measure after the first local maximum. Ray and Turi Plots for the Ruspini Data We applied the above method to the 2-dimensional Ruspini data which was depicted in Figure 24.1. To avoid the type of poor clusterings that were displayed in Figure ??, for each value of \\(k\\), the \\(k\\)-means algorithm was run 25 times and the best solution (that is, the solution with the lowest objective function) was chosen to represent that value of \\(k\\). Figure 24.6 shows the plot of Ray and Turi’s validity metric computed on each solution. If one were to pick the global minimum from this set of clusterings, the optimal number of clusters would be \\(k^*=2\\). However, according to the modified minimum favored in the original paper [14], the optimal number of clusters for the Ruspini data is \\(k^*=5\\). Neither of these solutions impose quite as obvious a clustering as the true number, 4. Figure 24.6: Ray and Turi Validity Plot for Ruspini Data Ray and Turi Plots for Medlars-Cranfield-CISI We can generate similar plots using the same clusterings found by spherical \\(k\\)-means that were used to generate the SSE plots in Figures 24.4 and 24.5. Obviously, the plots of Ray and Turi’s validity metric are far more definitive in their determination of \\(k^*\\), although it is left to the user to determine whether to pick the global minimum or modified minimum [14]. Figure 24.7: Ray &amp; Turi’s Validity Plots for Medlars-Cranfield-CISI Clusterings on Raw Data and SVD Reductions to \\(r=\\{8,12,20\\}\\) Dimensions Respectively. The results from Figures 24.6 and 24.7 are summarized in the following table, which shows the number of clusters that would be chosen if one were to pick the global minimum validity or the modified minimum validity along with the actual number of clusters. Data Input Global Min Modified Min. Actual \\(k\\) Medlars-Cranfield-CISI 4 6 3 or 5 Ruspini 2 5 4 Table 24.1: Approximated Number of Clusters via Ray and Turi’s Method 24.2.3 The Gap Statistic The gap statistic is an index devised by Tibshirani, Walther, and Hastie in 2000 that has received massive amounts of attention in the literature. This method is quite similar to the stopping methods previously discussed, only now the objective is to compare the cluster cohesion values with what is expected under some null reference distribution [7]. Supposing the \\(n\\) data points \\(\\x_1,\\dots,\\x_n\\) are clustered in to \\(k\\) clusters, \\(C_1,C_2,\\dots,C_k\\) and \\(|C_j|=n_j\\) some measure of cohesion is defined as \\[W_k = \\sum_{j=1}^k \\frac{1}{2n_j} \\sum_{\\x_p,\\x_q \\in \\C_j} d(\\x_p,\\x_q)\\] where \\(d(x,y)\\) is a distance function. The idea is then to compare the graph of \\(\\log(W_k), \\,\\,k=1,\\dots,K\\) to its expectation under some null reference distribution and to choose the value of \\(k, 1\\leq k \\leq K\\) for which \\(\\log(W_k)\\) falls the farthest below its expectation. This distance between \\(\\log(W_k)\\) and its expectation under the reference distribution, denoted by \\(E^*\\), is called the gap: \\[\\mbox{Gap}(k) = E^*(\\log(W_k)) - \\log(W_k).\\] This expectation is estimated by drawing a Monte Carlo sample, \\(\\X^*_1,\\X^*_2,\\dots,\\X^*_B\\) from the reference distribution. Each dataset in the sample is clustered, and the values of \\(\\log(W^*_k), \\,\\,k=1,\\dots,K\\) are averaged over the samples. The sampling distribution of the gap statistic is controlled using the standard deviation, \\(sd_k\\), of the B Monte Carlo replicates of \\(\\log(W^*_k)\\). Accounting for the simulation error in \\(E^*(\\log(W_k))\\) yields the standard error \\[s_k = sd_k\\sqrt{1+\\frac{1}{B}} .\\] Using the common “one standard error” rule, the number of clusters \\(k^*\\) is chosen to be the smallest \\(k\\) such that \\[Gap(k)\\geq Gap(k+1)-s_{k+1}.\\] The authors in [7] suggest, both for simplicity and performance, using a uniform distribution as the null reference distribution. This process is summarized in Table 24.2. Cluster the observed data in \\(X\\) (which contains \\(n\\) objects and \\(m\\) features), varying the total number of clusters from \\(k=1,2,\\dots K\\), recording within dispersion measures (SSE function values) \\(W_k, \\,\\,k=1,2,\\dots,K\\). Generate \\(B\\) reference datasets, each with \\(n\\) objects having \\(m\\) reference features generated uniformly over the range of the observed values for the original features in \\(X\\). Cluster each of the \\(B\\) reference datasets, recording within dispersion measures (SSE function values) \\(W^*_{kb},\\,\\, b=1,2,\\dots,B,\\,\\,k=1,2,\\dots,K.\\) Compute the estimated Gap statistic: \\[Gap(k)=(1/B)\\sum_b \\log(W^*_{kb})-\\log(W_k)\\] Let \\(\\bar{\\mathit{l}} = (1/B)\\sum_b \\log(W^*_{kb})\\) and compute the standard deviation: \\[sd_k = [(1/B)\\sum_b \\left(\\log(W^*_{kb})-\\bar{\\mathit{l}} \\right)^2]^{1/2}.\\] Define \\(s_k = sd_k \\sqrt{1+\\frac{1}{B}}.\\) Finally choose the number of clusters to be the smallest value of \\(k\\) such that \\[Gap(k) \\geq Gap(k+1) - s_{k+1}\\]. Table 24.2: Computation of the Gap Statistic [7] In Figure 24.8 we provide the results from the gap statistic procedure on the ruspini data. Our Monte Carlo simulation involved \\(B=10\\) generated datasets. The gap statistic indicates the presence of \\(k^*=4\\) clusters. Figure 24.8: Results for gap statistic procedure on Ruspini data. Observed vs. Expected values of \\(\\log(W_k)\\) (left) and Width of Gap (right). The maximum gap occurs at \\(k^*=4\\) clusters. 24.3 Graph Methods Based on Eigenvalues (Perron Cluster Analysis) Another commonly used methodology for determining the number of clusters relies upon the examination of eigenvalues of a graph Laplacian. Keeping with our focus in Chapter 22.1 we consider only undirected graphs. The methodology contained herein is motivated by the following observation: suppose we had an undirected graph consisting of \\(k\\) connected components (i.e. \\(k\\) distinct components, none of which are connected to any other). The adjacency matrix of such a graph would be block diagonal with \\(k\\) diagonal blocks \\(\\A_1, \\dots, \\A_k\\), and each diagonal block would itself be an adjacency matrix for one connected component. \\[\\begin{equation} \\A = \\left[ \\begin{array}{ccccc} \\A_1 &amp; 0 &amp; 0&amp; \\dots &amp; 0 \\\\ 0 &amp; \\A_2 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; \\A_3 &amp; \\ddots &amp; 0 \\\\ 0 &amp; 0&amp; 0 &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\dots &amp; \\A_k \\end{array} \\right] \\tag{24.1} \\end{equation}\\] Thus, the Laplacian matrix \\(\\mathbf{L}=\\mathbf{D}-\\A\\) would also be block diagonal and each diagonal block would be the Laplacian matrix for one component of the graph. \\[\\begin{equation} \\mathbf{L} = \\left[ \\begin{array}{ccccc} \\mathbf{L}_1 &amp; 0 &amp; 0&amp; \\dots &amp; 0 \\\\ 0 &amp; \\mathbf{L}_2 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; \\mathbf{L}_3 &amp; \\ddots &amp; 0 \\\\ 0 &amp; 0&amp; 0 &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\dots &amp; \\mathbf{L}_k \\end{array} \\right] \\hspace{.5cm} \\mbox{ with } \\mathbf{L}_i \\e = \\mathbf{0} \\mbox{ for } i=1,\\dots, k \\tag{24.2} \\end{equation}\\] Thus, if each component is connected, the multiplicity of the smallest eigenvalue, \\(\\lambda_1 = 0\\), will count the number of diagonal blocks and thus the number of components. Of course the situation depicted in Equation (24.2) is ideal and unlikely to be encountered in practice. However when the graph is nearly decomposable into disconnected components, continuity of the eigenvalues suggests that one may be able to count the number of tightly connected components by counting the number of eigenvalues near \\(\\lambda_1 =0\\). In order to be able to characterize eigenvalues as being near \\(\\lambda_1 =0\\), it is necessary to transform (normalize) the Laplacian matrix so that its spectrum is contained in the interval \\([0,1]\\). This type of analysis is usually done using one of the two normalized Laplacian matrices discussed in Chapter 22.1 and defined again here. The random-walk Laplacian \\[\\mathbf{L}_{rw} = \\mathbf{D}^{-1}\\mathbf{L} = \\mathbf{I}-\\mathbf{D}^{-1}\\mathbf{A} = \\mathbf{I}-\\mathbf{P}\\] The symmetric Laplacian \\[\\mathbf{L}_{sym} = \\mathbf{D}^{-1/2}\\mathbf{L}\\mathbf{D}^{-1/2}=\\mathbf{I}-\\mathbf{D}^{-1/2}\\mathbf{A}\\mathbf{D}^{-1/2}.\\] The normalized Laplacians, like the Laplacian matrix itself, are both positive definite. Furthermore, \\(\\mathbf{L}_{rw}\\) and \\(\\mathbf{L}_{sym}\\) have the same spectrum. The following well-known and easily verified fact characterizes the relationship between the eigenvalues and eigenvectors of these two matrices [58]. Theorem 24.1 (Eigenvalues of \\(\\mathbf{L}_{sym}\\) and \\(\\mathbf{L}_{rw}\\)) \\(\\lambda\\) is an eigenvalue of \\(\\mathbf{L}_{rw}\\) with eigenvector \\(\\mathbf{v}\\) if and only if \\(\\lambda\\) is an eigenvalue of \\(\\mathbf{L}_{sym}\\) with eigenvector \\(\\mathbf{w}=\\mathbf{D}^{1/2}\\mathbf{v}\\). In light of this fact, we will limit our discussion to the properties of the transition probability matrix of a random walk on the graph associated with the adjacency matrix \\(\\A\\), denoted \\[\\mathbf{P}=\\mathbf{D}^{-1} \\A = \\mathbf{I}-\\mathbf{L}_{rw},\\] since \\[\\lambda \\in \\sigma(\\mathbf{P}) \\quad \\Rightarrow \\quad (1-\\lambda) \\in \\sigma(\\mathbf{L}_{rw}).\\] Random walks on undirected graphs are reversible Markov chains, which satisfy the so-called detailed balance equations [66], [70]: \\[\\mathbf{Q}\\mathbf{P}=\\mathbf{P}^T \\mathbf{Q} \\hspace{.2cm} \\mbox{ where } \\mathbf{Q}=diag(\\mathbf{\\pi}).\\] The stationary distribution for \\(\\mathbf{P}\\) given by \\(\\mathbf{\\pi}^T= \\frac{\\e^T\\mathbf{D}}{\\e^T\\mathbf{D}\\e}\\). We assume the underlying graph (which we aim to partition) is connected so that the matrix \\(\\mathbf{P}\\) is irreducible. If the graph is composed of connected components, like the one associated with Equation (24.1), the resulting random walk is equivalently referred to as completely reducible, uncoupled, or completely decomposable and there simple efficient algorithms available to identify the connected components [2]. In our connected graph, we assume that there exists some cluster or community structure, i.e. that there are \\(k\\) groups of vertices, \\(C_1,C_2, \\dots, C_k\\) with \\(|C_k|=n_k\\), for which edges exist more frequently and with higher weight within each group than between each group. With this assumption, we can reorder the rows and columns of the transition probability matrix \\(\\mathbf{P}\\) according to group membership so that the result is block-diagonally dominant. By this we essentially mean that \\(\\mathbf{P}\\) is a perturbation of a block-diagonal matrix \\(\\mathbf{B}\\), such that \\[\\begin{equation} \\mathbf{P}=\\mathbf{B}+\\mathbf{E} = \\left[ \\begin{array}{ccccc} \\mathbf{B}_{11} &amp; \\mathbf{E}_{12} &amp; \\mathbf{E}_{13}&amp; \\dots &amp; \\mathbf{E}_{1k} \\\\ \\mathbf{E}_{21} &amp; \\mathbf{B}_{22} &amp; \\mathbf{E}_{23} &amp; \\dots &amp; \\mathbf{E}_{2k} \\\\ \\mathbf{E}_{31} &amp; \\mathbf{E}_{32} &amp; \\mathbf{B}_{33} &amp; \\ddots &amp; \\mathbf{E}_{3k} \\\\ \\vdots&amp; \\vdots&amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{E}_{k1} &amp; \\mathbf{E}_{k2}&amp; \\mathbf{E}_{k3} &amp; \\dots &amp; \\mathbf{B}_{kk} \\end{array} \\right] \\tag{24.3} \\end{equation}\\] where the off-diagonal blocks, \\(\\mathbf{E}_{ij}\\), are much smaller in magnitude than the the diagonal blocks. In fact, the entries in the off-diagonal blocks are small enough that the diagonal blocks are nearly stochastic, i.e. \\(\\mathbf{B}_{ii} \\e \\approx 1\\) for \\(i=1,2,\\dots,k\\). A transition probability matrix taking this form describes a nearly uncoupled or nearly completely reducible Markov Chain. The degree to which a matrix is considered nearly uncoupled is dependent on one’s criteria for measuring the level of coupling (interconnection) between the aggregates (clusters of states) of the Markov chain [11], [63], [65]. In [11], the deviation from complete reducibility is defined as follows: Definition 24.1 (Deviation from Complete Reducibility) For an \\(m\\times n\\) irreducible stochastic matrix with a \\(k\\)-level partition \\[\\mathbf{P} = \\left[ \\begin{array}{ccccc} \\mathbf{P}_{11} &amp; \\mathbf{P}_{12} &amp; \\mathbf{P}_{13}&amp; \\dots &amp; \\mathbf{P}_{1k} \\\\ \\mathbf{P}_{21} &amp; \\mathbf{P}_{22} &amp; \\mathbf{P}_{23} &amp; \\dots &amp; \\mathbf{P}_{2k} \\\\ \\mathbf{P}_{31} &amp; \\mathbf{P}_{32} &amp; \\mathbf{P}_{33} &amp; \\ddots &amp; \\mathbf{P}_{3k} \\\\ \\vdots&amp; \\vdots&amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{P}_{k1} &amp; \\mathbf{P}_{k2}&amp; \\mathbf{P}_{k3} &amp; \\dots &amp; \\mathbf{P}_{kk} \\end{array} \\right]\\] the number \\[\\delta=2\\max_{i} \\|\\mathbf{P}_{i*}\\|_{\\infty}\\] is called the deviation from complete reducibility. It is important to point out that the parameter \\(\\delta\\), or any other parameter that measures the level of coupling between clusters in a graph (like those suggested in [1], [63], [65]) cannot be computed without knowing a priori the clusters in the graph. Such parameters are merely tools for the perturbation analysis, used to prove the next theorem regarding the spectrum of block-diagonally dominant stochastic matrices [1], [5], [10]–[12], [62], [65]. Theorem 24.2 (The Spectrum of a Block-Diagonally Dominant Stochastic Matrix) For sufficiently small \\(\\delta \\neq 0\\), the eigenvalues of \\(\\mathbf{P}(\\delta)\\) are continuous in \\(\\delta\\), and can be divided into 3 parts [10], [11], [62], [65]: The Perron root, \\(\\lambda_1(\\delta)=1\\), a cluster of \\(k-1\\) eigenvalues \\(\\lambda_2(\\delta),\\lambda_3(\\delta),\\dots,\\lambda_k(\\delta)\\) that approach 1 as \\(\\delta \\to 0\\), and the remaining eigenvalues, which are bounded away from 1 as \\(\\delta \\to 0\\). The cluster of \\(k\\) eigenvalues surrounding and including the Perron root \\(\\lambda_1=1\\) is known as the Perron cluster [10]. The analysis in [62] explains that if there is no further decomposition (or meaningful sub-clustering) of the diagonal blocks, a relatively large gap between the eigenvalues \\(\\lambda_k\\) and \\(\\lambda_{k+1}\\) is expected. Thus, we can determine the number of clusters in the state space of a nearly uncoupled Markov chain (i.e. the number of clusters in a graph) by counting the number of eigenvalues in this Perron Cluster. This method is extremely effective when the graph to be partitioned is sufficiently close to being uncoupled. Problems arise when either high levels of coupling (intercluster linkage) are in play or when some vertices within a cluster are weakly connected to that cluster (for example, dangling nodes - vertices with degree 1). The examples in Figure 24.9 illustrate this point. Firts we show a synthetic example of a graph exhibiting cluster structure and the eigenvalues of the associated transition probability matrix respectively. The thickness of the edges in the graph correspond to their respective weights. Because there is a limited amount of coupling (intercluster connection) in this first example, the Perron cluster of eigenvalues is easy to identify. Because there are 3 eigenvalues near 1, the user would conclude that the graph has 3 clusters. Occasionally a user can get a sense of the cluster structure in a graph with an appropriate layout of the nodes and edges. Force-directed graph drawing algorithms are common in this practice. The basic idea behind these algorithms is to model the edges as springs connecting the nodes and then to somehow minimize the total amount of tension in the system. Thus, densely connected groups of nodes are placed proximal to each other and the edges which loosely connect these groups are stretched. The graph drawings in Figure 24.9 are all examples of force-directed layouts. Graph drawing algorithms are beyond the scope of this paper, but for information the interested reader should see, for example, [8]. The second two rows of Figure 24.9 display a real-world example using the hyperlink graph between a sample of 1222 American political blogs. Based upon the force-directed drawing of the graph, it is clear that there are 2 large communities or clusters in this graph. These clusters correspond to the liberal and conservative division of American politics. The Perron cluster is not easily identified on the eigenvalue plot in Figure 24.9, and thus no conclusion should be drawn regarding the number of clusters in this data. However, after removing a large number of dangling nodes from the graph, or blogs which link only to a single neighboring page in the sampled population, a different picture comes to light. In the final row of Figure 24.9 we illustrate the effect of removing these dangling nodes (about 200 in total) on the eigenvalues of the transition probability matrix. Luckily, for this particular graph, removing the dangling nodes did not create more, a situation that is not guaranteed in general. The third eigenvalue in the Perron cluster likely identifies the small group of 3 blogs that is now visible in the force directed drawing of the graph. Such small clusters are generally undesirable in graph partitioning, and since the eigenvalues tell the user nothing about the size or composition of the graph communities counted by the eigenvalues in the Perron cluster, this method must be used with caution! Figure 24.9: Some Examples of Perron Cluster Identification on various Network Datasets In the next Chapter, we will introduce a similarity matrix that is well suited for this Perron-cluster analysis. Our method has the ability of estimating the number of clusters in very noisy and high-dimensional data when other methods fail. References "],["references.html", "Chapter 25 References", " Chapter 25 References "]]
