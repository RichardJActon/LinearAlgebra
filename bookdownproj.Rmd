--- 
author: "Shaina Race Bennett, PhD"
date: "`r Sys.Date()`"
link-citations: yes
github-repo: rstudio/linalg-master
title: "Linear Algebra for Data Science"
description: "A traditional textbook fused with a collection of data science case studies that was engineered to weave practicality and applied problem solving into a linear algebra curriculum"
always_allow_html: true
bibliography: Dissertation.bib
csl: ieee.csl
---

#  {-}
<!-- | -->
<!--        ![](data:image/gif;base64,R0lGODlhAQABAIAAAP///wAAACH5BAEAAAAALAAAAAABAAEAAAICRAEAOw==){height=200px width=2px} -->
<!--<img src="figs/matrixlogo.jpg" style="position:absolute; top:300px; height:800px;  align:center;" /> -->
```{r id='matrixlogo.jpg', fig.align='center', echo=F, out.width="100%"}
knitr::include_graphics("figs/titlematrix.jpeg")
``` 

## Preface {-} 

This course is meant to instill a working knowledge of linear algebra terminology and to lay the foundations of advanced data mining techniques like Principal Component Analysis, Factor Analysis, Collaborative Filtering, Correspondence Analysis, Network Analysis, Support Vector Machines and many more. In order to fully comprehend these important tools and techniques, we will need to understand the language in which they are presented: Linear Algebra. This is NOT a rigorous proof-based mathematics course. It is an intuitive introduction to the most important definitions and concepts that are needed to understand and effectively implement these important data mining methodologies. So that we know _how_ to stir the pile...

```{r id='matrixlogo.jpg', fig.align='center', echo=F, out.width="40%"}
knitr::include_graphics("figs/xkcd.png")
``` 
<center>Image source: [https://xkcd.com/1838/](https://xkcd.com/1838/) </center>

## Structure of the book {-}

This project is the fusion of a traditional textbook (with definitions, theorems, examples and exercises) with a collection of interactive programming exercises (designed for in-class demonstration) that was engineered to weave practicality and applied problem solving into the curriculum right from the start. 


This is a work in progress; please check back frequently for updates.

## About the author {-}

Shaina Race Bennett earned her PhD in Operations Research from NC State in 2014 where she focused on matrix theory and clustering high-dimensional data sets. She was a teaching assistant professor at the Institute for Advanced Analytics from 2014 until 2021 when she joined Fidelity Investments as a Principal Data Scientist. She enjoys bringing linear algebra to life with animations and applications, and would love to hear from you about your experience with this text.

```{r id='matrixlogo.jpg', fig.align='center', echo=F, out.width="40%"}
knitr::include_graphics("figs/profile.jpeg")
```

## Acknowledgements {-}

The author would like to acknowledge and celebrate the work of her PhD advisor Dr. Carl Meyer who wrote the most _thorough_ and complete proof-based presentation of the material in this book. If you're looking for more details, we strongly suggest the book that contains all of them:

Meyer, Carl D. _Matrix analysis and applied linear algebra_. Vol. 71. Siam, 2000.


```{r echo=F}

knitr::knit_engines$set(thm = function(options) {
code <- paste(options$code, collapse = "\n")
paste('
<div class="Smain">
<div class="Scontainer">
<div class="Stext-header">Theorem \\@ref(',options$cid,').', options$num,': ', options$title,'</div>
<div class="Stext">' 
,code
,'</div>
</div>
</div>',sep='')
})

knitr::knit_engines$set(def = function(options) {
code <- paste(options$code, collapse = "\n")
paste('
<div class="Smain">
<div class="Scontainer">
<div class="Stext-header">Definition \\@ref(',options$cid,').', options$num,': ', options$title,'</div>
<div class="Stext">' 
,code
,'</div>
</div>
</div>',sep='')
})

knitr::knit_engines$set(ex = function(options) {
code <- paste(options$code, collapse = "\n")
paste('
<div class="S2main">
<div class="S2container">
<div class="S2text-header">Example \\@ref(',options$cid,').', options$num,': ', options$title,'</div>
<div class="S2text">' 
,code
,'</div>
</div>
</div>',sep='')
})

knitr::knit_engines$set(exer = function(options) {
code <- paste(options$code, collapse = "\n")
return(cat('
<div class="S2main">
<div class="S2container">
<div class="S2text-header">Check your Understanding: ', options$title,'</div>
<div class="S2text">' 
,paste(code)
,'</div>
</div>
</div>'))
})


```



```{r, echo=F} 
thmcounter=0
excounter=0
library(shiny)
```




<!-- ```{r, echo=F}  -->
<!-- thmcounter=thmcounter+1 -->
<!-- ``` -->



<!-- ```{thm title='Boom. A Theorem.', num=thmcounter} -->
<!-- Boom I just made a custom language interpreter chunk. can I number it though -->
<!-- ``` -->



<!-- ```{r, echo=F}  -->
<!-- thmcounter=thmcounter+1 -->
<!-- ``` -->



<!-- ```{def title='Boom again. A Definition too.', num=thmcounter} -->
<!-- I made a definition too, and I numbered them.  -->

<!-- Check! -->
<!-- ``` -->

<!-- ```{r echo=F} -->
<!-- library(knitr) -->
<!-- ``` -->





<!-- ## ermehgerd -->

<!-- ```{r,echo=F}  -->
<!-- excounter=excounter+1 -->
<!-- ``` -->




<!-- ```{ex name = 'Cool Example, Girl', num=excounter} -->
<!-- Insert-mind-blowing-example-text. -->
<!-- Maybe an image for Good Measure: -->
<!--   <img src="_book/iaaicon.png" width="140"> -->
<!-- ``` -->

<!--chapter:end:index.Rmd-->

# Introduction {#intro}


```{r, echo=F}
thmcounter=0
excounter=0
cid='intro'
```
## What is Linear Algebra?

At its heart, Linear Algebra is the study of **linear functions**. The term **linear** should be understood to mean _straight_ or _flat_. A  line on the two dimensional coordinate plane, written $$y=mx+b$$ represents a linear equation - the set of points $(x,y)$ satisfying this equation form a straight line. On the other hand, a quadratic equation like $$y=ax^2+bx+c$$ is *nonlinear* - it is not straight, it has curvature. You may have learned that the equation of a plane in 3-dimensional space (the coordinate cube $(x,y,z)$) is written as
$$ax+by+cz=d.$$
Such a plane is a classic example - the set of points $(x,y,z)$ satisfying this equation form a flat surface (a plane). 

A linear function involves only multiplication by scalars and addition/subtraction, for example:
$$2x+3y+6z=9 \quad \mbox{or} \quad 4x_1-3x_2+9x_3+x_4-x_5+2x_6 = 2.$$
The second equation above brings us to an important point: we don't have to restrict ourselves to a  3-dimensional world. While the "flatness" of linear equations is evident when we can graph them in 2 and 3-dimensions, with 6 variables we can no longer conceptualize the "flatness" of our equation. We take on principal that the surface that contains all solutions to the equation $4x_1-3x_2+9x_3+x_4-x_5+2x_6 = 2$ is flat, without curvature, existing in a 6-dimensional space (or higher!). You may be asking now: _what is 6-dimensional space?_  We'll get to the the definition of $n$-space soon (Definition \@ref(def:nspace)), but it should satisfy your intuition to extend your basic notion of coordinate axes. If you have 3 variables of interest, say _height_, _weight_ and _circumference_, you can make a 3-d scatter plot because we have 3 physical dimensions to map to each characteristic. Add in a forth variable, say _cost_, and suddenly we cannot _physically_ consider the plot (because our perception is limited to 3-dimensions) but we ought to be able to suspend our disbelief and assume that a "forth axes" (forth dimension) can be considered to represent cost. 

In some cases, this course will challenge you to think geometrically about data. Not in terms of the geometry you learned in high school regarding polygons and circles, but in terms of the layout and patterns of data. Linear algebra allows us to develop concepts of distance and similarity when our data has more than 3 variables and we can no longer look at a scatter plot and use our eyeballs to declare "these two observations are far away from each other". 


The second term in the phrase is, of course, **algebra.** While you are likely familiar with the term,  this course will challenge your initial familiarity. Linear Algebra deals with the algebra of matrices, which is likely different from any algebra you've experienced before. For example, when given an equation for an unknown value, like 
$$2x=3,$$ you probably immediately think "I should divide both sides by 2 to determine the unknown value x". Our equations in this course will be quite different because the expressions will involve matrices and vectors. For example,
$$\left(\begin{matrix} 2 & 3\\1&4 \end{matrix}\right) \left(\begin{matrix} x_1\\x_2 \end{matrix}\right) = \left(\begin{matrix} 5\\6 \end{matrix}\right)$$
is one type of equation we will learn to solve in this course. In this situation, we cannot simply divide both sides by the left hand matrix to solve for the unknowns - in fact it should look quite confusing to consider what that would even mean! 

Learning to work with matrices will be like learning a new language - the only way to succeed will be to practice and struggle and practice and struggle. Keep pace with the course and learn the terminology and definitions foremost - without the language and notation firmly in place, the techniques will seem far more difficult than they actually are.

## Why Linear Algebra 

If you want to understand the foundations of data science, it is imperative that you be familiar with Linear Algebra. As you've probably already noticed, data tends to come in rows and columns. By its very nature, data forms a matrix. A **matrix** is just an array of numbers, logically ordered by rows and columns. For example take the following data:


\begin{center}
\begin{tabular}{l|c|c}
Name & Credit Score & Income\\
\hline
John & 780 & 95000\\
Sam & 600 & 60000\\
Elena & 550 & 65000\\
Jim & 400 & 35000\\
Eric & 450 & 40000\\
Helen & 750 & 80000
\end{tabular}
\end{center}

To do anything to this data, we need a way to store it mathematically. This is done by creating a matrix:

\bordermatrix{  Credit Score & Income}{John \\Sam \\Elena \\Jim \\Eric \\Helen}{ 
\left(\begin{matrix} 780 & 95000\\
600 & 60000\\
550 & 65000\\
 400 & 35000\\
 450 & 40000\\
750 & 80000 \end{matrix} \right)}


The rows of this matrix correspond to observations, in this case a sample of people for whom we have collected data. The columns of this matrix correspond to the variables we are measuring. Some manipulation and pre-processing is usually required to turn nominal/categorical/qualitative variables into numerical data, often using binary dummy variables. Most every tool in data science involves some form of linear algebra on a data matrix.  In this course, we will learn some of the foundations of these tools. If you master the material in this course, you will be able to understand many more advanced data techniques as you progress through your careers. With that in mind, let's start at the beginning.

## Describing Matrices and Vectors

As we alluded to earlier, **matrices** are simply arrays of numbers which correspond in some way to their rows and columns. The following are examples of matrices:
$$\A=\left(\begin{matrix} 1 & 2\\3&5 \end{matrix}\right) \qquad \bo{H}= \left(\begin{matrix} 6 & 5& 10\\0.1 & 0.5 & 0.9\\1&4&1\\1&1&1\\2&0.4&9.99 \end{matrix}\right)$$

In this text, when we denote a matrix, we will always use a bold capital letter like $\bo{A}, \bo{M}, \mbox{or } \bo{D}$. To start, lets cover some basic properties and notation.

### Dimension/Size of a Matrix

:::{.definition #dim name='Dimension/Size of a Matrix'}
The **dimension** of a matrix is the number of rows and number of columns in the matrix. This is sometimes referred to as the **size** of the matrix. We say a matrix in $m\times n$ if it has $m$ rows and $n$ columns. If $\A$ is a matrix, we might specify the dimensions of $\A$ with a subscript: $\A_{m\times n}$ should be read _$\A$ is an $m\times n$ matrix_.

An $n\times n$ matrix is called a **square matrix** because it has the same number of rows and columns. A matrix without this characteristic is called a **rectangular matrix.**
:::


:::{.example name='Dimensions of a Matrix' #dimensions}
Consider the data matrix presented previously, containing observations on the two variables *Credit Score* and *Income*:

$$
\A\,=\left(\begin{matrix}
780 & 95000\cr
600 & 60000\cr
550 & 65000\cr
400 & 35000\cr
450 & 40000\cr
750 & 80000\end{matrix}\right)
$$

The dimension of the matrix $\A$ is $6\times 2$ because $\A$ has 6 rows and 2 columns. Thus when referring to $\A$ we might write $\A_{6\times 2}$ when the size is important. Note that the number of rows _always} comes first when specifying the size of a matrix!
:::


:::{.exercise name='Determining dimension of a matrix' #dimensions}
For the following matrices, determine the dimension:

$$\bo{B} = \left( \begin{matrix}1 & 2 & 0 \cr 2&1&0\cr3&1&1 \end{matrix}\right) \quad \bo{C} = \left(\begin{matrix} .01&.5&1.6&1.7\\ .1&3.5&4&2\\.61&.55&.46&.17\cr1.2&1.5&1.6&1\cr.31&.35&1.3&2.3\\2.3&3.5&.06&.7\\.3&.2&2.1&1.8\end{matrix}\right) \quad \bo{T} = \left(\begin{matrix} 1\cr1.3\cr0.8\cr2\cr2.5\cr0.8\cr0.9 \end{matrix} \right)$$
\vspace{1cm}
:::

### $(i,j)$ Notation

Now beyond just knowing how many rows and columns a matrix has, we frequently want to refer to a specific row or column that matrix. In Example \@ref(exm:dimensions) above, you may have noticed that the names of the individuals did not appear in our matrix - those names were merely _identifiers_. Of course we did not throw them away and erase them from our hard drive, we merely replaced them with an implied numerical index $i=\{1,2,3,4,5,6\}$ that corresponds to a row of the matrix for every person. As long as we don't reorder the rows of our matrix, we will know that the correspondence is as follows:
$$\left\lbrace \begin{array}{c}
row 1\\ row 2\\ row 3\\ row 4\\ row 5\\ row 6
\end{array}  \right\rbrace \Longleftrightarrow
\left\lbrace \begin{array}{c}
John\\ Sam\\ Elena\\ Jim\\ Eric\\ Helen
\end{array}  \right\rbrace$$

So, if I'd like to focus on data about _Eric_ in particular, I'd have to look no further than the $5^{th}$ row of my matrix $\A$.  Similarly, if I want to know the _Credit Scores_ of all individuals, I'd simply focus on the $1^{st}$ column of the matrix. Hence, to find _Eric's Credit Score_, I'd aim my sight on the element of the matrix located in the $5^{th}$ row and $1^{st}$ column of the matrix $\A$.

Now, rather than write an entire sentence each time we want to refer to an individual element or row, we will develop some notation. As a general rule, the letter $i$ is used to index the rows of a matrix and the letter $j$ is used to index the columns.

:::{.definition name='Notation for Elements of a Matrix' #ijnotdef}
Two subscripts are used to identify individual elements of a matrix:
- The element of matrix $\A$ corresponding to _row_ $i$ and _column_ $j$ is written $A_{ij}$
- The **diagonal** elements of a square matrix are those that have identical row and column indices: $\A_{ii}$
:::


Beyond these basic conventions, there are other common notational tricks that we will become familiar with. The first of these is writing a **partitioned matrix**. We will often want to consider a matrix as a collection of either rows or columns rather than individual elements. As we will see in the future, when we partition matrices in this form, we can view their multiplication in simplified form. This often leads us to a new view of the data which can be helpful for interpretation. 

When we write $\A=( \A_1 | \A_2 | \dots | \A_n )$ we are viewing the matrix $\A$ as collection of column vectors, $\A_i$, in the following way:
$$\A=( \A_1 | \A_2 | \dots | \A_n )=\left(\begin{matrix} \uparrow & \uparrow &\uparrow&\dots & \uparrow \\
			\A_1&\A_2&\A_3&\dots&\A_p \\
			\downarrow &\downarrow &\downarrow &\dots&\downarrow   \end{matrix}\right) $$

Similarly, we can write $\A$ as a collection of row vectors:
$$\A=\left(\begin{matrix} \A_1 \\ \A_2 \\ \vdots \\  \A_m \end{matrix}\right) = 
\left(\begin{matrix} \longleftarrow & \A_1 & \longrightarrow \\
 \longleftarrow & \A_2 & \longrightarrow \\
  \vdots & \vdots & \vdots \\
   \longleftarrow & \A_m & \longrightarrow \end{matrix}\right)$$
			
Sometimes, we will want to refer to both rows and columns in the same context. The above notation is not sufficient for this as we have $\A_j$ referring to either a column or a row. In these situations, we may use
$\A_{\star j}$ to reference the $j^{th}$ column and $\A_{i \star}$ to reference the $i^{th}$ row:

$$\bordermatrix{\blue{\acol{1}}&\acol{2}&\dots&\dots &\acol{n}}{~\\~\\~\\~\\~}{\pm \blue{a_{11}} & a_{12} &\dots & \dots & a_{1n} \\
\blue{\vdots} & \vdots &  &  & \vdots \\
\blue{a_{i1}} & \dots& a_{ij} & \dots & a_{in}\\
\blue{\vdots} & \vdots &  &  & \vdots \\
\blue{a_{m1}} & \dots& \dots & \dots & a_{mn} \mp}$$
$$\bordermatrix{&&&&&}{\blue{\arow{1}} \\\vdots \\ \arow{i} \\ \vdots \\\arow{m}}{\pm  \blue{a_{11}} & \blue{a_{12} }&\blue{\dots} & \blue{\dots} & \blue{a_{1n}} \cr
 \vdots& \vdots &  &  & \vdots \cr
 a_{i1} & \dots& a_{ij} & \dots & a_{in}\cr
 \vdots & \vdots &  &  & \vdots \cr
  a_{m1} & \dots& \dots & \dots & a_{mn} \mp}$$

	
:::{.definition name='Rows and Columns of a Matrix' #rowcol}
To refer to entire rows or columns, a placeholder $\star$ is often used to represent the idea that we take _all_ rows or columns after narrowing in on a given column or row respectively:
- To refer to the $k^{th}$ row of $\A$ we will use the notation $\arow{k}$ ("$k^{th} row, _all_ columns") \
- Similarly, to refer to the $k^{th}$ column of $\A$ we will use the notation $\acol{k}$.\

Often, when there is no confusion about whether we are referring to rows or columns, the above notation will be shortened to simply $\A_k$. In such a scenario it will be made clear from context whether this represents the $k^{th}$ row or $k^{th}$ column. For example, 
$$\A=( \A_1 | \A_2 | \dots | \A_n )$$
represents a matrix $\A$ with $n$ columns $\{\A_1,\A_2,\dots,\A_n\}$.
:::

:::{.example name='Rows, Columns, and Elements of a Matrix' #ijnot}
Consider the following table of data and corresponding matrix:
\begin{center}
\begin{tabular}{c|c|c|c}
Obs. & Height (in.) & Weight (lbs.) & Age \\
\hline
1 & 63 & 120 & 23\\
2 & 69 & 170 & 30\\
3 & 72 & 190 & 41\\
4 & 64 & 150 & 27\\
5 & 64 & 175 & 35\\
6 & 68 & 165 & 25\\
7 & 70 & 180 & 21
\end{tabular} $\Longrightarrow \,\, \B=\left(\begin{matrix}63 & 120 & 23\\
69 & 170 & 30\\
72 & 190 & 41\\
64 & 150 & 27\\
64 & 175 & 35\\
68 & 165 & 25\\
70 & 180 & 21\end{matrix}\right)$
\end{center}

Element $B_{42}=150$ corresponds to the _weight_ (second column) of _observation 4_ (forth row).

The column $$\bcol{3} = \left(\begin{matrix} 23\\30\\41\\27\\35\\25\\21 \end{matrix}\right)$$ corresponds to the variable _age_ and the row $$\brow{6} = \left(\begin{matrix} 68&165&25 \end{matrix}\right)$$ corresponds to the measurements for _observation 6_.
:::

:::{.exercise name='Rows, Columns, and Elements of a Matrix' #ijnot}
For the matrix
$$ \bo{L}=\left(\begin{matrix} 3 & -6 & -2\\1 & -4 & 5\\0 & 4 & -5 \end{matrix}\right)$$
Write the following elements, rows, or columns:
\vspace{.5cm}
$$L_{23}= \qquad\qquad L_{31}= \qquad\qquad \bo{L}_{\star 2}= \qquad\qquad \bo{L}_{1 \star}=\qquad\qquad$$
\vspace{1cm}
:::

:::{.definition name='Equality of Matrices' #equality}
Two matrices are **equal** if and only if they have the same size and all of their corresponding entries are equal. That is,
$$\A=\B \Longleftrightarrow A_{ij} = B_{ij} \quad \mbox{for all i and j}$$
:::


### Example: Defining social networks {-}

One advantage to using (i,j)-notation is that it allows us to define an entire matrix by describing one arbitrary element according to its row ($i$) and column ($j$). Let's consider a classic example from network analysis. Suppose we have a group of 6 students and we want to examine how often they have worked together in teams. Rather than use the students names, let's just number them Student 1 through Student 6. These students were put into teams in the summer semester and then reassigned to new teams for the fall and the spring semester:


\begin{center}
\begin{tabular}{|c|c||c|c||c|c|}
\hline
\multicolumn{2}{|c||}{Summer Teams} &\multicolumn{2}{c||}{Fall Teams} & \multicolumn{2}{c|}{Spring Teams}\\
\hline
Team 1 & Team 2 & Team 1 & Team 2& Team 1 & Team 2\\
\hline
Student 1 & Student 4 & Student 2& Student 5 & Student 2& Student 3\\
Student 2 & Student 5 & Student 3& Student 1 & Student 4& Student 5\\
Student 3 & Student 6 & Student 4& Student 6 & Student 1& Student 6\\
\hline
\end{tabular}
\end{center}


Now, we can define what is called an _adjacency_ or _association_ matrix for this data as follows:

$$\A_{ij} =\left\{ \begin{array}{l}
\mbox{# of times Student i has worked with Student j}\,\,\,\mbox{if } i\neq j\\
0 \,\,\,\mbox{ if } i=j 
\end{array} \right.$$

Breaking apart this matrix definition, we see both the rows (indexed by $i$) and columns (indexed by $j$) will correspond to students. For example, the element in the $2^{nd}$ row and $3^{rd}$ column, ($A_{23}$), will be the number of times Student 2 has worked with Student 3. Thus, 
$$A_{23}=2.$$ 
When the row and column numbers are the same ($i=j$), which happens along the _diagonal_ of the matrix, the entries will be 0 (this number was chosen arbitrarily - one could also use `3' along the diagonal). The result is a square matrix with 6 rows and columns:
$$\A=\left(\begin{matrix} 0&2&1&1&1&1\\2&0&2&2&0&0\\1&2&0&1&1&1\\1&2&1&0&1&1\\1&0&1&1&0&3\\1&0&1&1&3&0\end{matrix}\right)$$
We can quickly see that there is symmetry in this matrix because the number of times Students $i$ and $j$ worked together is, of course, the same as the number of times Students $j$ and $i$ worked together. Formally, a matrix is called **symmetric** if $A_{ij}=A_{ji}$ (this important concept will be defined again on page \pageref{symmetric}). We can also see straight from this matrix that Students 5 and 6 worked together every semester while neither of them worked with Student 2 at all.

An adjacency matrix like the one listed above is often used to describe a _network_ or _graph._ A **graph** is a collection of **vertices** (nodes) that each represent some entity, and **edges** which connect vertices that are related in some way.

Below is a network of the students, where the thickness of the edge between two vertices indicates the number of times they have worked together.  


```{r id='studentgraph', fig.align='center', fig.cap = 'Network of Student Team Membership', echo=F, out.width="50%"}
knitr::include_graphics("figs/studentgraph.jpg")
```

If we had chosen to put 3's along the diagonal, each vertex in this graph would have a bold edge that loops back to itself. Graphs and the adjacency matrices which define them are at the heart of many problems in social network analysis. For example, websites like Facebook and LinkedIn are just enormous networks of nodes (individual users) connected by edges ("friendships" or "connections"). In the case of Twitter, the network is even more complicated because the relationships are _directed_: the "follower" connection is not symmetric, the people I "follow" do not have to follow me back. We may have a chance to discuss more of this later; in general, it makes the analysis much more difficult!

### Example: Correlation matrices {#introcorr}

When we have several variables to analyze, it is good practice to measure the pairwise correlations between variables. Suppose we have 4 variables, $x_1, x_2, x_3,\,\mbox{and}\, x_4$. We will often examine the _correlation matrix_, $\C$, which is defined as follows:
$$\C_{ij}=correlation(x_i,x_j).$$
For example, suppose our correlation matrix is as follows:
$$\C=\left(\begin{matrix} 1 & 0.3 & -0.9 & 0.1\\0.3 & 1 & 0.8 & -0.5\\-0.9&0.8&1&-0.6\\0.1&-0.5&-0.6&1 \end{matrix}\right)$$

It is clear that the diagonal elements of this matrix, $C_{ii}$, should always equal 1 because every variable is perfectly correlated with itself. We can also see that

$$C_{ij}=C_{ji}\quad \mbox{Because }\,correlation(x_i,x_j)=correlation(x_j,x_i)$$ 
And in this particular example,
$$C_{13}=-0.9 \quad \mbox{Indicates that }\,x_1\,\,\mbox{and}\,\,x_3\,\,\mbox{have a strong negative correlation.}$$






## Vectors

Now that we've introduced the concept of a matrix, let's talk about vectors. A **vector** are merely a special type of matrix, one that consists of only a single row or column. They are essentially ordered lists of numbers. For example:

$$\x=\left(\begin{matrix} 5\\6\\7\\8 \end{matrix}\right) \qquad \bo{z}=\left(\begin{matrix} 3 & 5 & 1 & 0 & 2 \end{matrix}\right) \qquad \y = \left(\begin{matrix} y_1\\y_2\\y_3 \end{matrix}\right)$$
are all examples of vectors; $\x$ and $\y$ are **column vectors** and $\bo{z}$ is a **row vector**. In this text, to denote a vector we will always use a _bold lower-case letter_ like $\x, \y, \mbox{or } \bo{a}$. Since vectors only have one row or column, we do not need two subscripts to refer to their elements - we can use a single subscript to refer to the elements, as shown in the vector $\y$. Thus, $x_3$ refers to the $3^{rd}$ element in the vector $\x$ above: $$x_3 = 7.$$ Notice that the elements of a vector are not written in bold. In fact, all **scalar** quantities (a quantity that is just a single number (like 2 or $\sqrt{5}$), not a matrix) will be represented by unbolded, usually lowercase (often times greek) letters. For example, letters like $\alpha, \beta, x_i, \mbox{or } a$ will be used to refer to scalars.

You may wonder now why the notation is so particular, but as we get into the subject it will be come clear that some convention must be retained if we are going to understand an equation. If we write an equation like
$$\A\x=\b \qquad \mbox{or} \qquad \bo{M}\y=\alpha\y$$
We have to know what it represents - in the first case, we are dealing with a matrix $\A$, and two vectors $\x$ and $\b$; in the second case we are dealing with a matrix $\bo{M}$, a vector $\y$, and a scalar, $\alpha$.

:::{.exercise name='Matrix, Vector, and Scalar Notation' #matnot}
For the following quantities, indicate whether the notation indicates a Matrix, Scalar, or Vector.
\vspace{.5cm}
$$\A \qquad \qquad A_{ij} \qquad \qquad \v \qquad \qquad  p_2 \qquad \qquad  \lambda  \qquad \qquad \p_2$$
\vspace{.5cm}
:::

In the previous exercise, we see an important difference: $p_2$ is not bold, and thus is automatically assumed to a scalar, while $\p_2$ is bold and lowercase meaning, despite the subscript, it refers to a vector. 

### Vector Geometry: $n$-space

You are already familiar with the concept of "ordered pairs" or coordinates $(x_1,x_2)$ on the two-dimensional plane (in Linear Algebra, we call this plane "$2$-space"). Fortunately, we do not live in a two-dimensional world! Our data will more often consist of measurements on a number (lets call that number $n$) of variables.  Thus, our data points belong to what is known as $n$-space. They are represented by **$n$-tuples** which are nothing more than ordered lists of numbers:
$$(x_1, x_2, x_3, \dots, x_n).$$
An $n$-tuple defines a **vector** with the same $n$ elements, and so these two concepts should be thought of interchangeably. The only difference is that the vector has a direction (usually depicted with an arrow), away from the origin and toward the $n$-tuple. This concept is obviously very difficult to visualize when $n>3$, but our mental visualizations of $2-$ and $3-$space will usually be sufficient when we want to consider higher dimensional spaces.

:::{.example name='2-space' #twospace}
Whereas we once thought of ordered pairs as _points_ in space, we can also consider the coordinates as defining _vectors_. In almost every scenario, these two concepts can be thought of equivalently, however the direction of vectors helps us define their arithmetic geometrically, as we will see in the next chapter.
<center>
<img src="figs/2spacePoints.pdf" width=300 /> 
<img src="figs/2space.pdf" width=300 /> 
</center>
:::

You will recall that the symbol $\Re$ is used to denote the set of real numbers. $\Re$ is simply $1$-space. It is a set of vectors with a single element. In this sense any real number, $x$, has a direction: if it is positive, it is to one side of the origin, if it is negative it is to the opposite side. That number, $x$, also has a magnitude: $|x|$ is the distance between $x$ and the origin, 0.

:::{.definition name='n-space' #nspace}
$n$-space (the set of real $n$-tuples) is denoted $\Re^n$.  In set notation, the formal mathematical definition is simply:
$$\Re^n = \left\lbrace (x_1,x_2,\dots,x_n) : x_i \in \Re, i=1,\dots, n\right\rbrace.$$
:::

We will often use this notation to define the size of an arbitrary vector. For example, $\x \in \Re^p$ simply means that $\x$ is a vector with $p$ entries:
$\x=(x_1,x_2,\dots,x_p)$.

Many (all, really) of the concepts we have previously considered in $2$- or $3$-space extend naturally to $n$-space and a few new concepts become useful as well. One very important concept is that of a **norm** or distance metric, as we will see in Chapter \@ref(norms). Before we get into the details of the vector space model, let's continue with some of the basic definitions we will need on that journey.

## Matrix Operations

### Transpose {#transpose}

One important transformation we will have to perform on a matrix is to switch the columns into rows. It is not necessary that you see the importance of this transformation right now, but trust that it is something we will need quite frequently.

:::{.definition name='Transpose of a Matrix or Vector' #transposedef}
For a given $m\times n$ matrix $\A$, the **transpose** of $\A$, written $\A^T$ (read as "$\A$-transpose") in this course and sometimes elsewhere denoted as $\A'$ is the $n\times m$ matrix whose rows are the corresponding columns of $\A$.

Thus, if $\A$ is a $3\times 4$ matrix then $\A^T$ is a $4\times 3$ matrix as follows:
$$\A = \left(\begin{matrix} A_{11} & A_{12} & A_{13} &A_{14}\\ A_{21} & A_{22} & A_{23} &A_{24}\\ A_{31} & A_{32} & A_{33} &A_{34}\end{matrix}\right)
\quad  \A^T = \left(\begin{matrix} A_{11} & A_{21} & A_{31}\\A_{12} & A_{22} & A_{32}\\A_{13} & A_{23} & A_{33}\\A_{14} & A_{24} & A_{34}\end{matrix}\right)$$

An equivalent way to state this definition is to say that 
$$(A^T)_{ij} = A_{ji}$$
_Note: If we transpose the transpose of a matrix, we will get back the original matrix. That is,_ $$(\A^T)^T = \A.$$
:::


:::{.example name='Transpose of a Matrix or Vector' #transpose}
For the following matrices and vectors, determine the transpose:
$$\B=\left(\begin{matrix} 2 & -3 & -4 \\5&-6&-7\\-8&9&0 \end{matrix}\right) \qquad \bo{M}=\left(\begin{matrix} -1&2\\-3&6\\7&-9\\5&-1 \end{matrix}\right) \qquad \x=\left(\begin{matrix}3\\-4\\5\\6\end{matrix}\right)$$
To find the transpose, we simply create new matrices whose rows are the corresponding columns of each matrix or vector:
$$\B^T=\left(\begin{matrix} 2 &5& -8\\-3&-6&9\\-4&-7&0\end{matrix}\right) \qquad \bo{M}^T = \left(\begin{matrix}-1&-3&7&5\\2&6&-9&-1 \end{matrix}\right) $$ $$\x^T = \left(\begin{matrix} 3&-4&5&6 \end{matrix}\right)$$
:::

:::{.definition name='Symmetric Matrix' #symdef}
Defining the matrix transpose allows us to define the notion of a _symmetric matrix_. A matrix $\A$ is called **symmetric** if and only if $$\A=\A^T.$$
This is equivalent to saying that $\A$ is symmetric if and only if
$$A_{ij}=A_{ji}.$$
It should be clear from the definition that in order for $\A$ to be a symmetric matrix, it _must_ be a square matrix - otherwise $\A$ and $\A^T$ would not even have the same size!
:::


:::{.example name='Symmetric Matrices' #sym}
The following matrix is symmetric because $\B^T = \B$.
$$\B=\left(\begin{matrix} 2 & 0 & 1 \\0&-6&-7\\1&-7&0 \end{matrix}\right)$$
:::

:::{.exercise name='Transpose and Symmetry' #transposesym}
Given that,
$$\A=\left(\begin{matrix} 2 & -4 \\-1&2\\3&-6 \end{matrix}\right) \quad \v^T = \left(\begin{matrix} 1 & 0 & -2 & 5\end{matrix}\right) \quad \B=\left(\begin{matrix} B_{11}&B_{12}&B_{13}\\B_{21}&B_{22}&B_{23}\\B_{31}&B_{32}&B_{33}\\B_{41}&B_{42}&B_{43}\end{matrix}\right)$$
 compute the following matrices:
\vspace{.5cm}
$$\A^T= \qquad \qquad \qquad \qquad (\A^T)^T= \qquad \qquad \qquad \qquad$$
\vspace{1cm}
$$ \v= \qquad \qquad \qquad \qquad \B^T = \qquad \qquad\qquad \qquad $$
\vspace{1cm}

Give an example of a $4\times 4$ symmetric matrix:\\
\vspace{1.5cm} 

Is a correlation matrix symmetric? (For a hint, see section \@ref(introcorr))\\
\vspace{.5cm}
:::

### Trace of a Matrix

Another important matrix operation that will come into play later is the _trace_ of a matrix. The **trace** of a square matrix is the sum of the diagonal elements of this matrix.

:::{.definition name='Trace of a Matrix' #tracedef}
For an $n\times n$ square matrix, $\A$, the **trace** of $\A$, written
$$trace(\A)\,\,\,\mbox{or}\,\,\,tr(\A)$$
is the sum of the diagonal elements:
$$tr(\A)=\sum_{i=1}^n A_{ii}$$

The trace of a rectangular matrix is undefined. 
:::


:::{.example name='Trace of a Matrix' #trace}
Let $$\A=\left(\begin{matrix} 3&4&1\\0&1&-2\\-1&\sqrt{2}&3\end{matrix}\right)$$
Then, the trace of $\A$ is the sum of the diagonal elements:
$$tr(\A)=\sum_{i=1}^3 A_{ii} = 3+1+3 = 7.$$
:::

While we're on the subject of diagonal elements, let's take the opportunity to introduce some special types of matrices, namely the **identity matrix** and a **diagonal matrix**. 

## Special Matrices and Vectors {#special}

:::{.definition name='Identity Matrix' #identity}
The bold capital letter $\bo{I}$ will always to denote the **identity matrix**. Sometimes this matrix has a single subscript to specify the size of the matrix. More often, the size of the identity is implied by the matrix equation in which it appears.
$$\bo{I}_4 = \left(\begin{matrix} 1 & 0 & 0 & 0 \\
						0 & 1 & 0 & 0\\
						0&0&1&0\\
						0&0&0&1 \end{matrix}\right)$$
:::


:::{.definition name='Elementary Vectors' #elemvectors}
The bold lowercase $\bo{e}_j$ is used to refer to the $j^{th}$ column of $\bo{I}$. It is simply a vector of zeros with a one in the $j^{th}$ position. We do not often specify the size of the vector $\bo{e}_j$, the number of elements is generally assumed from the context of the problem.

$$\bo{e}_j=\begin{matrix}
\begin{matrix} ~ \\~\\~\\~\\j^{th}\text{row} \rightarrow \\ ~\\~\\~ \end{matrix} & 
  \begin{pmatrix}0\\0\\ \vdots \\0\\1\\0\\ \vdots\\0\end{pmatrix}\\\\
\end{matrix}$$

The vector $\bo{e}$ with no subscript refers to a vector of all ones. In some texts, this vector is written as a bold faced $\textbf{1}$.

$$\bo{e} = \left(\begin{matrix} 1\\1\\1\\ \vdots \\ 1\end{matrix}\right)$$
:::		

The elementary vectors described in Definition \@ref(def:elemvectors) create the coordinate axes of $n$-space. For illustrative purposes, let's consider $2$-space. The elementary vectors in $2$-space are:
$$\e_1 = \left(\begin{matrix} 1\\0\end{matrix}\right) \quad \mbox{and} \quad \e_2 = \left(\begin{matrix} 0\\1 \end{matrix}\right) $$
These vectors correspond to the directions of the coordinate axes as illustrated in Figure \@ref(fig:elemvectors)

```{r label='elemvectors', fig.align='center', fig.cap = 'Elementary cectors represent our "usual" coordinate axes', echo=F, out.width="50%"}
knitr::include_graphics("figs/elemvectors.jpg")
```

						
:::{.definition name='Diagonal Matrix' #diag}
A **diagonal matrix** is a matrix for which off-diagonal elements, $\A_{ij},\,i\ne j$ are zero.
For example:
$$\bo{D} = \left(\begin{matrix} \sigma_1 & 0 & 0 & 0 \\
						0 & \sigma_2 & 0 & 0\\
						0&0&\sigma_3&0\\
						0&0&0&\sigma_4 \end{matrix}\right)$$
Since the off diagonal elements are 0, we need only define the diagonal elements for such a matrix. Thus, we will frequently write
$$\bo{D}=diag\{\sigma_1,\sigma_2,\sigma_3,\sigma_4\}$$
or simply $$D_{ii} = \sigma_i.$$
:::


From the preceding definitions, it should be clear that diagonal and identity matrices are always _square_, meaning they have the same number of rows and columns, and {symmetric}, meaning they do not change under the transpose operation.

:::{.exercise name='Trace and Special Matrices' #traceexer}
Write out the following matrices and then compute their Trace, if possible:

$$\I_5 \qquad \bo{D}=diag\{2,6,1\} \qquad \e_2 \in \Re^4$$
\vspace{1.5cm}
:::


:::{.definition name='Triangular Matrices' #triangulardef}
Triangular matrices are square matrices whose elements are zero either below or above the main diagonal. If the matrix has zeros below the main diagonal, then it's called **upper triangular**. The following illustration depicts an upper triangular matrix, where the asterisk symbol is used to denote any number (including potential zeros).
$$\left(\begin{matrix} *&*&*&\dots&*\\0&*&*&\dots&*\\0&0&*&\dots&*\\ \vdots &\vdots &\ddots &\ddots&*\\0&0&0&0&*\end{matrix}\right)$$

If a matrix has zeros above the main diagonal, then it's called **lower triangular**. The following illustration depicts a lower triangular matrix.
$$\left(\begin{matrix} 0&0&0&\dots&0\\ *&0&0&\dots&0\\ *&*&0&\dots&0\\ \vdots &\vdots &\ddots &\ddots&0\\ *&*&*&*&0\end{matrix}\right)$$
:::

:::{.exercise name='Triangular Matrices' #triangularexer}
The following are examples of triangular matrices. Are they upper or lower triangular?

$$\left(\begin{matrix} 1&2&3&4\\0&5&6&7\\0&0&8&9\\0&0&0&1 \end{matrix}\right) \quad \left(\begin{matrix} -1&0&0\\0&-2&0\\1&-1&2 \end{matrix}\right) \quad \left(\begin{matrix} 0&0&0\\0&0&0\\0&0&0 \end{matrix}\right)$$
:::

## Summary of Conventional Notation 

Linear Algebra has some conventional ways of representing certain types of numerical objects.  Throughout this course, we will stick to the following basic conventions:

- Bold and uppercase letters like $\A$, $\X$, and $\U$ will be used to refer to matrices. 
- Occasionally, the size of the matrix will be specified by subscripts, like $\A_{m\times n}$, which means that $\A$ is a matrix with $m$ rows and $n$ columns. 
- Bold and lowercase letters like $\x$ and $\y$ will be used to reference vectors. Unless otherwise specified, these vectors will be thought of as columns, with $\x^T$ and $\y^T$ referring to the row equivalent.
- The individual elements of a vector or matrix will often be referred to with subscripts, so that $A_{ij}$ (or sometimes $a_{ij}$) denotes the element in the $i^{th}$ row and $j^{th}$ column of the matrix $\A$.  Similarly, $x_k$ denotes the $k^{th}$ element of the vector $\x$. These references to individual elements are not generally bolded because they refer to scalar quantities. 
- Scalar quantities are written as unbolded greek letters like $\alpha$, $\delta$, and $\lambda$. 
- The notation $\x \in \Re^n$ simply means that $\x$ is a vector in $n$-space, or a vector with $n$ elements.
- The **transpose** of an $m\times n$ matrix $\A$ is the $n\times m$ matrix $\A^T$ whose rows are the columns of $\A$.
- The **trace** of a square matrix $\A_{n\times n}$, denoted $Tr(\A)$ or $Trace(\A)$, is the sum of the diagonal elements of $\A$,
$$Tr(\A)=\sum_{i=1}^n A_{ii}.$$



## Exercises


<ol> 
<li>Use the following matrices or vectors to answer the following questions:
$$\mathbf{A}=\left(\begin{matrix} 1&3&8\\3&0&-2\\4&1&-3 \end{matrix}\right) \quad \mathbf{M}=\left(\begin{matrix} 1&8&-2&5\\2&8&1&7 \end{matrix}\right) \quad 
\mathbf{D} = \left(\begin{matrix} 1&0&0\\0&5&0\\0&0&3\end{matrix}\right) \\

\mathbf{X}=\left(\begin{matrix} 780 & 95000\\600 & 60000\\550 & 65000\\400 & 35000\\450 & 40000\\750 &80000\end{matrix}\right) \quad
\mathbf{t} = \left(\begin{matrix} 1\\1.3\\0.8\\2\\2.5\\0.8\\0.9 \end{matrix}\right) \quad 
\mathbf{v}=\left(\begin{matrix} 6\\3\\-1\\2\end{matrix}\right) \quad \mathbf{u}=\left(\begin{matrix} 6&4&8&1\end{matrix}\right)$$

<ol style="list-style-type:lower-alpha">
  <li> Write the appropriate size/dimensions next to each matrix:
<ol style="list-style-type:lower-roman">
  <li> $\mathbf{A}$  
  <li> $\mathbf{M}$  
  <li> $\mathbf{D}$  
  <li> $\mathbf{X}  $  
  <li> $\mathbf{t}  $  
  <li> $\mathbf{v}  $  
  <li> $\mathbf{u}  $  
</ol>

  <li> Which of these matrices are square? Which are rectangular?

  <li> Give the following quantities:
 <ol style="list-style-type:lower-roman">   
  <li> $A_{12}=$   
  <li> $M_{21}=$   
  <li> $\D_{\star3}=$  
  <li> $\bo{M}_{2\star}=$   
  <li> $X_{42}=$  
  <li> $t_5=$   
  <li> $v_3=$  
</ol>

  <li> What are the diagonal elements of $\A$?
</ol>



<li> For each of the following matrices and vectors, give their dimension. Label each as a matrix or vector. For each matrix, indicate whether the matrix is square or rectangular.
<ol style="list-style-type:lower-alpha">
  <li>  $$\A=\left(\begin{matrix} 2 & 3 & -1\\1&-1&1\\2&2&1 \end{matrix}\right)$$  
  <li>  $$\bo{h}=\left(\begin{matrix} -1\\-4\\1\\2 \end{matrix}\right)$$  
  <li>  $$\B=\left(\begin{matrix}   B_{11}&B_{12}&B_{13}\\B_{21}&B_{22}&B_{23}\\B_{31}&B_{32}&B_{33}\\B_{41}&B_{42}&B_{43}\end{matrix}\right)$$  
  <li>  $$\C=\left(\begin{matrix} 1 & 0.3 & -0.9 & 0.1\\0.3 & 1 & 0.8 & -0.5\\-0.9&0.8&1&-0.6\\0.1&-0.5&-0.6&1 \end{matrix}\right)$$  
  <li>  $$\A = [A_{ij}] \quad \mbox{where } i=1,2,3 \quad \mbox{and   } j=1,2$$  
</ol>
<li> For the following quantities, use what you know about notation to tell if they are matrices, vectors, or scalars:
<ol style="list-style-type:lower-alpha">
  <li> $\bo{H}$  
  <li> $\bo{W}$  
  <li> $n$  
  <li> $\v_2  $  
  <li> $v_2  $  
  <li> $\bo{M}_{\star 2} $  
  <li> $\lambda $  
  <li> $A_{ij} $  
  <li> $\bo{r} $  
</ol>

<li> The matrix $\C$ from exercise 2d is the correlation matrix discussed earlier in this chapter. What is the trace of $\C$? For any correlation matrix computed using $p$ variables, what should we expect the trace to be?

<li> If $$\v^T = \left(\begin{matrix} 1&6&-1&\sqrt{2} \end{matrix}\right),$$ then what is $\v$?

<li> For each of the following, write the vector or matrix that is specified:
<ol style="list-style-type:lower-alpha">
  <li> $\e_3 \in \Re^4$
  <li> $\D=diag\{2, \sqrt{3}, -1\}$
  <li> $\e \in \Re^3$
  <li> $\I_2$
</ol>

<li> How do you know if a matrix is symmetric? Give an example of a symmetric matrix.

<li> Give an example of a $4\times 4$ upper triangular matrix and a $3\times 3$ lower triangular matrix.

<li> Suppose we measure the heights of 10 people, $person_1, person_2, \dots, person_{10}$. 
<ol style="list-style-type:lower-alpha">
  <li> If we define a matrix $\S$ as
$$S_{ij} = height(person_i) - height(person_j)$$
is the matrix $\S$ symmetric? What is the trace($\S$)?  
  <li> If instead we create a matrix $\bo{G}$ where
$$G_{ij} = [height(person_i) - height(person_j)]^2$$
is the matrix $\bo{G}$ symmetric? What is the trace($\bo{G}$)?  
</ol>


<li> Refer to the network/graph shown below. This particular network has 6 numbered vertices (the circles) and edges which connect the vertices. Each edge has a certain {weight} (perhaps reflecting some level of association between the vertices) which is given as a number.

<li> Use the network shown below to answer the following questions.  
```{r fig.align='center', fig.cap = 'Graph (Network) for exercise 11', echo=F, fig.width=6}
knitr::include_graphics('figs/graphex.jpg')
```
<ol style="list-style-type:lower-alpha">  
  <li>  Write down the adjacency matrix, $\A$, for this graph where $\A_{ij}$ reflects the weight of the edge connecting vertex $i$ and vertex $j$.   
  <li>  The **degree** of a vertex is defined as the sum of the weights of the edges connected to that vertex. Create a vector $\bo{d}$ such that $d_i$ is the degree of node $i$.  
</ol>


<!-- ## List of Key Terms  -->

<!-- - linear -->
<!-- - matrix -->
<!-- - vector -->
<!-- - scalar -->
<!-- - $A_{ij}$ -->
<!-- - $\A_{\star j}$ -->
<!-- - $\A_{i \star}$ -->
<!-- - dimensions -->
<!-- - diagonal element -->
<!-- - square matrix -->
<!-- - rectangular matrix -->
<!-- - network -->
<!-- - graph -->
<!-- - adjacency matrix -->
<!-- - correlation matrix -->
<!-- - transpose -->
<!-- - symmetric matrix -->
<!-- - trace -->
<!-- - diagonal matrix -->
<!-- - identity matrix -->
<!-- - upper triangular matrix -->
<!-- - lower triangular matrix -->




<!--chapter:end:00-introLA.Rmd-->

# Matrix Arithmetic {#mult}

```{r, echo=F} 
thmcounter=0
excounter=0
cid='mult'
```

## Matrix Addition, Subtraction, and Scalar Multiplication


Addition, subtraction, and scalar multiplication are the only operations which act *element-wise* on matrices - they are performed in a way you might expect given your previous studies. 

:::{.definition name='Addition, Subtraction, and Scalar Multiplication' #addsubdef}
Two matrices can be added or subtracted only when they have the same dimensions. If $\A$ and $\B$ are both $m\times n$ matrices then the (i,j) element of the sum (or difference), written $\A _-^+ \B)_{ij}$ is:

$$(\A+\B)_{ij}=A_{ij}+B_{ij}$$
similarly,
$$(\A-\B)_{ij}=A_{ij}-B_{ij}$$
Multiplying a scalar by a matrix or vector also works element-wise:
$$(\alpha\A)_{ij}=\alpha A_{ij}$$
:::


:::{.example name='Addition, Subtraction, and Scalar Multiplication' #addsub}

a. Compute $\A+\B$, if possible: $$\A=\pm 2 & 3 & -1\\1&-1&1\\2&2&1 \mp \quad \B=\pm 4 & 5 & 6\\-1&0&4\\3&4&3 \mp$$
*We can add the matrices because they have the same size.*
$$\A+\B = \pm 6 & 8 & 5\\0&-1&5\\5&6&4\mp$$
b. Compute $\A-\bo{H}$, if possible:
$$\A=\pm 1 & 2\\3&5 \mp \qquad \bo{H}= \pm 6 & 5& 10\\0.1 & 0.5 & 0.9 \mp$$
*We cannot subtract these matrices because they don't have the same size.*

c. Compute $2\A$:
$$\A=\pm 2 & 3 & -1\\1&-1&1\\2&2&1 \mp$$
 *We simply multiply every element in $\A$ by 2,*
$$2\A=\pm 4 & 6 & -2\\2&-2&2\\4&4&2 \mp$$
:::

:::{.exercise name='Addition, Subtraction, and Scalar Multiplication' #addsubexer}

a.  Compute $\v-\y$, if possible: $$\v=\pm 2\\-3\\4 \mp \quad \y=\pm 1\\4\\1 \mp$$

b. Compute $\v+\bo{h}$, if possible:
$$\v=\pm 4\\-5\\3 \mp \quad \bo{h}=\pm -1\\-4\\1\\2 \mp$$


c. Compute $\frac{1}{\sqrt{2}}\v$:
$$\v=\pm 4\\-5\\3 \mp$$
:::

## Geometry of Vector Addition and Scalar Multiplication {#sec:vectoradd}

You've already learned how vector addition works algebraically: it occurs element-wise between two vectors of the same length:
$$
 \a+\b =\pm a_1\\ a_2\\ a_3\\ \vdots \\ a_n \mp +\pm b_1\\ b_2\\ b_3\\ \vdots \\ b_n \mp = \pm a_1+b_1\\a_2+b_2\\a_3+b_3\\ \vdots \\a_n+b_n \mp
$$

Geometrically, vector addition is witnessed by placing the two vectors, $\a$ and $\b$, _tail-to-head_. The result, $\a+\b$, is the vector from the open tail to the open head. This is called the parallelogram law and is demonstrated in Figure \@ref(fig:vectoradd).


<!-- ![](figs/vectoradd.pdf)     | ![](figs/vectorsub.pdf) -->
<!-- :-------------------------:|:-------------------------: -->
<!-- Addition of vectors  |  Subtraction of Vectors -->

```{r, fig=T, label='vectoradd', fig.show="hold", out.width="50%", echo=F,fig.align='center',fig.cap = 'Geometry of Vector Addition'}
knitr::include_graphics("figs/vectoradd.png")
```

When subtracting vectors as $\a-\b$ we simply add $-\b$ to $\a$. The vector $-\b$ has the same length as $\b$ but points in the opposite direction. This vector has the same length as the one which connects the two heads of $\a$ and $\b$ as shown in Figure \@ref(fig:vectorsub). 

```{r, fig=T, label='vectorsub', fig.show="hold", out.width="50%", echo=F,fig.align='center', fig.cap = 'Geometry of Vector Subtraction'}
knitr::include_graphics("figs/vectorsub.jpg")
```

:::{.example name='Centering Data' #centering}

One thing we will do frequently in this course is deal with centered and/or standardized data. To center a group of data points, we merely subtract the mean of each variable from each measurement on that variable. Geometrically, this amounts to a *translation* (shift) of the data so that its center (or mean) is at the origin. Figure \@ref(fig:centerall) illustrates this process using 4 data points.

```{r, fig=T, label='centerall', fig.show="hold", out.width="80%", echo=F,fig.align='center', fig.cap = 'Centering a Data Cloud as a Geometric Translation'}
knitr::include_graphics("figs/centerall.jpg")
```
:::

__Scalar multiplication__ is another operation which acts element-wise:
$$\alpha \a = \alpha \pm a_1\\a_2\\a_3\\ \vdots \\a_n \mp = \pm \alpha a_1 \\ \alpha a_2\\ \alpha a_3 \\ \vdots \\ \alpha a_n\mp $$

Scalar multiplication changes the length of a vector but not the overall direction (although a negative scalar will scale the vector in the opposite direction through the origin). We can see this geometric interpretation of scalar multiplication in Figure \@ref(fig:vectormult).


```{r fig=T, out.width='50%', echo=F,fig.align='center', label='vectormult', fig.cap='Geometric Effect of Scalar Multiplication'} 
knitr::include_graphics('figs/vectormult.jpg')
```

<!-- :::{.example name='Vector Scaling: Standardizing Data' #scaling} -->
<!-- Once data has been centered, it is also common to then scale the variables according to their standard deviation (or some other normalization factor). Geometrically this amounts to a proportional shrinking of the data. The following graphic illustrates this process using the same 4 data points from Example \@ref(exm:centering).  -->
<!-- ```{r, fig=T, label='scaleall', fig.show="hold", out.width="80%", echo=F,fig.align='center', fig.cap = 'Standardizing a Data Cloud.'} -->
<!-- knitr::include_graphics("figs/scaleall.jpg") -->
<!-- ``` -->
<!-- Note that it's the _variable_ vectors undergo the scalar multiplication whereas what's depicted in Figure \@ref(fig:scaleall) is the coordinates of _observations_. This is the first time we might contemplate the fact that any data matrix can have two equivalent geometric views: For an $m \times n$ matrix, the rows create vectors (points) that live in $\Re^n$ and the columns create vectors (points) that live in $\Re^m$. Depending on our task, either vantage point can provide analytical insights. -->
<!-- ::: -->


## Linear Combinations

:::{.definition name='Linear Combination' #lincombdef}

A **linear combination** is constructed from a set of terms $\v_1, \v_2, \dots, \v_n$ by multiplying each term by a scalar constant and adding the result:
$$\bo{c}=\alpha_1\v_1+\alpha_2 \v_2+ \dots+ \alpha_n\v_n = \sum_{i=1}^n \alpha_i \v_n$$
The coefficients $\alpha_i$ are scalar constants and the terms, $\{\v_i\}$ can be scalars, vectors, or matrices. Most often, we will consider linear combinations where the terms $\{\v_i\}$ are vectors. 
:::

Linear combinations are quite simple to understand. Once the equation is written, we can consider the expression as a breakdown into parts. 

:::{.example name='Linear Combination' #lincomb}
The simplest linear combination might involve columns of the identity matrix:
$$\pm 3 \\ -2\\4 \mp = 3\pm 1\\0\\0 \mp -2 \pm 0\\1\\0 \mp +4 \pm 0\\0\\1 \mp$$
We can easily picture this linear combination as a "breakdown into parts where the parts give directions along the 3 coordinate axis with which we are all familiar.
:::


We don't necessarily have to use vectors as the terms for a linear combination. Example \@ref(exm:matrixlincomb) shows how we can write any $m\times n$ matrix as a linear combination of $nm$ elementary matrices.

:::{.example name='Linear Combination of Matrices' #matrixlincomb}
Write the matrix $\A=\pm 1 & 3\\4&2 \mp$ as a linear combination of the following matrices:
$$\left\lbrace \pm 1 & 0\\0&0 \mp,\pm 0 & 1\\0&0 \mp,\pm 0 & 0\\1&0 \mp,\pm 0 & 0\\0&1 \mp \right\rbrace$$
Solution:
$$\A=\pm 1 & 3\\4&2 \mp = 1\pm 1 & 0\\0&0 \mp+3\pm 0 & 1\\0&0 \mp+4\pm 0 & 0\\1&0 \mp+2\pm 0 & 0\\0&1 \mp$$
:::



## Matrix Multiplication

When we multiply matrices, we do not perform the operation element-wise as we did with addition and scalar multiplication. Matrix multiplication is, in itself, a very powerful tool for summarizing information. In fact, many of the analytical tools we will focus on in this course, like Markov Chains, Principal Components Analysis, Factor Analysis, and the Singular Value Decomposition, can all be understood more clearly with a firm grasp on matrix multiplication. Because this operation is so important, we will spend a considerable amount of energy breaking it down in many ways. 

### The Inner Product
We'll begin by defining the multiplication of a row vector times a column vector, known as an inner product (sometimes called the _dot product_ in applied sciences). For the remainder of this course, unless otherwise specified, we will consider vectors to be columns rather than rows. This makes the notation more simple because if $\x$ is a column vector,
$$\x=\pm x_1\\x_2\\\vdots\\ x_n\mp$$
then we can automatically assume that $\x^T$ is a row vector:
$$\x^T = \pm x_1&x_2&\dots&x_n\mp.$$

:::{.definition name='Inner Product' #innerproddef}
The **inner product** of two vectors, $\x$ and $\y$, written $\x^T\y$, is defined as the sum of the product of corresponding elements in $\x$ and $\y$:

$$\x^T\y = \sum_{i=1}^n x_i y_i.$$
If we write this out for two vectors with 4 elements each, we'd have:

$$\x^T\y=\pm x_1 & x_2 & x_3 & x_4 \mp \pm y_1\\y_2\\y_3\\y_4 \mp = x_1y_1+x_2y_2+x_3y_3+x_4y_4$$

*Note: The inner product between vectors is only possible when the two vectors have the same number of elements!*\
:::


:::{.example name='Vector Inner Product' #innerprod}
Let $$\x=\pm -1 \\2\\4\\0 \mp \quad \y=\pm 3 \\5\\1\\7 \mp \quad \v=\pm -3 \\-2\\5\\3\\-2 \mp \quad \u= \pm 2\\-1\\3\\-3\\-2 \mp$$

If possible, compute the following inner products:

a. $\x^T\y$
\begin{eqnarray}
\x^T\y &=&\pm -1 &2&4&0 \mp \pm 3 \\5\\1\\7 \mp \cr &=& (-1)(3)+(2)(5)+(4)(1)+(0)(7) \cr &=& -3+10+4=\framebox{11}
\end{eqnarray}
b. $\x^T\v$
This is not possible because $\x$ and $\v$ do not have the same number of elements
c. $\v^T\u$
\begin{eqnarray}
\v^T\u &=& \pm -3 &-2&5&3&-2 \mp \pm 2\\-1\\3\\-3\\-2 \mp  \cr &=& (-3)(2)+(-2)(-1)+(5)(3)+(3)(-3)+(-2)(-2) \cr &=& -6+2+15-9+4 = \framebox{6}
\end{eqnarray}
:::

:::{.exercise name='Vector Inner Product' #innerprodexer}
Let $$\bo{v}=\pm 1 \\2\\3\\4\\5 \mp \quad \e=\pm 1 \\1\\1\\1\\1 \mp \quad \bo{p}=\pm 0.5 \\0.1\\0.2\\0\\0.2 \mp \quad \u= \pm 10\\4\\3\\2\\1 \mp \quad \bo{s} = \pm 2\\2\\-3 \mp$$

If possible, compute the following inner products:

a. $\bo{v}^T\e$
b. $\bo{e}^T\bo{v}$
c. $\bo{v}^T\bo{s}$
d. $\bo{p}^T\u$
e. $\bo{v}^T\bo{v}$
:::

It should be clear from the definition and from the previous exercise, that for all vectors $\x$ and $\y$,
$$\x^T\y = \y^T\x.$$
Also, if we take the inner product of a vector with itself, the result is the sum of squared elements in that vector:
$$\x^T\x = \sum_{i=1}^n x_i^2 = x_1^2 + x_2^2+ \dots + x_n^2.$$

Now that we are comfortable multiplying a row vector ($\x^T$ in the definition) and a column vector ($\y$ in the definition), we can define multiplication for matrices in general.


### Matrix Product

Matrix multiplication is nothing more than a collection of inner products done simultaneously in one operation. We must be careful when multiplying matrices because, as with vectors, the operation is not always possible. Unlike the vector inner product, the order in which you multiply matrices makes a big difference!

:::{.definition name='Matrix Multiplication' #matmultdef}
Let $\A_{m\times n}$ and $\B_{k\times p}$ be matrices. The matrix product $\A\B$ is possible if and only if $n=k$; that is, when the number of columns in $\A$ is the same as the number of rows in $\B$. If this condition holds, then the dimension of the product, $\A\B$ is $m\times p$ and the (ij)-entry of the product $\A\B$ is the inner product of the $i^{th}$ row of $\A$ and the $j^{th}$ column of $\B$:

$$(\A\B)_{ij} = \A_{i\star}\B_{\star j}$$
:::

This definition may be easier to dissect using an example:

:::{.example name='Steps to Compute a Matrix Product' #matmult}

Let $$\A=\pm 2 & 3 \\ -1 & 4 \\ 5 & 1 \mp \quad \mbox{and} \quad \B=\pm  0 & -2 \\ 2 & -3 \mp$$

When we first get started with matrix multiplication, we often follow a few simple steps:

1. Write down the matrices and their dimensions. Make sure the "inside" dimensions match - those corresponding to the columns of the first matrix and the rows of the second matrix:
$$\underset{(3\times \red{2})}{\A} \underset{(\red{2} \times 2)}{\B}$$
If these dimensions match, then we can multiply the matrices. If they don't, we stop right there - multiplication is not possible.
2. Now, look at the "outer" dimensions - this will tell you the size of the resulting matrix.
$$\underset{(\blue{3}\times 2)}{\A} \underset{(2\times \blue{2})}{\B}$$
So the product $\A\B$ is a $3\times 2$ matrix.
3. Finally, we compute the product of the matrices by multiplying each row of $\A$ by each column of $\B$ using inner products. The element in the first row and first column of the product (written $(\A\B)_{11}$) will be the inner product of the first row of $\A$ and the first column of $\B$. Then, $(\A\B)_{12}$ will be the inner product of the first row of $\A$ and the second column of $\B$, etc.

\begin{eqnarray}
\A\B &=&\pm (2)(0)+(3)(2) & (2)(-2)+(3)(-3)\\
 			 (-1)(0)+(4)(2) & (-1)(-2)+(4)(-3)\\
 			  (5)(0)+(1)(2) & (5)(-2)+(1)(-3) \mp \cr
 	&=& \pm 6&-13\\8 & -10\\2&-13\mp
\end{eqnarray}
:::

Matrix multiplication is incredibly important for data analysis. You may not see why all these multiplications and additions are so useful at this point, but we will visit some basic applications shortly. For now, let's practice so that we are prepared for the applications!

:::{.exercise name='Matrix Multiplication' #matmultexer}
Suppose we have $$\A_{4\times 6} \quad \B_{5\times 5} \quad \M_{5\times 4} \quad \bP_{6\times 5}$$
Circle the matrix products that are possible to compute and write the dimension of the result.
$$\A\M \qquad \M\A \qquad \B\M  \qquad \M\B \qquad \bP\A \qquad \bP\M \qquad \A\bP \qquad \A^T\bP \qquad \M^T\B$$
Let 
\begin{equation}
\A=\pm 1&1&0&1\\0&1&1&1\\1&0&1&0\mp \quad \M = \pm -2&1&-1&2&-2\\1&-2&0&-1&2\\2&1&-3&-2&3 \\ 1&3&2&-1&2\mp  \end{equation}

\begin{equation}
\C=\pm -1&0&1&0\\1&-1&0&0\\0&0&1&-1 \mp \end{equation}

Determine the following matrix products, if possible:

a $\A\C$

b $\A\M$

c. $\A^T\C$
:::

One very important thing to keep in mind is this:
 <p font-color:red><strong> matrix multiplication is NOT commutative! </strong></p>
 As we see from the previous exercises, it's quite common to be able to compute a product $\A\B$ where the reverse product, $\B\A$ is not even possible to compute. Even if both products are possible it is almost _never_ the case that $\A\B$ equals $\B\A$.
 
#### Multiplication by a Diagonal Matrix
 
 As we will see in the next example, multiplication by a diagonal matrix causes a very specific effect on a matrix.

:::{.example name='Multiplication by a Diagonal Matrix' #diagmult}
 Compute the following matrix product and comment on what you find in the results:
 $$\D=\pm 2&0&0\\0&3&0\\0&0&-2 \mp \A= \pm 1&2&3\\1&1&2\\2&1&3 \mp$$
 $$\D\A=\pm 2&4&6\\3&3&6\\-4&-2&-6 \mp$$
 In doing this multiplication, we see that the effect of multiplying the matrix $\A$ by a diagonal matrix on the left is that the rows of the matrix $\A$ are simply scaled by the entries in the diagonal matrix. You should work this computation out by hand to convince yourself that this effect will happen every time. Diagonal scaling can be important, and from now on when you see a matrix product like $\D\A$ where $\D$ is diagonal, you should automatically put together that the result is just a row-scaled version of $\A$.
:::
 
:::{.exercise name='Multiplication by a Diagonal Matrix' #diagmultexer}
 What happens if we were to compute the product from Example \@ref(exm:diagmult) in the reversed order, with the diagonal matrix on the right:
 $$\A\D?$$
 Would we expect the same result? Is multiplication by a diagonal matrix commutative? Work out the calculation and comment on what you've found.
:::

### Matrix-Vector Product

A matrix-vector product works exactly the same way as matrix multiplication; after all, a vector $\x$ is nothing but an $n\times 1$ matrix. In order to multiply a matrix by a vector, again we must match the dimensions to make sure they line up correctly. For example, if we have an $m\times n$ matrix $\A$, we can multiply by a $1\times m$ row vector $\v^T$ on the left:
$$\v^T\A \quad \mbox{works because } \underset{ (1\times \red{m})}{\v^T} \underset{(\red{m}\times n)}{\A}$$
$$\Longrightarrow \mbox{The result will be a   } 1 \times n \mbox{ row vector.}$$
or we can multiply by an $n\times 1$ column vector $\x$ on the right:

$$\A\x \quad \mbox{works because } \underset{(m\times \red{n})}{\A}\underset{(\red{n}\times 1)}{\x} $$
$$\Longrightarrow \mbox{The result will be a   } m\times 1 \mbox{ column vector.}$$

Matrix-vector multiplication works the same way as matrix multiplication: we simply multiply rows by columns until we've completed the answer. In the case of $\v^T\A$, we'd multiply the row $\v$ by each of the $n$ columns of $\A$, carving out our solution, one entry at a time :

$$\v^T\A = \pm \v^T\A_{*1} & \v^T\acol{2} & \dots & \v^T\acol{n} \mp.$$

In the case of $\A\x$, we'd multiply each of the $m$ rows of $\A$ by the column $\x$:

$$\A\x = \pm \arow{1}\x \\ \arow2{x} \\ \vdots \\ \arow{m}\x \mp.$$

Let's see an example of this:

:::{.example name='Matrix-Vector Products' #matvecprod}
Let $$\A=\pm 2 & 3 \\ -1 & 4 \\ 5 & 1 \mp  \quad \v=\pm 3\\2 \mp \quad \bo{q}=\pm 2\\-1\\3\mp$$

Determine whether the following matrix-vector products are possible. When possible, compute the product.

a. $\A\bo{q}$ 
$$\mbox{Not Possible: Inner dimensions do not match} \quad \underset{(3\times \red{2})}{\A}\underset{(\red{3}\times 1)}{\bo{q}}$$
b. $\A\v$
$$
\pm 2 & 3 \\ -1 & 4 \\ 5 & 1 \mp \pm 3\\2 \mp = \pm 2(3)+3(2) \\  -1(3)+4(2)\\5(3)+1(2) \mp = \pm 12\\5\\17\mp
$$
c. $\bo{q}^T\A$
<center>Rather than write out the entire calculation, the blue text highlights one of the two inner products required:</center>
$$
\pm \blue{2} & \blue{-1} & \blue{3}\mp \pm \blue{2} & 3 \\ \blue{-1} & 4 \\ \blue{5} & 1 \mp  =  \pm \blue{20} & 5  \mp
$$

d. $\v^T\A$
$$\mbox{Not Possible: Inner dimensions do not match} \quad \underset{(1\times \red{2})}{\v^T}\underset{(\red{3}\times 2)}{\A}$$
:::

:::{.exercise name='Matrix-Vector Products' #matvecprodexer}
Let
$$ \A=\pm 1&1&0&1\\0&1&1&1\\1&0&1&0\mp \quad \B=\pm  0 & -2 \\ 1 & -3 \mp $$ $$ \x=\pm 2\\1\\3 \mp \quad \y = \pm 1\\1 \mp \quad \z = \pm 3\\1\\2\\3 \mp$$
Determine whether the following matrix-vector products are possible. When possible, compute the product.

a. $\A\z$

b. $\z^T\A$

c. $\y^T\B$

d. $\B\y$

e. $\x^T\A$
:::

### Linear Combination view of Matrix Products {-}

All matrix products can be viewed as linear combinations or a collection of linear combinations. This vantage point is _extremely_ crucial to our understanding of data science techniques that are based on matrix-factorization. Let's start with matrix-vector product and see how we can depict it as a linear combination of the columns of the matrix.

:::{.definition name='Matrix-Vector Product as a Linear Combination' #matvecprodlincomb}
Let $\A$ be an $m\times n$ matrix partitioned into columns, 
$$\A = [\A_1 | \A_2 | \dots | \A_n]$$
and let $\x$ be a vector in $\Re^n$. Then,
$$\A\x = x_1\A_1 + x_2\A_2 + \dots + x_n\A_n$$\
:::

We use the animation in Figure \@ref(fig:matvecprodlincombanim) to illustrate Definition \@ref(def:matvecprodlincomb).

(ref:matvecprodlincombanim) Illustration of Definition \@ref(def:matvecprodlincomb)

```{r, fig=T, label='matvecprodlincombanim', fig.show="hold", out.width="50%", echo=F,fig.align='center',fig.cap = '(ref:matvecprodlincombanim)'}
knitr::include_graphics("figs/matvecprodlincombanim.gif")
```

Definition \@ref(def:matvecprodlincomb) extends to _any_ matrix product. If $\A\B=\mathbf{C}$ then the columns of $\mathbf{C}$ can be viewed as linear combinations of the columns of $\A$ and, likewise, the rows of $\C$ can be viewed as linear combinations of the rows of $\B$. We leave the latter fact for the reader to explore independently (see end-of-chapter exercise 5), and animate the former in Figure \@ref(fig:multlincombanim).

```{r, fig=T, label='multlincombanim', fig.show="hold", out.width="50%", echo=F,fig.align='center',fig.cap = '(ref:matvecprodlincombanim)'}
knitr::include_graphics("figs/multlincombanim.gif")
```

## Vector Outer Products

Whereas inner products were the product of a row vector with a column vector (think $\x^T\y$),  **outer products**  are the product of a *column* vector with a *row* vector (think $\x\y^T$).
Let's first consider the dimensions of the outcome:

$$\underset{(m\times \red{1})}{\x} \underset{(\red{1} \times n)}{\y^T} = \bo{M}_{m\times n}$$

So the result is a matrix! We'll want to treat this product in the same way we treat any matrix product, by multiplying row $\times$ column until we've run out of rows and columns. Let's take a look at an example:

:::{.example name='Vector Outer Product' #outerprod}
Let $\x = \pm 3\\4\\-2 \mp$ and $\y=\pm 1\\5\\3 \mp$. Then,
$$\x\y^T = \pm \red{3}\\4\\-2 \mp \pm \red{1}&5&3 \mp = \pm \red{3}&15&9\\4&20&12\\-2&-10&-3\mp$$
As you can see by performing this calculation, a vector outer product will _always_ produce a matrix whose rows are multiples of each other!
:::

## The Identity and the Matrix Inverse

The *identity matrix*, introduced in Section \@ref(special), is to matrices as the number `1' is to scalars. It is the **multiplicative identity**. For any matrix (or vector) $\A$, multiplying $\A$ by the identity matrix on either side does not change $\A$:
\begin{align*}
\A\I&=\A \\
\I\A &= \A 
\end{align*}

This fact is easy to verify in light of Example \@ref(exm:diagmult). Since the identity is simply a diagonal matrix with ones on the diagonal, when we multiply it by any matrix it merely scales each row or column of that matrix by 1.  The size of the identity matrix is generally implied in context. If $\A$ is $m\times n$ then writing $\A\I$ implies that $\I$ is $n \times n$, where as writing $\I\A$ implies $\I$ is $m\times m$.

For *certain* square matrices $\A$, an inverse matrix, written $\A^{-1}$, exists such that
$$\A\A^{-1} = \I$$
$$\A^{-1}\A = \I$$
It is very important to understand that not all matrices have inverses. There are 2 very important conditions that must be satisfied:
\begin{itemize}
\item The matrix $\A$ must be square
\item The matrix $\A$ must be full-rank. 
\end{itemize}

We have not yet discussed the notion of matrix rank, so the present discussion is aimed only at defining the concept of a matrix inverse rather than defining when it exists or how it is determined. For now, we want to see the analogy of the matrix inverse to our previous understanding of scalar algebra. Recall that the inverse of a non-zero scalar number is its reciprocal:
$$a^{-1} = \frac{1}{a}$$
Multiplying a scalar by its inverse yields the multiplicative identity, 1:
$$(a)(a^{-1}) = (a)(\frac{1}{a}) = 1$$
All scalars have an inverse with the exception of 0. For matrices, the idea of an inverse is quite the same - multiply a matrix on the left or right by its inverse to get the multiplicative identity, $\I$. However, as previously stated, the matrix inverse only exists for a small subset of matrices, those that are square and full rank. Such matrices are equivalently called **invertible** or **non-singular**. 


(ref:canceltitle) Don't Cancel That!!

:::{.example name='(ref:canceltitle)' #dontcancel}
We must be careful in linear algebra to remember the basics and not confuse our equations with scalar equations. When we see an equation like
$$\A\x=\lambda\x$$
We <font-folor:red><strong> CANNOT </strong> <font-folor:black>cancel terms from both sides. Mathematically, this operation amounts to multiplying both sides by an inverse. When the term we are canceling is a non-zero scalar, then we can proceed as usual. However, we must be careful not to assume that a matrix/vector quantity has an inverse. For example, the following operation is **nonsense:** 
$$\require{cancel} \A\cancel{\x}=\lambda\cancel{\x}$$
Note that, while this equation made sense to begin with, after erroneously canceling terms, it no longer makes sense as it equates a matrix, $\A$, with a scalar, $\lambda$.
:::



## Exercises
<ol>
<li> On a coordinate plane, draw the vectors $\a = \pm 1\\2\mp$ and $\b=\pm 0\\1\mp$ and then draw $\bo{c}=\a+\b$. Make dotted lines which illustrate how the point/vector $\bo{c}$ can be reached by connecting the vectors a and b "tail-to-head".
<li> Use the following vectors to answer the questions:
$$
\v=\pm 6\\-1\mp \quad \bo{u}=\pm -2\\1\mp \quad \x=\pm 4\\2\\1\mp \quad \y=\pm-1\\-2\\-3\mp \quad \e=\pm 1\\1\\1\mp
$$
  <ol style="list-style-type:lower-alpha">
      <li> Compute the following linear combinations, if possible:
        <ol style="list-style-type:lower-roman">
          <li> $2\u+3\v=$ 
          <li> $\x-2\y+\e=$ 
          <li> $-2\u-\v+\e=$ 
          <li> $\u+\e=$
        </ol>  
        
<li> Compute the following inner products, if possible:
  <ol style="list-style-type:lower-roman">
    <li> $\u^T\v=$ 
    <li> $\x^T\x=$ 
    <li> $\e^T\y=$ 
    <li> $\x^T\u=$
    <li> $\x^T\e=$ 
    <li> $\y^T\e=$
    <li> $\v^T\x=$ 
    <li> $\e^T\v=$  
</ol>  
<li> What happens when you take the inner product of a vector with $\e$?   
<li> What happens when you take the inner product of a vector with itself (as in $\x^T\x$)?  
</ol>  
<li> Use the following matrices to answer the questions:
$$\A=\pm 1&3&8\\3&0&-2\\8&-2&-3 \mp \quad \bo{M}=\pm 1&8&-2&5\\2&8&1&7 \mp \quad 
\D = \pm 1&0&0\\0&5&0\\0&0&3\mp $$ 
$$
\bo{H}=\pm 2&-1\\1&3 \mp \quad \bo{W}=\pm 1&1&1&1\\2&2&2&2\\3&3&3&3\mp
$$
<ol style="list-style-type:lower-alpha">
<li> Circle the matrix products that are possible and specify their resulting dimensions:
<ol style="list-style-type:lower-roman">
  <li> $\A\M$ 
  <li> $\A\bo{W}$ 
  <li> $\bo{W}\D$
  <li> $\bo{W}^T\D$
  <li> $\bo{H}\M$
  <li> $\bo{M}\bo{H}$
  <li> $\bo{M}^T\bo{H}^T$
  <li> $\D\bo{W}$  
</ol>  
<li> Compute the following matrix products:  
    
$$\bo{H}\M\quad \mbox{and} \quad \A\D$$
<li> From the previous computation, $\A\D$, do you notice anything interesting about multiplying a matrix by a diagonal matrix on the right? Can you generalize what happens in words? (*Hint:* see Example \@ref(exm:diagmult) and Exercise \@ref(exr:diagmultexer).  
</ol>  
<li> Is matrix multiplication commutative?
<li> **Different Views of Matrix Multiplication:** Consider the matrix product 
$\A\B$ where $$\A = \pm 1 & 2\\3&4 \mp \quad \B = \pm 2&5\\1&3\mp$$
Let $\C=\A\B$.
<ol style="list-style-type:lower-roman">
<li> Compute the matrix product $\C$.
<li> Compute the matrix-vector product $\A\B_{\star 1}$ and show that this is the first column of $\C$. (Likewise, $\A\B_{\star 2}$ is the second column of $\C$.) (_Matrix multiplication can be viewed as a collection of matrix-vector products._)
<li> Compute the two outer products using columns of $\A$ and rows of $\B$ and show that
$$\acol{1}\brow{1} + \acol{2}\brow{2} = \C$$ (_Matrix multiplication can be viewed as the sum of outer products._)
<li> Since $\A\B_{\star 1}$ is the first column of $\C$, show how $\C_{\star 1}$ can be written as a linear combination of columns of $\A$. (_Matrix multiplication can be viewed as a collection of linear combinations of columns of the first matrix._)
<li> Finally, note that $\arow{1}\B$ will give the first row of $\C$. (_This amounts to a linear combination of rows - can you see that?_)
</ol>
</ol>

## List of Key Terms {-}

- addition
- subtraction
- equal matrices
- scalar multiplication
- inner product
- matrix product
- linear combination
- outer product
- multiplicative identity
- matrix inverse


<!--chapter:end:015-mult.Rmd-->

# Applications of Matrix Multiplication {#multapp}

As we will begin to see here, matrix multiplication has a number of uses in data modeling and problem solving. It expresses a rather large number of operations in a surprisingly compact way. The more comfortable we can be with this compact notation and what it entails, the more understanding we can have with analytical tools like Principal Components Analysis, Factor Analysis, Markov Chains, and Optimization (to name a few).

## Systems of Equations

Matrix multiplication creates a __system of equations__, which is nothing more than a collection of equations  which hold true simultaneously. Suppose we take the matrix-vector product:
$$\A\x=\b$$
Where 
$$\A=\pm 1&2&3&1\\0&3&2&1\\1&1&1&4\mp \quad \x=\pm x_1\\x_2\\x_3\\x_4 \mp \quad \mbox{and} \quad \b=\pm 10\\15\\6\mp$$
Let's take a look at what happens when we write the equation $\A\x=\b$ the old-fashioned way, without matrices:
$$
\pm 1&2&3&1\\0&3&2&1\\1&1&1&4\mp\pm x_1\\x_2\\x_3\\x_4 \mp = \pm 10\\15\\6\mp 
$$
$$
\Longrightarrow\begin{cases}\begin{align}
x_1+2x_2+3x_3+x_4 = 10\\
3x_2+2x_3+x_4 = 15\\
x_1+x_2+x_3+4x_4 = 6\end{align}\end{cases}
$$

We get a system of three equations. In general, a system of equations is nothing more than a matrix equation, $\A\x=\b$ where the matrix $\A$ contains the coefficients on the parameters you wish you find, $\x$ is a vector containing those unknown parameters and $\b$ is a vector containing the right hand sides of the equations. These systems of equations pop-up in all types of data applications from regression analysis to optimization. Let's consider a scenario which mimics the real-world and try to model it using a matrix-vector product.

:::{.example name='System of Equations' #syseq}
A large manufacturing company has recently signed a deal to manufacture trail mix for a well-known food label. This label makes 3 versions of its product - one for airlines, one for grocery stores, and one for gas stations. Each version has a different mixture of peanuts, raisins, and chocolate which serves as the base of the trail mix. The base mixtures are made in 15 kg batches and sent to a second building for packaging. 

The following table contains the information about the mixes, each row containing the recipe for a 15 kg batch. There is also some additional information on the costs of the ingredients, the price the manufacturer can charge for the mixtures and the amount of storage allocated for each ingredient.

| |Raisins <br> (kg/batch)| Peanuts <br> (kg/batch)|Chocolate <br>(kg/batch) | Sale Price<br>(\$/kg) |
|:-------------:|:------------:|:------------:|:--------------:|:-------------:|
| Airline (a) |7|6|2|4.99|
| Grocery (g) |2|5|8|6.50|
| Gas Station (s) |6|4|5|5.50|
 ---
|Storage (kg) | 380 | 500| 620| |
|Cost (\$/kg) | 2.55|4.65|4.80| |




__a. If the manufacturer wanted to use up all the ingredients in storage each day, how many batches of each mixture (airline, gas station, and grocery) should be made? __


We can gather from the table that 1 batch of the airline mixture contains 7 kgs of raisins. We want the total number of kgs of raisins from each of the 3 mixtures to match the storage capacity of raisins, which is 380 kg. We can set this up as a system of equations, one for each ingredient, where
\begin{eqnarray}
a&=&\mbox{batches of airline mixture}\\
g&=&\mbox{batches of grocery mixture}\\
s&=&\mbox{batches of gas station mixture}\\
a,g,s &\geq & 0 
\end{eqnarray}
as follows:
$$\begin{cases}\begin{eqnarray}
7 a+6 s+2g &=& 380 \quad \mbox{(Raisins)}\\
6 a+4 s+5g &=& 500 \quad \mbox{(Peanuts)}\\
2 a+5 s+8g &=& 620 \quad \mbox{(Chocolate)}\end{eqnarray}\end{cases}$$

We can then transform this system of equations into matrix form:
$$\pm 7&2&6\\6&5&4\\2&8&5 \mp \pm a\\g\\s \mp = \pm 380\\500\\620 \mp$$

While we haven't yet discussed how to solve such a system of equations, you can verify that 
$$a=20\,batches \quad g=60\,batches \quad s=20\,batches$$
is indeed a solution - in fact, it is the only possible solution. Determining solutions such as this, and establishing that they are unique (i.e. that they are the _only_ possible solution) is one of the many tools that the study of linear algebra will provide.



__b. Use matrix-vector multiplication to determine how much it costs the manufacturer to produce 1 batch of each mixture
__


For 1 batch of airline mixture, the manufacturer will spend $$7 kg\times \$2.55/kg = \$17.85\,\,\mbox{ on raisins.}$$

Of course, we need to add in the cost of peanuts and chocolate and then repeat this calculation for both grocery and gas station mixtures.

This is conveniently done in one matrix-vector multiplication:
$$\pm 7&6&2\\2&5&8 \\6&4&5 \mp \pm 2.55\\4.65\\4.80 \mp = \pm 55.35\\66.75\\57.90\mp$$
Thus, the cost of 1 batch of each type of mixture is:


|Mixture | Cost (\$/batch)|
|:--------|:--------:|
|airline | 55.35    |
|grocery | 66.75    |
|gas station | 57.90|

:::


### _Big_ Systems of Equations

When we expand our minds to the possibilities associated with matrix-matrix products, the systems that we can generate get very large very quickly.

For example, let's consider a $2\times 2$ example, $$\A\X=\B$$ where $$\A=\pm 2&3\\1&4 \mp \qquad \X=\pm x_{11} & x_{12} \\x_{21} & x_{22} \mp \qquad \B=\pm 7 & 6 \\ 5& 9 \mp$$

Let's take a look at what the simple equation $\A\X=\B$ is really saying in terms of all the matrix values that we have. All we have to do is write out the multiplication "the long way". For example, to get the element in the first row and first column of $\B$ (in this case, 7) we would compute the inner product of the first row of $\A$ with the first column of $\X$:

$$\pm 2 & 3 \mp \pm x_{11} \\ x_{21} \mp = \red{2x_{11}+3x_{21} = 7}$$

Now the equation in red above is just one of 4. We have one equation for each element of $\B$!. Let's make sure we understand how to get all 4 of these equations:
\begin{eqnarray}
2x_{11}+3x_{21} &=& 7 \\
2x_{12}+3x_{22} &=&6\\
1x_{11}+4x_{21} &=& 5 \\
1x_{12}+4x_{22} &=&9\\
\end{eqnarray}

Now, a list of 4 equations does not seem that big. But what if the dimensions of the matrix $\B$ were $9\times10$? By now you should be able to see that we'd have a system of 90 equations! The number of equations generated will always equal the number of elements in the right hand side matrix $\B$.




## Regression Analysis

In statistics, the solution to these systems of equations is exactly what we are trying to find when we do regression analysis. Take, for example, a regression analysis with some dependent variable, $\y$, and two independent variables, $\h,\w$. The preliminary goal of this analysis is to find unknown parameters $\beta_0, \beta_1, \dots$ such that
\begin{equation}
\y= \beta_0+\beta_1\h+\beta_2\w
 (\#eq:hwmodel)
\end{equation}

This is the single equation we usually consider when talking about regression analysis - but what about all those data points? Suppose, for simplicity, we have only 4 observations as listed in the following table:


|$\h$|$\w$|$\y$ |
|:---:|:---:|:---:|
|3|3|6|
|2|3|6|
|5|6|10|
|6|5|9|


When we write the model from Equation \@ref(eq:hwmodel), what we are really saying is that the equation holds true for each of the 4 observations in our dataset. So rather than 1 single equation, what we really have here is 4 equations - 1 for each observation:

\begin{eqnarray}
\beta_0 + 3 \beta_1 + 3 \beta_2 &=& 6 \quad \mbox{(obs. 1)}\\
\beta_0 + 2 \beta_1 + 3 \beta_2 &=& 6 \quad \mbox{(obs. 2)}\\
\beta_0 + 5 \beta_1 + 6 \beta_2 &=& 10 \,\,\, \mbox{(obs. 3)}\\
\beta_0 + 6 \beta_1 + 5 \beta_2 &=& 9 \quad \mbox{(obs. 4)}\\
\end{eqnarray}

Rather than writing all these equations out, we instead represent the situation in matrix format as 
$$\X\bbeta = \y,$$
Where
$$\X=\pm 1&3&3\\1&2&3\\1&5&6\\1&6&5 \mp \quad \bbeta = \pm \beta_0 \\\beta_1 \\ \beta_2 \mp \quad \mbox{and}\quad \y = \pm 6 \\6 \\10\\9\mp$$

We know from our experiences with data that this situation will not have an exact solution: our data does not fall exactly on some straight line or surface. Instead, we have to consider some error, $\boldsymbol\epsilon$ and try to minimize it:
$$\X\bbeta + \boldsymbol\epsilon = \y,$$
where 
$$\boldsymbol\epsilon = \pm \epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \epsilon_4 \mp$$ is a vector containing the _residuals._ We will get into the exact details of this shortly, but for now it is important that we see how to set up the regression equation in terms of matrices and vectors. The typical regression equation with the intercept will always involve adding a column of 1's to the matrix of independent variables, as seen in the previous example.

## Linear Combinations

Let's revisit the second part of Example \@ref(exm:syseq), where the task was to use matrix-vector multiplication to determine how much it would cost for the manufacturer to produce 1 batch of each mixture.

Essentially what we want to do is take a linear combination of the amounts of raisins, peanuts, and chocolate where the scalar weights are the cost of each ingredient:
$$\bordermatrix{~& \mbox{Cost of 1kg}}{}{\begin{pmatrix}\mbox{airline}\\ \mbox{grocery}\\ \mbox{gas station} \end{pmatrix}} = \$2.55 \bordermatrix{~& raisins}{}{ \begin{pmatrix} 7\\ 2\\ 6\end{pmatrix}} + \$4.65 \bordermatrix{~& peanuts}{}{ \begin{pmatrix} 6\\ 5\\ 4\end{pmatrix}}  + \$4.80 \bordermatrix{~& chocolate}{}{ \begin{pmatrix} 2\\ 8\\ 5\end{pmatrix}}$$
This linear combination is exactly the same as the matrix-vector product originally used:
$$\pm 7&6&2\\2&5&8 \\6&4&5 \mp \pm 2.55\\4.65\\4.80 \mp = \pm 55.35\\66.75\\57.90\mp$$

Matrix multiplication is nothing more than a series of linear combinations. Let's develop another quick example.

:::{.example name='Linear Combinations of Variables' #lincombvar}
Suppose we have data for 100 postal packages using 3 variables: height $\bo{h}$, weight $\bo{w}$, and volume $\bo{v}$. If we create a data matrix, $\X$, the size of the matrix will be $100\times 3$ and the three columns will be composed of the variables height, weight, and volume. The previous sentence is written mathematically by creating a partitioned matrix:
$$\X = \left( \bo{h} | \bo{w} | \v \right)$$

If we wanted to create a new variable vector, $\bo{c}$, which equaled the height plus twice the weight of the package, we'd want to compute the following linear combination:
$$\bo{c} = \bo{h} + 2\bo{w} + 0\v$$

This could be accomplished by multiplying our whole data matrix by the vector $\pm 1\\2\\0\mp$.

\begin{eqnarray*}
\bo{c}&=&\underset{(100\times 3)}{\X}\pm 1\\2\\0\mp \cr
&=&\left( \bo{h} | \bo{w} | \v \right)\pm 1\\2\\0\mp \cr
&=& \bo{h} + 2\bo{w} + 0\v
\end{eqnarray*}

If this example confuses you, you ought to write out a smaller matrix of values for the three variables, height, weight and volume. Write down 3 observations or so and see how the linear combination of these columns is precisely the same as the matrix-vector product. Being able to think of these two ideas as interchangeable will be fundamental when we start talking about factor analysis and principal components analysis.
:::



If we dissect our formula for a system of linear equations, $\A\x=\bo{b}$, we will find that the right-hand side vector $\bo{b}$ can be expressed as a linear combination of the columns in the coefficient matrix, $\A$.

\begin{eqnarray*}
\bo{b}&=& \A\x\\
\bo{b}&=& (\A_1|\A_2|\dots|\A_n)\pm x_1\\x_2\\ \vdots\\x_3 \mp  \\
\bo{b}&=& x_1\A_1 + x_2\A_2 + \dots + x_n\A_n 
\end{eqnarray*}
A concrete example of this expression is given in Example \@ref(exm:lincom).

:::{.example name='Systems of Equations as Linear Combinations' #lincom}
Consider the following system of equations:
\begin{eqnarray}
3x_1 + 2x_2 + 9x_3 &=& 1\\
4x_1 + 2x_2 + 3x_3 &=& 5\\
2x_1 + 7x_2 + \,x_3 &=& 0
\end{eqnarray}
We can write this as a matrix vector product $\A\x=\bo{b}$ where
$$\A=\pm 3 & 2 & 9\\4 & 2 & 3\\2 &7&1\mp \,\,\,\x=\pm x_1\\x_2\\x_3\mp \mbox{   and   } \bo{b}=\pm 1\\5\\0 \mp$$
We can also write $\bo{b}$ as a linear combination of columns of $\A$:
$$x_1 \pm 3\\4\\2 \mp +x_2 \pm 2\\2\\7\mp + x_3 \pm9\\3\\1 \mp = \pm 1\\5\\0 \mp$$
:::


Similarly, if we have a matrix-matrix product, we can write each column of the result as a linear combination of columns of the first matrix. Let $\A_{m\times n}$, $\X_{n\times p}$, and $\B_{m\times p}$ be matrices. If we have $\A\X=\B$ then
$$
(\A_1 | \A_2 | \dots | \A_n) \pm x_{11} & x_{12} & \dots & x_{1p} \\x_{21} & x_{22} & \dots & x_{2n}\\ \vdots & \vdots & \ddots & \vdots \\ x_{n1} & x_{n2}&\dots &x_{np} \mp = (\B_1 | \B_2 | \dots | \B_n) 
$$

and we can write 
$$\B_j = \A\X_j = x_{1j}\A_1 + x_{2j}\A_2 + x_{3j}\A_3 + \dots + x_{nj}\A_n.$$
A concrete example of this expression is given in Example \@ref(exm:matmat).

:::{.example name='Linear Combinations in Matrix-Matrix Products' #matmat}
Suppose we have the following matrix formula:
$$\A\X=\B$$
Where $\A=\pm 2 & 1 & 3\\1 & 4 & 2\\ 3 & 2 & 1 \mp$, $\X=\pm 5&6\\9&5\\7&8 \mp$. 
Then 
\begin{eqnarray}
\B &=&\pm 2 & 1 & 3\\1 & 4 & 2\\ 3 & 2 & 1 \mp \pm 5&6\\9&5\\7&8 \mp \\
~	&=& \pm 2(5)+1(9)+3(7)&2(6) +1(5)+3(8)\\1(5)+4(9)+2(7)&1(6)+4(5)+2(8)\\3(5)+2(9)+1(7)&3(6)+2(5)+1(8) \mp 
\end{eqnarray}
and we can immediately notice that the columns of $\B$ are linear combinations of columns of $\A$:
$$\B_1 = 5\pm 2\\1\\3\mp+9\pm 1\\4\\2 \mp + 7 \pm 3\\2\\1 \mp$$
$$\B_2 = 6\pm 2\\1\\3\mp+5\pm 1\\4\\2 \mp + 8 \pm 3\\2\\1 \mp$$

We may also notice that the _rows_ of $\B$ can be expressed as a linear combination of _rows_ of $\X$:

$$\B_{1\star} = 2\pm 5& 6 \mp +  1\pm 9& 5 \mp + 3\pm 7& 8 \mp $$
$$\B_{2\star} = 1\pm 5& 6 \mp +  4\pm 9& 5 \mp + 2\pm 7& 8 \mp $$
$$\B_{3\star} = 3\pm 5& 6 \mp +  2\pm 9& 5 \mp + 1\pm 7& 8 \mp $$

Linear combinations are everywhere, and they can provide subtle but important meaning in the sense that they can break data down into a sum of parts. 

You should convince yourself of one final view of matrix multiplication, as the _sum of outer products_. In this case $\B$ is the sum of 3 outer products (3 matrices of rank 1) involving the columns of $\A$ and corresponding rows of $\X$:
$$\B=\acol{1}\X_{1\star}+\acol{2}\X_{2\star}+\acol{3}\X_{3\star}.$$
:::

Example \@ref(exm:matmat) turns out to have important implications for our interpretation of matrix factorizations. In this context we'd call $\A\X$ a _factorization_ of the matrix $\B$. We will see how to use these expressions to our advantage in later chapters. 

## Exercises {#multapp-ex}
<ol type='1'>
<li>  A florist offers three sizes of flower arrangements (small, medium, large) containing three types of flowers (roses, daisies, and chrysanthemums). The number of each type of flower in each size arrangement is given in the table below, along with the selling price of each arrangement and the cost of each individual flower.

<table>
<tr>
<td style="text-align: center"> <td style="text-align: center"> __Roses__ <td style="text-align: center"> __Daisies__ <td style="text-align: center"> __Chrys.__ <td style="text-align: center"> __Price__</td>
<tr>
<td >Small  <td style="text-align: center"> 1 <td style="text-align: center"> 3 <td style="text-align: center"> 3<td style="text-align: center"> \$10</td>
<tr>
<td >Medium <td style="text-align: center">  2<td style="text-align: center">  4<td style="text-align: center"> 6<td style="text-align: center">\$15 </td>
<tr>
<td>Large  <td style="text-align: center">  4<td style="text-align: center">  8<td style="text-align: center">6 <td style="text-align: center"> \$20</td>
<tr >
<td style="border-top: double">Cost   <td style="text-align: center; border-top: double">  \$0.50 <td style="text-align: center; border-top: double"> \$0.25 <td style="text-align: center; border-top: double"> \$0.10 <td style="text-align: center; border-top: double"> </td>
</tr>
</table>


Let $$\A = \pm 1 &3 & 3\\2& 4& 6\\4& 8&6 \mp \quad \bo{p}= \pm 10\\15\\20 \mp \quad \bo{c} = \pm 0.50\\0.25\\0.10 \mp.$$
  <ol type='a'>
    <li> Determine the matrix-vector product that produces a vector, $\y$ which gives the total cost of creating       each size arrangement (small, medium, and large).
    <li> Suppose that an order came in for 2 small arrangements and 2 large arrangements. Let $$\bo{v} = \pm 2\\0\\2 \mp$$
Using matrix arithmetic (and writing out the formula) determine both the price of this order and the total profit to the florist. </ol>


<li> Write the following system of equations as a matrix-vector product $\A\x=\b$:
\begin{eqnarray}
2x_2 +3x_3&=& 8,
2x_1+3x_2+1x_3 &=& 5,
x_1-x_2-2x_3 &=&-5
\end{eqnarray}


<li> A model is being developed to predict a student's SAT score based upon some numeric attributes. The data being used for this model is provided below:

|Observation | PSAT score | Mother's SAT score | SAT Score |
|:----------:|:----------:|:------------------:|:---------:|
|1|1600|1700|1750|
|2|1800|1250|1750|
|3|1750|1300|1600|
|4|1200|1800|1450|
|5|1350|1950|1500|


If our regression model is
$$SAT\_score = \beta_0 + \beta_1* PSAT\_score + \beta_2* Mothers\_SAT\_Score + \epsilon$$
Show how we'd set up the underlying matrix equation for regression analysis,
$$\y = \X\bbeta $$
by defining the matrices/vectors $\X, \bbeta, \mbox{ and } \y$.


<li> Suppose a company collected daily data regarding the sales and revenue of particular products for which prices fluctuate daily:


<table>
  <tr>
    <td></td>
    <td colspan="2" style="text-align: center">Monday</td>
    <td colspan="2" style="text-align: center">Tuesday</td>
    <td colspan="2" style="text-align: center">Wednesday</td>
    <td colspan="2" style="text-align: center">Thursday</td>
    <td colspan="2" style="text-align: center">Friday</td>
  </tr>
  <tr>
    <td>Product</td>
    <td style="text-align: center">Sales</td>
    <td style="text-align: center">Rev.</td>
    <td style="text-align: center">Sales</td>
    <td style="text-align: center">Rev.</td>
    <td style="text-align: center">Sales</td>
    <td style="text-align: center">Rev.</td>
    <td style="text-align: center">Sales</td>
    <td style="text-align: center">Rev.</td>
    <td style="text-align: center">Sales</td>
    <td style="text-align: center">Rev.</td>
  </tr>
  <tr>
  <td>Widgets <td style="text-align: center"> 1 <td style="text-align: center"> 195 <td style="text-align: center"> 5 <td style="text-align: center"> 945 <td style="text-align: center"> 2 <td style="text-align: center"> 400<td style="text-align: center"> 2 <td style="text-align: center"> 450<td style="text-align: center"> 5 <td style="text-align: center"> 790</td>
  </tr>
  <tr>
  <td>Gadgets <td style="text-align: center"> 35 <td style="text-align: center"> 350 <td style="text-align: center"> 13 <td style="text-align: center"> 110 <td style="text-align: center"> 25 <td style="text-align: center"> 300 <td style="text-align: center"> 45 <td style="text-align: center"> 497 <td style="text-align: center"> 90 <td style="text-align: center"> 789 </td>
  </tr>
</table>

<ol style="list-style-type:lower-alpha">
  <li>  Show how you could use matrix addition to compute the total weekly sales and revenue of each product.
  <li>  Now suppose you find out that both the sales and the revenue numbers in the table above were listed in hundreds (i.e. that 100 widgets were sold on Monday, bringing in \$19,500 in revenue). Using your answer from part a. show how you would use scalar multiplication to represent the exact weekly numbers for revenue and units sold.
</ol>
<li> Let 

$$\A=\pm 3 & 2 & 9\\4 & 2 & 3\\2 &7&1\mp \quad$$


<li> For a general matrix $\A_{m\times n}$ describe what the following products will provide.  Also give the size of the result (i.e. "$n\times 1$ vector" or "scalar").
<ol style="list-style-type:lower-alpha">
  <li> $\A\e_j$  
  <li> $\e_i^T\A$  
  <li> $\e_i^T\A\e_j$  
  <li> $\A\e$  
  <li> $\e^T\A$  
  <li>  $\frac{1}{n}\e^T\A$  
</ol>
<li> Let $\bo{D}_{n\times n}$ be a diagonal matrix with diagonal elements $D_{ii}$. What effect does multiplying a matrix $\A_{n\times m}$ on the left by $\bo{D}$ have? What effect does multiplying a matrix $\A_{m\times n}$ on the right by $\bo{D}$ have? If you cannot see this effect in a general sense, try writing out a simple $3\times 3$ matrix as an example first.


</ol>

<!--chapter:end:016-multapp.Rmd-->

# R Programming Basics

Before we get started, you will need to know the basics of matrix manipulation in the R programming language:

- Generally matrices are entered in as one vector, which R then breaks apart into rows and columns in they way that you specify (with nrow/ncol). The default way that R reads a vector into a matrix is down the columns. To read the data in across the rows, use the byrow=TRUE option). This is only relevant if you're entering matrices from scratch.

```{r}
Y=matrix(c(1,2,3,4),nrow=2,ncol=2)
Y
X=matrix(c(1,2,3,4),nrow=2,ncol=2,byrow=TRUE)
X
```

- The standard multiplication symbol, '\*', will unfortunately provide unexpected results if you are looking for matrix multiplication. '\*' will multiply matrices _elementwise_.  In order to do matrix multiplication, the function is '%\*%'.

```{r}
X*X
X%*%X
```

- To transpose a matrix or a vector $\X$, use the function t($\X$).
```{r}
t(X)
```

- R indexes vectors and matrices starting with $i=1$ (as opposed to $i=0$ in python).
- X[i,j] gives element $\X_{ij}$. You can alter individual elements this way.

```{r}
X[2,1]
X[2,1]=100
X
```

- To create a vector of all ones, $\e$, use the ```  rep()``` function

```{r}
e=rep(1,5)
e
```

- To compute the mean of a vector, use the mean function. To compute the column means of a matrix (or data frame), use the ```  colMeans() ```  function. You can also use the ``` apply``` function, which is necessary if you want column standard deviations (```  sd() ``` function). ```  apply(X,dim,function)``` applies the specified function to the specified dimension ```  dim``` (1 for rows, 2 for columns) of the matrix or data frame X.
```{r}
# Start by generating random ~N(0,1) data:
A=replicate(2,rnorm(5))
colMeans(A)
# (Why aren't the means close to zero?)
A=replicate(2,rnorm(100))
colMeans(A)
#LawOfLargeNumbers.

apply(A,2,sd)
# To apply a "homemade function" you must create it as a function
# Here we apply a sum of squares function for the first 5 rows of A:
apply(A[1:5, ],1,function(x) x%*%x)
# Here we center the data by subtracting the mean vector:
B=apply(A,2,function(x) x-mean(x))
colMeans(B)
# R doesn't tell you when things are zero to machine precision. "Machine zero" in
# R is given by the internal variable .Machine$double.eps
colMeans(B) < .Machine$double.eps
```

- To invert a matrix, use the ```  solve()``` command.

```{r}
Xinv=solve(X)
X%*%Xinv
```

- To determine size of a matrix, use the ```  dim()``` function. The result is a vector with two values: ```  dim(x)[1]``` provides the number of rows and ```  dim(x)[2]``` provides the number of columns. You can label rows/columns of a matrix using the ```  rownames()``` or ```  colnames()``` functions.

```{r}
dim(A)
nrows=dim(A)[1]
ncols=dim(A)[2]
colnames(A)=c("This","That")
A[1:5, ]
```

- Most arithmetic functions you apply to a vector act elementwise. In R, $\x^2$ will be a vector containing the square of the elements in $\x$. You can add a column to a matrix (or a data frame) by using the ```  cbind()``` function.

```{r}
# Add a column containing the square of the second column
A=cbind(A,A[ ,2]^2)
colnames(A)
colnames(A)[3]="That Squared"
colnames(A)
```

- You can compute vector norms using the ```  norm() ``` function. Unfortunately, the default norm is _not_ the $2$-norm (it should be!) so we must specify the ``` type="2" ``` as the second argument to the function.

```{r}
x=c(1,1,1)
y=c(1,0,0)
norm(x,type="2")
# It's actually fewer characters to work from the equivalent definition:
sqrt(x%*%x)
norm(y,type="2")
norm(x-y,type="2")
```

You'll learn many additional R techniques throughout this course, but our strategy in this text will be to pick them up as we go as opposed to trying to remember them from the beginning.


<!--chapter:end:018-Intro.Rmd-->

# Solving Systems of Equations {#solvesys}

```{r, echo=F}
thmcounter=0
excounter=0
cid='solvesys'
```



In this section we will learn about solving the systems of equations that were presented in Chapter \@ref(multapp). There are three general situations we may find ourselves in when attempting to solve systems of equations:

1. The system could have one unique solution.
2. The system could have infinitely many solutions (sometimes called _underdetermined_).
3. The system could have no solutions (sometimes called _overdetermined_ or _inconsistent_).


Luckily, no matter what type of system we are dealing with, the method to arriving at the answer (should it exist) is the same. The process is called Gaussian (or Gauss-Jordan) Elimination.

## Gaussian Elimination

Gauss-Jordan Elimination is essentially the same process of elimination you may have used in an Algebra class in primary school. Suppose, for example, we have the following simple system of equations:
$$\begin{cases}\begin{eqnarray}
x_1+2x_2 &=& 11\\
x_1+x_2 &=& 6\end{eqnarray}\end{cases}$$

One simple way to solve this system of equations is to subtract the second equation from the first. By this we mean that we'd perform subtraction on the left hand and right hand sides of the equation:
$$\pm &x_1&+&2x_2 \\ -&(x_1&+&x_2) \\ \hline &&&x_2 \mp = \pm 11 \\-6\\\hline 5 \mp$$
This operation is clearly allowed because the two subtracted quantities are equal (by the very definition of an equation!). What we are left with is one much simpler equation, 
$$x_2=5$$
using this information, we can return to the first equation, substitute and solve for $x_1$:

$$\begin{eqnarray}
x_1+2(5)&=&11 \\
x_1 &=& 1
\end{eqnarray}$$

This final process of substitution is often called **back substitution.** Once we have a sufficient amount of information, we can use that information to substitute and solve for the remainder.

### Row Operations
In the previous example, we demonstrated one operation that can be performed on systems of equations without changing the solution: one equation can be added to a multiple of another (in that example, the multiple was -1). For any system of equations, there are 3 operations which will not change the solution set:

1. Interchanging the order of the equations.
2. Multiplying both sides of one equation by a constant.
3. Replace one equation by a linear combination of itself and of another equation.

Taking our simple system from the previous example, we'll examine these three operations concretely:

$$\begin{cases}\begin{eqnarray}
x_1+2x_2 &=& 11\\
x_1+x_2 &=& 6\end{eqnarray}\end{cases}$$

1. Interchanging the order of the equations.
<table style="width:auto; margin-left: auto; 
  margin-right: auto; border:none;" >
<tr>
<td style="text-align:center; vertical-align:center">
 \begin{cases}\begin{align}
x_1+2x_2 &= 11\\
x_1+x_2 &= 6\end{align}\end{cases}
<td style="text-align:center; vertical-align:center; width:10px">
 $\Leftrightarrow$ 
 <td style="text-align:center; vertical-align:center">
 \begin{cases}\begin{align}
x_1+x_2 =& 6\\
x_1+2x_2 =& 11\end{align}\end{cases}
</tr>
</table>
2. Multiplying both sides of one equation by a constant. _(Multiply the second equation by -1)_.
<table style="width:auto; margin-left: auto; 
  margin-right: auto; border:none;" >
<tr>
<td style="text-align:center; vertical-align:center">
\begin{cases}\begin{align}
x_1+2x_2 &=& 11\\
x_1+x_2 &=& 6\end{align}\end{cases}
<td style="text-align:center; vertical-align:center; width:10px">
 $\Leftrightarrow$ 
 <td style="text-align:center; vertical-align:center">
 \begin{cases}\begin{align}
x_1+2x_2 &=& 11\\
-1x_1-1x_2 &=& -6\end{align}\end{cases}
</tr>
</table>


3. Replace one equation by a linear combination of itself and of another equation. _(Replace the second equation by the first minus the second.)_
<table style="width:auto; margin-left: auto; 
  margin-right: auto; border:none;" >
<tr>
<td style="text-align:center; vertical-align:center">
\begin{cases}\begin{eqnarray}
x_1+2x_2 &=& 11\\
x_1+x_2 &=& 6\end{eqnarray}\end{cases}
<td style="text-align:center; vertical-align:center; width:10px">
 $\Leftrightarrow$ 
 <td style="text-align:center; vertical-align:center">
 \begin{cases}\begin{eqnarray}
x_1+2x_2 &=& 11\\
x_2 &=& 5\end{eqnarray}\end{cases}
</tr>
</table>

Using these 3 row operations, we can transform any system of equations into one that is _triangular_. A **triangular system** is one that can be solved by back substitution. For example,
$$\begin{cases}\begin{align}
x_1+2x_2 +3x_3= 14\\
x_2+x_3 =6\\
x_3 = 1\end{align}\end{cases}$$
is a triangular system. Using substitution, the second equation will give us the value for $x_2$, which will allow for further substitution into the first equation to solve for the value of $x_1$.  Let's take a look at an example of how we can transform any system to a triangular system.

:::{.example name='Transforming a System to a Triangular System via 3 Operations' #rowopeq}

Solve the following system of equations:
$$\begin{cases}\begin{eqnarray}
x_1+x_2 +x_3&=& 1\\
x_1-2x_2+2x_3 &=&4\\
x_1+2x_2-x_3 &=& 2\end{eqnarray}\end{cases}$$
To turn this into a triangular system, we will want to eliminate the variable $x_1$ from two of the equations. We can do this by taking the following operations:

a. Replace equation 2 with (equation 2 - equation 1).
b. Replace equation 3 with (equation 3 - equation 1).

Then, our system becomes:
$$\begin{cases}\begin{eqnarray}
x_1+x_2 +x_3&=& 1\\
-3x_2+x_3 &=&3\\
x_2-2x_3 &=& 1\end{eqnarray}\end{cases}$$

Next, we will want to eliminate the variable $x_2$ from the third equation. We can do this by replacing equation 3 with (equation 3 + $\frac{1}{3}$ equation 2). _However_, we can avoid dealing with fractions if instead we:

c.  Swap equations 2 and 3.

$$\begin{cases}\begin{eqnarray}
x_1+x_2 +x_3 &=& 1\\
x_2-2x_3  &=& 1\\
-3x_2+x_3  &=&3\end{eqnarray}\end{cases}$$

Now, as promised our math is a little simpler: 

d. Replace equation 3 with (equation 3 + 3*equation 2).

$$\begin{cases}\begin{eqnarray}
x_1+x_2 +x_3 &=& 1 \\
x_2-2x_3  &=& 1 \\
-5x_3  &=&6 \end{eqnarray}\end{cases}$$

Now that our system is in triangular form, we can use substitution to solve for all of the variables:
$$x_1 = 3.6 \quad x_2 = -1.4 \quad x_3 = -1.2 $$

This is the procedure for Gaussian Elimination, which we will now formalize in it's matrix version.
:::

### The Augmented Matrix

When solving systems of equations, we will commonly use the **augmented matrix**. 

:::{.definition name='The Augmented Matrix' #augmat}
The **augmented matrix** of a system of equations is simply the matrix which contains all of the coefficients of the equations, augmented with an extra column holding the values on the right hand sides of the equations. If our system is:

$$\begin{cases}\begin{eqnarray}
a_{11}x_1+a_{12}x_2 +a_{13}x_3&=& b_1
a_{21}x_1+a_{22}x_2 +a_{23}x_3&=& b_2
a_{31}x_1+a_{32}x_2 +a_{33}x_3&=&b_3 \end{eqnarray}\end{cases}$$

Then the corresponding augmented matrix is
$$\left(\begin{array}{rrr|r}
a_{11}&a_{12}&a_{13}& b_1\\
a_{21}&a_{22}&a_{23}& b_2\\
a_{31}&a_{12}&a_{33}& b_3\\
\end{array}\right)$$
:::

Using this augmented matrix, we can contain all of the information needed to perform the three operations outlined in the previous section. We will formalize these operations as they pertain to the rows (i.e. individual equations) of the augmented matrix (i.e. the entire system) in the following definition.

:::{.definition name='Row Operations for Gaussian Elimination' #rowops}
Gaussian Elimination is performed on the rows, $\arow{i},$ of an augmented matrix, $$\A = \pm \arow{1}\\\arow{2}\\\arow{3}\\\vdots\\\arow{m}\mp$$ by using the three **elementary row operations**:

1. Swap rows $i$ and $j$.
2. Replace row $i$ by a nonzero multiple of itself.
3. Replace row $i$ by a linear combination of itself plus a multiple of row $j$.


The ultimate goal of Gaussian elimination is to transform an augmented matrix into an **upper-triangular matrix** which allows for backsolving. 
$$\A \rightarrow \left(\begin{array}{rrrr|r}
 t_{11}& t_{12}& \dots& t_{1n}&c_1\cr 
								0& t_{22}& \dots& t_{2n}&c_2\cr
								\vdots& \vdots& \ddots& \vdots&\vdots\cr
								0& 0& \dots& t_{nn}&c_n\end{array}\right)$$
:::

 The key to this process at each step is to focus on one position, called the _pivot position_ or simply the _pivot_, and try to eliminate all terms below this position using the three row operations. Only nonzero numbers are allowed to be pivots. If a coefficient in a pivot position is ever 0, then the rows of the matrix should be interchanged to find a nonzero pivot. If this is not possible then we continue on to the next possible column where a pivot position can be created.

Let's now go through a detailed example of Gaussian elimination using the augmented matrix. We will use the same example (and same row operations) from the previous section to demonstrate the idea.

:::{.example name='Row Operations on the Augmented Matrix' #rowopmat}
We will solve the system of equations from Example \@ref(exm:rowopeq) using the Augmented Matrix.
\begin{equation*}\begin{cases}\begin{align}
x_1+x_2 +x_3= 1\\
x_1-2x_2+2x_3 =4\\
x_1+2x_2-x_3 = 2\end{align}\end{cases}
\end{equation*}

Our first step will be to write the augmented matrix and identify the current pivot. Here, a square is drawn around the pivot and the numbers below the pivot are circled. It is our goal to eliminate the circled numbers using the row with the pivot. 
\begin{equation*}
\left(\begin{array}{rrr|r}
 1 & 1 & 1 & 1\\
 1 & -2 & 2 &4\\
 1&2&-1 &2
\end{array}\right)
 \xrightarrow{Current Pivot}\left(\begin{array}{rrr|r}
 \fbox{1} & 1 & 1 & 1\\
\enclose{circle}[mathcolor="red"]{\color{black}{1}} & -2 & 2 &4\\
 \enclose{circle}[mathcolor="red"]{\color{black}{1}}&2&-1 &2
\end{array}\right)
\end{equation*}
We can eliminate the circled elements by making combinations with those rows and the pivot row. For instance, we'd replace row 2 by the combination (row 2 - row 1). Our shorthand notation for this will be R2' = R2-R1. Similarly we will replace row 3 in the next step.
\begin{equation*}
\xrightarrow{R2'=R2-R1}
  \left(\begin{array}{rrr|r}
 \fbox{1} & 1 & 1 & 1\\
\red{0} & \red{-3} & \red{1} &\red{3}\\
 \enclose{circle}[mathcolor="red"]{\color{black}{1}}&2&-1 &2
\end{array}\right)
 \xrightarrow{R3'=R3-R1} \left(\begin{array}{rrr|r}
 \fbox{1} & 1 & 1 & 1\\
0 & -3 & 1 &3\\
\red{0}&\red{1}&\red{-2}&\red{1}
\end{array}\right)
\end{equation*}
 
 Now that we have eliminated each of the circled elements below the current pivot, we will continue on to the next pivot, which is -3. Looking into the future, we can either do the operation $R3'=R3+\frac{1}{3}R2$ or we can interchange rows 2 and 3 to avoid fractions in our next calculation. To keep things neat, we will do the latter. (_note: either way you proceed will lead you to the same solution!_)
\begin{equation*} \xrightarrow{Next Pivot}
    \left(\begin{array}{rrr|r}
\fbox{1} & 1 & 1 & 1\\
0 & \fbox{-3} & 1 &3\\
0&\enclose{circle}[mathcolor="red"]{\color{black}{1}}&-2&1
\end{array}\right)
 \xrightarrow{R2 \leftrightarrow R3}      \left(\begin{array}{rrr|r}
 \fbox{1} & 1 & 1 & 1\\
 0&\fbox{1}&-2&1\\
0 & \enclose{circle}[mathcolor="red"]{\color{black}{-3}} & 1 &3
\end{array}\right)
\end{equation*}
Now that the current pivot is equal to 1, we can easily eliminate the circled entries below it by replacing rows with combinations using the pivot row. We finish the process once the last pivot is identified (the final pivot has no eliminations to make below it).
\begin{equation*}
 \xrightarrow{R3'=R3+3R2}
     \left(\begin{array}{rrr|r}
 \fbox{1} & 1 & 1 & 1\\
 0&\fbox{1}&-2&1\\
\red{0} & \red{0} & \red{\fbox{-5}} &\red{6}
\end{array}\right)
\end{equation*} 

At this point, when all the pivots have been reached, the augmented matrix is said to be in **row-echelon form**. This simply means that all of the entries below the pivots are equal to 0. The augmented matrix can be transformed back into equation form now that it is in a triangular form:

\begin{equation*}
\begin{cases}\begin{align}
x_1+x_2 +x_3= 1\\
x_2-2x_3 = 1\\
5x_3 =-6\end{align}\end{cases}
\end{equation*}
Which is the same system we finally solved in Example \@ref(exm:rowopeq) to get the final solution:
$$x_1 = 3.6 \quad x_2 = -1.4 \quad x_3 = -1.2 $$
:::
 
 
 
### Gaussian Elimination Summary

 Let's summarize the process of Gaussian elimination step-by-step:
<ol>
<li> We work from the upper-left-hand corner of the matrix to the lower-right-hand corner
<li> Focusing on the first column, identify the first pivot element. The first pivot element should be located in the first row (if this entry is zero, we must interchange rows so that it is non-zero). 
<li> Eliminate (zero-out) all elements below the pivot using the combination row operation.
<li> Determine the next pivot and go back to step 2.
<ul>
<li> Only nonzero numbers are allowed to be pivots. 
<li> If a coefficient in the next pivot position is 0, then the rows of the matrix should be interchanged to find a nonzero pivot. 
<li> If this is not possible then we continue on to the next column to determine a pivot.
</ul>
<li> When the entries below all of the pivots are equal to zero, the process stops. The augmented matrix is said to be in _row-echelon form_, which corresponds to a _triangular_ system of equations, suitable to solve using back substitution.
</ol>

:::{.exercise name='Gaussian Elimination and Back Substitution' #gaussianelim}
Use Gaussian Elimination and back substitution to solve the following system.
$$\begin{cases}\begin{align}
2x_1-x_2=1\\
-x_1+2x_2-x_3=0\\
-x_2+x_3=0\end{align}\end{cases}$$
\vspace{3cm}
:::
 
 

## Gauss-Jordan Elimination

 Gauss-Jordan elimination is Gaussian elimination taken one step further. In Gauss-Jordan elimination, we do not stop when the augmented matrix is in row-echelon form. Instead, we force all the pivot elements to equal 1 and we continue to eliminate entries _above_ the pivot elements to reach what's called **reduced row echelon form**.
 
 Let's take a look at another example:

:::{.example name='Gauss-Jordan elimination' #gaussjordan}
 We begin with a system of equations, and transform it into an augmented matrix:
 $$\begin{cases}\begin{align}
x_2 -x_3= 3\\
-2x_1+4x_2-x_3 = 1\\
-2x_1+5x_2-4x_3 =-2\end{align}\end{cases}
 \Longrightarrow \left(\begin{array}{rrr|r}0&1&-1&3\\-2&4&-1&1\\-2&5&-4&-2\end{array}\right) $$ 
 
We start by locating our first pivot element. This element cannot be zero, so we will have to swap rows to bring a non-zero element to the pivot position. 
$$\left(\begin{array}{rrr|r}0&1&-1&3\\-2&4&-1&1\\-2&5&-4&-2\end{array}\right) \xrightarrow{R1\leftrightarrow R2}
\left(\begin{array}{rrr|r}\fbox{-2}&4&-1&1\\0&1&-1&3\\-2&5&-4&-2\end{array}\right)$$
Now that we have a non-zero pivot, we will want to do two things: 
\begin{itemize}
\item Use the pivot to eliminate all of the elements below it (as with Gaussian elimination) \item Make the pivot element equal to 1.
\end{itemize}
It does not matter what order we perform these two tasks in. Here, we will have an easy time eliminating using the -2 pivot:
$$\left(\begin{array}{rrr|r}\fbox{-2}&4&-1&1\\0&1&-1&3\\-2&5&-4&-2\end{array}\right)\xrightarrow{R3'=R3-R1} \left(\begin{array}{rrr|r}\fbox{-2}&4&-1&1\\0&1&-1&3\\\red{0}&\red{1}&\red{-3}&\red{-3}\end{array}\right)$$
Now, as promised, we will make our pivot equal to 1.
$$\left(\begin{array}{rrr|r}\fbox{-2}&4&-1&1\\0&1&-1&3\\0&1&-3&-3\end{array}\right) \xrightarrow{R1'=-\frac{1}{2} R1} \left(\begin{array}{rrr|r}\red{\fbox{1}}&\red{-2}&\red{\frac{1}{2}}&\red{-\frac{1}{2}}\\0&1&-1&3\\0&1&-3&-3\end{array}\right)$$
We have finished our work with this pivot, and now we move on to the next one. Since it is already equal to 1, the only thing left to do is use it to eliminate the entries below it:
$$\left(\begin{array}{rrr|r}1&-2&\frac{1}{2}&-\frac{1}{2}\\0&\fbox{1}&-1&3\\0&1&-3&-3\end{array}\right)\xrightarrow{R3'=R3-R2} \left(\begin{array}{rrr|r}1&-2&\frac{1}{2}&-\frac{1}{2}\\0&\fbox{1}&-1&3\\\red{0}&\red{0}&\red{-2}&\red{-6}\end{array}\right)$$
And then we move onto our last pivot. This pivot has no entries below it to eliminate, so all we must do is turn it into a 1:
$$\left(\begin{array}{rrr|r}1&-2&\frac{1}{2}&\frac{-1}{2}\\0&1&-1&3\\0&0&\fbox{-2}&-6\end{array}\right)\xrightarrow{R3'=-\frac{1}{2}R3}\left(\begin{array}{rrr|r}1&-2&\frac{1}{2}&-\frac{1}{2}\\0&1&-1&3\\\red{0}&\red{0}&\red{\fbox{1}}&\red{3}\end{array}\right) $$
Now, what really differentiates Gauss-Jordan elimination from Gaussian elimination is the next few steps. Here, our goal will be to use the pivots to eliminate all of the entries _above_ them. While this takes a little extra work, as we will see, it helps us avoid the tedious work of back substitution.

We'll start at the southeast corner on the current pivot. We will use that pivot to eliminate the elements above it:

$$\left(\begin{array}{rrr|r} 1&-2&\frac{1}{2}&-\frac{1}{2}\\0&1&-1&3\\0&0&\fbox{1}&3\end{array}\right) \xrightarrow{R2'=R2+R3} \left(\begin{array}{rrr|r} 1&-2&\frac{1}{2}&-\frac{1}{2}\\\red{0}&\red{1}&\red{0}&\red{6}\\0&0&\fbox{1}&3\end{array}\right)$$

$$ \left(\begin{array}{rrr|r} 1&-2&\frac{1}{2}&-\frac{1}{2}\\0&1&0&6\\0&0&\fbox{1}&3\end{array}\right)\xrightarrow{R1'=R1-\frac{1}{2}R3}\left(\begin{array}{rrr|r} \red{1}&\red{-2}&\red{0}&\red{-2}\\0&1&0&6\\0&0&\fbox{1}&3\end{array}\right)$$
We're almost done! One more pivot with elements above it to be eliminated:

$$\left(\begin{array}{rrr|r} 1&-2&0&-2\\0&\fbox{1}&0&6\\0&0&1&3\end{array}\right) \xrightarrow{R1'=R1+2R2}
\left(\begin{array}{rrr|r}\red{1}&\red{0}&\red{0}&\red{10}\\0&\fbox{1}&0&6\\0&0&1&3\end{array}\right)$$

And we've reached **reduced row echelon form**. How does this help us? Well, let's transform back to a system of equations:
 $$\begin{cases}\begin{align}
x_1 = 10\\
x_2= 6\\
x_3 =3\end{align}\end{cases}$$
The solution is simply what's left in the right hand column of the augmented matrix.
:::

As you can see, the steps to performing Gaussian elimination and Gauss-Jordan elimination are very similar. Gauss-Jordan elimination is merely an extension of Gaussian elimination which brings the problem as close to completion as possible. 




### Gauss-Jordan Elimination Summary


1. Focusing on the first column, identify the first pivot element. The first pivot element should be located in the first row (if this entry is zero, we must interchange rows so that it is non-zero). Our goal will be to use this element to eliminate all of the elements below it.  
2. The pivot element should be equal to 1. If it is not, we simply multiply the row by a constant to make it equal 1 (or interchange rows, if possible).
3. Eliminate (zero-out) all elements below the pivot using the combination row operation.
4. Determine the next pivot and go back to step 2.
  - Only nonzero numbers are allowed to be pivots. If a coefficient in a pivot position is ever 0, then the rows of the matrix should be interchanged to find a nonzero pivot. If this is not possible then we continue on to the next possible column where a pivot position can be created.
5. When the last pivot is equal to 1, begin to eliminate all the entries above the pivot positions.
6. When all entries above and below each pivot element are equal to zero, the augmented matrix is said to be in _reduced row echelon form_ and the Gauss-Jordan elimination process is complete.

:::{.exercise name='Gauss-Jordan Elimination' #gaussjordanexer}
Use the Gauss-Jordan method to solve the following system:
$$\begin{cases}\begin{align}
4x_2-3x_3=3\\
-x_1+7x_2-5x_3=4\\
-x_1+8x_2-6x_3=5\end{align}\end{cases}$$

:::


## Three Types of Systems

 As was mentioned earlier, there are 3 situations that may arise when solving a system of equations:

-  The system could have one __unique solution__ (this is the situation of our examples thus far).
-  The system could have no solutions (sometimes called _overdetermined_ or ___inconsistent___).
-  The system could have __infinitely many solutions__ (sometimes called _underdetermined_).


### The Unique Solution Case {#uniquesol}

Based on our earlier examples, we already have a sense for systems which fall into the first case.

(ref:case1title) Case 1: Unique solution

:::{.theorem name='(ref:case1title)' #case1}
A system of equations $\A\x=\b$ has a unique solution if and only if _both_ of the following conditions hold:

1. The number of equations is equal to the number of variables (i.e. the coefficient matrix $\A$ is _square_).
2. The number of pivots is equal to the number of rows/columns. In other words, under Gauss-Jordan elimination, the coefficient matrix is transformed into the identity matrix:
$$\A \xrightarrow{Gauss-Jordan} I$$

In this case, we say that the matrix $\A$ is **invertible** because it is full-rank (the rank of a matrix is the number of pivots after Gauss-Jordan elimination) _and_ square. 
:::

### The Inconsistent Case {#inconsistent}

The second case scenario is a very specific one. In order for a system of equations to be **inconsistent** and have no solutions, it must be that after Gaussian elimination, a situation occurs where at least one equation reduces to $0=\alpha$ where $\alpha$ is  nonzero. Such a situation would look as follows (using asterisks to denote any nonzero numbers):

$$\left(\begin{array}{rrr|r} *&*&*&*\\0&*&*&*\\0&0&0&\alpha\end{array}\right) $$

The third row of this augmented system indicates that $$0x_1+0x_2+0x_3=\alpha$$ where $\alpha\neq 0$, which is a contradiction. When we reach such a situation through Gauss-Jordan elimination, we know the system is inconsistent.

:::{.example name='Identifying an Inconsistent System' #inconsistent}
$$\begin{cases}\begin{align}
x-y+z=1\\
x-y-z=2\\
x+y-z=3\\
x+y+z=4\end{align}\end{cases}$$
Using the augmented matrix and Gaussian elimination, we take the following steps:

$$\left(\begin{array}{rrr|r} 1&-1&1&1\\1&-1&-1&2\\1&1&-1&3\\1&1&1&4\end{array}\right) \xrightarrow{\substack{R2'=R2-R1 \\ R3'=R3-R1 \\ R4'=R4-R1}} \left(\begin{array}{rrr|r} 1&-1&1&1\\0&0&-2&1\\0&2&-2&2\\0&2&0&3\end{array}\right) $$
$$\xrightarrow{ R4\leftrightarrow R2}\left(\begin{array}{rrr|r} 1&-1&1&1\\0&2&0&3\\0&2&-2&2\\0&0&-2&1\end{array}\right)\xrightarrow{R3'=R3-R2} \left(\begin{array}{rrr|r} 1&-1&1&1\\0&2&0&3\\0&0&-2&-1\\0&0&-2&1\end{array}\right)$$
$$\xrightarrow{R4'=R4-R3} \left(\begin{array}{rrr|r} 1&-1&1&1\\0&2&0&3\\0&0&-2&-1\\0&0&0&2\end{array}\right)$$

In this final step, we see our contradiction equation, $0=2$. Since this is obviously impossible, we conclude that the system is inconsistent.
:::

Sometimes inconsistent systems are referred to as _over-determined_. In this example, you can see that we had more equations than variables. This is a common characteristic of over-determined or inconsistent systems. You can think of it as holding too many demands for a small set of variables! In fact, this is precisely the situation in which we find ourselves when we approach linear regression. Regression systems do not have an exact solution: there are generally no set of $\beta_i's$ that we can find so that our regression equation exactly fits every observation in the dataset - the regression system is inconsistent. Thus, we need a way to get _as close as possible_ to a solution; that is, we need to find a solution that minimizes the residual error. This is done using the Least Squares method, the subject of Chapter \@ref(leastsquares).

### The Infinite Solutions Case {#infinitesol}

For the third case, consider the following system of equations written as an augmented matrix, and its reduced row echelon form after Gauss-Jordan elimination. As an exercise, it is suggested that you confirm this result.

$$\left(\begin{array}{rrr|r} 1&2&3&0\\2&1&3&0\\1&1&2&0\end{array}\right) \xrightarrow{Gauss-Jordan} \left(\begin{array}{rrr|r} 1&0&1&0\\0&1&1&0\\0&0&0&0\end{array}\right) $$

There are several things you should notice about this reduced row echelon form. For starters, it has a row that is completely 0. This means, intuitively, that one of the equations was able to be completely eliminated - it contained redundant information from the first two. The second thing you might notice is that there are only 2 pivot elements. Because there is no pivot in the third row, the last entries in the third column could not be eliminated! This is characteristic of what is called a **free-variable**. Let's see what this means by translating our reduced system back to equations:
 $$\begin{cases}\begin{align}
x_1+x_3 = 0\\
x_2+x_3= 0\end{align}\end{cases}$$
Clearly, our answer to this problem depends on the variable $x_3$, which is considered _free_ to take on any value. Once we know the value of $x_3$ we can easily determine that
\begin{align}
x_1 &= -x_3 \\
x_2 &= -x_3 \end{align}

Our convention here is to __parameterize__ the solution and simply declare that $x_3=s$ (or any other placeholder variable for a constant). Then our solution becomes:
$$\pm x_1\\x_2\\x_3 \mp = \pm -s \\ -s \\ s \mp = s \pm -1\\-1\\1 \mp$$
What this means is that any scalar multiple of the vector $\pm -1\\-1\\1 \mp$ is a solution to the system. Thus there are infinitely many solutions!



(ref:case3title) Case 3: Infinitely Many Solutions

:::{.theorem name='(ref:case3title)' #case3}
A system of equations $\A\x=\b$ has infinitely many solutions if the system is consistent and _any_ of the following conditions hold:

1. The number of variables is greater than the number of equations.
2. There is at least one _free variable_ presented in the reduced row echelon form.
3. The number of pivots is less than the number of variables.
:::


:::{.example name='Infinitely Many Solutions' #infsolutions}
For the following reduced system of equations, characterize the set of solutions in the same fashion as the previous example. 
$$\left(\begin{array}{rrrr|r}
 1&0&1&2&0\\0&1&1&-1&0\\0&0&0&0&0\\0&0&0&0&0\end{array}\right) $$
A good way to start is sometimes to write out the corresponding equations:
$$\begin{cases}\begin{align}
x_1+x_3+2x_4 = 0\\
x_2+x_3-x_4= 0\end{align}\end{cases} \Longrightarrow \systeme{
x_1=-x_3-2x_4\\
x_2=-x_3+x_4\end{align}\end{cases}$$

Now we have _two_ variables which are free to take on any value. Thus, let 
$$x_3 = s \quad \mbox{and} \quad x_4 = t$$
Then, our solution is:
$$\pm x_1\\x_2\\x_3\\x_4 \mp = \pm -s-2t \\ -s+t\\s\\t \mp = s\pm -1\\-1\\1\\0 \mp + t\pm -2\\1\\0\\1 \mp$$
so any linear combination of the vectors 
$$\pm -1\\-1\\1\\0 \mp \quad \mbox{and} \quad \pm -2\\1\\0\\1 \mp$$
will provide a solution to this system.
:::


### Matrix Rank

The **rank** of a matrix is the number of linearly independent rows or columns in the matrix (the number of linearly independent rows will always be the same as the number of linearly independent columns). It can be determined by reducing a matrix to row-echelon form and counting the number of pivots. A matrix is said to be **full rank** when its rank is maximal, meaning that either all rows or all columns are linearly independent. In other words, an $m\times n$ matrix $\A$ is full rank when the rank($\A$)$=\min(m,n)$. A square matrix that is full rank will always have an inverse. 


## Solving Matrix Equations

One final piece to the puzzle is what happens when we have a matrix equation like 
$$\A\X=\B$$
This situation is an easy extension of our previous problem because we are essentially solving the same system of equation with several different right-hand-side vectors (the columns of $\B$).

Let's look at a $2\times 2$ example to get a feel for this! We'll dissect the following matrix equation into two different systems of equations:

$$\pm 1&1\\2&1\mp \pm x_{11} & x_{12} \\ x_{21} & x_{22} \mp = \pm 3&3\\4&5 \mp.$$

Based on our previous discussions, we ought to be able to see that this matrix equation represents 4 separate equations which we'll combine into two systems:

$$\pm  1&1\\2&1\mp \pm x_{11} \\x_{21} \mp = \pm 3\\4 \mp \quad \mbox{and}\quad \pm  1&1\\2&1\mp \pm x_{12} \\x_{22} \mp = \pm 3\\5 \mp$$

Once you convince yourself that the unknowns can be found in this way, let's take a look at the augmented matrices for these two systems:

$$\left(\begin{array}{rr|r}
 1&1&3\\2&1&4\end{array}\right) \quad\mbox{and}\quad \left(\begin{array}{rr|r}
 1&1&3\\2&1&5\end{array}\right)$$

When performing Gauss-Jordan elimination on these two augmented matrices, how are the row operations going to differ? They're not! The same row operations will be used for each augmented matrix - the only thing that will differ is how these row operations will affect the right hand side vectors. Thus, it is possible for us to keep track of those differences in one larger augmented matrix :

$$\begin{pmatrix}
\begin{array}{cc|cc}
1&1&3&3\\
2&1&4&5
\end{array}
\end{pmatrix}$$

We can then perform the row operations on both right-hand sides at once:

$$\begin{pmatrix}
\begin{array}{cc|cc}
1&1&3&3\\
2&1&4&5
\end{array}
\end{pmatrix}\xrightarrow{R2'=R2-2R1}\begin{pmatrix}
\begin{array}{cc|cc}
1&1&3&3\\
0&-1&-2&-1
\end{array}
\end{pmatrix} $$
$$\xrightarrow{R2'=-1R2}\begin{pmatrix}
\begin{array}{cc|cc}
1&1&3&3\\
0&1&2&1
\end{array}
\end{pmatrix}\xrightarrow{R1'=R1-R2}\begin{pmatrix}
\begin{array}{cc|cc}
1&0&1&2\\
0&1&2&1
\end{array}
\end{pmatrix}$$

Now again, remembering the situation from which we came, we have the equivalent system:
$$\pm 1&0\\0&1 \mp \pm x_{11} & x_{12} \\ x_{21} & x_{22} \mp = \pm 1&2\\2&1\mp$$

So we can conclude that $$\pm x_{11} & x_{12} \\ x_{21} & x_{22} \mp = \pm 1&2\\2&1\mp$$ and we have solved our system. This method is particularly useful when finding the inverse of a matrix. 

### Solving for the Inverse of a Matrix

For any square matrix $\A$, we know the inverse matrix ($\A^{-1}$), if it exists, satisfies the following matrix equation,
$$\A\A^{-1} = \I.$$

Using the Gauss-Jordan method with multiple right hand sides, we can solve for the inverse of any matrix. We simply start with an augmented matrix with $\A$ on the left and the identity on the right, and then use Gauss-Jordan elimination to transform the matrix $\A$ into the identity matrix.
$$\left(\begin{array}{r|r}
 \bo{A} & \I\end{array}\right)\xrightarrow{Gauss-Jordan}\left(\begin{array}{r|r} \bo{I} & \A^{-1}\end{array}\right)$$
 If this is possible then the matrix on the right is the inverse of $\A$. If this is not possible then $\A$ does not have an inverse. Let's see a quick example of this.
 
:::{.example name='Finding a Matrix Inverse' #findinverse}
  Find the inverse of $$\A = \pm -1&2&-1\\0&-1&1\\2&-1&0 \mp$$ using Gauss-Jordan Elimination.
 
 Since $\A\A^{-1} = \I$, we set up the augmented matrix as $\left(\begin{array}{r|r} \bo{A} & \I\end{array}\right)$:
 
 $$\begin{pmatrix}
\begin{array}{ccc|ccc}-1&2&-1&1&0&0\\0&-1&1&0&1&0\\2&-1&0&0&0&1 \end{array}\end{pmatrix} \xrightarrow{R3'=R3+2R1}
\begin{pmatrix}
\begin{array}{ccc|ccc} -1&2&-1&1&0&0\\0&-1&1&0&1&0\\0&3&-2&2&0&1 \end{array}\end{pmatrix}$$
 
 $$\begin{pmatrix}
\begin{array}{ccc|ccc} -1&2&-1&1&0&0\\0&-1&1&0&1&0\\0&3&-2&2&0&1 \end{array}\end{pmatrix}
\xrightarrow{\substack{R1'=-1R1\\R3'=R3+3R2}}\begin{pmatrix}\begin{array}{ccc|ccc} 1&-2&1&-1&0&0\\0&-1&1&0&1&0\\0&0&1&2&3&1 \end{array}\end{pmatrix}$$
$$\begin{pmatrix}\begin{array}{ccc|ccc} 1&-2&1&-1&0&0\\0&-1&1&0&1&0\\0&0&1&2&3&1 \end{array}\end{pmatrix}\xrightarrow{\substack{R1'=R1-R3\\R2'=R2-R3}}\begin{pmatrix}\begin{array}{ccc|ccc} 1&-2&0&-3&-3&-1\\0&-1&0&-2&-2&-1\\0&0&1&2&3&1 \end{array}\end{pmatrix}$$
$$\begin{pmatrix}\begin{array}{ccc|ccc} 1&-2&0&-3&-3&-1\\0&-1&0&-2&-2&-1\\0&0&1&2&3&1 \end{array}\end{pmatrix}\xrightarrow{\substack{R2'=-1R2\\R1'=R1+2R2}}\begin{pmatrix}\begin{array}{ccc|ccc} 1&0&0&1&1&1\\0&1&0&2&2&1\\0&0&1&2&3&1 \end{array}\end{pmatrix}$$

Finally, we have completed our task. The inverse of $\A$ is the matrix on the right hand side of the augmented matrix!
$$\A^{-1} = \pm 1&1&1\\2&2&1\\2&3&1 \mp$$
:::


:::{.exercise name='Finding a Matrix Inverse' #inverseexer}
Use the same method to determine the inverse of
$$\B=\pm 1&1&1\\2&2&1\\2&3&1 \mp$$
(_hint:  Example \@ref(exm:findinverse) should tell you the answer you expect to find!_)
:::


:::{.example name='Inverse of a Diagonal Matrix' #diaginverse}
A full rank diagonal matrix (one with no zero diagonal elements) has a particularly neat and tidy inverse. Here we motivate the definition by working through an example. Find the inverse of the digaonal matrix $\D$,
$$\D = \pm 3&0&0\\0&-2&0\\0&0&\sqrt{5} \mp $$
To begin the process, we start with an augmented matrix and proceed with Gauss-Jordan Elimination. In this case, the process is quite simple! The elements above and below the diagonal pivots are already zero, we simply need to make each pivot equal to 1!

$$\pm\begin{array}{ccc|ccc} 3&0&0&1&0&0\\0&-2&0&0&1&0\\0&0&\sqrt{5}&0&0&1 \end{array}\mp
\xrightarrow{\substack{R1'=\frac{1}{3}R1 \\R2' = -\frac{1}{2} R2\\R3'=\frac{1}{\sqrt{5}} R3}}
\pm\begin{array}{ccc|ccc} 1&0&0&\frac{1}{3}&0&0\\0&1&0&0&-\frac{1}{2}&0\\0&0&1&0&0&\frac{1}{\sqrt{5}} \end{array}\mp$$

Thus, the inverse of $\D$ is:
$$\D^{-1} = \pm \frac{1}{3}&0&0\\0&-\frac{1}{2}&0\\0&0&\frac{1}{\sqrt{5}} \mp $$

As you can see, all we had to do is take the scalar inverse of each diagonal element! 
:::


:::{.definition name='Inverse of a Diagonal Matrix' #diaginverse}
An $n\times n$ diagonal matrix $\D = diag\{d_{11},d_{22},\dots,d_{nn}\}$ with no nonzero diagonal elements is invertible with inverse
$$\D^{-1} = diag\{\frac{1}{d_{11}},\frac{1}{d_{22}},\dots,\frac{1}{d_{nn}}\}$$
:::



## Exercises
<ol>
<li> Using Gaussian Elimination on the augmented matrices, reduce each system of equations to a triangular form and solve using back-substitution.
<ol style="list-style-type:lower-alpha">
 <li>$$\begin{cases}
x_1 +2x_2= 3\\
-x_1+x_2=0\end{cases}$$
  <li>$$\begin{cases}
x_1+x_2 +2x_3= 7\\
x_1+x_3 = 4\\
-2x_1-2x_2 =-6\end{cases}$$
  <li>$$\begin{cases}\begin{align}
2x_1-x_2 +x_3= 1\\
-x_1+2x_2+3x_3 = 6\\
x_2+4x_3 =6 \end{align}\end{cases}$$
</ol>

<li> Using Gauss-Jordan Elimination on the augmented matrices, reduce each system of equations from the previous exercise to reduced row-echelon form and give the solution as a vector.

<li> Use either Gaussian or Gauss-Jordan Elimination to solve the following systems of equations. Indicate whether the systems have a unique solution, no solution, or infinitely many solutions. If the system has infinitely many solutions, exhibit a general solution in vector form as we did in Section \@ref(infinitesol).
<ol style="list-style-type:lower-alpha">
  <li> $$\begin{cases}\begin{align}
		2x_1+2x_2+6x_3=4\\
		2x_1+x_2+7x_3=6\\
		-2x_1-6x_2-7x_3=-1\end{align}\end{cases}$$
  <li> $$\begin{cases}\begin{align}
		1x_1+2x_2+2x_3=0\\
		2x_1+5x_2+7x_3=0\\
		3x_1+6x_2+6x_3=0\end{align}\end{cases}$$	
  <li> $$\begin{cases}\begin{align}
		1x_1+3x_2-5x_3=0\\
		1x_1-2x_2+4x_3=2\\
		2x_1+1x_2-1x_3=0\end{align}\end{cases}$$	
  <li> $$\begin{cases}\begin{align}
w-h+l=1\\
w-h-l=2\\
w+h-l=3\\
w+h+l=4\end{align}\end{cases}$$
  <li> $$\begin{cases}\begin{align}
w-h+l=1\\
w-h-l=2\\
w+h-l=3\\
w+h+l=2\end{align}\end{cases}$$
<li> $$\begin{cases}\begin{align}
x_1+2x_2+2x_3+3x_4=0\\
2x_1+4x_2+x_3+3x_4=0\\
3x_1+6x_2+x_3+4x_4=0\end{align}\end{cases}$$
</ol>

<li> Use Gauss-Jordan Elimination to find the inverse of the following matrices, if possible.
<ol style="list-style-type:lower-alpha">
<li> $\A=\pm 2&3\\2&2\mp$
<li> $\B=\pm 1&2\\2&4\mp$
<li> $\C=\pm 1&2&3\\4&5&6\\7&8&9\mp$
<li> $\D=\pm 4&0&0\\0&-4&0\\0&0&2 \mp$
</ol>

<li> What is the inverse of a diagonal matrix, $\bo{D}=diag\{\sigma_{1},\sigma_{2},
\dots,\sigma_{n}\}$?

<li> Suppose you have a matrix of data, $\A_{n\times p}$, containing $n$ observations on $p$ variables. Suppose the standard deviations of these variables are contained in a diagonal matrix
$$\bo{S}= diag\{\sigma_1, \sigma_2,\dots,\sigma_p\}.$$ Give a formula for a matrix that contains the same data but with each variable divided by its standard deviation. _Hint: This problem connects Text Exercise \@ref(exr:diagmultexer) and Example \@ref(exm:diaginverse)_. 
</ol>

## List of Key Terms

- systems of equations
- row operations
- row-echelon form
- pivot element
- Gaussian elimination
- Gauss-Jordan elimination
- reduced row-echelon form
- rank
- unique solution
- infinitely many solutions
- inconsistent
- back-substitution


<!--chapter:end:0189-GaussJordan.Rmd-->

## Gauss-Jordan Elimination in R

It is important that you understand what is happening in the process of Gauss-Jordan Elimination. Once you have a handle on how the procedure works, it is no longer necessary to do every calculation by hand. We can skip to the reduced row echelon form of a matrix using the ```pracma``` package in R. \\

We'll start by creating our matrix as a variable in R.  Matrices are entered in as one vector, which R then breaks apart into rows and columns in they way that you specify (with nrow/ncol). The default way that R reads a vector into a matrix is down the columns. To read the data in across the rows, use the byrow=TRUE option). Once a matrix is created, it is stored under the variable name you give it (below, we call our matrices $\Y$ and $\X$). We can then print out the stored matrix by simply typing $\Y$ or $\X$ at the prompt:

```{r}
(Y=matrix(c(1,2,3,4),nrow=2,ncol=2))
(X=matrix(c(1,2,3,4),nrow=2,ncol=2,byrow=TRUE))
```

To perform Gauss-Jordan elimination, we need to install the ```pracma``` package which contains the code for this procedure. 

```{r eval=F}
install.packages("pracma")
```

After installing a package in R, you must always add it to your library (so that you can actually use it in the current session). This is done with the library command:

```{r}
library("pracma")
```

Now that the library is accessible, we can use the \textbf{rref} command to get the reduced row echelon form of an augmented matrix, $\A$:
```{r}
A= matrix(c(1,1,1,1,-1,-1,1,1,1,-1,-1,1,1,2,3,4), nrow=4, ncol=4)
A
rref(A)
```
And we have the reduced row echelon form for one of the problems from the worksheets! You can see this system of equations is inconsistent because the bottom row amounts to the equation
$$0\x_1+0\x_2+0\x_3 = 1.$$
This should save you some time and energy by skipping the arithmetic steps in Gauss-Jordan Elimination.
  
  
  
  


<!--chapter:end:01899-GaussJordanInR.Rmd-->

# Norms, Similarity, and Distance  {#norms}

## Norms and Distances {#sec-norms}

In applied mathematics, Norms are functions which measure the magnitude or length of a vector. They are commonly used to determine similarities between observations by measuring the distance between them. As we will see, there are many ways to define distance between two points. 

:::{.definition name='Vector Norms and Distance Metrics' #normdef}
A Norm, or distance metric, is a function that takes a vector as input and returns a scalar quantity ($f: \Re^n \to \Re$). A vector norm is typically denoted by two vertical bars surrounding the input vector, $\|\bo{x}\|$, to signify that it is not just any function, but one that satisfies the following criteria:

1.  If $c$ is a scalar, then $$\|c\x\|=|c|\|x\|$$\
2. The triangle inequality: $$\|\x+\bo{y}\| \leq \|\x\|+\|\bo{y}\|$$\
3. $\|\x\|=0$ if and only if $\x=0$.\
4. $\|\x\|\geq 0$ for any vector $\x$\

:::

We will not spend any time on these axioms or on the theoretical aspects of norms, but we will put a couple of these functions to good use in our studies, the first of which is the __Euclidean norm__ or __2-norm__.

(ref:norm2title) Euclidean Norm, $\|\star\|_2$

:::{.definition name='(ref:norm2title)' #twonormdef}
The __Euclidean Norm__, also known as the __2-norm__ simply measures the Euclidean length of a vector (i.e. a point's distance from the origin). Let $\x = (x_1,x_2,\dots,x_n)$. Then,
$$\|\x\|_2 = \sqrt{x_1^2 + x_2^2 + \dots + x_n^2} $$
If $\x$ is a column vector, then $$\|\x\|_2= \sqrt{\x^T\x}.$$
Often we will simply write $\|\star\|$ rather than $\|\star\|_2$ to denote the $2$-norm, as it is by far the most commonly used norm.
:::

This is merely the distance formula from undergraduate mathematics, measuring the distance between the point $\x$ and the origin. To compute the distance between two different points, say $\x$ and $\y$, we'd calculate 
$$\|\x-\y\|_2= \sqrt{(x_1-y_1)^2 + (x_2-y_2)^2 + \dots + (x_n-y_n)^2}$$

:::{.example name='Euclidean Norm and Distance' #twonorm}
Suppose I have two vectors in $3$-space: 
$$\x=(1,1,1) \mbox{   and   } \bo{y}=(1,0,0)$$
Then the magnitude of $\x$ (i.e. its length or distance from the origin) is
$$\|\x\|_2=\sqrt{1^2+1^2+1^2}=\sqrt{3}$$
and the magnitude of $\bo{y}$ is
$$\|\bo{y}\|_2=\sqrt{1^2+0^2+0^2}=1$$
and the distance between point $\x$ and point $\bo{y}$ is
$$ \|\x-\bo{y}\|_2=\sqrt{(1-1)^2 + (1-0)^2 + (1-0)^2} =\sqrt{2}.$$
The Euclidean norm is crucial to many methods in data analysis as it measures the closeness of two data points.
:::

Thus, to turn any vector into a __unit vector__, a vector with a length of 1, we need only to divide each of the entries in the vector by its Euclidean norm. This is a simple form of standardization used in many areas of data analysis.  For a unit vector $\x$, $\x^T\x=1$. 

Perhaps without knowing it, we've already seen many formulas involving the norm of a vector. Examples \@ref(exm:sdnorm) and \@ref(exm:lsreg) show how some of the most important concepts in statistics can be represented using vector norms.

:::{.example name='Standard Deviation and Variance' #sdnorm}
Suppose a group of individuals has the following heights, measured in inches: (60, 70, 65, 50, 55). The mean height for this group is 60 inches. The formula for the __sample standard deviation__ is typically given as
$$s = \frac{\sqrt{\sum_{i=1}^n (x_i-\bar{x})^2}}{\sqrt{n-1}}$$ 

We want to subtract the mean from each observation, square the numbers, sum the result, take the square root and divide by $\sqrt{n-1}$.

If we let $\bar{\x}=\bar{x}\e=(60,60,60,60,60)$ be a vector containing the mean, and $\x=(60, 70, 65, 50, 55)$ be the vector of data then the standard deviation in matrix notation is:
$$s=\frac{1}{\sqrt{n-1}}\|\x-\bar{\x}\|_2=7.9$$

The __sample variance__ of this data is merely the square of the sample standard deviation:
$$s^2 = \frac{1}{n-1}\|\x-\bar{\x}\|_2^2$$
:::

:::{.example name='Residual Sums of Squares' #lsreg}
Another place we've seen a similar calculation is in linear regression. You'll recall the objective of our regression line is to minimize the sum of squared residuals between the predicted value $\hat{y}$ and the observed value $y$:
$$\sum_{i=1}^n (\hat{y}_i-y_i )^2.$$
In vector notation, we'd let $\y$ be a vector containing the observed data and $\hat{\y}$ be a vector containing the corresponding predictions and write this summation as
$$\|\hat{\y}-\y\|_2^2$$

In fact, any situation where the phrase "sum of squares" is encountered, the $2$-norm is generally implicated.
:::

(ref:rsquaredtitle) Coefficient of Determination, $R^2$

:::{.example name='(ref:rsquaredtitle)' #rsquared}
Since variance can be expressed using the Euclidean norm, so can the __coefficient of determination__ or $R^2$. 
$$R^2 = \frac{SS_{reg}}{SS_{tot}}= \frac{\sum_{i=1}^n (\hat{y_i}-\bar{y})^2}{\sum_{i=1}^n (y_i-\bar{y})^2} = \frac{\|\hat{\y}-\bar{\y}\|^2}{\|\y-\bar{\y}\|^2}$$
:::

## Other useful norms and distances

### 1-norm, $\|\star\|_1$.

If $\x=\pm x_1 & x_2 & \dots &x_n \mp$ then the $1$-norm of $\X$ is
$$\|\x\|_1 = \sum_{i=1}^n |x_i|.$$
This metric is often referred to as _Manhattan distance_, _city block distance_, or _taxicab distance_ because it measures the distance between points along a rectangular grid (as a taxicab must travel on the streets of Manhattan, for example). When $\x$ and $\y$ are binary vectors, the $1$-norm is called the __Hamming Distance__, and simply measures the number of elements that are different between the two vectors.

```{r id='fig-1norm', fig.align='center', fig.cap = 'The lengths of the red, yellow, and blue paths represent the 1-norm distance between the two points. The green line shows the Euclidean measurement (2-norm).', echo=F, out.width="50%"}
knitr::include_graphics("figs/1norm.png")
```


### $\infty$-norm, $\|\star\|_{\infty}$.
The infinity norm, also called the Supremum, or Max distance, is:
$$\|\x\|_{\infty} =  \max\{|x_1|,|x_2|,\dots,|x_p|\}$$


## Inner Products
The inner product of vectors is a notion that we've already seen in Chapter \@ref(mult), it is what's called the _dot product_ in most physics and calculus text books.  


:::{.definition name='Vector Inner Product' #innerproddef}
 The inner product of two $n\times 1$ vectors $\x$ and $\y$ is written $\x^T\y$ (or sometimes as $\langle \x,\y \rangle$) and is the sum of the product of corresponding elements.
$$\x^T\y = \pm x_1 & x_2 & \dots & x_n \mp \pm y_1 \\y_2 \\ \vdots \\ y_n \mp = x_1y_1+x_2y_2+\dots+x_ny_n=\sum_{i=1}^n x_i y_i.$$

When we take the inner product of a vector with itself, we get the square of the 2-norm:
$$\x^T\x=\|\x\|_2^2.$$
:::

Inner products are at the heart of every matrix product. When we multiply two matrices, $\X_{m\times n}$ and $\bo{Y}_{n\times p}$, we can represent the individual elements of the result as inner products of rows of $\X$ and columns of $\Y$ as follows:

$$
\X\Y = \pm \xrow{1} \\ \xrow{2}    \\ \vdots \\ \xrow{m}   \mp 
\pm \ycol{1}&\ycol{2}&\dots&\ycol{p}  \mp \\
= \pm \xrow{1}\ycol{1} & \xrow{1}\ycol{2}   & \dots & \xrow{1}\ycol{p}  \\
\xrow{2}\ycol{1} & \xrow{2}\ycol{2}    & \dots & \xrow{2}\ycol{p}  \\
\xrow{3}\ycol{1}  &\xrow{3}\ycol{2}    &\dots & \xrow{3}\ycol{p}  \\
\vdots & \vdots  & \ddots & \vdots \\
\xrow{m}\ycol{1} & \dots & \ddots & \xrow{m}\ycol{p}  \mp 
$$

### Covariance
Another important statistical measurement that is represented by an inner product is __covariance.__ Covariance is a measure of how much two random variables change together. The statistical formula for covariance is given as
\begin{equation}
Covariance(\x,\y)=E[(\x-E[\x])(\y-E[\y])]
  (\#eq:cov)
\end{equation}
where $E[\star]$ is the expected value of the variable.
 If larger values of one variable correspond to larger values of the other variable and at the same time smaller values of one correspond to smaller values of the other, then the covariance between the two variables is positive. In the opposite case, if larger values of one variable correspond to smaller values of the other and vice versa, then the covariance is negative. Thus, the _sign_ of the covariance shows the tendency of the linear relationship between variables, however the _magnitude_ of the covariance is not easy to interpret. Covariance is a population parameter - it is a property of the joint distribution of the random variables $\x$ and $\y$.  Definition \@ref(def:covariancedef) provides the mathematical formulation for the _sample_ covariance. This is our best estimate for the population parameter when we have data sampled from a population.


:::{.definition name='Sample Covariance' #covariancedef}

If $\x$ and $\y$ are $n\times 1$ vectors containing $n$ observations for two different variables, then the __sample covariance__ of $\x$ and $\y$ is given by
$$\frac{1}{n-1}\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y}) = \frac{1}{n-1}(\x-\bar{\x})^T(\y-\bar{\y})$$
Where again $\bar{\x}$ and $\bar{\y}$  are vectors that contain $\bar{x}$ and $\bar{y}$ repeated $n$ times. It should be clear from this formulation that $$cov(\x,\y)=cov(\y,\x).$$

When we have $p$ vectors, $\v_1,\v_2,\dots,\v_p$, each containing $n$ observations for $p$ different variables, the sample covariances are most commonly given by the __sample covariance matrix__, $\ssigma$, where $$\ssigma_{ij}=cov(\v_i,\v_j).$$ This matrix is symmetric, since $\ssigma_{ij}=\ssigma_{ji}$. If we create a matrix $\V$ whose columns are the vectors $\v_1,\v_2,\dots \v_p$ _once the variables have been centered to have mean 0_, then the covariance matrix is given by:
$$cov(\V)=\ssigma = \frac{1}{n-1}\V^T\V.$$
The $j^{th}$ diagonal element of this matrix gives the variance $\v_j$ since
\begin{eqnarray}
\ssigma_{jj}=cov(\v_j,\v_j) &=&\frac{1}{n-1}(\v_j-\bar{\v}_j)^T(\v_j-\bar{\v}_j) \\
&=&\frac{1}{n-1}\|\v_j-\bar{\v}_j\|_2^2\\
&=& var(\v_j)
\end{eqnarray}
:::

When two variables are completely uncorrelated, their covariance is zero. This lack of correlation would be seen in a covariance matrix with a diagonal structure. That is, if $\v_1, \v_2,\dots, \v_p$ are uncorrelated with individual variances $\sigma_1^2,\sigma_2^2,\dots,\sigma_p^2$ respectively then the corresponding covariance matrix is:
$$\ssigma = \pm \sigma_1^2 & 0 & 0& \dots & 0\\
										0 & \sigma_2^2 & 0 & \dots & 0\\
										0 & 0 & \ddots & \vdots & 0 \\
										\vdots & \vdots &\vdots & \ddots & \vdots \\
										0 & 0 & 0 & 0 & \sigma_p^2 \mp$$
Furthermore, for variables which are independent and identically distributed (take for instance the error terms in a linear regression model, which are assumed to independent and normally distributed with mean 0 and constant variance $\sigma$), the covariance matrix is a multiple of the identity matrix:
$$\ssigma = \pm \sigma^2 & 0 & 0& \dots & 0\\
										0 & \sigma^2 & 0 & \dots & 0\\
										0 & 0 & \ddots & \vdots & 0 \\
										\vdots & \vdots &\vdots & \ddots & \vdots \\
										0 & 0 & 0 & 0 & \sigma^2 \mp =\sigma^2\bo{I}$$
										
Transforming our variables in a such a way that their covariance matrix becomes diagonal will be our goal in Chapter \@ref(pca).


:::{.theorem name='Properties of Covariance Matrices' #propcov}
The following mathematical properties stem from Equation \@ref(eq:cov). Let $\X_{n\times p}$ be a matrix of data containing $n$ observations on $p$ variables.  If $\A$ is a constant matrix (or vector, in the first case) then
$$cov(\X\A)=\A^Tcov(\X)\A \quad \mbox{ and } \quad cov(\X+\A)=cov(\X)$$
:::


### Mahalanobis Distance

Mahalanobis Distance is similar to Euclidean distance, but takes into account the correlation of the variables. This metric is relatively common in data mining applications like classification. Suppose we have $p$ variables which have some covariance matrix, $\cov$. Then the Mahalanobis distance between two observations, $\x=\pm x_1& x_2 &\dots & x_p \mp^T$ and $\y = \pm y_1 & y_2 & \dots & y_p \mp^T$ is given by
$$d(\x,\y)=\sqrt{(\x-\y)^T\cov^{-1}(\x-\y)}.$$
If the covariance matrix is diagonal (meaning the variables are uncorrelated) then the Mahalanobis distance reduces to Euclidean distance normalized by the variance of each variable:
$$d(\x,\y)=\sqrt{\sum_{i=1}^p\frac{(x_i-y_i)^2}{s_i^2}}=\|\cov^{-1/2}(\x-\y)\|_2.$$
										

### Angular Distance

The inner product between two vectors can provide useful information about their relative orientation in space and about their similarity. For example, to find the cosine of the angle between two vectors in $n$-space, the inner product of their corresponding unit vectors will provide the result. This cosine is often used as a measure of similarity or correlation between two vectors.


:::{.definition name='Cosine of Angle between Vectors' #cosine}
The cosine of the angle between two vectors in $n$-space is given by
$$\cos(\theta)=\frac{\x^T\y}{\|\x\|_2\|\y\|_2}$$
![](figs/cos.gif)
:::

This angular distance is at the heart of __Pearson's correlation coefficient__.

### Correlation

Pearson's correlation is a normalized version of the covariance, so that not only the _sign_ of the coefficient is meaningful, but its _magnitude_ is meaningful in measuring the strength of the linear association.

(ref:cosinecorrtitle) Pearson's Correlation and Cosine Distance

:::{.example name='(ref:cosinecorrtitle)' #cosinecorr}
You may recall the formula for Pearson's correlation between variable $\x$ and $\y$ with a sample size of $n$ to be as follows:
$$r = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^{n} (x_i - \bar{x})^2}\sqrt{\sum_{i=1}^{n} (y_i - \bar{y})^2}}$$
If we let $\bar{\x}$ be a vector that contains $\bar{x}$ repeated $n$ times, like we did in Example \@ref(exm:sdnorm), and let $\bar{\y}$ be a vector that contains $\bar{y}$ then Pearson's coefficient can be written as:
$$r=\frac{(\x-\bar{\x})^T(\y-\bar{\y})}{\|\x-\bar{\x}\|\|\y-\bar{\y}\|}$$
In other words, it is just the cosine of the angle between the two vectors once they have been _centered_ to have mean 0.

This makes sense: correlation is a measure of the extent to which the two variables share a line in space. If the cosine of the angle is positive or negative one, this means the angle between the two vectors is $0^{\circ}$ or $180^{\circ}$, thus, the two vectors are perfectly correlated or _collinear._ 
:::

It is difficult to visualize the angle between two variable vectors because they exist in $n$-space, where $n$ is the number of observations in the dataset. Unless we have fewer than 3 observations, we cannot draw these vectors or even picture them in our minds. As it turns out, this angular measurement does translate into something we can conceptualize: Pearson's correlation coefficient is the angle formed between the two possible regression lines using the centered data: $\y$ regressed on $\x$ and $\x$ regressed on $\y$. This is illustrated in Figure \@ref(fig:corrangle).

(ref:corranglecap) Correlation Coefficient $r$ and Angle between Regression Lines

```{r fig=T, label='corrangle', fig.align='center', fig.cap = '(ref:corranglecap)', echo=F, out.width="40%"}
knitr::include_graphics("figs/corrangle.jpg")
```


To compute the matrix of pairwise correlations between variables $\x_1,\x_2,\x_3,\dots,\x_p$ (columns containing $n$ observations for each variable), we'd first center them to have mean zero, then normalize them to have length $\|\x_i\|=1$ and then compose the matrix
$$\X=[\x_1|\x_2|\x_3|\dots|\x_p].$$

Using this centered and normalized data, the correlation matrix is simply
$$\C=\X^T\X.$$
## Outer Products

The outer product of two vectors $\x \in \Re^m$ and $\y \in \Re^n$, written $\x\y^T$, is an $m\times n $ matrix with rank 1. To see this basic fact, lets just look at an example.

:::{.example name='Outer Product' #outerprod}
Let $\x=\pm 1\\2\\3\\4\mp$ and let $\y=\pm2\\1\\3\mp$. Then the outer product of $\x$ and $\y$ is:
$$\x\y^T = \pm 1\\2\\3\\4\mp \pm 2&1&3\mp = \pm 2&1&3\\4&2&6\\6&3&9\\8&4&12 \mp$$
which clearly has rank 1.  It should be clear from this example that computing an outer product will always result in a matrix whose rows and columns are multiples of each other.
:::

:::{.example name='Centering Data with an Outer Product' #centerouter}
As we've seen in previous examples, many statistical formulas involve the _centered_ data, that is, data from which the mean has been subtracted so that the new mean is zero. Suppose we have a matrix of data containing observations of individuals' heights (h) in inches, weights (w), in pounds and wrist sizes (s), in inches:

$$\A=\bm{ ~ & h & w & s \cr 
			person_1 & 60 & 102 & 5.5 \cr
			person_2 & 72 & 170 &  7.5 \cr
			person_3 & 66 & 110 & 6.0\cr
			person_4 & 69 & 128 & 6.5\cr
			person_5 & 63 & 130 &  7.0\cr}$$
			
The average values for height, weight, and wrist size are as follows:
\begin{eqnarray}
\bar{h}&=&66\\
\bar{w}&=&128\\
\bar{s}&=&6.5
\end{eqnarray}			

To center all of the variables in this data set simultaneously, we could compute an outer product using a vector containing the means and a vector of all ones:

 $$\pm 60 & 102 & 5.5 \cr
			 72 & 170 &  7.5 \cr
			66 & 110 & 6.0\cr
			69 & 128 & 6.5\cr
			63 & 130 &  7.0\cr \mp - \pm 1\\1\\1\\1\\1 \mp \pm 66 & 128 & 6.5 \mp$$
$$= \pm 60 & 102 & 5.5 \cr
			 72 & 170 &  7.5 \cr
			66 & 110 & 6.0\cr
			69 & 128 & 6.5\cr
			63 & 130 &  7.0\cr \mp - \pm  66 & 128 & 6.5 \\66 & 128 & 6.5 \\66 & 128 & 6.5 \\66 & 128 & 6.5 \\66 & 128 & 6.5 \mp$$

$$= \pm    -6.0000 & -26.0000  & -1.0000\\
    6.0000 &  42.0000   & 1.0000\\
         0 & -18.0000 &  -0.5000\\
    3.0000    &     0       &  0\\
   -3.0000 &   2.0000 &   0.5000 \mp$$
:::


## Exercises

<ol>
<li> Let $\u=\pm 1\\2\\-4\\-2\mp$ and $\v=\pm 1\\-1\\1\\-1\mp$. 
<ol style="list-style-type:lower-alpha">
<li>  Determine the Euclidean distance between $\u$ and $\v$.
<li>  Find a vector of unit length in the direction of $\u$.
<li>  Determine the cosine of the angle between $\u$ and $\v$.
<li>  Find the 1- and $\infty$-norms of $\u$ and $\v$.
<li>  Suppose these vectors are observations on four independent variables, which have the following covariance matrix:
$$\cov=\pm 2&0&0&0\\0&1&0&0\\0&0&2&0\\0&0&0&1 \mp$$
Determine the Mahalanobis distance between $\u$ and $\v$.
</ol>

<li>  Write a matrix expression for the correlation matrix, $\C$,  for a matrix of _centered_ data, $\X$, where $\C_{ij}=r_{ij}$ is Pearson's correlation measure between variables $\x_i$ and $\x_j$. To do this, we need more than an inner product, we need to normalize the rows and columns by
the norms $\|\x_i\|$. For a hint, revisit the exercises in Chapter \@ref(mult).

<li>  Suppose you have a matrix of data, $\A_{n\times p}$, containing $n$ observations on $p$ variables. Develop a matrix formula for the standardized data (where the mean of each variable should be subtracted from the corresponding column before dividing by the standard deviation). _Hint: use Exercises 1(f) and 4 from Chapter \@ref(mult) along with Example \@ref(exm:centerouter)._

<li>  Explain why, for any norm or distance metric,
$$\|\x-\y\|=\|\y-\x\|$$

</ol>



<!-- % -->
<!-- % -->
<!-- %\section{Homogeneous Linear Equations} -->
<!-- % -->
<!-- %Linear Algebra is the study of linear equations. As you may recall, a homogeneous linear equation is one in which the right hand side of the equation is 0 (whether it be the scalar 0, a vector of zeros, or a matrix of zeros). In two dimensions, the set of solutions to a linear equation is a one-dimensional line, something we are all familiar with, for example: -->
<!-- % -->
<!-- %$$3x_1 + 2x_2 = 0.$$ -->
<!-- % -->
<!-- %We can write this simple homogeneous equation in  matrix notation by letting $\bo{a}=\pm 3 \\ 2 \mp$ and $\x=\pm x_1 \\ x_2 \mp$ so our equation becomes -->
<!-- %$$\bo{a}^T\x=\pm 3 & 2 \mp \pm x_1 \\x_2 \mp = 0.$$ -->
<!-- % -->
<!-- %This equation has infinitely many solutions - any point on the line will do. Gaussian Elimination will obviously not change this simple equation.  In this situation, $x_1$ is called a basic variable (because it corresponds to a basic column) and $x_2$ is called a free variable (because it does \textit{not} correspond to a basic column and it is free to take on any value). And so to represent the solution set, we have the following: -->
<!-- %$$\pm -\frac{2}{3} x_1 \\ x_1 \mp = x_1 \pm -frac{2}{3} \\ 1 \mp = \alpha \pm -\\frac{2}{3} \\ 1 \mp = span \left\lbrace\pm -\frac{2}{3} \\ 1 \mp \right\rbrace$$  -->
<!-- % -->





<!--chapter:end:019-norms.Rmd-->

# Linear Independence {#linind}

```{r, echo=F}
thmcounter=0
excounter=0
cid='linind'
```

One of the most central ideas in all of Linear Algebra is that of **linear independence**. For regression problems, it is repeatedly stressed that _multicollinearity_ is problematic. Multicollinearity is simply a statistical term for linear dependence. It's bad. Having a firm understanding of linear combinations, we can develop the important concept of linear independence. 

## Linear Independence

:::{.definition name='Linear Dependence and Linear Independence' #lininddef}
A set of vectors $\{\v_1,\v_2,\dots, \v_n\}$ is **linearly dependent** if we can express the zero vector, $\bo{0}$, as non-trivial linear combination of the vectors.  In other words there exist some constants $\alpha_1,\alpha_2,\dots \alpha_n$  (non-trivial means that these constants are not _all_ zero) for which
\begin{equation}
\alpha_1\v_1 +\alpha_2\v_2 + \dots +\alpha_n\v_n=\bo{0}.
  (\#eq:triv)
\end{equation}

A set of terms is **linearly independent** if Equation \@ref(eq:triv) has only the trivial solution ($\alpha_1=\alpha_2=\dots=\alpha_n = 0$).
:::

Another way to express linear dependence is to say that we can write one of the vectors as a linear combination of the others. If there exists a non-trivial set of coefficients $\alpha_1,\alpha_2, \dots, \alpha_n$ for which
$$\alpha_1\v_1 +\alpha_2\v_2 + \dots +\alpha_n\v_n=\bo{0}$$
then for $\alpha_j \neq 0$ we could write
$$\v_j=-\frac{1}{\alpha_j} \sum_{\substack{i=1\\i \neq j}}^n \alpha_i \v_i$$

:::{.example name='Linearly Dependent Vectors' #lindep}
The vectors $\v_1 =\pm 1\\2\\2 \mp, \v_2 = \pm 1\\2\\3 \mp, \mbox{  and  } \v_3 = \pm 3\\6\\7 \mp$ are linearly dependent because 
$$\v_3 = 2\v_1+\v_2$$
or, equivalently, because
$$2\v_1+\v_2-\v_3 = \bo{0}$$
:::

### Determining Linear Independence

You should realize that the linear combination expressed Definition \@ref{def:linind} can be written as a matrix vector product. Let $\A_{m\times n} = (\A_1|\A_2|\dots|\A_n)$ be a matrix. Then by Definition \@ref{def:linind}, the columns of $\A$ are linearly independent if and only if the equation
\begin{equation}
\A\x = \bo{0}
(\#eq:homo)
\end{equation}
has only the trivial solution, $\x=0$. Equation \@ref{eq:homo} is commonly known as the homogeneous linear equation. For this equation to have only the trivial solution, it must be the case that under Gauss-Jordan elimination, the augmented matrix $(\A|\bo{0})$ reduces to $(\bo{I}|0)$. We have already seen this condition in our discussion about matrix inverses - if a square matrix $\A$ reduces to the identity matrix under Gauss-Jordan elimination then it is equivalently called _full rank, nonsingular, or invertible_. Now we add an additional condition equivalent to the others - the matrix $\A$ has linearly independent columns (_and rows_).

In Theorem \@ref(thm:equivcond) we provide an important list of equivalent conditions regarding the existence of a matrix inverse.

:::{.theorem name='Equivalent Conditions for Matrix Invertibility' #equivcond}
Let $\A$ be an $n\times n$ matrix - a _square_ matrix. Then the following statements are _equivalent_. (If one these statements is true, then all of these statements are true)

- $\A$ is invertible ($\A^{-1} exists$)
- $\A$ has full rank ($rank(\A)=n$)
- The columns of $\A$ are linearly independent
- The rows of $\A$ are linearly independent
- The system $\A\x=\b$, $\b\neq \bo{0}$ has a unique solution
- $\A\x=\bo{0} \Longrightarrow \x=\bo{0}$
- $\A$ is nonsingular
- $\A \xrightarrow{Gauss-Jordan} \bo{I}$
:::

## Span of Vectors {#span}

:::{.definition name='Vector Span' #spandef}
The **span** of a single vector $\v$ is the set of all scalar multiples of $\v$:
$$span(\v)=\{\alpha\v\ \mbox{  for any constant  } \alpha\}$$
 The **span** of a collection of vectors, $\V=\{\v_1,\v_2,\dots,\v_n\}$ is the set of all linear combinations of these vectors:
 $$span(\V)=\{\alpha_1\v_1+\alpha_2\v_2+\dots+\alpha_n\v_n \mbox{ for any constants }\alpha_1,\dots,\alpha_n\}$$
:::

Recall that addition of vectors can be done geometrically using the _head-to-tail_ method shown in Figure \@ref(fig:vectoradd2).

```{r label='vectoradd2', fig.align='center', fig.cap = 'Geometrical addition of vectors: Head-to-tail', echo=F, out.width=300}
knitr::include_graphics("figs/vectoradd.png")
```


If we have two linearly independent vectors on a coordinate plane, then any third vector can be written as a linear combination of them. This is because two vectors is sufficient to _span_ the entire 2-dimensional plane. You should take a moment to convince yourself of this geometrically.

In 3-space, two linearly independent vectors can still only span a plane. Figure \@ref(fig:spanfig) depicts this situation. The set of all linearly combinations of the two vectors $\a$ and $\b$ (i.e. the $span(\a,\b)$) carves out a plane. We call this a two-dimensional collection of vectors a **subspace** of $\Re^3$. A subspace is formally defined in Definition \@ref(def:subspacedef).

(ref:spancap) The $span(\a,\b)$ in $\Re^3$ creates a plane (a 2-dimensional _subspace_)

```{r label='spanfig', fig.align='center', fig.cap = '(ref:spancap)', echo=F, out.width=300}
knitr::include_graphics("figs/span.png")
```


:::{.definition name='Subspace' #subspacedef}
A **subspace**, $\mathcal{S}$ of $\Re^n$ is thought of as a "flat"  (having no curvature) surface within $\Re^n$. It is a collection of vectors which satisfies the following conditions:

1. The origin ($\bo{0}$ vector) is contained in $\mathcal{S}$
2. If $\x$ and $\y$ are in $\mathcal{S}$ then the sum $\x+\y$ is also in $\mathcal{S}$
3. If $\x$ is in $\mathcal{S}$ and $\alpha$ is a constant then $\alpha\x$ is also in $\mathcal{S}$
:::

The span of two vectors $\a$ and $\b$ is a subspace because it satisfies these three conditions. (Can you prove it formally? See exercise 4).

:::{.example name='Span' #span}
Let $\a=\pm 1\\3\\4 \mp$ and $\b=\pm 3\\0\\1 \mp$. Explain why or why not each of the following vectors is contained in the $span(\a,\b)$? 

a. $\x=\pm 5\\6\\9 \mp$ 

  - To determine if $\x$ is in the $span(\a,\b)$ we need to find coefficients $\alpha_1, \alpha_2$ such that $$\alpha_1\a+\alpha_2\b=\x.$$ Thus, we attempt to solve the system
$$\pm 1&3\\3&0\\4&1 \mp \pm \alpha_1\\ \alpha_2 \mp = \pm 5\\6\\9\mp.$$
After Gaussian Elimination, we find that the system is consistent with the solution
$$\pm\alpha_1\\ \alpha_2 \mp=\pm 2\\1\mp$$
and so $\x$ is in fact in the $span(\a,\b)$.

b. $\y=\pm 2\\4\\6 \mp$ 

  - We could follow the same procedure as we did in part (a) to learn that the corresponding system is _not_ consistent and thus that $\y$ is not in the $span(\a,\b)$.

:::

##  Exercises
<ol>
<li> **Six views of matrix multiplication:**  _This notational exercise turns out to contain one of (well, six of) the most fundamentally important concepts to grasp for applied linear algebra. We must be able to intuitively create matrix multiplications from every angle in order for us to be strong practitioners. This is not something that we develop immediately. It comes through practice, visualization, and experience. Do not skip this exercise and keep it in your pocket as you proceed through this book. _

Let $\A_{m\times k}$, $\B_{k\times n}$, and $\C_{m\times n}$ be matrices such that
$$\A\B=\C.$$
<ol style="list-style-type:lower-alpha">
  <li> Express the first column of $\C$ as a linear combination of the columns of $\A$.
  <li> Express the first column of $\C$ as a matrix-vector product.
  <li> Express $\C$ as a sum of outer products.
  <li> Express the first row of $\C$ as a linear combination of the rows of $\B$.
  <li> Express the first row of $\C$  as a matrix-vector product.
  <li> Express the element $\C_{ij}$ as an inner product of row or column vectors from $\A$ and $\B$.
</ol>
<li> Determine whether or not the vectors $$\x_1=\pm 1\\3\\1\mp,\x_2=\pm 0\\1\\1\mp,\x_3=\pm 2\\1\\0\mp$$ are linearly independent.
<li> Let $\a=\pm 1\\3\\4 \mp$ and $\b=\pm 3\\0\\1 \mp$. 
<ol style="list-style-type:lower-alpha">
  <li> Show that the zero vector, $\pm 0\\0\\0 \mp$ is in the $span(\a,\b)$. 
  <li> Determine whether or not the vector $\pm 1\\0\\1 \mp$ is in the $span(\a,\b)$.
</ol>
<li> Describe the span of one vector in $\Re^3$.
<li> Describe the span of two linearly _independent_ vectors in $\Re^3$.
<li> Describe the span of two linearly _dependent_ vectors in $\Re^3$.
<li> What is the span of the zero vector, $\bo{0}=(0,0,\dots, 0)$?
<li> Compare the $span \left{\pm 1\\1 \mp\right}$ to the $span \left{\pm 1\\1 \mp , \pm 2\\2 \mp \right}$.
<li> What is the dimension of the $span \left{\pm 1\\1 \mp , \pm 2\\2 \mp \right}$
<li> What is the definition of the dimension of a subspace?
<li> How would you describe a hyperplane?
<li> Draw the $span(\a,\b)$ if $\a=\pm 1\\2 \mp$ and $\b=\pm 3\\6 \mp$.
<li> Prove that the span of vectors is a subspace by showing that it satisfies the three conditions from Definition \@ref(def:spandef). To make a formal proof, the strategy should be as follows: (1) Take two arbitrary elements from the span and show that when you add them together, the resulting vector is also in the span. (2) Take one arbitrary vector from the span and show that when you multiply it by a constant, the resulting vector is also in the span. (3) Show that the zero vector is contained in the span. You can simply show this fact for the span of two vectors and notice how the concept will hold for more than two vectors.
<li>  _True/False_ Mark each statement as true or false. Justify your response.
<ol style="list-style-type:lower-alpha">
  <li> If $\A\x=\b$ has a solution then $\b$ can be written as a linear combination of the columns of $\A$.
  <li> If $\A\x=\b$ has a solution then $\b$ is in the span of the columns of $\A$.
  <li> If  $\v_1$ is in the $span(\v_2,\v_3)$, then $\v_1, \v_2, \,\mbox{ and},\v_3$ form a linearly independent set.
</ol>
<li> Two vectors are linearly independent only if they are not perfectly correlated, $-1<\rho<1$, where $\rho$ is Pearson's correlation coefficient.
</ol>


## List of Key Terms {-}

- linearly independent
- linearly dependent
- full rank
- perfect multicollinearity
- severe multicollinearity
- invertible
- nonsingular
- linear combination geometrically       
- linear (in)dependence geometrically    
- vector span                            
- subspace                               
- dimension of subspace                  
- hyperplane                             
               


<!--chapter:end:025-linind.Rmd-->

# Basis and Change of Basis {#basis}

When we think of coordinate pairs, or coordinate triplets, we tend to think of them as points on a grid where each axis represents one of the coordinate directions:\\

```{r id='coordplane', fig.align='center', fig.cap = 'The Coordinate Plane', echo=F, out.width="40%"}
knitr::include_graphics("figs/coordplane.jpg")
```

We may not have previously formalized it, but even in this elementary setting, we are considering these points (vectors) as linear combinations of the elementary __basis vectors__
$$\e_1 = \pm 1\\0\mp \mbox{  and  } \e_2=\pm 0\\1 \mp.$$
For example, the point $(2,3)$ can be written as
\begin{equation}
\pm 2\\3 \mp = 2\pm 1\\0\mp+3\pm 0\\1\mp = 2\e_1+3\e_2.
  (\#eq:coord)
\end{equation}

We consider the coefficients (the scalars 2 and 3) in this linear combination as __coordinates__ in the basis $\mathcal{B}_1=\{\e_1,\e_2\}$.  The coordinates, in essence, tell us how much "information" from the vector/point $(2 ,3 )$ lies along each basis direction: to create this point, we must travel 2 units along the direction of $\e_1$ and then 3 units along the direction of $\e_2$. 

We can also view Equation \@ref(eq:coord) as a way to separate the vector $(2,3)$ into orthogonal components. Each component is an __orthogonal projection__ of the vector onto the span of the corresponding basis vector. The orthogonal projection of vector $\bo{a}$ onto the span another vector $\v$ is simply the closest point to $\bo{a}$ contained on the span($\v$), found by _projecting_ $\bo{a}$ onto $\v$ at a $90^\circ$ angle. Figure \@ref(fig:orthogproj) shows this explicitly for $\bo{a}=(2,3)$.

```{r fig=T,label='orthogproj', fig.align='center', fig.cap = 'Orthogonal Projections onto basis vectors', echo=F, out.width="40%"}
knitr::include_graphics("figs/orthogproj.jpg")
```

:::{.definition name='Elementary Basis' #elembasisdef}
For any vector $\a=(a_1,a_2,\dots, a_n)$, the basis $\mathcal{B} = \{\e_1,\e_2,\dots,\e_n\}$ (recall $\e_i$ is the $i^{th}$ column of the identity matrix $\bo{I}_n$) is the \textbf{elementary basis} and $\a$ can be written in this basis using the \textbf{coordinates} $a_1,a_2,\dots, a_n$ as follows:
$$\a=a_1\e_1+a_2\e_2+\dots a_n\e_n.$$
:::

The elementary basis $\mathcal{B}_1$ is convenient for many reasons, one being its __orthonormality__:
\begin{eqnarray*}
\e_1^T\e_1 &=& \e_2^T\e_2 = 1\\
\e_1^T\e_2 &=& \e_2^T\e_1 = 0
\end{eqnarray*}

However, there are many (infinitely many, in fact) ways to represent the data points on different axes. If I wanted to view this data in a different way, I could use a different basis. Let's consider, for example, the following orthonormal basis, drawn in green over the original grid in Figure \@ref(fig:coordplaneB2):
$$\mathcal{B}_2 = \{\v_1,\v_2\}=\left\lbrace {\textstyle\frac{\sqrt{2}}{2}}\pm 1\\ 1\mp ,{\textstyle\frac{\sqrt{2}}{2}}\pm 1\\-1\mp\right\rbrace$$

(ref:coordcap) New basis vectors, $\v_1$ and $\v_2$, shown on original plane

```{r fig=T,label='coordplaneB2', fig.align='center', fig.cap = '(ref:coordcap)', echo=F, out.width="40%"}
knitr::include_graphics("figs/coordplaneB2.jpg")
```


The scalar multipliers $\frac{\sqrt{2}}{2}$ are simply normalizing factors so that the basis vectors have unit length. You can convince yourself that this is an orthonormal basis by confirming that
\begin{eqnarray*}
\v_1^T\v_1 &=& \v_2^T\v_2 = 1\\
\v_1^T\v_2 &=& \v_2^T\v_1 = 0
\end{eqnarray*}

If we want to _change the basis_ from the elementary $\mathcal{B}_1$ to the new green basis vectors in $\mathcal{B}_2$, we need to determine a new set of coordinates that direct us to the point using the green basis vectors as a frame of reference. In other words we need to determine $(\alpha_1,\alpha_2)$ such that travelling $\alpha_1$ units along the direction $\v_1$ and then $\alpha_2$ units along the direction $\v_2$ will lead us to the point in question. For the point $(2,3)$ that means
$$
\pm 2\\3 \mp = \alpha_1\v_1+\alpha_2\v_2 = \alpha_1\pm \frac{\sqrt{2}}{2}\\ \frac{\sqrt{2}}{2}\mp+\alpha_2\pm \frac{\sqrt{2}}{2}\\ -\frac{\sqrt{2}}{2}\mp .
$$

This is merely a system of equations $\bo{V}\bo{a}=\b$:
$$
{\textstyle\frac{\sqrt{2}}{2}}\pm 1&1 \\1& -1\mp \pm \alpha_1\\ \alpha_2 \mp = \pm 2\\3 \mp
$$

The $2\times 2$ matrix $\V$ on the left-hand side has linearly independent columns and thus has an inverse. In fact, $\V$ is an orthonormal matrix which means its inverse is its transpose. Multiplying both sides of the equation by $\V^{-1}=\V^T$ yields the solution
$$\bo{a} =\pm \alpha_1 \\ \alpha_2 \mp = \V^T\b= \pm \frac{5\sqrt{2}}{2} \\ -\frac{\sqrt{2}}{2} \mp$$

This result tells us that in order to reach the red point (formerly known as (2,3) in our previous basis), we should travel $\frac{5\sqrt{2}}{2}$ units along the direction of $\v_1$ and then $-\frac{\sqrt{2}}{2}$ units along the direction $\v_2$ (Note that $\v_2$ points toward the southeast corner and we want to move northwest, hence the coordinate is negative). Another way (a more mathematical way) to say this is that _the length of the orthogonal projection of $\bo{a}$ onto the span of $\v_1$ is $\frac{5\sqrt{2}}{2}$, and the length of the orthogonal projection of $\bo{a}$ onto the span of $\v_2$ is $-\frac{\sqrt{2}}{2}$_. While it may seem that these are difficult distances to plot, they work out quite well if we examine our drawing in Figure \@ref(fig:coordplaneB2), because the diagonal of each square is $\sqrt{2}$. 

In the same fashion, we can re-write all 3 of the red points on our graph in the new basis by solving the same system simultaneously for all the points. Let $\B$ be a matrix containing the original coordinates of the points and let $\A$ be a matrix containing the new coordinates:
$$\B=\pm -4 & 2 & 5 \\ -2 & 3 & 2 \mp\,\, \A=\pm \alpha_{11} & \alpha_{12} & \alpha_{13}\\ \alpha_{21} & \alpha_{22} & \alpha_{23} \mp$$

Then the new data coordinates on the rotated plane can be found by solving:
$$\V\A=\B$$
And thus $$\A=\V^T\B =\frac{\sqrt{2}}{2} \pm   -6 & 5 &7\\ -2&-1&3\mp$$

Using our new basis vectors, our alternative view of the data is that in Figure \@ref(fig:coordplaneB2rotate).

(ref:coordB2cap) Points plotted in the new basis, $\mathcal{B}$

```{r fig=T,label='coordplaneB2rotate', fig.align='center', fig.cap = '(ref:coordB2cap)', echo=F, out.width="50%"}
knitr::include_graphics("figs/coordplaneB2rotate.jpg")
```


In the above example, we changed our basis from the original elementary basis to a new orthogonal basis which provides a different view of the data. All of this amounts to a rotation of the data around the origin. No real information has been lost - the points maintain their distances from each other in nearly every distance metric. __Our new variables, $\v_1$ and $\v_2$ are linear combinations of our original variables $\e_1$ and $\e_2$__, thus we can transform the data _back_ to its original coordinate system by again solving a linear system (in this example, we'd simply multiply the new coordinates again by $\V$).

In general, we can change bases using the procedure outlined in Theorem \@ref(thm:changebasedef).

:::{.theorem name='Changing Bases' #changebasedef}
Given a matrix of coordinates (in columns), $\A$, in some basis, $\mathcal{B}_1=\{\x_1,\x_2,\dots,\x_n\}$, we can change the basis to $\mathcal{B}_2=\{\v_1,\v_2,\dots,\v_n\}$ with the new set of coordinates in a matrix $\B$ by solving the system
$$\X\A=\V\B$$
where $\X$ and $\V$ are matrices containing (as columns) the basis vectors from $\mathcal{B}_1$ and $\mathcal{B}_2$ respectively.

Note that when our original basis is the elementary basis, $\X=\bo{I}$, our system reduces to
$$\A=\V\B.$$

When our new basis vectors are orthonormal, the solution to this system is simply
$$\B=\V^T\A.$$
:::

:::{.definition name='Basis' #basisdef}
A __basis__ for an arbitrary vector space $\mathcal{V}$ is any set of linearly independent vectors $\{\mathcal{B}_1,\dots, \mathcal{B}_r\}$ such that $$span(\{\mathcal{B}_1,\dots, \mathcal{B}_r\}) = \mathcal{\mathbf{V}}.$$
$r$ is said to be the __dimension__ of the vector space $\Re^n$.
  
A __basis__ for the vector space $\Re^n$ is then any set of $n$ linearly independent vectors in $\Re^n$; $n$ is said to be the __dimension__ of the vector space $\Re^n$. When the basis vectors are orthonormal (as in our prior examples), the set is called an __orthonormal basis__. 
:::

The preceding discussion dealt entirely with bases for $\Re^n$ (our example was for points in $\Re^2$). However, we will need to consider bases for _subspaces_ of $\Re^n$. Recall that the span of two linearly independent vectors in $\Re^3$ is a plane. This plane is a 2-dimensional subspace of $\Re^3$. Its dimension is 2 because 2 basis vectors are required to represent this space. However, not all points from $\Re^3$ can be written in this basis - only those points which exist on the plane. Chapter, we will discuss how to proceed in a situation where the point we'd like to represent does not actually belong to the subspace we are interested in. This is the foundation for Least Squares.

## Exercises
<ol>
<li> What are the coordinates of the vector $\x=\pm 4\\3 \mp$ in the basis $\left{\pm -1\\-1 \mp , \pm 1\\-1 \mp \right}$? Draw a picture to make sure your answer lines up with intuition.
<li> In the following picture what would be the signs (+/-) of the coordinates of the green point in the basis $\{\v_1, \v_2\}$? Pick another point at random and answer the same question for that point.
<center> ![](figs/basiscoordsexer.png) </center>
<li> Write the orthonormal basis vectors from exercise 1 as linear combinations of the original elementary basis vectors.
<li> What is the length of the orthogonal projection of $\a_1$ onto $\v_1$?

</ol>

## List of Key Terms {-}

- basis vectors
- orthonormal basis
- coordinates in different bases         
             
                            
    

<!--chapter:end:026-basis.Rmd-->

# Orthogonality {#orthog}
Orthogonal (or perpendicular) vectors have an angle between them of $90^{\circ}$, meaning that their cosine (and subsequently their inner product) is zero. 

:::{.definition name='Orthogonality' #orthogdef}
Two vectors, $\x$ and $\y$, are __orthogonal__ in $n$-space if their inner product is zero:
$$\x^T\y=0$$
:::

Combining the notion of orthogonality and unit vectors we can define an orthonormal set of vectors, or an orthonormal matrix. Remember, for a unit vector, $\x^T\x = 1$.\

:::{.definition name='Orthonormal Set' #orthsetdef}
The $n\times 1$ vectors $\{\x_1,\x_2,\x_3,\dots,\x_p\}$ form an __orthonormal set__ if and only if

1. $\x_i^T\x_j = 0\,$  when $i \ne j$ and \
2. $\x_i^T\x_i = 1\,$  (equivalently $\|\mathbf{x}_i\|=1$)

In other words, an orthonormal set is a collection of _unit vectors which are mutually orthogonal_.
:::

If we form a matrix, $\X=(\x_1|\x_2|\x_3|\dots|\x_p )$, having an orthonormal set of vectors as columns, we will find that multiplying the matrix by its transpose provides a nice result:

\begin{eqnarray*}
\X^T\X = \pm \x_1^T \\ \x_2^T \\ \x_3^T \\ \vdots \\ \x_p^T  \mp 
\pm \x_1&\x_2&\x_3&\dots&\x_p  \mp 
&=& \pm \x_1^T\x_1 & \x_1^T\x_2 & \x_1^T\x_3 & \dots & \x_1^T\x_p \\
\x_2^T\x_1 & \x_2^T\x_2 & \x_2^T\x_3 & \dots & \x_2^T\x_p \\
\x_3^T\x_1 & \x_3^T\x_2 & \x_3^T\x_3 &\dots & \x_3^T\x_p \\
\vdots & \vdots & \ddots & \ddots & \vdots \\
\x_p^T\x_1 & \dots & \dots & \ddots & \x_p^T\x_p \mp \\
&=&  \pm 1 & 0 & 0 & \dots & 0\\
            0 & 1 & 0 & \dots & 0 \\
             0 & 0 & 1 & \dots & 0 \\
             \vdots & \vdots & \ddots & \ddots & \vdots \\
            0 & 0 & 0 & \dots & 1 \mp 
= \bo{I}_p 
\end{eqnarray*}
We will be particularly interested in these types of matrices when they are square. If $\X$ is a square matrix with orthonormal columns, the arithmetic above means that the inverse of $\X$ is $\X^T$ (i.e. $\X$ also has orthonormal rows):
$$\X^T\X=\X\X^T = I.$$ 
Square matrices with orthonormal columns are called orthogonal matrices - this an unfortunate naming mismatch, we agree.         

:::{.definition name='Orthogonal Matrix' #orthmatdef}
A _square_ matrix, $\bo{U}$ with orthonormal columns also has orthonormal rows and is called an __orthogonal matrix__. Such a matrix has an inverse which is equal to it's transpose,
$$\U^T\U=\U\U^T = \bo{I} $$
:::

## Orthonormal Basis

Our primary interest in orthonormal sets will be in the formulation of new bases for our data. An orthonormal basis has two advantages over other types of bases, with one advantage stemming from each of the two criteria for orthonormality:

1. The basis vectors are mutually perpendicular. They are just rotations of the elementary basis vectors and thus when we examine our data in these new bases, we are merely observing a rotation of our data. This allows us to plot the new coordinates of our data on a plane without much regard for the basis vectors themselves. Basis vectors that are _not_ orthogonal would create a distorted image of our data if we did this. We'd need to factor in the angle between the basis vectors to grasp our previously intuitive notions of distance and similarity. \

2. The basis vectors have length 1. So when we look at the coordinates associated with each basis vector, they tell us how many _units_ to go in each direction. This way, again, we can ignore the basis vectors and focus on the coordinates alone. We truly just rotated the axes. 

These two advantages combine to mean that we can use our new coordinates in place of our original data without any loss of "signal". We can rotate my multidimensional data cloud, put that rotated data into a regression or clustering model, and then draw conclusions that about my original data points (of course our "variables" (basis vectors) are no longer the same so we wouldn't be able to draw any conclusions about our original variables from my regression analysis until we _un_-rotated the data, but the predictions would all match exactly).

However, the _real_ power of these advantages are the ease with which we can perform orthogonal projections and the ease with which we can transform from our original space to the new space and back again. 

## Orthogonal Projection

Think about taking a giant flashlight and shining all of the data onto a subspace at a right angle. The resulting shadow would be the __orthogonal projection__ of the data onto the subspace. Figure \@ref(fig:orthogproganim) brings this definition to life.

```{r, fig=T, label='orthogproganim', fig.show="hold", out.width="100%", echo=F,fig.align='center',fig.cap = 'Omitting a variable from an orthonormal basis amounts to orthogonal projection onto the span of the other basis vectors.'}
knitr::include_graphics("figs/orthogproganimlow.gif")
```

Of course, data can exist _below_ and all around the subspace in question, so it might be helpful to imagine two flashlights or many flashlights that project each data point down to the closest point on the subspace (an orthogonal projection onto a subspace of interest always gets you as _close_ to your original data as possible, under the constraint that the projection be contained in the subspace).

When we have a cloud of data and we "drop" one of our variables, geometrically this amounts to an orthogonal projection of the data onto the span of the other axes - reducing the dimensionality of the space in which it exists by 1. Figure \@ref(fig:orthogproganim2) strives to make this clear.

```{r, fig=T, label='orthogproganim2', fig.show="hold", out.width="100%", echo=F,fig.align='center',fig.cap = 'Omitting a variable from an orthonormal basis amounts to orthogonal projection onto the span of the other basis vectors.'}
knitr::include_graphics("figs/orthogproganim2.gif")
```

## Why??

My favorite question. "Whyyy do we have to learn this?!" It's time to build some intuition toward that question. Consider the following 3-D scatter plot, which is interactive. Turn it around with your mouse and see what you notice.

```{r, fig=T, label='pcafig', fig.show="hold", out.width="100%", echo=F,fig.align='center',fig.cap = '3-Dimensional data cloud that suffers from severe multicollinearity.', warning=F, message=F}
library(MASS)
mu=c(0,0,0)
Sigma=matrix(c(1,.5,.5,.5,1,.99,.5,.99,1),3)
dat=mvrnorm(5000,mu=mu,Sigma=Sigma)
# x= rnorm(500,0,1)
# y=rnorm(500,0,1)
# z=2*x+4*y
# dat = cbind(x,y,z)
# library(rgl)
library(plotly)
library(processx)
graph = plot_ly(x=dat[,1],y=dat[,2],z=dat[,3],type='scatter3d')
# plot3d(x=dat[,1],y=dat[,2],z=dat[,3], xlab='',ylab='',zlab='',xaxt='n',yaxt='n',zaxt='n', box=F)
graph
```
Does it look like this data is 3-dimensional in nature? It appears that it could be well-summarized if it existed on a plane. However, what _is_ that plane? We can't merely drop a variable in this instance, as doing so is quite likely to destroy a good proportion of the information from the data. Still, it seems clear that by rotating the data to the _right_ set of axes, we could then squish it down and use 2 coordinates to describe the data without losing much of the information at all. What do we mean by "information"? In this context, using the word "variance" works well. We want to keep as much of the original variance as possible when we squish the cloud down to a plane with an orthogonal projection. Can you find the (approximate) rotation that gives you the _best_ view of the data points? 

Figure \@ref(fig:pcafig) is really the jumping off point. Once we can intuitively see _why_ we might benefit from a new basis and _how_ one might be crafted, we're ready to start digging in to PCA. Once we've exposed the basic terminology in Chapters \@ref(eigen) and \@ref(pca), we can explore the magnificent world of dimension reduction and its many benefits in the case studies in Chapters \@ref(PCA_apps_ukfood)-\@ref(otherdimred).


## Exercises

<ol>
<li>  Let $$\U=\frac{1}{3}\pm -1&2&0&-2\\2&2&0&1\\0&0&3&0\\-2&1&0&2\mp$$
<ol style="list-style-type:lower-alpha">
<li>  Show that $\U$ is an orthogonal matrix.
<li>  Let $\bo{b}=\pm 1\\1\\1\\1\mp$. Solve the equation $\U\x=\bo{b}$.
</ol>
<li> Draw the orthogonal projection of the points onto the subspace $span \left{ \pm 1\\-1\mp \right}$
<center>![](figs/orthogprojex.png)</center>
<li> Are the vectors $\v_1=\pm 1\\-1\\1 \mp$ and $\v_2=\pm 0\\1\\1 \mp$ orthogonal? How do you know?
<li> What are the two conditions necessary for a collection of vectors to be orthonormal?
<li> Show that the vectors $\v_1 = \pm 3 \\ 1\mp$ and $\v_2=\pm -2 \\6\mp$ are orthogonal. Create an orthonormal basis for $\Re^2$ using these two direction vectors.
<li> Consider $\a_1=(1,1)$ and $\a_2=(0,1)$ as coordinates for points in the elementary basis. Write the coordinates of $\a_1$ and $\a_2$ in the orthonormal basis found in the previous exercise. Draw a picture which reflects the old and new basis vectors. 
<li> Briefly explain why an orthonormal basis is important.
<li> Find two vectors which are orthogonal to $\x=\pm 1\\1\\1\mp$
<li>  __Pythagorean Theorem.__ Show that $\x$ and $\y$ are orthogonal if and only if
$$\|\x+\y\|_2^2 = \|\x\|_2^2 + \|\y\|_2^2$$
_(Hint: Recall that $\|\x\|_2^2 = \x^T\x$)_

</ol>

<!--chapter:end:0269-orthogonality.Rmd-->

# Least Squares {#leastsquares}

## Introducing Error

The least squares problem arises in almost all areas of applied mathematics. In data science, the idea is generally to find an approximate mathematical relationship between predictor and target variables such that the sum of squared errors between the true target values and the predicted target values is minimized. In two dimensions, the goal would be to develop a line as depicted in Figure \@ref(fig:leastsquaresillustrated) such that the sum of squared vertical distances (the residuals, in green) between the true data (in red) and the mathematical prediction (in blue) is minimized.

```{r fig=T,fig.align='center', label='leastsquaresillustrated', fig.cap = 'Least Squares Illustrated in 2 dimensions', echo=F, fig.width=10}
knitr::include_graphics('figs/lsreg.pdf')
```

If we let $\bo{r}$ be a vector containing the residual values $(r_1,r_2,\dots,r_n)$ then the sum of squared residuals can be written in linear algebraic notation as
$$\sum_{i=1}^n r_i^2 = \bo{r}^T\bo{r}=(\y-\hat{\y})^T(\y-\hat{\y}) = \|\y-\hat{\y}\|^2$$

 Suppose we want to regress our target variable $\y$ on $p$ predictor variables, $\x_1,\x_2,\dots,\x_p$. If we have $n$ observations, then the ideal situation would be to find a vector of parameters $\boldsymbol\beta$ containing an intercept, $\beta_0$ along with $p$ slope parameters, $\beta_1,\dots,\beta_p$ such that
\begin{equation}
(\#eq:lssystem)
\underbrace{\bordermatrix{1&\x_1 & \x_2 & \dots & \x_p}{obs_1\\obs_2\\ \vdots \\ obs_n}{\left(\begin{matrix}
					  1 &  x_{11} & x_{12} & \dots & x_{1p}\\
					 1 & x_{21} & x_{22} & \dots & x_{2p}\\
					  \vdots & \vdots & \vdots & \vdots &\vdots\\
					 1 & x_{n1} & x_{n2} & \dots & x_{np} \end{matrix}\right) }}_{\LARGE\mathbf{X}}
					  \underbrace{\left(\begin{matrix} \beta_0\\ \beta_1\\  \vdots \\ \beta_p \end{matrix}\right)}_{\LARGE \boldsymbol\beta} 
					  =  \underbrace{\pm y_0\\ y_1 \\ \vdots \\ y_n \mp}_{\LARGE \y} 
\end{equation}					  

With many more observations than variables, this system of equations will not, in practice, have a solution. Thus, our goal becomes finding a vector of parameters $\hat{\bbeta}$ such that $\X\hat{\bbeta}=\hat{\y}$ comes as close to $\y$ as possible.
Using the design matrix, $\X$, the least squares solution $\hat{\boldsymbol\beta}$ is the one for which
$$\|\y-\X\hat{\boldsymbol\beta} \|^2=\|\y-\hat{\y}\|^2$$ is minimized. Theorem \@ref(thm:leastsquares) characterizes the solution to the least squares problem.

:::{.theorem name='Least Squares Problem and Solution' #leastsquares}
For an $n\times m$ matrix $\X$ and $n\times 1$ vector $\y$, let $\bo{r} = \X\widehat{\boldsymbol\beta} - \y$. The least squares problem is to find a vector $\widehat{\boldsymbol\beta}$ that minimizes the quantity
$$\sum_{i=1}^n r_i^2 = \|\y-\X\widehat{\boldsymbol\beta} \|^2.$$

Any vector $\widehat{\bbeta}$ which provides a minimum value for this expression is called a _least-squares solution_.

- The set of all least squares solutions is precisely the set of solutions to the so-called **normal equations**, $$\X^T\X\widehat{\bbeta} = \X^T\y.$$
- There is a unique least squares solution if and only if $rank(\X)=m$ (i.e. linear independence of variables or no perfect multicollinearity!), in which case $\X^T\X$ is invertible and the solution is given by
$$\widehat{\bbeta} = (\X^T\X)^{-1}\X^T\y$$
:::


:::{.example name='Solving a Least Squares Problem' #lsex}
In 2014, data was collected regarding the percentage of linear algebra exercises done by students and the grade they received on their examination. Based on this data, what is the expected effect of completing an additional 10\% of the exercises on a students exam grade?\\
\begin{center}
\begin{tabular}{l | c c}
ID & \% of Exercises & Exam Grade\\
\hline 
1 & 20 & 55\\
2 & 100 & 100\\
3 & 90 & 100\\
4 & 70 & 70\\
5 & 50 & 75\\
6 & 10 & 25\\
7 & 30 & 60
\end{tabular}	

\end{center}

To find the least squares regression line, we want to solve the equation $\X\bbeta = \y$:
$$
\pm 1 & 20\\
 1 & 100\\
  1 & 90\\
   1 & 70\\
    1 & 50\\
     1 & 10\\
      1 & 30\mp \pm \beta_0 \\ \beta_1 \mp = \pm 55\\100\\100\\70\\75\\25\\60 \mp
$$

This system is obviously inconsistent. Thus, we want to find the least squares solution $\hat{\bbeta}$ by solving $\X^T\X\hat{\bbeta}=\X^T\y$:

\begin{eqnarray}
\small
\pm 1&1&1&1&1&1&1 \\20&100&90&70&50&10&30 \mp\pm 1 & 20\\
 1 & 100\\
  1 & 90\\
   1 & 70\\
    1 & 50\\
     1 & 10\\
      1 & 30\mp \pm \beta_0 \\ \beta_1 \mp &=&\pm 1&1&1&1&1&1&1 \\20&100&90&70&50&10&30 \mp \pm 55\\100\\100\\70\\75\\25\\60 \mp \cr
\pm 7 & 370\\370&26900 \mp     \pm \beta_0 \\ \beta_1 \mp &=& \pm 485 \\ 30800\mp
\end{eqnarray}

Now, since multicollinearity was not a problem, we can simply find the inverse of $\X^T\X$ and multiply it on both sides of the equation:
$$\pm 7 & 370\\370&26900 \mp^{-1}= \pm 0.5233 &  -0.0072\\ -0.0072 & 0.0001 \mp$$
and so $$\pm \beta_0 \\ \beta_1 \mp = \pm  0.5233 &  -0.0072\\ -0.0072 & 0.0001 \mp \pm  485 \\ 30800\mp = \pm 32.1109 \\0.7033\mp$$ 

Thus, for each additional 10\% of exercises completed, exam grades are expected to increase by about 7 points. The data along with the regression line $$grade=32.1109+0.7033percent\_exercises$$ is shown below.

<center>
<img src=figs/grades.jpg width=50%/>
</center>
:::
		 
## Why the normal equations?

The normal equations can be derived using matrix calculus (demonstrated at the end of this section) but the solution of the normal equations also has a nice geometrical interpretation.  It involves the idea of orthogonal projection, a concept which will be useful for understanding future topics.

### Geometrical Interpretation
 
 In order for a system of equations, $\A\x=\b$ to have a solution, $\b$ must be a linear combination of columns of $\A$. That is simply the definition of matrix multiplication and equality. If $\A$ is $m\times n$ then
 $$\A\x=\b \Longrightarrow \b = x_1\A_1+x_2\A_2+\dots+x_n\A_n.$$
 As discussed in Chapter \@ref(linind), another way to say this is that $\b$ is in the $span$ of the columns of $\A$. The $span$ of the columns of $\A$ is called the **column space** of $\A$. In Least-Squares applications, the problem is that $\b$ is _not_ in the column space of $\A$. In essence, we want to find the vector $\hat{\b}$ that is _closest_ to $\b$ but exists in the column space of $\A$. Then we know that $\A\hat{\x}=\hat{\b}$ _does_ have a unique solution, and that the right hand side of the equation comes as close to the original data as possible. By multiplying both sides of the original equation by $\A^T$ what we are really doing is _projecting_ $\b$ orthogonally onto the column space of $\A$. We should think of the column space as a flat surface (perhaps a plane) in space, and $\b$ as a point that exists off of that flat surface. There are many ways to draw a line from a point to plane, but the shortest distance would always be travelled perpendicular (orthogonal) to the plane. You may recall from undergraduate calculus or physics that a _normal_ vector to a plane is a vector that is orthogonal to that plane. The normal equations, $\A^T\A\x=\A^T\b$, help us find the closest point to $\b$ that belongs to the column space of $\A$ by means of an orthogonal projection. This geometrical vantage point is depicted in Figure \@ref(fig:lsproj).

(ref:lsprojcap) The normal equations yield the vector $\hat{\b}$ in the column space of $\A$ which is closest to the original right hand side $\b$ vector.

```{r label='lsproj',fig.align='center', fig.cap = '(ref:lsprojcap)', echo=F, out.width="60%"}
knitr::include_graphics('figs/lsproj.jpg')
```

### Calculus Derivation

If you've taken a course in undergraduate calculus, you recall that finding minima and maxima of functions typically involves taking their derivatives and setting them equal to zero. That approach to the derivation of the normal equations will be fruitful, but we first need to understand how to take derivatives of matrix equations. Without teaching vector calculus, we will simply provide the following required formulas for matrix derivatives. If you've taken some undergraduate calculus, perhaps you'll see some parallels. 

While $\frac{\partial}{\partial \mathbf{x}}$ would commonly be the formula reported, we've swapped out the $\mathbf{x}$ for $\boldsymbol \beta$ in the table below in an effort to make our current problem more recognizable.

<table>
<tr>
<td> Condition
<td> Formula
<tr>
<td style="text-align:center"> $\mathbf{a}$ is not a function of $\boldsymbol \beta$
<td> $\frac{\partial \mathbf{a}}{\partial \boldsymbol \beta}= \mathbf{0}$
<tr>
<tr>
<td style="text-align:center"> $\mathbf{a}$ is not a function of $\boldsymbol \beta$
<td> $\frac{\partial \boldsymbol \beta}{\partial \boldsymbol \beta}= \mathbf{I}$
<tr>
<td style="text-align:center"> $\A$ is not a function of $\boldsymbol \beta$
<td> $\frac{\partial \boldsymbol \beta^T\mathbf{A}}{\partial \boldsymbol \beta}= \mathbf{A}T$
<tr>
<td style="text-align:center"> $\A$ is not a function of $\boldsymbol \beta$
<td> $\frac{\partial \boldsymbol \beta^T\mathbf{A}\boldsymbol \beta}{\partial \boldsymbol \beta}= (\mathbf{A}+\mathbf{A}^T)\boldsymbol \beta$
<tr>
<td style="text-align:center"> $\A$ is not a function of $\boldsymbol \beta$ <br> $\A$ is symmetric
<td> $\frac{\partial \boldsymbol \beta^T\mathbf{A}\boldsymbol \beta}{\partial \boldsymbol \beta}= 2\mathbf{A}\boldsymbol \beta$
</td></tr>
</table>


Now let's start with our objective, which is to minimize sum of squared error, by writing it as the inner product of the vector of residuals with itself:
$$\boldsymbol \varepsilon^T \boldsymbol \varepsilon = (\mathbf{y}-\mathbf{X}\boldsymbol \beta)^T(\mathbf{y}-\mathbf{X}\boldsymbol \beta)$$
We'd like to minimize this function with respect to $\boldsymbol \beta$, our vector of unknowns. Thus, our procedure will be to take the derivative with respect to $\boldsymbol \beta$ and set it equal to 0. Note, on the second line of this proof, we take advantage of the fact that $\mathbf{a}^T\mathbf{b} = \mathbf{b}^T\mathbf{a}$ for all $\mathbf{a},\mathbf{b} \in \Re^n$

\begin{eqnarray*}
\frac{\partial}{\partial \boldsymbol \beta} \left(\boldsymbol \varepsilon^T \boldsymbol \varepsilon\right) 
&=&
\frac{\partial}{\partial \boldsymbol \beta} \left(\mathbf{y}^T\mathbf{y} - \mathbf{y}^T(\mathbf{X}\boldsymbol \beta) - (\mathbf{X}\boldsymbol \beta)^T\mathbf{y} + (\mathbf{X}\boldsymbol \beta)^T(\mathbf{X}\boldsymbol \beta)\right)\\
&=&
\frac{\partial}{\partial \boldsymbol \beta} \left(\mathbf{y}^T\mathbf{y} -2\mathbf{y}^T(\mathbf{X}\boldsymbol \beta) + \boldsymbol \beta\mathbf{X}^T\mathbf{X}  \boldsymbol \beta \right)\\
&=& 0 - 2\mathbf{X}^Ty + 2\mathbf{X}^T\mathbf{X}\boldsymbol \beta
\end{eqnarray*}

Setting the last line equal to zero and solving for $\boldsymbol \beta$ completes the derivation. $\Box$

In Chapter \@ref(lsapp), we'll take a deeper dive into the utility of least squares for applied data science. 

<!--chapter:end:029-OLStext.Rmd-->

# Applications of Least Squares {#lsapp}

```{r, echo=F}
thmcounter=0
excounter=0
cid='lsapp'
```

## Simple Linear Regression
### Cars Data

The `cars' dataset is included in the datasets package. This dataset contains observations of speed and stopping distance for 50 cars. We can take a look at the summary statistics by using the **summary** function.

```{r}
summary(cars)
```

We can plot these two variables as follows:

```{r fig=T, fig.width=5, fig.align = 'center'}
plot(cars)
```

### Setting up the Normal Equations

Let's set up a system of equations
$$\mathbf{X}\boldsymbol\beta=\mathbf{y}$$
to create the model
$$stopping\_distance=\beta_0+\beta_1speed.$$

To do this, we need a design matrix $\mathbf{X}$ containing a column of ones for the intercept term and a column containing the speed variable. We also need a vector $\mathbf{y}$ containing the corresponding stopping distances. The ``` model.matrix()``` function will be of use to us here. ``` model.matrix()``` takes a formula and data matrix as input and exports the matrix that we represent as $\mathbf{X}$ in the normal equations. For datasets with categorical (factor) inputs, this function would also create dummy variables for each level, leaving out a reference level by default. You can override the default to leave out a reference level (when you override this default, you **one-hot-encode** your categorical variable) by including the following option as a third input to the function: ``` contrasts.arg = lapply(starwars[,sapply(starwars,is.factor) ], contrasts, contrasts=FALSE ```

```{r}
# Create matrix X and label the columns
X=model.matrix(dist~speed, data=cars)

# Create vector y and label the column
y=cars$dist
```

Let's print the first 10 rows of each to see what we did:

```{r}
# Show first 10 rows, all columns. To show only observations 2,4, and 7, for
# example, the code would be X[c(2,4,7), ]
X[1:10, ]
y[1:10]
```

### Solving for Parameter Estimates and Statistics

Now lets find our parameter estimates by solving the normal equations,
$$\mathbf{X}^T\mathbf{X}\boldsymbol\beta = \mathbf{X}^T\mathbf{y}$$
using the built in **solve** function. To solve the system $\mathbf{A}\mathbf{x}=\mathbf{b}$ we'd use ``` solve(A,b)```.

```{r}
(beta=solve(t(X) %*% X ,t(X)%*%y))
```

At the same time we can compute the residuals,
$$\mathbf{r}=\mathbf{y}-\mathbf{\hat{y}}$$
the total sum of squares (SST),
$$\sum_{i=1}^n (y-\bar{y})^2=(\mathbf{y}-\mathbf{\bar{y}})^T(\mathbf{y}-\mathbf{\bar{y}})=\|\mathbf{y}-\mathbf{\bar{y}}\|^2$$
the regression sum of squares (SSR or SSM)
$$\sum_{i=1}^n (\hat{y}-\bar{y})^2=(\mathbf{\hat{y}}-\mathbf{\bar{y}})^T(\mathbf{\hat{y}}-\mathbf{\bar{y}})=\|\mathbf{\hat{y}}-\mathbf{\bar{y}}\|^2$$
the residual sum of squares (SSE)
$$\sum_{i=1}^n r_i =\mathbf{r}^T\mathbf{r}=\|\mathbf{r}\|^2$$
and the unbiased estimator of the variance of the residuals, using the model degrees of freedom which is $n-2=48$:
$$\widehat{\sigma_{\varepsilon}}^2 =\frac{SSE}{d.f.} = \frac{\|\mathbf{r}\|^2}{48}$$

Then $R^2$:
$$R^2 = \frac{SSR}{SST}$$

We can also compute the standard error of $\widehat{\boldsymbol\beta}$ since

\begin{eqnarray*}
\widehat{\boldsymbol\beta} &=& (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\\
var(\widehat{\boldsymbol\beta})&=&var((\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y})\\
 &=&(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T var(\mathbf{y}) \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1} \\
 &=&(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T (\widehat{\sigma_{\varepsilon}}^2) \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1} \\
  &=& \widehat{\sigma_{\varepsilon}}^2 (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1} \\
  &=& \widehat{\sigma_{\varepsilon}}^2(\mathbf{X}^T\mathbf{X})^{-1}
\end{eqnarray*}

The variances of each $\widehat\beta$ are given by the diagonal elements of their covariance matrix (see Definition \@ref(def:covariancedef)), and the standard errors of each $\widehat\beta$ are thus obtained by taking the square roots of these diagonal elements:
$$s.e.(\widehat{\beta_i})=\sqrt{\widehat{\sigma_{\varepsilon}}[(\mathbf{X}^T\mathbf{X})^{-1}]_{ii}}$$

```{r results='hide'}
meany=mean(y)
XXinv=solve(t(X)%*%X)
yhat=X%*%XXinv%*%t(X)%*%y
resid=y-yhat
SStotal=norm(y-meany,type="2")^2
### OR  SStotal=t(y-meany)%*%(y-meany)
SSreg=norm(yhat-meany,type="2")^2
### OR  SSreg=t(yhat-meany)%*%(yhat-meany)
SSresid=norm(resid,type="2")^2
### OR SSresid=t(resid)%*%resid
Rsquared=SSreg/SStotal
StdErrorResiduals=norm(resid/sqrt(48), type="2") #=sqrt(SSresid/48)
CovBeta=SSresid*XXinv/48
StdErrorIntercept = sqrt(CovBeta[1,1])
StdErrorSlope = sqrt(CovBeta[2,2])
```

```{r echo=F}
sprintf("Rsquared: %s",Rsquared)
sprintf("SSresid: %s",SSresid)
sprintf("SSmodel: %s",SSreg)
sprintf("StdErrorResiduals: %s",StdErrorResiduals)
sprintf("StdErrorIntercept: %s",StdErrorIntercept)
sprintf("StdErrorIntercept: %s",StdErrorSlope)
```

Let's plot our regression line over the original data:

```{r fig=T,fig.width=7, fig.height=6}
plot(cars)
abline(beta[1],beta[2],col='blue')
```


### OLS in R via ```lm()```

Finally, let's compare our results to the built in linear model solver, ``` lm()```:

```{r}
fit = lm(dist ~ speed, data=cars)
summary(fit)
anova(fit)
```

## Multiple Linear Regression

### Bike Sharing Dataset

<!--chapter:end:0299-OLS.Rmd-->

# Eigenvalues and Eigenvectors {#eigen}

Eigenvalues and eigenvectors are (scalar, vector)-pairs that form the "essence" of a matrix. The prefix eigen- is adopted from the German word _eigen_ which means "characteristic, inherent, own" and was introduced by David Hilbert in 1904, but the study of these characteristic directions and magnitudes dates back to Euler's study of the rotational motion of rigid bodies in the $18^{th}$ century.

:::{.definition name='Eigenvalues and Eigenvectors' #eigsdef}
For a square matrix $\A_{n\times n}$, a scalar $\lambda$ is called an __eigenvalue__ of $\A$ if there is a nonzero vector $\x$ such that $$\A\x=\lambda\x.$$ Such a vector, $\x$ is called an __eigenvector__ of $\A$ corresponding to the __eigenvalue__ $\lambda$. We sometimes refer to the pair $(\lambda,\x)$ as an __eigenpair.__
:::

Eigenvalues and eigenvectors have numerous applications from graphic design to quantum mechanics to geology to epidemiology. The main application of note for data scientists is Principal Component Analysis, but we will also see eigenvalue equations used in social network analysis to determine important players in a network and to detect communities in the network.  Before we dive into those applications, let's first get a handle on the definition by exploring some examples.

:::{.example name='Eigenvalues and Eigenvectors' #eig1}
Determine whether $\x=\pm 1\\1 \mp$ is an eigenvector of $\A=\pm 3 & 1 \\1&3 \mp$ and if so, find the corresponding eigenvalue.\\
To determine whether $\x$ is an eigenvector, we want to compute $\A\x$ and observe whether the result is a multiple of $\x$. If this is the case, then the multiplication factor is the corresponding eigenvalue:
$$\A\x=\pm  3 & 1 \\1&3 \mp \pm 1\\1 \mp =\pm 4\\4 \mp=4\pm 1\\1 \mp$$
From this it follows that $\x$ _is_ an eigenvector of $\A$ and the corresponding eigenvalue is $\lambda = 4$.\\

Is the vector $\y=\pm 2\\2 \mp$ an eigenvector? 
$$\A\y=\pm  3 & 1 \\1&3 \mp \pm 2\\2 \mp =\pm 8\\8 \mp=4\pm 2\\2 \mp = 4\y$$
Yes, it is and it corresponds to the _same_ eigenvalue, $\lambda=4$
:::

Example \@ref(exm:eig1) shows a very important property of eigenvalue-eigenvector pairs. If $(\lambda,\x)$ is an eigenpair then any scalar multiple of $\x$ is also an eigenvector corresponding to $\lambda$. To see this, let $(\lambda,\x)$ be an eigenpair for a matrix $\A$ (which means that $\A\x=\lambda\x$) and let $\y=\alpha\x$ be any scalar multiple of $\x$. Then we have,
$$\A\y = \A(\alpha\x)=\alpha(\A\x) = \alpha(\lambda\x) = \lambda(\alpha\x)=\lambda\y$$
which shows that $\y$ (or any scalar multiple of $\x$) is also an eigenvector associated with the eigenvalue $\lambda$.

Thus, for each eigenvalue we have infinitely many eigenvectors. In the preceding example, the eigenvectors associated with $\lambda = 4$ will be scalar multiples of $\x=\pm 1\\1 \mp$. You may recall from Chapter \@ref(linind) that the set of all scalar multiples of $\x$ is denoted $span(\x)$.  The $span(\x)$ in this example represents the __eigenspace__ of $\lambda$.
_Note: when using software to compute eigenvectors, it is standard practice for the software to provide the normalized/unit eigenvector._

In some situations, an eigenvalue can have multiple eigenvectors which are linearly independent. The number of linearly independent eigenvectors associated with an eigenvalue is called the __geometric multiplicity__ of the eigenvalue. Example \@ref(exm:eig2) clarifies this concept.

:::{.example name='Geometric Multiplicity' #eig2}
Consider the matrix $\A=\pm 3 & 0 \\0 & 3 \mp$. It should be straightforward to see that $\x_1 =\pm 1 \\0 \mp$ and $\x_2=\pm 0\\1\mp$ are both eigenvectors corresponding to the eigenvalue $\lambda = 3$. $\x_1$ and $\x_2$ are linearly independent, therefore the geometric multiplicity of $\lambda=3$ is 2.\\

What happens if we take a linear combination of $\x_1$ and $\x_2$? Is that also an eigenvector?
Consider $\y=\pm 2 \\ 3 \mp = 2\x_1+3\x_2$. Then 
$$\A\y = \pm 3 & 0 \\0 & 3 \mp \pm 2 \\ 3 \mp = \pm 6 \\ 9 \mp = 3 \pm 2\\3\mp = 3\y$$
shows that $\y$ is also an eigenvector associated with $\lambda=3$.

The __eigenspace__ corresponding to $\lambda=3$ is the set of all linear combinations of $\x_1$ and $\x_2$, i.e. the $span(\x_1,\x_2)$.
:::

We can generalize the result that we saw in Example \@ref(exm:eig2) for any square matrix and any geometric multiplicity. Let $\A_{n\times n}$ have an eigenvalue $\lambda$ with geometric multiplicity $k$. This means there are $k$ linearly independent eigenvectors, $\x_1,\x_2,\dots,\x_k$ such that $\A\x_i=\lambda\x_i$ for each eigenvector $\x_i$.  Now if we let $\y$ be a vector in the $span(\x_1,\x_2,\dots,\x_k)$ then $\y$ is some linear combination of the $\x_i$'s:
$$\y=\alpha_1\x_2+\alpha_2\x_2+\dots+\alpha_k\x_k$$
Observe what happens when we multiply $\y$ by $\A$:
\begin{eqnarray*}
\A\y &=&\A(\alpha_1\x_2+\alpha_2\x_2+\dots+\alpha_k\x_k) \\  
&=& \alpha_1(\A\x_1)+\alpha_2(\A\x_2)+\dots +\alpha_k(\A\x_k) \\ 
&=& \alpha_1(\lambda\x_1)+\alpha_2(\lambda\x_2)+\dots +\alpha_k(\lambda\x_k) \\ 
&=& \lambda(\alpha_1\x_2+\alpha_2\x_2+\dots+\alpha_k\x_k) \\
&=& \lambda\y
\end{eqnarray*}
which shows that $\y$ (or any vector in the $span(\x_1,\x_2,\dots,\x_k)$) is an eigenvector of $\A$ corresponding to $\lambda$.

This proof allows us to formally define the concept of an eigenspace.

:::{.definition name='Eigenspace' #eigenspace}
Let $\A$ be a square matrix and let $\lambda$ be an eigenvalue of $\A$. The set of all eigenvectors corresponding to $\lambda$, together with the zero vector, is called the __eigenspace__ of $\lambda$.  The number of basis vectors required to form the eigenspace is called the __geometric multiplicity__ of $\lambda$.
:::

Now, let's attempt the eigenvalue problem from the other side. Given an eigenvalue, we will find the corresponding eigenspace in Example \@ref(exm:eig3).

:::{.example name='Eigenvalues and Eigenvectors' #eig3}
Show that $\lambda=5$ is an eigenvalue of $\A=\pm 1 & 2 \\4&3 \mp$ and determine the eigenspace of $\lambda=5$.\\

Attempting the problem from this angle requires slightly more work. We want to find a vector $\x$ such that $\A\x=5\x$. Setting this up, we have:
$$\A\x = 5\x.$$
What we want to do is move both terms to one side and factor out the vector $x$. In order to do this, we must use an identity matrix, otherwise the equation wouldn't make sense (we'd be subtracting a constant from a matrix). 
\begin{eqnarray*}
\A\x-5\x &=& \bo{0}\\
(\A-5\bo{I})\x &=& \bo{0} \\
\left( \pm 1 & 2 \\4&3 \mp - \pm 5 & 0 \\ 0 & 5 \mp \right) \pm x_1 \\ x_2 \mp &=& \pm 0 \\0 \mp \\
\pm -4 & 2\\ 4 & -2 \mp  \pm x_1 \\ x_2 \mp &=& \pm 0 \\0 \mp \\
\end{eqnarray*}
Clearly, the matrix $\A-\lambda\bo{I}$ is singular (i.e. does not have linearly independent rows/columns). This will always be the case by the definition $\A\x=\lambda\x$, and is often used as an alternative definition.\\
In order to solve this homogeneous system of equations, we use Gaussian elimination:
$$\left(\begin{array}{rr|r}
 -4 & 2 & 0 \\4 & -2 & 0 \end{array}\right)\longrightarrow\left(\begin{array}{rr|r} 1 & -\frac{1}{2} & 0 \\0 & 0 & 0 \end{array}\right)$$
This implies that any vector $\x$ for which $x_1-\frac{1}{2}\x_2=0$ satisfies the eigenvector equation. We can pick any such vector, for example $\x=\pm 1\\2\mp$, and say that the eigenspace of $\lambda=5$ is
$$span\left\lbrace\pm 1\\2 \mp\right\rbrace$$
:::

If we didn't know either an eigenvalue or eigenvector of $\A$ and instead wanted to find both, we would first find eigenvalues by determining all possible $\lambda$ such that $\A-\lambda\bo{I}$ is singular and then find the associated eigenvectors. There are some tricks which allow us to do this by hand for $2\times 2$ and $3\times 3$ matrices, but after that the computation time is unworthy of the effort. Now that we have a good understanding of how to interpret eigenvalues and eigenvectors algebraically, let's take a look at some of the things that they can do, starting with one important fact.

:::{.definition name='Eigenvalues and the Trace of a Matrix' #eigtrace}
Let $\A$ be an $n\times n$ matrix with eigenvalues $\lambda_1,\lambda_2,\dots,\lambda_n$. Then the sum of the eigenvalues is equal to the trace of the matrix (recall that the trace of a matrix is the sum of its diagonal elements).
$$Trace(\A)=\sum_{i=1}^n \lambda_i.$$
:::

:::{.example name='Trace of Covariance Matrix' #tracecov}
Suppose that we had a collection of $n$ observations on $p$ variables, $\x_1,\x_2,\dots,\x_p$. After centering the data to have zero mean, we can compute the sample variances as:
$$var(\x_i)=\frac{1}{n-1}\x_i^T\x_i =\|\x_i\|^2$$
These variances form the diagonal elements of the sample covariance matrix,
$$\ssigma = \frac{1}{n-1}\X^T\X$$
Thus, the total variance of this data is
$$\frac{1}{n-1}\sum_{i=1}^n \|\x_i\|^2 = Trace(\ssigma) = \sum_{i=1}^n \lambda_i.$$

In other words, the sum of the eigenvalues of a covariance matrix provides the total variance in the variables $\x_1,\dots,\x_p$.
:::

## Diagonalization
Let's take another look at Example \@ref(exm:eig3). We already showed that $\lambda_1=5$ and $\v_1=\pm 1\\2\mp$ is an eigenpair for the matrix $\A=\pm 1 & 2 \\4&3 \mp$. You may verify that $\lambda_2=-1$ and $\v_2=\pm 1\\-1 \mp$ is another eigenpair. Suppose we create a matrix of eigenvectors:
$$\V=(\v_1,\v_2) = \pm 1&1\\2&-1 \mp$$
and a diagonal matrix containing the corresponding eigenvalues:
$$\D=\pm 5 & 0 \\ 0 & -1 \mp$$
Then it is easy to verify that $\A\V=\V\D$:
\begin{eqnarray*}
\A\V &=& \pm 1 & 2 \\4&3 \mp \pm 1&1\\2&-1 \mp\\
		&=& \pm 5&-1\\10&1 \mp\\
		&=&  \pm 1&1\\2&-1 \mp\pm 5 & 0 \\ 0 & -1 \mp\\
		&=&\V\D
\end{eqnarray*}
If the columns of $\V$ are linearly independent, which they are in this case, we can write:
$$\V^{-1}\A\V = \D$$

What we have just done is develop a way to transform a matrix $\A$ into a diagonal matrix $\D$.  This is known as __diagonalization.__

:::{.definition name='Diagonalizable' #diagable}
An $n\times n$ matrix $\A$ is said to be __diagonalizable__ if there exists an invertible matrix $\bP$ and a diagonal matrix $\D$ such that
$$\bP^{-1}\A\bP=\D$$
This is possible if and only if the matrix $\A$ has $n$ linearly independent eigenvectors (known as a _complete set of eigenvectors_). The matrix $\bP$ is then the matrix of eigenvectors and the matrix $\D$ contains the corresponding eigenvalues on the diagonal.
:::

Determining whether or not a matrix $\A_{n\times n}$ is diagonalizable is a little tricky. Having $rank(\A)=n$ is _not_ a sufficient condition for having $n$ linearly independent eigenvectors. The following matrix stands as a counter example:
$$\A=\pm -3& 1 & -3 \\20& 3 & 10 \\2& -2 & 4 \mp$$
This matrix has full rank but only two linearly independent eigenvectors. Fortunately, for our primary application of diagonalization, we will be dealing with a symmetric matrix, which can always be diagonalized. In fact, symmetric matrices have an additional property which makes this diagonalization particularly nice, as we will see in Chapter \@ref(pca).

## Geometric Interpretation of Eigenvalues and Eigenvectors
Since any scalar multiple of an eigenvector is still an eigenvector, let's consider for the present discussion unit eigenvectors $\x$ of a square matrix $\A$ - those with length $\|\x\|=1$. By the definition, we know that
$$\A\x = \lambda\x$$
We know that geometrically, if we multiply $\x$ by $\A$, the resulting vector points in the same direction as $\x$. Geometrically, it turns out that multiplying the unit circle or unit sphere by a matrix $\A$ carves out an ellipse, or an ellipsoid. We can see eigenvectors visually by watching how multiplication by a matrix $\A$ changes the unit vectors. Figure \@ref(fig:eigenarrows) illustrates this. The blue arrows represent (a sampling of) the unit circle, all vectors $\x$ for which $\|\x\|=1$. The red arrows represent the image of the blue arrows after multiplication by $\A$, or $\A\x$ for each vector $\x$.  We can see how almost every vector changes direction when multiplied by $\A$, except the eigenvector directions which are marked in black.  Such a picture provides a nice geometrical interpretation of eigenvectors for a general matrix, but we will see in Chapter \@ref(pca) just how powerful these eigenvector directions are when we look at symmetric matrix.

(ref:eigenarrowscap) Visualizing eigenvectors (in black) using the image (in red) of the unit sphere (in blue) after multiplication by $\A$.

```{r fig=T,label='eigenarrows', fig.align='center', fig.cap = '(ref:eigenarrowscap)', echo=F, out.width="50%"}
knitr::include_graphics("figs/eigenarrows.jpg")
```


## Exercises
<ol>
<li> Show that $\v$ is an eigenvector of $\A$ and find the corresponding eigenvalue:
<ol style="list-style-type:lower-alpha">
<li> $\A=\pm 1 & 2 \\2 & 1 \mp \quad \v=\pm 3\\-3 \mp $
<li> $\A=\pm -1 & 1 \\6 & 0 \mp \quad \v=\pm 1\\-2 \mp $
<li> $\A=\pm 4 & -2 \\5 & -7 \mp \quad \v=\pm 4\\2 \mp $
</ol>
<li> Show that $\lambda$ is an eigenvalue of $\A$ and list two eigenvectors corresponding to this eigenvalue:
<ol style="list-style-type:lower-alpha">
<li>$\A=\pm 0& 4\\-1&5\mp \quad \lambda = 4$
<li> $\A=\pm 0& 4\\-1&5\mp \quad \lambda = 1$
</ol>
<li> Based on the eigenvectors you found in exercises 2, can the matrix $\A$ be diagonalized? Why or why not? If diagonalization is possible, explain how it would be done.
<li> Can a rectangular matrix have eigenvalues/eigenvectors?
</ol>

<!--chapter:end:02999-eigen.Rmd-->

# Principal Components Analysis {#pca}


We now have the tools necessary to discuss one of the most important concepts in mathematical statistics: **Principal Components Analysis (PCA)**. PCA involves the analysis of eigenvalues and eigenvectors of the covariance or correlation matrix. Its development relies on the following important facts:
 
:::{.theorem name='Diagonalization of Symmetric Matrices' #eigsym}
All $n\times n$ real valued symmetric matrices (like the covariance and correlation matrix) have two very important properties:

1. They have a complete set of $n$ linearly independent eigenvectors, $\{\v_1,\dots,\v_n\}$, corresponding to eigenvalues $$\lambda_1 \geq \lambda_2 \geq\dots\geq \lambda_n.$$
2. Furthermore, these eigenvectors can be always be chosen to be _orthonormal_ so that if $\V=[\v_1|\dots|\v_n]$ then
$$\V^{T}\V=\bo{I}$$
or equivalently, $\V^{-1}=\V^{T}$.

Letting $\D$ be a diagonal matrix with $D_{ii}=\lambda_i$, by the definition of eigenvalues and eigenvectors we have for any symmetric matrix $\bo{S}$,
$$\bo{S}\V=\V\D$$
Thus, any symmetric matrix $\bo{S}$ can be diagonalized in the following way:
$$\V^{T}\bo{S}\V=\D$$
Covariance and Correlation matrices (when there is no perfect multicollinearity in variables) have the additional property that all of their eigenvalues are positive (nonzero). They are _positive definite_ matrices.
:::

Now that we know we have a complete set of eigenvectors, it is common to order them according to the magnitude of their corresponding eigenvalues. From here on out, we will use $(\lambda_1,\v_1)$ to represent the __largest__ eigenvalue of a matrix and its corresponding eigenvector. When working with a covariance or correlation matrix, this eigenvector associated with the largest eigenvalue is called the **first principal component** and points in the direction for which the variance of the data is maximal.  Example \@ref(exm:coveigs) illustrates this point.

:::{.example name='Eigenvectors of the Covariance Matrix' #coveigs}
Suppose we have a matrix of data for 10 individuals on 2 variables, $\x_1$ and $\x_2$. Plotted on a plane, the data appears as follows:

<center>
 ![](figs/pcpoints.png =100x)
 </center>
  
Our data matrix for these points is:
$$\X=\pm 1 & 1\\2&1\\2&4\\3&1\\4&4\\5&2\\6&4\\6&6\\7&6\\8&8 \mp$$
the means of the variables in $\X$ are:
$$\bar{\x}=\pm 4.4 \\ 3.7 \mp. $$
When thinking about variance directions, our first step should be to center the data so that it has mean zero. Eigenvectors measure the spread of data around the origin. Variance measures spread of data around the mean. Thus, we need to equate the mean with the origin. To center the data, we simply compute 
$$\X_c=\X-\e\bar{\x}^T = \pm 1 & 1\\2&1\\2&4\\3&1\\4&4\\5&2\\6&4\\6&6\\7&6\\8&8 \mp - \pm 4.4 & 3.7 \\4.4 & 3.7 \\4.4 & 3.7 \\4.4 & 3.7 \\4.4 & 3.7 \\4.4 & 3.7 \\4.4 & 3.7 \\4.4 & 3.7 \\4.4 & 3.7 \\4.4 & 3.7  \mp = \pm -3.4&-2.7\\-2.4&-2.7\\-2.4& 0.3\\-1.4&-2.7\\ -0.4&  0.3\\0.6&-1.7\\1.6& 0.3\\1.6& 2.3\\2.6& 2.3\\3.6&  4.3\mp.$$
Examining the new centered data, we find that we've only translated our data in the plane - we haven't distorted it in any fashion.
<center>
 ![pcpoints](figs/pcpointscenter.png =100x)
</center>
Thus the covariance matrix is:
$$\ssigma=\frac{1}{9}(\X_c^T\X_c)= \pm 5.6 & 4.8\\4.8&6.0111 \mp $$
The eigenvalue and eigenvector pairs of $\ssigma$ are (rounded to 2 decimal places) as follows:
$$(\lambda_1,\v_1)=\left( 10.6100 ,  \begin{bmatrix} 0.69 \\ 0.72 \end{bmatrix}\right) \mbox{  and  } (\lambda_2,\v_2)= \left( 1.0012,\begin{bmatrix}-0.72\\0.69 \end{bmatrix}\right)$$
Let's plot the eigenvector directions on the same graph:

<center>
 ![pcpoints](figs/pcpointspc.png =100x)
</center> 


The eigenvector $\v_1$ is called the **first principal component**. It is the direction along which the variance of the data is maximal. The eigenvector $\v_2$ is the **second principal component**.  In general, the second principal component is the direction, orthogonal to the first, along which the variance of the data is maximal (in two dimensions, there is only one direction possible.) 
:::

Why is this important? Let's consider what we've just done. We started with two variables, $\x_1$ and $\x_2$, which appeared to be correlated. We then derived *new variables*, $\v_1$ and $\v_2$, which are linear combinations of the original variables:
\begin{eqnarray}
\v_1 &=& 0.69\x_1 + 0.72\x_2 \\
(\#eq:pcacomb)
\v_2 &=& -0.72\x_1 + 0.69\x_2
\end{eqnarray}
 These new variables are completely uncorrelated. To see this, let's represent our data according to the new variables - i.e. let's change the basis from $\mathcal{B}_1=[\x_1,\x_2]$ to $\mathcal{B}_2=[\v_1,\v_2]$.  
 
:::{.example name='The Principal Component Basis' #pcabasis}
Let's express our data in the basis defined by the principal components. We want to find coordinates (in a $2\times 10$ matrix $\A$) such that our original (centered) data can be expressed in terms of principal components. This is done by solving for $\A$ in the following equation (see Chapter \@ref(basis) and note that the _rows_ of $\X$ define the points rather than the columns):
\begin{eqnarray}
 \X_c &=& \A \V^T \\
 \pm -3.4&-2.7\\-2.4&-2.7\\-2.4& 0.3\\-1.4&-2.7\\ -0.4&  0.3\\0.6&-1.7\\1.6& 0.3\\1.6& 2.3\\2.6& 2.3\\3.6&  4.3 \mp &=&  \pm a_{11} & a_{12} \\ a_{21} & a_{22} \\ a_{31} & a_{32}\\ a_{41} & a_{42}\\ a_{51} & a_{52}\\ a_{61} & a_{62}\\ a_{71} & a_{72}\\ a_{81} & a_{82}\\ a_{91} & a_{92}\\ a_{10,1} & a_{10,2} \mp \pm \v_1^T \\ \v_2^T \mp
\end{eqnarray}
 
 Conveniently, our new basis is orthonormal meaning that $\V$ is an orthogonal matrix, so
 $$\A=\X\V .$$
 The new data coordinates reflect a simple rotation of the data around the origin:
<center>
<img src=figs/pcpointsrotate.pdf width=50%>
</center>
 
Visually, we can see that the new variables are uncorrelated. You may wish to confirm this by calculating the covariance. In fact, we can do this in a general sense. If $\A=\X_c\V$ is our new data, then the covariance matrix is diagonal:
\begin{eqnarray*}
\ssigma_A &=& \frac{1}{n-1}\A^T\A  \\ 
	&=& \frac{1}{n-1}(\X_c\V)^T(\X_c\V) \\
	&=& \frac{1}{n-1}\V^T((\X_c^T\X_c)\V\\
	&=&\frac{1}{n-1}\V^T((n-1)\ssigma_X)\V\\
	&=&\V^T(\ssigma_X)\V\\
	&=&\V^T(\V\D\V^T)\V\\
	&=& \D
\end{eqnarray*}
Where $\ssigma_X=\V\D\V^T$ comes from the diagonalization in Theorem \@ref(thm:eigsym).
By changing our variables to principal components, we have managed to **"hide"** the correlation between $\x_1$ and $\x_2$ while keeping the spacial relationships between data points in tact. Transformation _back_ to variables $\x_1$ and $\x_2$ is easily done by using the linear relationships in from Equation \@ref(eq:pcacomb).
:::
 
## Geometrical comparison with Least Squares

 In least squares regression, our objective is to maximize the amount of variance explained in our target variable. It may look as though the first principal component from Example \@ref(exm:coveigs) points in the direction of the regression line. This is not the case however. The first principal component points in the direction of a line which minimizes the sum of squared _orthogonal_ distances between the points and the line. Regressing $\x_2$ on $\x_1$, on the other hand, provides a line which minimizes the sum of squared _vertical_ distances between points and the line. This is illustrated in Figure \@ref(fig:pcvsreg).
 
 ```{r fig=T,fig.align='center', fig.cap = 'Principal Components vs. Regression Lines', echo=F, out.width='60%', label='pcvsreg'}
knitr::include_graphics('figs/pcvsreg.jpg')
```


The first principal component about the mean of a set of points can be represented by that line which most closely approaches the data points. Let this not conjure up images of linear regression in your head, though.  In contrast, linear least squares tries to minimize the distance in a single direction only (the direction of your target variable axes). Thus, although the two use a similar error metric, linear least squares is a method that treats one dimension of the data preferentially, while PCA treats all dimensions equally.

You might be tempted to conclude from Figure \@ref(fig:pcvsreg) that the first principal component and the regression line "ought to be similar." This is a terrible conclusion if you consider a large multivariate dataset and the various regression lines that would predict each variable in that dataset. In PCA, there is no target variable and thus no single regression line that we'd be comparing to. 


## Covariance or Correlation Matrix?

Principal components analysis can involve eigenvectors of either the covariance matrix or the correlation matrix. When we perform this analysis on the covariance matrix, the geometric interpretation is simply centering the data and then determining the direction of maximal variance. When we perform this analysis on the correlation matrix, the interpretation is _standardizing_ the data and then determining the direction of maximal variance. The correlation matrix is simply a scaled form of the covariance matrix. In general, these two methods give different results, especially when the scales of the variables are different.

The covariance matrix is the default for (most) $\textsf{R}$ PCA functions. The correlation matrix is the default in SAS and the covariance matrix method is invoked by the option:

```
proc princomp data=X cov; 
var x1--x10;
run;
```

Choosing between the covariance and correlation matrix can sometimes pose problems. The rule of thumb is that the correlation matrix should be used when the scales of the variables vary greatly. In this case, the variables with the highest variance will dominate the first principal component.  The argument against automatically using correlation matrices is that it turns out to be quite a brutal way of standardizing your data - forcing all variables to contain the same amount of information (after all, don't we equate variance to information?) seems naive and counterintuitive when it is not absolutely necessary for differences in scale. We hope that the case studies outlined in Chapter \@ref(pcaapp) will give those who _always_ use the correlation option reason for pause, and we hope that, in the future, they will consider multiple presentations of the data and their corresponding low-rank representations of the data.  


## PCA in R

Let's find Principal Components using the iris dataset. This is a well-known dataset, often used to demonstrate the effect of clustering algorithms. It contains numeric measurements for 150 iris flowers along 4 dimensions. The fifth column in the dataset tells us what species of Iris the flower is. There are 3 species.

1. Sepal.Length
2. Sepal.Width
3. Petal.Length
4. Petal.Width
5. Species
 - Setosa
 - Versicolor
 - Virginica
 
Let's first take a look at the scatterplot matrix:

```{r fig.width=9, fig.height=8}
pairs(~Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,data=iris,col=c("red","green3","blue")[iris$Species])
```

It is apparent that some of our variables are correlated. We can confirm this by computing the correlation matrix with the `cor()` function. We can also check out the individual variances of the variables and the covariances between variables by examining the covariance matrix (`cov()` function). Remember - when looking at covariances, we can really only interpret the _sign_ of the number and not the magnitude as we can with the correlations.

```{r}
cor(iris[1:4])
cov(iris[1:4])
```

We have relatively strong positive correlation between Petal Length, Petal Width and Sepal Length. It is also clear that Petal Length has more than 3 times the variance of the other 3 variables. How will this effect our analysis?

The scatter plots and correlation matrix provide useful information, but they don't give us a true sense for how the data looks when all 4 attributes are considered simultaneously.

In the next section we will compute the principal components directly from eigenvalues and eigenvectors of the covariance or correlation matrix. __It's important to note that this method of _computing_ principal components is not actually recommended - the answer provided is the same, but the numerical stability and efficiency of this method may be dubious for large datasets. The Singular Value Decomposition (SVD), which will be discussed in Chapter \@ref(svd), is generally a preferred route to computing principal components. In fact, the numerical computation of principal components is outside of the scope of this book. We recommend [@cite1,@cite2, or @cite3] for a treatment of the topic.__ using both the covariance matrix and the correlation matrix, and see what we can learn about the data. Let's start with the covariance matrix which is the default setting for the `prcomp` function in R.

### Covariance PCA

 Let's start with the covariance matrix which is the default setting for the `prcomp` function in R. It's worth repeating that a dedicated principal component function like `prcomp()` is superior in numerical stability and efficiency to the lines of code in the next section. __The only reason for directly computing the covariance matrix and its eigenvalues and eigenvectors (as opposed to  `prcomp()`) is for edification. Computing a PCA in this manner, just this once, will help us grasp the exact mathematics of the situation and discover the nuances of them.__

### Principal Components, Loadings, and Variance Explained 

```{r}
covM = cov(iris[1:4])
eig=eigen(covM,symmetric=TRUE,only.values=FALSE)
c=colnames(iris[1:4])
eig$values
rownames(eig$vectors)=c(colnames(iris[1:4]))
eig$vectors
```

The eigenvalues tell us how much of the total variance in the data is directed along each eigenvector. Thus, the amount of variance along $\mathbf{v}_1$ is $\lambda_1$ and the _proportion_ of variance explained by the first principal component is
$$\frac{\lambda_1}{\lambda_1+\lambda_2+\lambda_3+\lambda_4}$$

```{r}
eig$values[1]/sum(eig$values)
```

Thus 92\% of the variation in the Iris data is explained by the first component alone. What if we consider the first and second principal component directions? Using this two dimensional representation (approximation/projection) we can capture the following proportion of variance:
$$\frac{\lambda_1+\lambda_2}{\lambda_1+\lambda_2+\lambda_3+\lambda_4}$$

```{r}
sum(eig$values[1:2])/sum(eig$values)
```

With two dimensions, we explain 97.8% of the variance in these 4 variables! The entries in each eigenvector are called the **loadings** of the variables on the component. The loadings give us an idea how important each variable is to each component. For example, it seems that the third variable in our dataset (Petal Length) is dominating the first principal component. This should not come as too much of a shock - that variable had (by far) the largest amount of variation of the four. In order to capture the most amount of variance in a single dimension, we should certainly be considering this variable strongly. The variable with the next largest variance, Sepal Length, dominates the second principal component.

**Note:**  *Had Petal Length and Sepal Length been correlated, they would not have dominated separate principal components, they would have shared one. These two variables are not correlated and thus their variation cannot be captured along the same direction.*

### Scores and PCA Projection 

Lets plot the *projection* of the four-dimensional iris data onto the two dimensional space spanned by the first 2 principal components. To do this, we need coordinates. These coordinates are commonly called **scores** in statistical texts. We can find the coordinates of the data on the principal components by solving the system
$$\mathbf{X}=\mathbf{A}\mathbf{V}^T$$
where $\mathbf{X}$ is our original iris data **(centered to have mean = 0)** and $\mathbf{A}$ is a matrix of coordinates in the new principal component space, spanned by the eigenvectors in $\mathbf{V}$.

Solving this system is simple enough - since $\mathbf{V}$ is an orthogonal matrix. Let's confirm this:

```{r}
eig$vectors %*% t(eig$vectors)
t(eig$vectors) %*% eig$vectors
```

We'll have to settle for precision at 15 decimal places. Close enough!

So to find the scores, we simply subtract the means from our original variables to create the data matrix $\mathbf{X}$ and compute
$$\mathbf{A}=\mathbf{X}\mathbf{V}$$

```{r }
# The scale function centers and scales by default
X=scale(iris[1:4],center=TRUE,scale=FALSE)
# Create data.frame from matrix for plotting purposes.
scores=data.frame(X %*% eig$vectors)
# Change default variable names
colnames(scores)=c("Prin1","Prin2","Prin3","Prin4")
# Print coordinates/scores of first 10 observations
scores[1:10, ]
```
 
To this point, we have simply computed coordinates (scores) on a new set of axis (principal components, eigenvectors, loadings). These axis are orthogonal and are aligned with the directions of maximal variance in the data. When we consider only a subset of principal components (like 2 components accounting for 97% of the variance), then we are projecting the data onto a lower dimensional space. Generally, this is one of the primary goals of PCA: Project the data down into a lower dimensional space (_onto the span of the principal components_) while keeping the maximum amount of information (i.e. variance).

Thus, we know that almost 98% of the data's variance can be seen in two-dimensions using the first two principal components. Let's go ahead and see what this looks like:

```{r fig=T, fig.width=5,fig.height=6}
plot(scores$Prin1, scores$Prin2, 
     main="Data Projected on First 2 Principal Components",
     xlab="First Principal Component", 
     ylab="Second Principal Component", 
     col=c("red","green3","blue")[iris$Species])
```

### PCA functions in R

```{r}
irispca=prcomp(iris[1:4])
# Variance Explained
summary(irispca)
# Eigenvectors:
irispca$rotation
# Coordinates of first 10 observations along PCs:
irispca$x[1:10, ]
```

All of the information we computed using eigenvectors aligns with what we see here, except that the coordinates/scores and the loadings of Principal Component 3 are of the opposite sign. In light of what we know about eigenvectors representing _directions_, this should be no cause for alarm. The `prcomp` function arrived at the unit basis vector pointing in the negative direction of the one we found directly from the `eig` function - which should negate all the coordinates and leave us with an equivalent mirror image in all of our projections.

### The Biplot

One additional feature that R users have created is the **biplot**. The PCA biplot allows us to see where our original variables fall in the space of the principal components. Highly correlated variables will fall along the same direction (or exactly opposite directions) as a change in one of these variables correlates to a change in the other. Uncorrelated variables will appear further apart. The length of the variable vectors on the biplot tell us the degree to which variability in variable is explained in that direction. Shorter vectors have less variability than longer vectors. So in the biplot below, petal width and petal length point in the same direction indicating that these variables share a relatively high degree of correlation. However, the vector for petal width is much shorter than that of petal length, which means you can expect a higher degree of change in petal length as you proceed to the right along PC1. PC1 explains more of the variance in petal length than it does petal width. If we were to imagine a third PC orthogonal to the plane shown, petal width is likely to exist at much larger angle off the plane - here, it is being projected down from that 3-dimensional picture.


```{r fig=T,fig.width=10, fig.height=8}
biplot(irispca, col = c("gray", "blue"))
```

We can examine some of the outlying observations to see how they align with these projected variable directions. It helps to compare them to the quartiles of the data. Also keep in mind the direction of the arrows in the plot. If the arrow points down then the positive direction is down - indicating observations which are greater than the mean. Let's pick out observations 42 and 132 and see what the actual data points look like in comparison to the rest of the sample population.
```{r}
summary(iris[1:4])
# Consider orientation of outlying observations:
iris[42, ]
iris[132, ]
```

## Variable Clustering with PCA

The direction arrows on the biplot are merely the coefficients of the original variables when combined to make principal components. Don't forget that principal components are simply linear combinations of the original variables.

For example, here we have the first principal component (the first column of $\V$), $\mathbf{v}_1$ as:

```{r}
eig$vectors[,1]
```

This means that the __coordinates of the data along__ the first principal component, which we'll denote here as $PC_1$ are given by a simple linear combination of our original variables after centering (for covariance PCA) or standardization (for correlation PCA)

$$PC_1 = 0.36Sepal.Length-0.08Sepal.Width+0.85Petal.Length +0.35Petal.Width$$
the same equation could be written for each of the vectors of coordinates along principal components, $PC_1,\dots, PC_4$.

Essentially, we have a system of equations telling us that the rows of $\V^T$ (i.e. the columns of $\V$) give us the weights of each variable for each principal component:
\begin{equation}
(\#eq:cpc1)
\begin{bmatrix} PC_1\\PC_2\\PC_3\\PC_4\end{bmatrix} = \mathbf{V}^T\begin{bmatrix}Sepal.Length\\Sepal.Width\\Petal.Length\\Petal.Width\end{bmatrix}
\end{equation}

Thus, if want the coordinates of our original variables in terms of Principal Components (so that we can plot them as we do in the biplot) we need to look no further than the rows of the matrix $\mathbf{V}$ as
\begin{equation}
(\#eq:cpc2)
\begin{bmatrix}Sepal.Length\\Sepal.Width\\Petal.Length\\Petal.Width\end{bmatrix} =\mathbf{V}\begin{bmatrix} PC_1\\PC_2\\PC_3\\PC_4\end{bmatrix}
\end{equation}

means that the rows of $\mathbf{V}$ give us the coordinates of our original variables in the PCA space. The transition from Equation \@ref(eq:cpc1) to Equation  \@ref(eq:cpc2) is provided by the orthogonality of the eigenvectors per Theorem \@ref(thm:eigsym).

```{r}
#First entry in each eigenvectors give coefficients for Variable 1:
eig$vectors[1,]
```

$$Sepal.Length = 0.361 PC_1 - 0.657 PC_2 - 0.582 PC_3 + 0.315 PC_4$$
You can see this on the biplot. The vector shown for Sepal.Length is (0.361, -0.656), which is the two dimensional projection formed by throwing out components 3 and 4.

Variables which lie upon similar directions in the PCA space tend to change together in a similar fashion. We might consider Petal.Width and Petal.Length as a cluster of variables because they share a direction on the biplot, which means they represent much of the same information (the underlying construct being the "size of the petal" in this case).


### Correlation PCA

We can complete the same analysis using the correlation matrix. I'll leave it as an exercise to compute the Principal Component loadings and scores and variance explained directly from eigenvectors and eigenvalues. You should do this and compare your results to the R output. _(Beware: you must transform your data before solving for the scores. With the covariance version, this meant centering - for the correlation version, this means standardization as well)_

```{r fig=T}
irispca2=prcomp(iris[1:4], cor=TRUE)
summary(irispca2)
irispca2$rotation
irispca2$x[1:10,]
plot(irispca2$x[,1],irispca2$x[,2],
     main="Data Projected on First 2  Principal Components",
     xlab="First Principal Component",
     ylab="Second Principal Component",
     col=c("red","green3","blue")[iris$Species])
```
```{r fig=T}
biplot(irispca2)
```


Here you can see the direction vectors of the original variables are relatively uniform in length in the PCA space. This is due to the standardization in the correlation matrix. However, the general message is the same: Petal.Width and Petal.Length Cluster together, and many of the same observations appear "on the fray" on the PCA space - although not all of them!

### Which Projection is Better? 

What do you think? It depends on the task, and it depends on the data. One flavor of PCA is not "better" than the other. Correlation PCA is appropriate when the scales of your attributes differ wildly, and covariance PCA would be inappropriate in that situation. But in all other scenarios, when the scales of our attributes are roughly the same, we should always consider both dimension reductions and make a decision based upon the resulting output (variance explained, projection plots, loadings). 

For the iris data, The results in terms of variable clustering are pretty much the same. For clustering/classifying the 3 species of flowers, we can see better separation in the covariance version.

### Beware of biplots 

Be careful not to draw improper conclusions from biplots. Particularly, be careful about situations where the first two principal components do not summarize the majority of the variance. If a large amount of variance is captured by the 3rd or 4th (or higher) principal components, then we must keep in mind that the variable projections on the first two principal components are flattened out versions of a higher dimensional picture. If a variable vector appears short in the 2-dimensional projection, it means one of two things:

- That variable has small variance
- That variable appears to have small variance when depicted in the space of the first two principal components, but truly has a larger variance which is represented by 3rd or higher principal components.


Let's take a look at an example of this. We'll generate 500 rows of data on 4 nearly independent normal random variables. Since these variables are uncorrelated, we might expect that the 4 orthogonal principal components will line up relatively close to the original variables. If this doesn't happen, then at the very least we can expect the biplot to show little to no correlation between the variables. We'll give variables $2$ and $3$ the largest variance. Multiple runs of this code will generate different results with similar implications.

```{r fig=TRUE, fig.align ='center', fig.cap = 'BiPlot of Iris Data', id='irisbiplot'}
means=c(2,4,1,3)
sigmas=c(7,9,10,8)
sample.size=500
data=mapply(function(mu,sig){rnorm(mu,sig, n=sample.size)},mu=means,sig=sigmas)
cor(data)

pc=prcomp(data,scale=TRUE)
summary(pc)
pc$rotation

biplot(pc)
```

Obviously, the wrong conclusion to make from this biplot is that Variables 1 and 4 are correlated. Variables 1 and 4 do not load highly on the first two principal components - in the _whole_ 4-dimensional principal component space they are nearly orthogonal to each other and to variables 1 and 2. Thus, their orthogonal projections appear near the origin of this 2-dimensional subspace.

The morals of the story: 
- Always corroborate your results using the variable loadings and the amount of variation explained by each variable.\
- When a variable shows up near the origin in a biplot, it is generally not well represented by your two-dimensional approximation of the data.



<!--chapter:end:03-PCA.Rmd-->

# Applications of Principal Components {#pcaapp}

```{r, echo=F}
thmcounter=0
excounter=0
cid='pcaapp'
```
 Principal components have a number of applications across many areas of statistics. In the next sections, we will explore their usefulness in the context of dimension reduction. In Chapter \ref{advanced} we will look at how PCA is used to solve the issue of multicollinearity in biased regression.


## Dimension reduction

It is quite common for an analyst to have too many variables. There are two different solutions to this problem:

1. **Feature Selection**: Choose a subset of existing variables to be used in a model. 
2. **Feature Extraction**: Create a new set of features which are combinations of original variables.


### Feature Selection 

Let's think for a minute about feature selection. What are we really doing when we consider a subset of our existing variables? Take the two dimensional data in Example \ref{ex:pcabasis} (while two-dimensions rarely necessitate dimension reduction, the geometrical interpretation extends to higher dimensions as usual!). The centered data appears as follows:

<img src={figs/pcpointscenter.pdf width=40%/>

Now say we perform some kind of feature selection (there are a number of ways to do this, chi-square tests for instances) and we determine that the variable $\x_2$ is more important than $\x_1$. So we throw out $\x_2$ and we've reduced the dimensions from $p=2$ to $k=1$. Geometrically, what does our new data look like? By dropping $\x_1$ we set all of those horizontal coordinates to zero. In other words, we **project the data orthogonally** onto the $\x_2$ axis:

\begin{figure}[h!]
\centering
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{figs/pcpointsselect1.pdf}
\caption{Projecting Data Orthogonally}
\label{select1}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{figs/pcpointsselect2.pdf}
\caption{New One-Dimensional Data}
\label{select2}
\end{subfigure}
\caption{Geometrical Interpretation of Feature Selection}
\end{figure}

Now, how much information (variance) did we lose with this projection? The total variance in the original data is 
$$\|\x_1\|^2+\|\x_2\|^2.$$
The variance of our data reduction is
$$\|\x_2\|^2.$$
Thus, the proportion of the total information (variance) we've kept is
$$\frac{\|\x_2\|^2}{\|\x_1\|^2+\|\x_2\|^2}=\frac{6.01}{5.6+6.01} = 51.7\%.$$
Our reduced dimensional data contains only 51.7\% of the variance of the original data. We've lost a lot of information!  

The fact that feature selection omits variance in our predictor variables does not make it a bad thing! Obviously, getting rid of variables which have no relationship to a target variable (in the case of _supervised_ modeling like prediction and classification) is a good thing. But, in the case of _unsupervised_ learning techniques, where there is no target variable involved, we must be extra careful when it comes to feature selection. In summary,

1. Feature Selection is important. Examples include:

  a. Removing variables which have little to no impact on a target variable in supervised modeling (forward/backward/stepwise selection).
  b. Removing variables which have obvious strong correlation with other predictors.
  c. Removing variables that are not interesting in unsupervised learning (For example, you may not want to use the words ``th'' and ``of'' when clustering text).

2. Feature Selection is an orthogonal projection of the original data onto the span of the variables you choose to keep.
3. Feature selection should always be done with care and justification. 

  d. In regression, could create problems of endogeneity (errors correlated with predictors - omitted variable bias).
  e. For unsupervised modelling, could lose important information. 


### Feature Extraction 

PCA is the most common form of feature extraction. The rotation of the space shown in Example \ref{ex:pcabasis} represents the creation of new features which are linear combinations of the original features. If we have $p$ potential variables for a model and want to reduce that number to $k$, then the first $k$ principal components combine the individual variables in such a way that is guaranteed to capture as much ``information'' (variance) as possible. Again, take our two-dimensional data as an example. When we reduce our data down to one-dimension using principal components, we essentially do the same orthogonal projection that we did in Feature Selection, only in this case we conduct that projection in the new basis of principal components. Recall that for this data, our first principal component $\v_1$ was $$\v_1 = \pm 0.69 \\0.73 \mp.$$
Projecting the data onto the first principal component is illustrated in Figure \ref{pcaproj}
\begin{figure}[h!]
\centering
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{figs/pcproj1.pdf}
\caption{Projecting Data Orthogonally}
\label{select1}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
\includegraphics[width=\textwidth]{figs/pcproj2.pdf}
\caption{New One-Dimensional Data}
\label{select2}
\end{subfigure}
\caption{Illustration of Feature Extraction via PCA}
\label{pcaproj}
\end{figure}
 How much variance do we keep with $k$ principal components? The proportion of variance explained by each principal component is the ratio of the corresponding eigenvalue to the sum of the eigenvalues (which gives the total amount of variance in the data).

```{r, echo=F} 
thmcounter=thmcounter+1
```
```{thm title='Proportion of Variance Explained', id='pcpropvar', num=thmcounter}
The proportion of variance explained by the projection of the data onto principal component $\v_i$ is
$$\frac{\lambda_i}{\sum_{j=1}^p \lambda_j}.$$
Similarly, the proportion of variance explained by the projection of the data onto the first $k$ principal components ($k<j$) is
$$ \frac{\sum_{i=1}^k\lambda_i}{\sum_{j=1}^p \lambda_j}$$
```

In our simple 2 dimensional example we were able to keep
$$\frac{\lambda_1}{\lambda_1+\lambda_2}=\frac{10.61}{10.61+1.00} = 91.38\%$$
of our variance in one dimension. 

## Exploratory Analysis

### UK Food Consumption

#### Explore the Data

The data for this example can be read directly from our course webpage. When we first examine the data, we will see that the rows correspond to different types of food/drink and the columns correspond to the 4 countries within the UK. Our first matter of business is transposing this data so that the 4 countries become our observations (i.e. rows).

```{r echo=F, id='ukfood'}
load(file='LAdata/ukfood.RData')
```

```{r eval=F}
food=read.csv("http://birch.iaa.ncsu.edu/~slrace/LinearAlgebra2021/Code/ukfood.csv",
              header=TRUE,row.names=1)
```

```{r}
library(reshape2) #melt data matrix into 3 columns
library(ggplot2) #heatmap
head(food)
food=as.data.frame(t(food))
head(food)
```

Next we will visualize the information in this data using a simple heat map. To do this we will standardize and then melt the data using the \blue{reshape2} package, and then use a \blue{ggplot()} heatmap.

```{r fig=T, fig.align='center'}
food.std = scale(food, center=T, scale = T)
food.melt = melt(food.std, id.vars = row.names(food.std), measure.vars = 1:17)
ggplot(data = food.melt, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile(color = "white")+
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-2,2), space = "Lab" 
                       ) +  theme_minimal()+ 
  theme(axis.title.x = element_blank(),axis.title.y = element_blank(),
        axis.text.y = element_text(face = 'bold', size = 12, colour = 'black'),
        axis.text.x = element_text(angle = 45, vjust = 1, face = 'bold',
                                   size = 12, colour = 'black', hjust = 1))+coord_fixed()
```

<!-- \includegraphics[width=.4\textwidth]{heatmap.png} -->

#### ```prcomp()``` function for PCA

The ```prcomp()``` function is the one I most often recommend for reasonably sized principal component calculations in R. This function returns a list with class "prcomp" containing the following components (from help prcomp):

\begin{enumerate}
\item ```sdev```: the standard deviations of the principal components (i.e., the square roots of the eigenvalues of the covariance/correlation matrix, though the calculation is actually done with the singular values of the data matrix).
\item ```rotation```: the matrix of \textit{variable loadings} (i.e., a matrix whose columns contain the eigenvectors). The function princomp returns this in the element loadings.
 \item ```x```:  if retx is true \textit{the value of the rotated data (i.e. the scores)} (the centred (and scaled if requested) data multiplied by the rotation matrix) is returned. Hence, cov(x) is the diagonal matrix $diag(sdev^2)$. For the formula method, napredict() is applied to handle the treatment of values omitted by the na.action.
\item ```center```, ``` scale```: the centering and scaling used, or FALSE.
\end{enumerate}

The option ```scale = TRUE``` inside the ```prcomp()``` function instructs the program to use **orrelation PCA**. *The default is covariance PCA*. 


```{r}
pca=prcomp(food, scale = T) 
```

This first plot just looks at magnitudes of eigenvalues - it is essentially the screeplot in barchart form.

```{r fig=T, fig.align='center'}
summary(pca)
plot(pca, main = "Bar-style Screeplot")
```


The next plot views our four datapoints (locations) projected onto the 2-dimensional subspace
 (from 17 dimensions) that captures as much information (i.e. variance) as possible.
 
```{r fig=T,fig.align='center'}
plot(pca$x, 
     xlab = "Principal Component 1",
     ylab = "Principal Component 2", 
     main = 'The four observations projected into 2-dimensional space')
text(pca$x[,1], pca$x[,2],row.names(food))
```

#### The BiPlot

Now we can also view our original variable axes projected down onto that same space!

```{r fig=T, ,fig.align='center', fig.cap='BiPlot: The observations and variables projected onto the same plane.'}
biplot(pca$x,pca$rotation, cex = c(1.5, 1), col = c('black','red'))#, 
      # xlim = c(-0.8,0.8), ylim = c(-0.6,0.7))
```
<!-- \includegraphics[width=\textwidth]{biplot.png} -->
<!-- \caption{} -->


#### Formatting the biplot for readability

I will soon introduce the ```autoplot()``` function from the ```ggfortify`` package, but for now I just want to show you that you can specify _which_ variables (and observations) to include in the biplot by directly specifying the loadings matrix and scores matrix of interest in the biplot function:

```{r fig=T, fig.align='center'}
desired.variables = c(2,4,6,8,10)
biplot(pca$x, pca$rotation[desired.variables,1:2], cex = c(1.5, 1), 
       col = c('black','red'), xlim = c(-6,5), ylim = c(-4,4))
```

#### What are all these axes?

Those numbers relate to the scores on PC1 and PC2 (sometimes normalized so that each new variable has variance 1 - and sometimes not) and the loadings on PC1 and PC2 (sometimes normalized so that each variable vector is a unit vector - and sometimes scaled by the eigenvalues or square roots of the eigenvalues in some fashion).

Generally, I've never found it useful to hunt down how each package is rendering the biplot, as they should be providing the same information regardless of the _numbers_ on the axes. We don't actually use those numbers to help us draw conclusions. We use the directions of the arrows and the layout of the points in reference to those direction arrows. 

```{r,  fig.align='center'}
vmax = varimax(pca$rotation[,1:2])
new.scores = pca$x[,1:2] %*% vmax$rotmat

biplot(new.scores, vmax$loadings[,1:2], 
       # xlim=c(-60,60),
       # ylim=c(-60,60),
       cex = c(1.5, 1),
       xlab = 'Rotated Axis 1',
       ylab = 'Rotated Axis 2')

vmax$loadings[,1:2]
```

<!--chapter:end:035-PCA_apps_ukfood.Rmd-->

## FIFA Soccer Players

#### Explore the Data

We begin by loading in the data and taking a quick look at the variables that we'll be using in our PCA for this exercise. You may need to install the packages from the following ```library()``` statements.

```{r results='hide'}
library(reshape2) #melt correlation matrix into 3 columns
library(ggplot2) #correlation heatmap
library(ggfortify) #autoplot bi-plot
library(viridis) # magma palette
library(plotrix) # color.legend
```

Now we'll read the data directly from the web, take a peek at the first 5 rows, and explore some summary statistics.

```{r echo=F}
load("LAdata/fifa.RData")
head(fifa)
summary(fifa[,13:46])
```

These variables are scores on the scale of [0,100] that measure 34 key abilities of soccer players. No player has ever earned a score of 100 on any of these attributes - no player is _perfect_!

It would be natural to assume some correlation between these variables and indeed, we see lots of it in the following heatmap visualization of the correlation matrix.

```{r fig = T, fig.align='center', fig.cap='Heatmap of correlation matrix for 34 variables of interest'}
cor.matrix = cor(fifa[,13:46])
cor.matrix = melt(cor.matrix)
ggplot(data = cor.matrix, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile(color = "white")+
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
    name="Correlation") +  theme_minimal()+ 
      theme(axis.title.x = element_blank(),axis.title.y = element_blank(),
            axis.text.x = element_text(angle = 45, vjust = 1, 
        size = 9, hjust = 1))+coord_fixed()
```



What jumps out right away are the "GK" (Goal Keeping) abilities - these attributes have _very_ strong positive correlation with one another and negative correlation with the other abilities. After all, goal keepers are not traditionally well known for their dribbling, passing, and finishing abilities!

Outside of that, we see a lot of red in this correlation matrix -- many attributes share a lot of information. This is the type of situation where PCA shines.


#### Principal Components Analysis

Let's take a look at the principal components analysis. Since the variables are on the same scale, I'll start with **covariance PCA** (the default in R's ```prcomp()``` function). 

```{r echo=F}
load('LAdata/fifapca.RData')
```
```{r eval=F }
fifa.pca = prcomp(fifa[,13:46] )
```

We can then print the summary of variance explained and the loadings on the first 3 components:

```{r}
summary(fifa.pca)
fifa.pca$rotation[,1:3]
```

It's clear we can capture a large amount of the variance in this data with just a few components. In fact **just 2 components yield 76% of the variance!**

Now let's look at some projections of the players onto those 2 principal components. The scores are located in the ```fifa.pca$x``` matrix.

```{r fig=T, fig.align='center', fig.cap="Projection of the FIFA players' skill data into 2 dimensions. Player positions are evident." }
plot(fifa.pca$x[,1],fifa.pca$x[,2], col=alpha(c('red','blue','green','black')[as.factor(fifa$position)],0.4), pch=16, xlab = 'Principal Component 1', ylab='Principal Component 2', main = 'Projection of Players onto 2 PCs, Colored by Position')
legend(125,-45, c('Forward','Defense','Midfield','GoalKeeper'), c('red','blue','green','black'), bty = 'n', cex=1.1)
```

The plot easily separates the field players from the goal keepers, and the forwards from the defenders. As one might expect, midfielders are sandwiched by the forwards and defenders, as they play both roles on the field. The labeling of player position was imperfect and done using a list of the players' preferred positions, and it's likely we are seeing that in some of the players labeled as midfielders that appear above the cloud of red points. 

We can also attempt a 3-dimensional projection of this data:

```{r fig=T, fig.align='center', fig.cap="Projection of the FIFA players' skill data into 3 dimensions. Player positions are evident.", message=F}
library(plotly)
library(processx)
colors=alpha(c('red','blue','green','black')[as.factor(fifa$position)],0.4)
graph = plot_ly(x = fifa.pca$x[,1], 
                y = fifa.pca$x[,2],
                z= fifa.pca$x[,3],
                type='scatter3d', 
                mode="markers",
                marker = list(color=colors))
graph
```

#### The BiPlot

BiPlots can be tricky when we have so much data and so many variables. As you will see, the default image leaves much to be desired, and will motivate our move to the ```ggfortify``` library to use the ```autoplot()``` function. The image takes too long to render and is practically unreadable with the whole dataset, so I demonstrate the default ```biplot()``` function with a sample of the observations.

(ref:fifabadplot) The default biplot function leaves much to be desired here

```{r fig=T, fig.align='center',fig.cap='(ref:fifabadplot)'}
biplot(fifa.pca$x[sample(1:16501,2000),],fifa.pca$rotation[,1:2], cex=0.5, arrow.len = 0.1)
```



The autoplot function uses the `ggplot2``` package and is superior when we have more data.

(ref:fifagoodplot) The ```autoplot()``` biplot has many more options for readability.

```{r fig=T,fig.align='center',fig.cap='(ref:fifagoodplot)'}
autoplot(fifa.pca, data = fifa, 
         colour = alpha(c('red','blue','green','orange')[as.factor(fifa$pos)],0.4),
         loadings = TRUE, loadings.colour = 'black',
         loadings.label = TRUE, loadings.label.size = 3.5, loadings.label.alpha = 1,
         loadings.label.fontface='bold',
         loadings.label.colour = 'black', 
         loadings.label.repel=T)
```


Many expected conclusions can be drawn from this biplot. The defenders tend to have stronger skills of _interception, slide tackling, standing tackling,_ and _marking_, while forwards are generally stronger when it comes to _finishing, long.shots, volleys, agility_ etc. Midfielders are likely to be stronger with _crossing, passing, ball.control,_ and _stamina._ 

#### Further Exploration

Let's see what happens if we color by the variable 'overall' which is designed to rank a player's overall quality of play.

(ref:fifaoverall) Projection of Players onto 2 PCs, Colored by "Overall" Ability

```{r fig=T, fig.align='center', fig.cap='(ref:fifaoverall)'}
palette(alpha(magma(100),0.6))

plot(fifa.pca$x[,1],fifa.pca$x[,2], col=fifa$Overall,pch=16, xlab = 'Principal Component 1', ylab='Principal Component 2')

color.legend(130,-100,220,-90,seq(0,100,50),alpha(magma(100),0.6),gradient="x")
```

We can attempt to label some of the outliers, too. First, we'll look at the 0.001 and 0.999 quantiles to get a sense of what coordinates we want to highlight. Then we'll label any players outside of those bounds and surely find some familiar names.

```{r fig=T, fig.align='center'}
# This first chunk is identical to the chunk above. I have to reproduce the plot to label it.
palette(alpha(magma(100),0.6))
plot(fifa.pca$x[,1], fifa.pca$x[,2], col=fifa$Overall,pch=16, xlab = 'Principal Component 1', ylab='Principal Component 2',
     xlim=c(-175,250), ylim = c(-150,150))
color.legend(130,-100,220,-90,seq(0,100,50),alpha(magma(100),0.6),gradient="x")

# Identify quantiles (high/low) for each PC
(quant1h = quantile(fifa.pca$x[,1],0.9997))
(quant1l = quantile(fifa.pca$x[,1],0.0003))

(quant2h = quantile(fifa.pca$x[,2],0.9997))
(quant2l = quantile(fifa.pca$x[,2],0.0003))
# Next I create a logical vector which identifies the outliers 
# (i.e. TRUE = outlier, FALSE = not outlier)
outliers = fifa.pca$x[,1] > quant1h | fifa.pca$x[,1] < quant1l |
                  fifa.pca$x[,2] > quant2h | fifa.pca$x[,2] < quant2l
# Here I label them by name, jittering the coordinates of the text so it's more readable
text(jitter(fifa.pca$x[outliers,1],factor=1), jitter(fifa.pca$x[outliers,2],factor=600), fifa$Name[outliers], cex=0.7)
```

What about by wage? First we need to convert their salary, denominated in Euros, to a numeric variable.

```{r fig=T, fig.align='center', fig.cap = 'Projection of Players onto 2 Principal Components, Colored by Wage'}
# First, observe the problem with the Wage column as it stands
head(fifa$Wage)
# Use regular expressions to remove the Euro sign and K from the wage column
# then covert to numeric
fifa$Wage = as.numeric(gsub('[€K]', '', fifa$Wage))
# new data:
head(fifa$Wage)

palette(alpha(magma(100),0.6))

plot(fifa.pca$x[,1], fifa.pca$x[,2], col=fifa$Wage,pch=16, xlab = 'Principal Component 1', ylab='Principal Component 2')

color.legend(130,-100,220,-90,c(min(fifa$Wage),max(fifa$Wage)),alpha(magma(100),0.6),gradient="x")
```


#### Rotations of Principal Components

We might be able to align our axes more squarely with groups of original variables that are strongly correlated and tell a story. Perhaps we might be able to find latent variables that indicate the position specific ability of players. Let's see what falls out after varimax and quartimax rotation. Recall that in order to employ rotations, we have to first decide on a number of components. A quick look at a screeplot or cumulative proportion variance explained should help to that aim.
 
```{r fig.align='center', fig.cap = 'Cumulative proportion of variance explained by rank of the decomposition (i.e. the number of components)' }
plot(cumsum(fifa.pca$sdev^2)/sum(fifa.pca$sdev^2),
     type = 'b',
     cex=.75,
     xlab = "# of components",
     ylab = "% variance explained")
```

Let's use 3 components, since the marginal benefit of using additional components seems small. Once we rotate the loadings, we can try to use a heatmap to visualize what they might represent.

```{r fig.align='center'}
vmax = varimax(fifa.pca$rotation[,1:3])
loadings = fifa.pca$rotation[,1:3]%*%vmax$rotmat
melt.loadings = melt(loadings)
ggplot(data = melt.loadings, aes(x=Var2, y=Var1, fill=value)) + 
  geom_tile(color = "white")+
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1))
```

<!--chapter:end:036-PCA-fifa.Rmd-->

## Cancer Genetics

Read in the data. The load() function reads in a dataset that has 20532 columns and may take some time. You may want to save and clear your environment (or open a new RStudio window) if you have other work open.

```{r}
load('LAdata/geneCancerUCI.RData')
table(cancerlabels$Class)
```

Original Source: _The cancer genome atlas pan-cancer analysis project_

- BRCA = Breast Invasive Carcinoma
- COAD = Colon Adenocarcinoma
- KIRC = Kidney Renal clear cell Carcinoma
- LUAD = Lung Adenocarcinoma
- PRAD = Prostate Adenocarcinoma

We are going to want to plot the data points according to their different classification labels. We should pick out a nice color palette for categorical attributes. We chose to assign palette `Dark2` but feel free to choose any categorical palette that attracts you in the code below!

```{r eval=F}
library(RColorBrewer)
display.brewer.all()
palette(brewer.pal(n = 8, name = "Dark2"))
```

The first step is typically to explore the data. Obviously we can't look at ALL the scatter plots of input variables. For the fun of it, let's look at a few of these scatter plots which we'll pick at random. First pick two column numbers at random, then draw the plot, coloring by the label. You could repeat this chunk several times to explore different combinations. Can you find one that does a good job of separating any of the types of cancer?

```{r fig=T, fig.cap = 'Random 2-Dimensional Projections of Cancer Data', fig.align='center'}
par(mfrow=c(2,3))
for(i in 1:6){
randomColumns = sample(2:20532,2)
plot(cancer[,randomColumns],col = cancerlabels$Class)
}
```
To restore our plot window from that 3-by-2 grid, we run ``dev.off()``
```{r fig=T, fig.cap = 'Random 2-Dimensional Projection of Cancer Data', fig.align='center'}
dev.off()
```

### Computing the PCA

 The \blue{prcomp()} function is the one I most often recommend for reasonably sized principal component calculations in R. This function returns a list with class "prcomp" containing the following components (from help prcomp):


1. \red{sdev}: the standard deviations of the principal components (i.e., the square roots of the eigenvalues of the covariance/correlation matrix, though the calculation is actually done with the singular values of the data matrix).
2. \red{rotation}: the matrix of _variable loadings_ (i.e., a matrix whose columns contain the eigenvectors). The function princomp returns this in the element loadings.
3. \red{x}:  if retx is true _the value of the rotated data (i.e. the scores)_ (the centred (and scaled if requested) data multiplied by the rotation matrix) is returned. Hence, cov(x) is the diagonal matrix $diag(sdev^2)$. For the formula method, napredict() is applied to handle the treatment of values omitted by the na.action.
4. \red{center, scale}: the centering and scaling used, or FALSE.


The option \blue{scale = TRUE} inside the \blue{prcomp()} function instructs the program to use **correlation PCA**. The **default is covariance PCA**. 

Now let's compute the _first three_ principal components and examine the data projected onto the first 2 axes. We can then look in 3 dimensions.
```{r echo=F}
load('LAdata/cancerpca.RData')
```

```{r eval=F}
pcaOut = prcomp(cancer,rank = 3, scale = F)
```

```{r fig=T, fig.cap = 'Covariance PCA of genetic data', fig.align='center'}

plot(pcaOut$x[,1], pcaOut$x[,2], 
     col = cancerlabels$Class, 
     xlab = "Principal Component 1",
     ylab = "Principal Component 2", 
     main = 'Genetic Samples Projected into 2-dimensions \n using COVARIANCE PCA')
```

### 3D plot with \blue{plotly} package 

Make sure the plotly package is installed for the 3d plot. To get the plot points colored by group, we need to execute the following command that creates a vector of colors (specifying a color for each observation).

```{r}
colors = factor(palette())
colors = colors[cancerlabels$Class]
table(colors, cancerlabels$Class)
```

```{r eval=F}
library(plotly)
graph = plot_ly(x = pcaOut$x[,1], 
                y = pcaOut$x[,2],
                z= pcaOut$x[,3],
                type='scatter3d', 
                mode="markers",
                marker = list(color=colors))
graph
```

### 3D plot with \blue{rgl} package
```{r, setup}
library(rgl)
knitr::knit_hooks$set(webgl = hook_webgl)
```

Make sure the rgl package is installed for the 3d plot.

```{r, webgl=TRUE}
plot3d(x = pcaOut$x[,1],
       y = pcaOut$x[,2],
       z= pcaOut$x[,3],
       col = colors, 
       xlab = "Principal Component 1", 
       ylab = "Principal Component 2", 
       zlab = "Principal Component 3")
```


### Variance explained

Proportion of Variance explained by 2,3 components:
```{r}
summary(pcaOut)
# Alternatively, if you had computed the ALL the principal components (omitted the rank=3 option) then 
# you could directly compute the proportions of variance explained using what we know about the 
# eigenvalues:

# sum(pcaOut$sdev[1:2]^2)/sum(pcaOut$sdev^2)
# sum(pcaOut$sdev[1:3]^2)/sum(pcaOut$sdev^2)
```

### Using Correlation PCA

The data involved in this exercise are actually on the same scale, and normalizing them may not be in your best interest because of this. However, it's always a good idea to explore both decompositions if you have time. 

```{r, eval=F}
pca.cor = prcomp(cancer, rank=3, scale =T)
```

An error message! Cannot rescale a constant/zero column to unit variance. Solution: check for columns with zero variance and remove them. Then, re-check dimensions of the matrix to see how many columns we lost.

```{r fig=T, fig.cap = 'Correlation PCA of genetic data', fig.align='center'}
cancer = cancer[,apply(cancer, 2, sd)>0 ]
dim(cancer)
```
```{r, echo=F}
load('LAdata/cancerpcacor.RData')
```

Once we've taken care of those zero-variance columns, we can proceed to compute the correlation PCA:

```{r, eval=F}
pca.cor = prcomp(cancer, rank=3, scale =T)
```

```{r fig=T, fig.cap = 'Correlation PCA of genetic data', fig.align='center'}
plot(pca.cor$x[,1], pca.cor$x[,2], 
     col = cancerlabels$Class, 
     xlab = "Principal Component 1",
     ylab = "Principal Component 2", 
     main = 'Genetic Samples Projected into 2-dimensions \n using CORRELATION PCA')
```

And it's clear just from the 2-dimensional projection that correlation PCA does not seem to work as well as covariance PCA when it comes to separating the 4 different types of cancer. 

Indeed, we can confirm this from the proportion of variance explained, which is substantially lower than that of covariance PCA:
```{r}
summary(pca.cor)
```

### Range standardization as an alternative to covariance PCA

We can also put all the variables on a scale of 0 to 1 if we're concerned about issues with scale (in this case, scale wasn't an issue - but the following approach still might be provide interesting projections in some datasets). This transformation would be as follows for each variable $\mathbf{x}$:
$$\frac{\mathbf{x} - \min(\mathbf{x})}{\max(\mathbf{x})-\min(\mathbf{x})}$$


```{r}
cancer = cancer[,apply(cancer,2,sd)>0]

min = apply(cancer,2,min)
range =   apply(cancer,2, function(x){max(x)-min(x)})
minmax.cancer=scale(cancer,center=min,scale=range)  
```

```{r, echo=F}
load('LAdata/cancerpcaminmax.RData')
```

Then we can compute the covariance PCA of that range-standardized data without concern:

```{r, eval=F}
minmax.pca = prcomp(minmax.cancer, rank=3, scale=F )  
```

```{r fig=T, fig.cap = 'Covariance PCA of range standardized genetic data', fig.align='center'}
plot(minmax.pca$x[,1],minmax.pca$x[,2],col = cancerlabels$Class, xlab = "Principal Component 1", ylab = "Principal Component 2")
```

<!--chapter:end:037-cancer.Rmd-->

# The Singular Value Decomposition (SVD) {#svd}

The Singular Value Decomposition (SVD) is one of the most important concepts in applied mathematics. It is used for a number of application including dimension reduction and data analysis. Principal Components Analysis (PCA) is a special case of the SVD. Let's start with the formal definition, and then see how PCA relates to that definition.

:::{.definition name='Singular Value Decomposition' #svddef}
For any $m\times n$ matrix $\A$ with $rank(\A)=r$, there are orthogonal matrices $\U_{m\times m}$ and $\V_{n\times n}$ and a diagonal matrix $\D_{r\times r}=diag(\sigma_1,\sigma_2,\dots,\sigma_r)$ such that
\begin{equation}
(\#eq:svd)
\A = \U \underbrace{\pm \D & \bo{0} \\\bo{0}&\bo{0} \mp}_{\text{$m\times n$}} \V^T \quad \mbox{with}\quad \sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_r\geq 0
\end{equation}
The $\sigma_i$'s are called the nonzero __singular values__ of $\A$. (When $r<p=\min\{m,n\}$ (i.e. when $\A$ is not full-rank), $\A$ is said to have an additional $p-r$ zero singular values). This factorization is called a __singular value decomposition__ of $\A$, and the columns of $\U$ and $\V$ are called the left- and right-hand __singular vectors__ for $\A$, respectively.\

__Properties of the SVD__
1. The left-hand singular vectors are a set of orthonormal eigenvectors for $\A\A^T$.\
2. The right-hand singular vectors are a set of orthonormal eigenvectors for $\A^T\A$.\
3. The singular values are the square roots of the eigenvalues for $\A^T\A \mbox{  and  } \A\A^T$, as these matrices have the same eigenvalues.\
4. The first singular value is equal to the matrix two-norm:
   $$\sigma_1 = \max_{\|\x\|=1} \|\A\x\|_2 = \|\A\|_2$$
5. The Frobenius norm of the matrix is also related to the singular values:
     $$\|\A\|_F = \sqrt{\sum_{i,j} A_{ij}^2} = \sqrt{\sum_{i=1}^r \sigma_i^2}$$
6. Singular values represent distances to lower rank matrices.  
   $$\sigma_{k+1}=\min_{rank(\bo{B})=k} \|\A-\bo{B}\|_2$$
7. The _truncated SVD_ (Equation \@ref(eq:truncsvd)) provides the _closest_ rank k approximation to our original matrix in the Euclidean sense. \
 
:::

When we studied PCA, one of the goals was to find the new coordinates, or _scores_, of the data in the principal components basis. If our original (centered or standardized) data was contained in the matrix $\X$ and the eigenvectors of the covariance/correlation matrix ($\X^T\X$) were columns of a matrix $\V$, then to find the scores (call these $\mathbf{S}$) of the observations on the eigenvectors we used the following equation (which is the transpose of Equation \@ref(eq:cpc2)):
$$\X=\mathbf{S}\V^T.$$
This equation mimics Equation \@ref(eq:svd) because the matrix $\V^T$ in Equation \ref(eq:svd) is also a matrix of eigenvectors for $\A^T\A$. This means that the principal component scores $\mathbf{S}$ are actually a set of unit eigenvectors for $\A\A^T$ scaled by the singular values in $\D$:
$$\mathbf{S}=\U \pm \D & \bo{0} \\ \bo{0}&\bo{0} \mp .$$

## Resolving a Matrix into Components

One of the primary goals of the singular value decomposition is to resolve the data in $\A$ into $r$ mutually orthogonal components by writing the matrix factorization as a sum of outer products using the corresponding columns of $\U$ and rows of $\V^T$:

$$\A = \U \pm \D & \bo{0} \\\bo{0}&\bo{0} \mp\V^T = \pm \u_1 & \u_2 & \dots &\u_m \mp \pm \sigma_1 & 0 & \dots & 0 & 0 \\ 0 & \ddots & 0 & \vdots & 0 \\ \vdots & 0& \sigma_r  & 0 & \vdots \\ 0 & 0 & 0 & 0 &0\\ \vdots & \vdots & \vdots & \vdots & \vdots\\ 0 & 0 & 0 & 0 &0 \mp \pm \v_1^T \\ \v_2^T \\ \vdots \\ \v_n^T \mp$$

$$= \sigma_1\u_1\v_1^T+\sigma_2\u_2\v_2^T+\dots+\sigma_r\u_r\v_r^T.$$
$$\sigma_1 \geq \sigma_2 \geq \dots \sigma_r$$
For simplicity, let $\Z_i=\u_i\v_i^T$ act as basis matrices for this expansion, so we have
\begin{equation}
(\#eq:svdsum)
\A=\sum_{i=1}^r \sigma_i \Z_i.
\end{equation}

This representation can be regarded as a Fourier expansion. The coefficient (singular value) $\sigma_i$ can be interpreted as the proportion of $\A$ lying in the "direction" of $\Z_i$. When $\sigma_i$ is small, omitting that term from the expansion will cause only a small amount of the information in $\A$ to be lost. This fact has important consequences for compression and noise reduction.

## Data Compression

We've already seen how PCA can be used to reduce the dimensions of our data while keeping the most amount of variance. The way this is done is by simply ignoring those components for which the proportion of variance is small. Supposing we keep $k$ principal components, this amounts to truncating the sum in Equation \@ref(eq:svdsum) after $k$ terms:
\begin{equation}
(\#eq:truncsvd)
\A \approx \sum_{i=1}^{k} \sigma_i \Z_i.
\end{equation}
As it turns out, this truncation has important consequences in many applications. One example is that of image compression. An image is simply an array of pixels. Supposing the image size is $m$ pixels tall by $n$ pixels wide, we can capture this information in an $m\times n$ matrix if the image is in grayscale, or an $m\times 3n$ matrix for a [r,g,b] color image (we'd need 3 values for each pixel to recreate the pixel's color). These matrices can get very large (a 6 megapixel photo is 6 million pixels). 

Rather than store the entire matrix, we can store an approximation to the matrix using only a few (well, more than a _few_) singular values and singular vectors.

This is the basis of image compression. An approximated photo will not be as crisp as the original - some information will be lost - but most of the time we can store much less than the original matrix and still get a good depiction of the image.

## Noise Reduction
Many applications arise where the relevant information contained in a matrix is contaminated by a certain level of noise. This is particularly common with video and audio signals, but also arises in text data and other types of (usually high dimensional) data. The __truncated SVD__ (Equation \@ref(eq:truncsvd)) can actually reduce the amount of noise in data and increase the overall __signal-to-noise__ ratio under certain conditions.

Let's suppose, for instance, that our matrix $\A_{m\times n}$ contains data which is contaminated by noise.  If that noise is assumed to be random (or nondirectional) in the sense that the noise is distributed more or less uniformly across the components $\Z_i$, then there is just as much noise "in the direction" of one $\Z_i$ as there is in the other. If the amount of noise along each direction is approximately the same, and the $\sigma_i$'s tell us how much (relevant) information in $\A$ is directed along each component $\Z_i$, then it must be that the ratio of "signal" (relevant information) to noise is decreasing across the ordered components, since
$$\sigma_1 \geq \sigma_2\geq \dots \geq \sigma_r$$
implies that the signal is greater in earlier components. So letting $SNR(\sigma_i\Z_i)$ denote the signal-to-noise ratio of each component, we have
$$SNR(\sigma_1\Z_1) \geq SNR(\sigma_2\Z_2)\geq \dots \geq SNR(\sigma_r\Z_r)$$

This explains why the __truncated SVD__, 
$$\A \approx \sum_{i=1}^{k} \sigma_i \Z_i \quad \mbox{where}\quad k<r$$
can, in many scenarios, filter out some of the noise without losing much of the significant information in $\A$.




<!--chapter:end:0399-SVD-text.Rmd-->

# Applications of SVD {#svdapp}

## Text Mining {#tm}

Text mining is another area where the SVD is used heavily. In text mining, our data structure is generally known as a __Term-Document Matrix__.  The _documents_ are any individual pieces of text that we wish to analyze, cluster, summarize or discover topics from. They could be sentences, abstracts, webpages, or social media updates. The _terms_ are the words contained in these documents. The term-document matrix represents what's called the "bag-of-words" approach - the order of the words is removed and the data becomes unstructured in the sense that each document is represented by the words it contains, not the order or context in which they appear. The $(i,j)$ entry in this matrix is the number of times term $j$ appears in document $i$.

:::{.definition name='Term-Document Matrix' #tdm}
 Let $m$ be the number of documents in a collection and $n$ be the number of terms appearing in that collection, then we create our __term-document matrix__ $\A$ as follows:
\begin{equation}
    \begin{array}{ccc}
        & & \text{term 1} \quad \text{term $j$} \,\, \text{term $n$} \\
        \A_{m\times n} = & \begin{array}{c}
            \hbox{Doc 1} \\
            \\
            \\
            \hbox{Doc $i$} \\
            \\
            \hbox{Doc $m$} \\
        \end{array} &
        \left(
        \begin{array}{ccccccc}
            & & & |&  & & \\
            & & & |&  & & \\
            & & & |&  & & \\
            & - & - &f_{ij}  &  & & \\
            & & & & & & \\
            & & & & & & \\
        \end{array}
        \right)
    \end{array}
\nonumber
\end{equation}
where $f_{ij}$ is the frequency of term $j$ in document $i$.  A __binary__ term-document matrix will simply have $\A_{ij}=1$ if term $j$ is contained in document $i$.
:::

### Note About Rows vs. Columns 

You might be asking yourself, "__Hey, wait a minute. Why do we have documents as columns in this matrix? Aren't the documents like our observations?__" Sure! Many data scientists insist on having the documents on the rows of this matrix. _But_, before you do that, you should realize something. Many SVD and PCA routines are created in a way that is more efficient when your data is long vs. wide, and text data commonly has more terms than documents. The equivalence of the two presentations should be easy to see in all matrix factorization applications. If we have 
$$\A = \U\mathbf{D}\V^T$$ then,
$$\A^T = \V\mathbf{D}\U^T$$
so we merely need to switch our interpretations of the left- and right-singular vectors to switch from document columns to document rows. 

Beyond any computational efficiency argument, we prefer to keep our documents on the columns here because of the emphasis placed earlier in this text regarding matrix multiplication viewed as a linear combination of columns. The animation in Figure \@ref(fig:multlincombanim) is a good thing to be clear on before proceeding here. 

### Term Weighting

Term-document matrices tend to be large and sparse. Term-weighting schemes are often used to downplay the effect of commonly used words and bolster the effect of rare but semantically important words \cite{termweighting, berryCIR}.  The most popular weighting method is known as  __Term Frequency-Inverse Document Frequency (TF-IDF)__. For this method, the raw term-frequencies $f_{ij}$ in the matrix $\A$ are multiplied by global weights called _inverse document frequencies_, $w_i$, for each term. These weights reflect the commonality of each term across the entire collection and ultimately quantify a term's ability to narrow one's search results (the foundations of text analysis were, after all, dominated by search technology). The inverse document frequency of term $i$ is:
$$w_i = \log \left( \frac{\mbox{total # of documents}}{\mbox{# documents containing term  } i} \right)$$
To put this weight in perspective, for a collection of $n=10,000$ documents we have $0\leq w_j \leq 9.2$, where $w_j=0$ means the word is contained in every document (rendering it useless for search) and $w_j=9.2$ means the word is contained in only 1 document (making it very useful for search). The document vectors are often normalized to have unit 2-norm, since their directions (not their lengths) in the term-space is what characterizes them semantically.\

### Other Considerations

In dealing with text, we want to do as much as we can do minimize the size of the dictionary (the collection of terms which enumerate the rows of our term-document matrix) for both computational and practical reasons.  The first effort we'll make toward this goal is to remove so-called __stop words__, or very common words that appear in a great many sentences like articles ("a", "an", "the") and prepositions ("about", "for", "at") among others. Many projects also contain domain-specific stop words. For example, one might remove the word "Reuters" from a corpus of [Reuters' newswires](https://shainarace.github.io/Reuters/). The second effort we'll often make is to apply a __stemming__ algorithm which reduces words to their _stem._ For example, the words "swimmer" and "swimming" would both be reduced to their stem, "swim". Stemming and stop word removal can greatly reduce the size of the dictionary and also help draw meaningful connections between documents.

### Latent Semantic Indexing

The noise-reduction property of the SVD was extended to text processing in 1990 by Susan Dumais et al, who named the effect _Latent Semantic Indexing (LSI)_. LSI involves the singular value decomposition of the term-document matrix defined in Definition \@ref(def:tdm). In other words, it is like a principal components analysis using the unscaled, uncentered inner-product matrix $\A^T\A$. If the documents are normalized to have unit length, this is a matrix of __cosine similarities__ (see Chapter \@ref(norms)). Cosine similarity is the most common measure of similarity between documents for text mining. If the term-document matrix is binary, this is often called the co-occurrence matrix because each entry gives the number of times two words occur in the same document.

 It certainly seems logical to view text data in this context as it contains both an informative signal and semantic noise.  LSI quickly grew roots in the information retrieval community, where it is often used for query processing. The idea is to remove semantic noise, due to variation and ambiguity in vocabulary and presentation style, without losing significant amounts of information. For example, a human may not differentiate between the words "car" and "automobile", but indeed the words will become two separate entities in the raw term-document matrix.  The main idea in LSI is that the realignment of the data into fewer directions should force related documents (like those containing "car" and "automobile") closer together in an angular sense, thus revealing latent semantic connections.
 
Purveyors of LSI suggest that the use of the Singular Value Decomposition to project the documents into a lower-dimensional space results in a representation which reflects the major associative patterns of the data while ignoring less important influences.  This projection is done with the simple truncation of the SVD shown in Equation \@ref(eq:truncsvd). 

As we have seen with other types of data, the very nature of dimension reduction makes possible for two documents with similar semantic properties to be mapped closer together. Unfortunately, the mixture of signs (positive and negative) in the singular vectors (think principal components) makes the decomposition difficult to interpret.  While the major claims of LSI are legitimate, this lack of interpretability is still conceptually problematic for some folks. In order to make this point as clear as possible, consider the original "term basis" representation for the data, where each document (from a collection containing $m$ total terms in the dictionary) could be written as:
$$\A_j = \sum_{i=1}^{m} f_{ij}\e_i$$
where $f_{ij}$ is the frequency of term $i$ in the document, and $\e_i$ is the $i^{th}$ column of the $m\times m$ identity matrix. The truncated SVD gives us a new set of coordinates (scores) and basis vectors (principal component features):
$$\A_j \approx \sum_{i=1}^r \alpha_i \u_i$$
but the features $\u_i$ live in the term space, and thus ought to be interpretable as a linear combinations of the original "term basis." However the linear combinations, having both positive and negative coefficients, tends to be semantically obscure in practice - These new features do not often form meaningful _topics_ for the text, although they often do organize in a meaningful way as we will demonstrate in the next section.

### Example

Let's consider a corpus of short documents, perhaps status updates from social media sites. We'll keep this corpus as minimal as possible to demonstrate the utility of the SVD for text. 

```{r label='studentgraph', fig.align='center', fig.cap = 'A corpus of 6 documents. Words occurring in more than one document appear in bold. Stop words removed, stemming utilized. Document numbers correspond to term-document matrix below.', echo=F, out.width="100%"}
knitr::include_graphics("figs/documents.png")
```
\begin{equation*}
    \begin{array}{cc}
         & \begin{array}{cccccc} \;doc_1\; & \;doc_2\;& \;doc_3\;& \;doc_4\;& \;doc_5\;& \;doc_6\; \end{array}\\
          \begin{array}{c}
            \hbox{cat} \\
            \hbox{dog}\\
            \hbox{eat}\\
            \hbox{tired} \\
            \hbox{toy}\\
            \hbox{injured} \\
            \hbox{ankle} \\
            \hbox{broken} \\
            \hbox{swollen} \\
            \hbox{sprained} \\
        \end{array} &
\left(
\begin{array}{cccccc}
\quad 1\quad   &  \quad 2\quad   &  \quad 2\quad   &  \quad 0\quad   &  \quad 0\quad   &  \quad 0\quad  \\
\quad 2\quad   &  \quad 3\quad   &  \quad 2\quad   &  \quad 0\quad   &  \quad 0\quad   &  \quad 0\quad  \\
\quad 2\quad   &  \quad 0\quad   &  \quad 1\quad   &  \quad 0\quad   &  \quad 0\quad   &  \quad 0\quad  \\
\quad 0\quad   &  \quad 1\quad   &  \quad 0\quad   &  \quad 0\quad   &  \quad 1\quad   &  \quad 0\quad  \\
\quad 0\quad   &  \quad 1\quad   &  \quad 1\quad   &  \quad 0\quad   &  \quad 0\quad   &  \quad 0\quad  \\
\quad 0\quad   &  \quad 0\quad   &  \quad 0\quad   &  \quad 1\quad   &  \quad 1\quad   &  \quad 0\quad  \\
\quad 0\quad   &  \quad 0\quad   &  \quad 0\quad   &  \quad 1\quad   &  \quad 1\quad   &  \quad 1\quad  \\
\quad 0\quad   &  \quad 0\quad   &  \quad 0\quad   &  \quad 1\quad   &  \quad 0\quad   &  \quad 1\quad  \\
\quad 0\quad   &  \quad 0\quad   &  \quad 0\quad   &  \quad 1\quad   &  \quad 0\quad   &  \quad 1\quad  \\
\quad 0\quad   &  \quad 0\quad   &  \quad 0\quad   &  \quad 1\quad   &  \quad 1\quad   &  \quad 0\quad  \\
\end{array}\right)
\end{array}
\end{equation*}
 
We'll start by entering this matrix into R. Of course the process of parsing a collection of documents and creating a term-document matrix is generally more automatic. The `tm` text mining library is recommended for creating a term-document matrix in practice. 

```{r}
A=matrix(c(1,2,2,0,0,0,
           2,3,2,0,0,0,
           2,0,1,0,0,0,
           0,1,0,0,1,0,
           0,1,1,0,0,0,
           0,0,0,1,1,0,
           0,0,0,1,1,1,
           0,0,0,1,0,1,
           0,0,0,1,0,1,
           0,0,0,1,1,0), 
         nrow=10, byrow=T)
A
```

Because our corpus is so small, we'll skip the step of term-weighting, but we _will_ normalize the documents to have equal length. In other words, we'll divide each document vector by its two-norm so that it becomes a unit vector:

```{r}
A_norm = apply(A, 2, function(x){x/c(sqrt(t(x)%*%x))})
A_norm
```

We then compute the SVD of `A_norm` and observe the left- and right-singular vectors. Since the matrix $\A$ is term-by-document, you might consider the terms as being the "units" of the rows of $\A$ and the documents as being the "units" of the columns. For example, $\A_{23}=2$ could logically be interpreted as "there are 2 units of the word _dog_ per _document number 3_". In this mentality, any factorization of the matrix should preserve those units. Similar to any ["Change of Units Railroad"](https://www.katmarsoftware.com/articles/railroad-track-unit-conversion.htm), matrix factorization can be considered in terms of units assigned to both rows and columns:
$$\A_{\text{term} \times \text{doc}} = \U_{\text{term} \times \text{factor}}\mathbf{D}_{\text{factor} \times \text{factor}}\V^T_{\text{factor} \times \text{doc}}$$
Thus, when we examine the rows of the matrix $\U$, we're looking at information about each term and how it contributes to each factor (i.e. the "factors" are just linear combinations of our elementary term vectors); When we examine the columns of the matrix $\V^T$, we're looking at information about how each document is related to each factor (i.e. the documents are linear combinations of these factors with weights corresponding to the elements of $\V^T$). And what about $\mathbf{D}?$ Well, in classical factor analysis the matrix $\mathbf{D}$ is often combined with either $\U$ or $\V^T$ to obtain a two-matrix factorization. $\mathbf{D}$ describes how much information or signal from our original matrix exists along each of the singular components.  It is common to use a __screeplot__, a simple line plot of the singular values in $\mathbf{D}$, to determine an appropriate _rank_ for the truncation in Equation \@ref(eq:truncsvd).

```{r}
out = svd(A_norm)
```
Let's first examine the left-singular vectors in $\U$. Remember, the _rows_ of this matrix describe how the terms load onto factors, and the columns are those mysterious "factors" themselves. 

```{r}
out$u
```


So the first "factor" of SVD is as follows:

$$\text{factor}_1 = 
 -0.530 \text{cat} -0.734 \text{dog}-0.344 \text{eat}-0.112 \text{tired} -0.208 \text{toy}-0.034 \text{injured} -0.046 \text{ankle}-0.024 \text{broken} -0.024 \text{swollen} -0.034 \text{sprained} $$
 We can immediately see why people had trouble with LSI as a topic model -- it's hard to intuit how you might treat a mix of positive and negative coefficients in the output.  If we ignore the signs and only investigate the absolute values, we can certainly see some meaningful topic information in this first factor: the largest magnitude weights all go to the words from the documents about pets. You might like to say that negative entries mean a topic is _anticorrelated_ with that word, and to some extent this is correct. That logic works nicely, in fact, for factor 2:   

$$\text{factor}_2 = -0.048\text{cat}-0.066\text{dog}-0.039\text{eat}+ 0.167\text{tired} -0.017\text{toy} 0.370\text{injured}+ 0.587\text{ankle}  +0.415\text{broken} + 0.415\text{swollen} + 0.370\text{sprained}$$
However, circling back to factor 1 then leaves us wanting to see different signs for the two groups of words. Nevertheless, the information separating the words is most certainly present. Take a look at the plot of the words' loadings along the first two factors in Figure \@ref(fig:lsiwords).

```{r label='lsiwords', fig.align='center', fig.cap = 'Projection of the Terms onto First two Singular Dimensions', echo=F, out.width="50%"}
terms= c('cat','dog','eat','tired','toy','injured','ankle','broken','swollen','sprained')

fig <- plot_ly(type = 'scatter', mode = 'markers') %>%
   add_trace(
    x = jitter(out$u[,1]),
    y = jitter(out$u[,2]),
    text = terms,
    hoverinfo = 'text',
    marker = list(color='green', opacity=0.6),
    showlegend = F
  )
fig

```

Moving on to the documents, we can see a similar clustering pattern in the columns of $\V^T$ which are the rows of $\V$, shown below:

```{r}
out$v
```
In fact, the ability to separate the documents with the first two singular vectors is rather magical here, as shown visually in Figure \@ref(fig:lsidocs). 

```{r label='lsidocs', fig.align='center', fig.cap = 'Projection of the Docuemnts onto First two Singular Dimensions', echo=F, out.width="50%"}
documents= c('1 (pets)','2 (pets)','3 (pets)','4 (injuries)','5 (injuries)','6 (injuries)')

fig <- plot_ly(type = 'scatter', mode = 'markers') %>%
   add_trace(
    x = jitter(out$v[,1]),
    y = jitter(out$v[,2]),
    text = documents,
    hoverinfo = 'text',
    marker = list(color='green', opacity=0.6),
    showlegend = F
  )
fig
```

Figure \@ref(fig:lsidocs) demonstrates how documents that live in a 10-dimensional term space can be compressed down to 2-dimensions in a way that captures the major information of interest. If we were to take that 2-truncated SVD of our term-document matrix and multiply it back together, we'd see an _approximation_ of our original term-document matrix, and we could calculate the error involved in that approximation. We could equivalently calculate that error by using the singular values. 

```{r} 
A_approx = out$u[,1:2]%*% diag(out$d[1:2])%*%t(out$v[,1:2])
# Sum of element-wise squared error 
(norm(A-A_approx,'F'))^2
# Sum of squared singular values truncated
(sum(out$d[3:6]^2))
```

However, multiplying back to the original data is not generally an action of interest to data scientists. What we are after in the SVD is the dimensionality reduced data contained in the columns of $\V^T$ (or, if you've created a document-term matrix, the rows of $\U$. 

## Image Compression {#rappasvd}

While multiplying back to the original data is not generally something we'd like to do, it does provide a nice illustration of noise-reduction and signal-compression when working with images. The following example is not designed to teach you how to work with images for the purposes of data science. It is merely a nice visual way to _see_ what's happening when we truncate the SVD and omit these directions that have "minimal signal." 


### Image data in R

Let's take an image of a leader that we all know and respect:

```{r fig.align='center', fig.cap = 'Michael Rappa, PhD, Founding Director of the Institute for Advanced Analytics and Distinguished Professor at NC State', echo=F, fig.width=10}
knitr::include_graphics('LAdata/rappa.jpg')
```
This image can be downloaded from the IAA website, after clicking on the link on the left hand side "Michael Rappa / Founding Director."

Let's read this image into R. You'll need to install the pixmap package:
```{r}
#install.packages("pixmap")
library(pixmap)
```
Download the image to your computer and then set your working directory in R as the same place you have saved the image:
```{r eval=F}
setwd("/Users/shaina/Desktop/lin-alg")
```
The first thing we will do is examine the image as an [R,G,B] (extension .ppm) and as a grayscale (extension .pgm). Let's start with the [R,G,B] image and see what the data looks like in R:
```{r id='rgbrappa'}
rappa = read.pnm("LAdata/rappa.ppm")
#Show the type of the information contained in our data:
str(rappa)
```
You can see we have 3 matrices - one for each of the colors: red, green, and blue.
Rather than a traditional data frame, when working with an image, we have to refer to the elements in this data set with @ rather than with $.
```{r}
rappa@size
```
We can then display a heat map showing the intensity of each individual color in each pixel:
```{r, fig.align='center', fig.cap = 'Intensity of green in each pixel of the original image'}
rappa.red=rappa@red
rappa.green=rappa@green
rappa.blue=rappa@blue
image(rappa.green)
```

Oops! Dr. Rappa is sideways. To rotate the graphic, we actually have to rotate our coordinate system. There is an easy way to do this (with a little bit of matrix experience), we simply transpose the matrix and then reorder the columns so the last one is first: (note that ``` nrow(rappa.green)``` gives the number of columns in the transposed matrix)

```{r fig.align= 'center' }
rappa.green=t(rappa.green)[,nrow(rappa.green):1]
image(rappa.green)
```

Rather than compressing the colors individually, let's work with the grayscale image:

```{r id='greyrappa'}
greyrappa = read.pnm("LAdata/rappa.pgm")
str(greyrappa)
rappa.grey=greyrappa@grey
#again, rotate 90 degrees
rappa.grey=t(rappa.grey)[,nrow(rappa.grey):1]
```

```{r fig.cap = 'Greyscale representation of original image'}
image(rappa.grey, col=grey((0:1000)/1000))
```

### Computing the SVD of Dr. Rappa

Now, let's use what we know about the SVD to compress this image. First, let's compute the SVD and save the individual components. Remember that the rows of $\mathbf{v}^T$ are the right singular vectors. R outputs the matrix $\mathbf{v}$ which has the singular vectors in columns.

```{r }
rappasvd=svd(rappa.grey)
U=rappasvd$u
d=rappasvd$d
Vt=t(rappasvd$v)
```

Now let's compute some approximations of rank 3, 10 and 50:
```{r,fig.align='center', fig.cap = 'Rank 3 approximation of the image data'}
rappaR3=U[ ,1:3]%*%diag(d[1:3])%*%Vt[1:3, ]
image(rappaR3, col=grey((0:1000)/1000))
```
```{r,fig.align='center', fig.cap = 'Rank 10 approximation of the image data'}
rappaR10=U[ ,1:10]%*%diag(d[1:10])%*%Vt[1:10, ]
image(rappaR10, col=grey((0:1000)/1000))
```
```{r,fig.align='center', fig.cap = 'Rank 50 approximation of the image data'}
rappaR25=U[ ,1:25]%*%diag(d[1:25])%*%Vt[1:25, ]
image(rappaR25, col=grey((0:1000)/1000))
```

How many singular vectors does it take to recognize Dr. Rappa? Certainly 25 is sufficient. Can you recognize him with even fewer? You can play around with this and see how the image changes.

### The Noise

One of the main benefits of the SVD is that the _signal-to-noise_ ratio of each component decreases as we move towards the right end of the SVD sum. If $\mathbf{x}$ is our data matrix (in this example, it is a matrix of pixel data to create an image) then,

\begin{equation}
\mathbf{X}= \sigma_1\mathbf{u}_1\mathbf{v}_1^T + \sigma_2\mathbf{u}_2\mathbf{v}_2^T + \sigma_3\mathbf{u}_3\mathbf{v}_3^T + \dots + \sigma_r\mathbf{u}_r\mathbf{v}_r^T
(\#eq:svdsum)
\end{equation}

where $r$ is the rank of the matrix. Our image matrix is full rank, $r=160$. This is the number of nonzero singular values, $\sigma_i$. But, upon examinination, we see many of the singular values are nearly 0. Let's examine the last 20 singular values:

```{r}
d[140:160]
```

We can think of these values as the amount of "information" directed along those last 20 singular components. If we assume the noise in the image or data is uniformly distributed along each orthogonal component $\mathbf{u}_i\mathbf{v}_i^T$, then there is just as much noise in the component $\sigma_1\mathbf{u}_1\mathbf{v}_1^T$ as there is in the component $\sigma_{160}\mathbf{u}_{160}\mathbf{v}_{160}^T$. But, as we've just shown, there is far less information in the component $\sigma_{160}\mathbf{u}_{160}\mathbf{v}_{160}^T$ than there is in the component $\sigma_1\mathbf{u}_1\mathbf{v}_1^T$. This means that the later components are primarily noise. Let's see if we can illustrate this using our image. We'll construct the parts of the image that are represented on the last few singular components

(ref:last25) The last 25 components, or the sum of the last 25 terms in equation \@ref(eq:svdsum)

```{r,fig.align='center', fig.cap = '(ref:last25)', fig.align='center' }
# Using the last 25 components:

rappa_bad25=U[ ,135:160]%*%diag(d[135:160])%*%Vt[135:160, ]
image(rappa_bad25, col=grey((0:1000)/1000))
```

(ref:last50) The last 50 components, or the sum of the last 50 terms in equation \@ref(eq:svdsum)

```{r,fig.align='center', fig.cap = '(ref:last50)', fig.align='center' }
# Using the last 50 components:

rappa_bad50=U[ ,110:160]%*%diag(d[110:160])%*%Vt[110:160, ]
image(rappa_bad50, col=grey((0:1000)/1000))
```

(ref:last100) The last 100 components, or the sum of the last 100 terms in equation \@ref(eq:svdsum)

```{r,fig.align='center', fig.cap = '(ref:last100)', fig.align='center' }
# Using the last 100 components: (4 times as many components as it took us to recognize the face on the front end)

rappa_bad100=U[ ,61:160]%*%diag(d[61:160])%*%Vt[61:160, ]
image(rappa_bad100, col=grey((0:1000)/1000))
```

Mostly noise. In the last of these images, we see the outline of Dr. Rappa. One of the first things to go when images are compressed are the crisp outlines of objects. This is something you may have witnessed in your own experience, particularly when changing the format of a picture to one that compresses the size.

<!--chapter:end:04-SVD.Rmd-->

# Factor Analysis {#fa}

Factor Analysis is about looking for underlying _relationships_ or _associations_.  In that way, factor analysis is a correlational study of variables, aiming to group or cluster variables along dimensions. It may also be used to provide an estimate (factor score) of a latent construct which is a linear combination of variables. For example, a standardized test might ask hundreds of questions on a variety of quantitative and verbal subjects. Each of these questions could be viewed as a variable. However, the quantitative questions collectively are meant to measure some _latent_ factor, that is the individual's _quantitative reasoning_. A Factor Analysis might be able to reveal these two latent factors (quantitative reasoning and verbal ability) and then also provide an estimate (score) for each individual on each factor.

Any attempt to use factor analysis to summarize or reduce a set to data should be based on a conceptual foundation or hypothesis. It should be remembered that factor analysis will produce factors for most sets of data. Thus, if you simply analyze a large number of variables in the hopes that the technique will ``figure it out", your results may look as though they are grasping at straws. The quality or meaning/interpretation of the derived factors is best when related to a conceptual foundation that existed prior to the analysis.


## Assumptions of Factor Analysis

1. No outliers in the data set
2. Adequate sample size 
  a. As a rule of thumb, maintain a ratio of variables to factors of at least 3 (some say 5). This depends on the application.
  b. You should have at least 10 observations for each variable (some say 20). This often depends on what value of factor loading you want to declare as significant. See Table \ref{table:factorsig} for the details on this.
3. No perfect multicollinearity
4. Homoskedasticity _not_ required between variables (all variances _not_ required to be equal)
5. Linearity of variables desired - only models linear correlation between variables
6. Interval data (as opposed to nominal)
7. Measurement error on the variables/observations has constant variance and is, on average, 0
8. Normality is not required


|Sample Size<br> Needed for Significance| Factor Loading |
|:-----------------------:|:---------------------:|
|350       |    .30|
|   250    |    .35|
|   200    |    .40|
|   150    |    .45|
|   120    |    .50|
|   100    |    .55|
|     85   |    .60|
|     70   |    .65|
|     60   |    .70|
|     50   |    .75|

Table: Factor loadings are the correlation of each variable and the factor. This table is a guide for the sample sizes necessary to consider a factor loading significant. For example, in a sample of 100, factor loadings of 0.55 are considered significant. In a sample size of 70, however, factor loadings must reach 0.65 to be considered significant. Significance based on 0.05 level, a power level of 80 percent. Source: _Computations made with SOLO Power Analysis, BMDP Statistical Software, Inc., 1993_



## Determining Factorability

Before we even begin the process of factor analysis, we have to do some preliminary work to determine whether or not the data even lends itself to this technique. If none of our variables are correlated, then we cannot group them together in any meaningful way! Bartlett's Sphericity Test and the KMO index are two statistical tests for whether or not a set of variables can be factored. These tests _do not_ provide information about the appropriate number of factors, only whether or not such factors even exist.

### Visual Examination of Correlation Matrix
Depending on how many variables you are working with, you may be able to determine whether or not to proceed with factor analysis by simply examining the correlation matrix. With this examination, we are looking for two things:

1. Correlations that are significant at the 0.01 level of significance. At least half of the correlations should be significant in order to proceed to the next step.
2. Correlations are "sufficient" to justify applying factor analysis. As a rule of thumb, at least half of the correlations should be greater than 0.30.


### Barlett's Sphericity Test

Barlett's sphericity test checks if the observed correlation matrix is significantly different from the identity matrix. Recall that the correlation of two variables is equal to 0 if and only if they are orthogonal (and thus completely uncorrelated). When this is the case, we cannot reduce the number of variables any further, neither PCA nor Factor Analysis will be able to compress the information reliably into fewer dimensions. For Barlett's test,
$$H_0 = \mbox{ The variables are orthogonal} $$
Which implies that there are no underlying factors to be uncovered. Obviously, we must be able to reject this hypothesis for a meaningful result in PCA.

### Kaiser-Meyer-Olkin (KMO) Measure of Sampling Adequacy

The goal of the Kaiser-Meyer-Olkin (KMO) measure of sampling adequacy is similar to that of Bartlett's test in that it checks if we can factorize efficiently the original variables. However, the KMO measure is based on the idea of _partial correlation_. The correlation matrix is always the starting point. We know that the variables are more or less correlated, but the correlation between two variables can be influenced by the others. So, we use the partial correlation in order to measure the relation between two variables by removing the effect of the remaining variables. The KMO index compares the raw values of correlations between variables and those of the partial correlations. If the KMO index is high ($\approx 1$), then PCA can act efficiently; if the KMO index is low ($\approx 0$), then PCA is not relevant. Generally a KMO index greater than 0.5 is considered acceptable to proceed with factor analysis. Table \@ref(tab:KMO) contains the information about interpretting KMO results that was provided in the original 1974 paper.


<table>
<tr>
<td> KMO value <td>Degree of <br> Common Variance</td>
<td>0.90 to 1.00  <td>Marvelous</td>
<td>0.80 to 0.89  <td>Middling</td>
<td>0.60 to 0.69   <td>Mediocre</td>
<td>0.50 to 0.59    <td>Miserable</td>
<td>0.00 to 0.49   <td>Don't Factor</td>
</table>
Table: (\#tab:KMO) Interpretting the KMO value. \cite{KMO}
\caption{Interpretting the KMO value. }
\label{table:KMO}
\end{center}
\end{table}

So, for example, if you have a survey with 100 questions/variables and you obtained a KMO index of 0.61, this tells you that the degree of common variance between your variables is mediocre, on the border of being miserable. While factor analysis may still be appropriate in this case, you will find that such an analysis will not account for a substantial amount of variance in your data. It may still account for enough to draw some meaningful conclusions, however.


## Communalities


You can think of **communalities** as multiple $R^2$ values for regression models predicting the variables of interest from the factors (the reduced number of factors that your model uses). The communality for a given variable can be interpreted as the proportion of variation in that variable explained by the chosen factors.

Take for example the SAS output for factor analysis on the Iris dataset shown in Figure \ref{factorOUT}. The factor model (which settles on only one single factor) explains 98% of the variability in _petal length_. In other words, if you were to use this factor in a simple linear regression model to predict petal length, the associated $R^2$ value should be 0.98. Indeed you can verify that this is true. The results indicate that this single factor model will do the best job explaining variability in _petal length, petal width, and sepal length_.

(ref:factorOUT) SAS output for PROC FACTOR using Iris Dataset

```{r fig=T, out.width="100%", echo=F,fig.align='center', id='factorOUT', fig.cap='(ref:factorOUT)'} 
knitr::include_graphics('figs/factorOUT.png')
```


One assessment of how well a factor model is doing can be obtained from the communalities. What you want to see is values that are close to one. This would indicate that the model explains most of the variation for those variables. In this case, the model does better for some variables than it does for others. 

If you take all of the communality values, $c_i$ and add them up you can get a total communality value:

$$\sum_{i=1}^p \widehat{c_i} = \sum_{i=1}^k \widehat{\lambda_i}$$



Here, the total communality is 2.918. The proportion of the total variation explained by the three factors is
$$\frac{2.918}{4}\approx 0.75.$$
The denominator in that fraction comes from the fact that the correlation matrix is used by default and our dataset has 4 variables. Standardized variables have variance of 1 so the total variance is 4. This gives us the percentage of variation explained in our model. This might be looked at as an overall assessment of the performance of the model. The individual communalities tell how well the model is working for the individual variables, and the total communality gives an overall assessment of performance. 

## Number of Factors

A good rule of thumb for determining the number of factors is to only choose factors with associated eigenvalue (or variance) greater than 1. Since the correlation matrix is used for factor analysis, we want our factors to explain more variance than any individual variable from our dataset. If this rule of thumb produces too many factors, it is reasonable to raise that limiting condition only if the number of factors still explains a reasonable amount of the total variance.


## Rotation of Factors

The purpose of rotating factors is to make them more interpretable. If factor loadings are relatively constant across variables, they don't help us find latent structure or clusters of variables. This will often happen in PCA when the goal is only to find directions of maximal variance. Thus, once the number of components/factors is fixed and a projection of the data onto a lower-dimensional subspace is done, we are free to rotate the axes of the result without losing any variance. The axes will no longer be principal components! The amount of variance explained by each factor will change, but the total amount of variance in the reduced data will stay the same because all we have done is rotate the basis. The goal is to rotate the factors in such a way that the loading matrix develops a more _sparse_ structure. A sparse loading matrix (one with lots of very small entries and few large entries) is far easier to interpret in terms of finding latent variable groups.

The two most common rotations are **varimax** and **quartimax**. The goal of _varimax_ rotation is to maximize the squared factor loadings in each factor, i.e. to simplify the columns of the factor matrix. In each factor, the large loadings are increased and the small loadings are decreased so that each factor has only a few variables with large loadings. In contrast, the goal of _quartimax_ rotation is to simply the rows of the factor matrix. In each variable the large loadings are increased and the small loadings are decreased so that each variable will only load on a few factors. Which of these factor rotations is appropriate 

<!--chapter:end:05-FA.Rmd-->

# Factor Analysis {#fa-apps}

```{r, echo=F}
thmcounter=0
excounter=0
cid='fa-apps'
```

Factor Analysis is much like PCA in that it attempts to find some latent variables (linear combinations of original variables) which can describe large portions of the total variance in data. There are numerous ways to compute factors for factor analysis, the two most common methods are:

1. The _principal axis_ method (i.e. PCA) and
2. Maximum Likelihood Estimation.

In fact, the default method for PROC FACTOR with no additional options is merely PCA. For some reason, the scores and factors may be scaled differently, involving the standard deviations of each factor, but nonetheless, there is absolutely nothing different between PROC FACTOR defaults and PROC PRINCOMP.

The difference between Factor Analysis and PCA is two-fold:

1. In factor analysis, the factors are usually rotated to obtain a more sparse (i.e. interprettable) structure _varimax_ rotation is the most common rotation. Others include _promax_, and _quartimax_.)
2. The factors try to only explain the "common variance" between variables. In other words, Factor Analysis tries to estimate how much of each variable's variance is specific to that variable and not "covarying" (for lack of a better word) with any other variables. This specific variance is then subtracted from the diagonal of the covariance matrix before factors or components are found.


We'll talk more about the first difference than the second because it generally carries more advantages.

## PCA Rotations
Let's first talk about the motivation behind principal component rotations. Compare the following sets of (fabricated) factors, both using the variables from the iris dataset. Listed below are the loadings of each variable on two factors. Which set of factors is more easily interpretted? 

<!-- \mathbf{b}egin{center} -->
<!-- \mathbf{b}egin{minipage}{\textwidth} -->
<!--   \mathbf{b}egin{minipage}[b]{0.47\textwidth} -->

<!-- \captionof*{table}{Factor Set 1} -->
<!-- \mathbf{b}egin{tabular}{c|c|c|} -->
<!--  Variable             & P1 & P2\\ -->
<!--               \hline -->
<!-- Sepal.Length  & -.3 & .7 \\ -->
<!-- Sepal.Width   & -.5 & .4 \\ -->
<!-- Petal.Length  & .7 & .3  \\ -->
<!-- Petal.Width   & .4 & -.5 \\ -->
<!-- \end{tabular} -->
<!-- \end{minipage} -->
<!-- \hfill -->
<!--   \mathbf{b}egin{minipage}[b]{0.47\textwidth} -->
<!-- \captionof*{table}{Factor Set 2} -->
<!-- \mathbf{b}egin{tabular}{c|c|c|} -->
<!--    Variable           & F1 & F2\\ -->
<!--               \hline -->
<!-- Sepal.Length  & 0 & .9 \\ -->
<!-- Sepal.Width   & -.9 & 0 \\ -->
<!-- Petal.Length  & .8 & 0  \\ -->
<!-- Petal.Width   & .1 & -.9 \\ -->
<!-- \end{tabular} -->
<!-- \end{minipage} -->
<!-- \end{minipage} -->
<!-- \end{center} -->

The difference between these factors might be described as ``sparsity". Factor Set 2 has more zero loadings than Factor Set 1. It also has entries which are comparitively larger in magnitude. This makes Factor Set 2 much easier to interpret! Clearly F1 is dominated by the variables Sepal.Width (positively correlated) and Petal.Length (negatively correlated), whereas F2 is dominated by the variables Sepal.Length (positively) and Petal.Width (negatively). Factor interpretation doesn't get much easier than that! With the first set of factors, the story is not so clear.

This is the whole purpose of factor rotation, to increase the interpretability of factors by encouraging sparsity. Geometrically, factor rotation tries to rotate a given set of factors (like those derived from PCA) to be more closely aligned with the original variables once the dimensions of the space have been reduced and the variables have been pushed closer together in the factor space. Let's take a look at the actual principal components from the iris data and then rotate them using a varimax rotation. In order to rotate the factors, we have to decide on some number of factors to use. If we rotated all 4 orthogonal components to find sparsity, we'd just end up with our original variables again!

```{r}
irispca = princomp(iris[,1:4],scale=T)
summary(irispca)
irispca$loadings
# Since 2 components explain a large proportion of the variation, lets settle on those two:
rotatedpca = varimax(irispca$loadings[,1:2])
rotatedpca$loadings
# Not a drastic amount of difference, but clearly an attempt has been made to encourage
# sparsity in the vectors of loadings.

# NOTE: THE ROTATED FACTORS EXPLAIN THE SAME AMOUNT OF VARIANCE AS THE FIRST TWO PCS
# AFTER PROJECTING THE DATA INTO TWO DIMENSIONS (THE BIPLOT) ALL WE DID WAS ROTATE THOSE
# ORTHOGONAL AXIS. THIS CHANGES THE PROPORTION EXPLAINED BY *EACH* AXIS, BUT NOT THE TOTAL
# AMOUNT EXPLAINED BY THE TWO TOGETHER.

# The output from varimax can't tell you about proportion of variance in the original data
# because you didn't even tell it what the original data was!
```

## Ex: Personality Tests

In this example, we'll use a publicly available dataset that describes personality traits of nearly 
Read in the Big5 Personality test dataset, which contains likert scale responses (five point scale where 1=Disagree, 3=Neutral, 5=Agree. 0 = missing) on 50 different questions in columns 8 through 57. The questions, labeled E1-E10 (E=extroversion), N1-N10 (N=neuroticism), A1-A10 (A=agreeableness), C1-C10 (C=conscientiousness), and O1-O10 (O=openness) all attempt to measure 5 key angles of human personality. The first 7 columns contain demographic information coded as follows:
<ol>
<li> **Race**	Chosen from a drop down menu. 
<ul>
<li> 1=Mixed Race
<li> 2=Arctic (Siberian, Eskimo)
<li> 3=Caucasian (European)
<li> 4=Caucasian (Indian)
<li> 5=Caucasian (Middle East)
<li> 6=Caucasian (North African, Other)
<li> 7=Indigenous Australian
<li> 8=Native American
<li> 9=North East Asian (Mongol, Tibetan, Korean Japanese, etc)
<li> 10=Pacific (Polynesian, Micronesian, etc)
<li> 11=South East Asian (Chinese, Thai, Malay, Filipino, etc)
<li> 12=West African, Bushmen, Ethiopian
<li> 13=Other (0=missed)
</ul>
<li> **Age**	Entered as text (individuals reporting age < 13 were not recorded)
<li> **Engnat**	Response to "is English your native language?"
<ul>
<li> 1=yes
<li> 2=no
<li> 0=missing
</ul>
<li> **Gender**	Chosen from a drop down menu
<ul>
<li> 1=Male
<li> 2=Female
<li> 3=Other
<li> 0=missing
</ul>
<li> **Hand**	"What hand do you use to write with?"
<ul>
<li> 1=Right
<li> 2=Left
<li> 3=Both
<li> 0=missing
</ul>
</ol>


```{r echo=F}
load('LAdata/big5.RData')
```

```{r id='big5', eval=F}
options(digits=2)
big5 = read.csv('http://birch.iaa.ncsu.edu/~slrace/LinearAlgebra2021/Code/big5.csv')
```

To perform the same analysis we did in SAS, we want to use Correlation PCA and rotate the axes with a varimax transformation. We will start by performing the PCA. We need to set the option ```scale=T} to perform PCA on the correlation matrix rather than the default covariance matrix. We will only compute the first 5 principal components because we have 5 personality traits we are trying to measure. We could also compute more than 5 and take the number of components with eigenvalues >1 to match the default output in SAS (without n=5 option).

### Raw PCA Factors

```{r}
options(digits=5)
pca.out = prcomp(big5[,8:57], rank = 5, scale = T)
```

Remember the only difference between the default PROC PRINCOMP output and the default PROC FACTOR output in SAS was the fact that the eigenvectors in PROC PRINCOMP were normalized to be unit vectors and the factor vectors in PROC FACTOR were those same eigenvectors scaled by the square roots of the eigenvalues. So we want to multiply each eigenvector column output in ```pca.out$rotation``` (recall this is the loading matrix or matrix of eigenvectors) by the square root of the corresponding eigenvalue given in ```pca.out$sdev```. You'll recall that multiplying a matrix by a diagonal matrix on the right has the effect of scaling the columns of the matrix. So we'll just make a diagonal matrix, $\textbf{S}$ with diagonal elements from the ```pca.out$sdev``` vector and scale the columns of the ```pca.out$rotation``` matrix.  Similarly, the coordinates of the data along each component then need to be _divided_ by the standard deviation to cancel out this effect of lengthening the axis. So again we will multiply by a diagonal matrix to perform this scaling, but this time, we use the diagonal matrix $\textbf{S}^{-1}=$ ```diag(1/(pca.out$sdev))```. \\

Matrix multiplication in R is performed with the ```\%\*\%``` operator.

```{r}
fact.loadings = pca.out$rotation[,1:5] %*% diag(pca.out$sdev[1:5])
fact.scores = pca.out$x[,1:5] %*%diag(1/pca.out$sdev[1:5])
# PRINT OUT THE FIRST 5 ROWS OF EACH MATRIX FOR CONFIRMATION.
fact.loadings[1:5,1:5]
fact.scores[1:5,1:5]
```

This should match the output from SAS and it does. Remember these columns are unique up to a sign, so you'll see factor 4 does not have the same sign in both software outputs. This is not cause for concern.

(ref:loads) Default (Unrotated) Factor Loadings Output by SAS

```{r fig.align='center', echo=F, fig.cap='(ref:loads)', out.width='100%'}
knitr::include_graphics('factorOutput.png')
```

(ref:scores) Default (Unrotated) Factor Scores Output by SAS

```{r fig.align='center', echo=F, fig.cap='(ref:scores)', out.width='100%'}
knitr::include_graphics('scoresOutput.png')
```


### Rotated Principal Components

The next task we may want to undertake is a rotation of the factor axes according to the varimax procedure. The most simple way to go about this is to use the `varimax()` function to find the optimal rotation of the eigenvectors in the matrix `pca.out$rotation`. The `varimax()` function outputs both the new set of axes in the matrix called `loadings` and the rotation matrix (`rotmat`) which performs the rotation from the original principal component axes to the new axes. (i.e. if $\textbf{V}$ contains the old axes as columns and $\hat{\textbf{V}}$ contains the new axes and $\textbf{R}$ is the rotation matrix then $\hat{\textbf{V}} = \textbf{V}\textbf{R}$.) That rotation matrix can be used to perform the same rotation on the scores of the observations. If the matrix $\textbf{U}$ contains the scores for each observation, then the rotated scores $\hat{\textbf{U}}$ are found by $\hat{\textbf{U}} = \textbf{U}\textbf{R}$


```{r}
varimax.out = varimax(fact.loadings)
rotated.fact.loadings = fact.loadings %*% varimax.out$rotmat
rotated.fact.scores = fact.scores %*% varimax.out$rotmat
# PRINT OUT THE FIRST 5 ROWS OF EACH MATRIX FOR CONFIRMATION.
rotated.fact.loadings[1:5,]
rotated.fact.scores[1:5,]
```

And again we can see that these line up with our SAS Rotated output, **however** the order does not have to be the same! SAS conveniently reorders the columns according to the variance of the data along that new direction. Since we have not done that in R, the order of the columns is not the same! Factors 1 and 2 are the same in both outputs, but SAS Factor 3 = R Factor 4 and SAS Factor 5 = (-1)* R Factor 4. The coordinates are switched too so nothing changes in our interpretation. Remember, when you rotate factors, you no longer keep the notion that the "first vector" explains the most variance unless you reorder them so that is true (like SAS does).


```{r fig=T, label='rotloads',fig.align='center', echo=F, fig.cap='Rotated Factor Loadings Output by SAS', out.width='100%'}
knitr::include_graphics('RotatedLoadings.png')
```


```{r fig=T, label='rotscores',fig.align='center', fig.cap='Rotated Factor Scores Output by SAS', out.width='100%'}
knitr::include_graphics('RotatedScores.png')
```


### Visualizing Rotation via BiPlots

Let's start with a peek at BiPlots of the first 2 \textit{pairs} of principal component loadings, prior to rotation. Notice that here I'm not going to bother with any scaling of the factor loadings as I'm not interested in forcing my output to look like SAS's output. I'm also downsampling the observations because 20,000 is far to many to plot. 

```{r fig.align='center',fig.cap='BiPlot of Projection onto PC1 and PC2'}
biplot(pca.out$x[sample(1:19719,1000),1:2], 
       pca.out$rotation[,1:2],
       cex=c(0.2,1))
```


```{r fig.align='center',fig.cap='BiPlot of Projection onto PC3 and PC4'}
biplot(pca.out$x[sample(1:19719,1000),3:4], 
       pca.out$rotation[,3:4],
       cex=c(0.2,1))
```


Let's see what happens to these biplots after rotation:

```{r fig.align='center',fig.cap='BiPlot of Projection onto Rotated Axes 1,2. Extroversion questions align with axis 1, Neuroticism with Axis 2'}

vmax = varimax(pca.out$rotation)
newscores = pca.out$x%*%vmax$rotmat
biplot(newscores[sample(1:19719,1000),1:2], 
       vmax$loadings[,1:2],
       cex=c(0.2,1),
       xlab = 'Rotated Axis 1',
       ylab = 'Rotated Axis 2')
```


```{r fig.align='center',fig.cap = 'BiPlot of Projection onto Rotated Axes 3,4. Agreeableness questions align with axis 3, Openness with Axis 4.'}
biplot(newscores[sample(1:19719,1000),3:4], 
       vmax$loadings[,3:4],
       cex=c(0.2,1),
       xlab = 'Rotated Axis 3',
       ylab = 'Rotated Axis 4')
```



After the rotation, we can see the BiPlots tell a more distinct story. The extroversion questions line up along rotated axes 1, neuroticism along rotated axes 2, and agreeableness and openness are reflected in rotated axes 3 and 4 respectively. The fifth rotated component can be confirmed to represent the last remaining category which is conscientiousness. 



<!--chapter:end:051-FA_apps.Rmd-->

# Dimension Reduction for Visualization {#otherdimred}

## Multidimensional Scaling

Multidimensional scaling is a technique which aims to represent higher-dimensional data in a lower-dimensional space while keeping the pairwise distances between points as close to their original distances as possible. It takes as input a distance matrix, $\D$ where $\D_{ij}$ is some measure of distance between observation $i$ and observation $j$ (most often Euclidean distance). The original observations may involve many variables and thus exist in a high-dimensional space. The output of MDS is a set of coordinates, usually in 2-dimensions for the purposes of visualization, such that the Euclidean distance between observation $i$ and observation $j$ in the new lower-dimensional representation is an approximation to $\D_{ij}$.

One of the outputs in R will be a measure that is akin to the "percentage of variation explained" by PCs. The difference is that the matrix we are representing is not a covariance matrix, so this ratio is not exactly a percentage of variation. It can, however, be used to judge how much information is retained by the lower dimensional representation. This is output in the `GOF` vector (presumably standing for _Goodness of Fit_). The first entry in `GOF` gives the ratio of the sum of the $k$ largest eigenvalues to the sum of the absolute values of all the eigenvalues, and the second entry in `GOF` gives the ratio of the sum of the $k$ largest eigenvalues to the sum of only the positive eigenvalues.

$$GOF[1] = \frac{\sum_{i=1}^k \lambda_i}{\sum_{i=1}^n |\lambda_i|}$$
and
$$GOF[2] = \frac{\sum_{i=1}^k \lambda_i}{\sum_{i=1}^n \max(\lambda_i,0)}$$
### MDS of Iris Data

Let's take a dataset we've already worked with, like the iris dataset, and see how this is done. Recall that the iris data contains measurements of 150 flowers (50 each from 3 different species) on 4 variables: Sepal.Length, Sepal.Width, Petal.Length, and Petal.Width. To examine a 2-dimensional representation of this data via Multidimensional Scaling, we simply compute a distance matrix and run the MDS procedure:

```{r fig.align='center', fig.cap = 'Multidimensional Scaling of the Iris Data', id='MDSofIris'}
D = dist(iris[,1:4])
fit = cmdscale(D, eig=TRUE, k=2) # k is the number of dimensions desired
fit$eig[1:12] # view first dozen eigenvalues
fit$GOF # view the Goodness of Fit measures

# plot the solution, colored by iris species:
x = fit$points[,1]
y = fit$points[,2]
# The pch= option controls the symbol output. 16=filled circles.
plot(x,y,col=c("red","green3","blue")[iris$Species], pch=16,
     xlab='Coordinate 1', ylab='Coordinate 2')
```

We can tell from the eigenvalues alone that two dimensions should be relatively sufficient to summarize this data. After two large eigenvalues, the remainder drop off and become small,  signifying a lack of further information. Indeed, the Goodness of Fit measurements back up this intuition: values close to 1 indicate a good fit with minimal error.

### MDS of Leukemia dataset

Let's take a look at another example, this time using the Leukemia dataset which has 5000 variables. It is unreasonable to expect that we can get as good of a fit of this data using only two dimensions! There will obviously be much more error. However, we can still get a visualization that should at least show us which observations are close to each other and which are far away.

```{r eval=F}
leuk=read.csv('http://birch.iaa.ncsu.edu/~slrace/Code/leukemia.csv')
```
```{r echo=F}
load('LAdata/leuk.RData')
```

As you may recall, this data has some variables with 0 variance; those entire columns are constant. To determine which ones, we first remove the last column which is a character vector that identifies the type of leukemia.

```{r fig=T, fig.align='center', fig.cap='Multidimensional Scaling of the Leukemia Data'}
type = leuk[ , 5001]
leuk = leuk[,1:5000]
# If desired, could supply names of columns that have 0 variance with
# names(leuk[, sapply(leuk, function(v) var(v, na.rm=TRUE)==0)])
# The na.rm=T would allow us to keep any missing information and still compute
# the variance using the non-missing values. In this instance, it is not necessary
# because we have no missing values.

# We can remove these columns from the data with:
leuk=leuk[,apply(leuk, 2, var, na.rm=TRUE) != 0]

# compute distances matrix
t=dist(leuk)
fit=cmdscale(t,eig=TRUE, k=2)

fit$GOF

x = fit$points[,1]
y = fit$points[,2]
#The cex= controls the size of the circles in the plot function.
plot(x,y,col=c("red","green","blue")[factor(type)], cex=3,
     xlab='Coordinate 1', ylab='Coordinate 2',
     main = 'Multidimensional Scaling of Raw Leukemia Data')
text(x,y,labels=row.names(leuk))
```


What if we standardize our data before running the MDS procedure? Will that effect our results? Let's see how it looks on the standardized version of the leukemia data.

```{r fig=T, fig.align='center'}
# We can experiment with standardization to see how it
# effects our results:

leuk2=scale(leuk,center=TRUE, scale=TRUE)
t2=dist(leuk2)
fit2=cmdscale(t2,eig=TRUE,k=2)
fit2$GOF
x2 = fit2$points[,1]
y2 = fit2$points[,2]
#The cex= controls the size of the circles in the plot function.
plot(x2,y2,col=c("red","green","blue")[factor(type)], cex=3,
     xlab='Coordinate 1', ylab='Coordinate 2',
     main = 'Multidimensional Scaling of Standardized Leukemia Data')
text(x2,y2,labels=row.names(leuk))
```

### A note on standardization {-}

Clearly, things have changed substantially. We shouldn't give to much creedence to the decreased Goodness of Fit statistics. I don't necessarily believe that we are explaining less information just because we scaled our data, the fact that this number has changed should likely be attributed to the fact that we have significantly decreased all of the eigenvalues of the matrix, and not in any predictable or meaningful way. It's more important to focus on what we are trying to represent and that is differences between samples. Perhaps if there are some genes for which values vary wildly between the different leukemia types, and other genes which don't show much variation, then we should keep this information in the data. By standardizing the data, we're making the variation of every gene equal to 1 - which stands to wash out some of the bigger, more discriminating factors in the distance calculations. This consideration is something that will need to be made for each dataset on a case-by-case basis. If our dataset had variables with wide scale variations (like income and number of cars) then standardization is a much more reasonable approach! 


There are several things to keep in mind when studying an MDS map.

1. The axis are, by themselves, meaningless.
2. The orientation of the picture is completely arbitrary.
3. All that matters is the relative proximity of the points in the map. Are they close? Are they far apart?




<!--chapter:end:06-OtherDimRed.Rmd-->

# Social Network Analysis {#sna}

## Working with Network Data

We'll use the popular ``igraph`` package to explore the student slack network in R. The data has been anonymized for use in this text. First, we load the two data frames that contain the information for our network: 
- ``SlackNetwork``  contains the interactions between pairs of students. An interaction between students was defined as either an emoji-reaction or threaded reply to a post. The _source_ of the interaction is the individual reacting or replying and the _target_ of the interaction is the user who originated the post. This data frame also contains the channel in which the interaction takes place, and 9 binary flags indicating the presence or absence of certain keywords or phrases of interest.
- ``users`` contains user-level attributes like the cohort to which a student belongs ('blue' or 'orange').


```{r , results=F}
library(igraph)
load('LAdata/slackanon2021.RData')
```

```{r}
head(SlackNetwork)
head(users)
```
Using this information, we can create an igraph network object using the ``graph_from_data_frame()`` function. We can then apply some functions from the igraph package to discover the underlying data as we've already seen it. Because this network has almost 42,000 edges overall, we'll subset the data and only look at interactions from the general channel.

## Network Visualization - ``igraph`` package

```{r }
SlackNetworkSubset = SlackNetwork[SlackNetwork$channel=='general',]
slack = graph_from_data_frame(SlackNetworkSubset, directed = TRUE, vertices = users)
plot(slack)
```
The default plots certainly leave room for improvement. We notice that one user is not connected to the rest of the network in the general channel, signifying that this user has not reacted or replied in a threaded fashion to any posts in this channel, nor have they created a post that received any interaction. We can delete this vertex from the network by taking advantage of the `delete.vertices()` function specifying that we want to remove all vertices with degree equal to zero. You'll recall that the degree of a vertex is the number of edges that connect to it.

```{r }
slack=delete.vertices(slack,degree(slack)==0)
```

There are various ways that we can improve the network visualization, but we will soon see that _layout_ is, by far, the most important. First, let's explore how we can use the plot options to change the line weight, size, and color of the nodes and edges to improve the visualization in the following chunk. 

```{r }
plot(slack, edge.arrow.size = .3, vertex.label=NA,vertex.size=10,
     vertex.color='gray',edge.color='blue')
```
### Layout algorithms for ``igraph`` package

The igraph package has many different layout algorithms available; type `?igraph::layout` for a list of them. By clicking on each layout in the help menu, you'll be able to distinguish which of the layouts are force-directed and which are not. Force-directed layouts generally provide the highest quality network visualizations. The Davidson-Harel (``layout_with_dh``), Fruchterman-Reingold  (``layout_with_fr``), DrL (``layout_with_drl``) and multidimensional scaling algorithms (``layout_with_mds``) are probably the most well-known algorithms available in this package.

We recommend that you compute the layout outside of the plot function so that you may use it again without re-computing it. After all, a layout is just a two dimensional array of coordinates that specifies where each node should be placed. If you compute the layout inside the plot function then every time you make a small adjustment like color or edge arrow size, you will have to your computer will have to re-compute the layout algorithm.

The following code chunk computes 4 different layouts and then plots the resulting networks on a 2x2 grid for comparison. We encourage you to substitute four _different_ layouts (listed in the help document at the bottom) in place of the ones chosen here as part of your exploration.


```{r }
#?igraph::layout
l = layout_with_lgl(slack)
l2 = layout_with_fr(slack)
l3 = layout_with_drl(slack)
l4 =  layout_with_mds(slack)
par(mfrow=c(2,2),mar=c(1,1,1,1)) 
# Above tells the graphic window to use the
# following plots to fill out a 2x2 grid with margins of 1 unit
# on each side. Must reset these options with dev.off() when done!
plot(slack, edge.arrow.size = .3, vertex.label=NA,vertex.size=10,
     vertex.color='lightblue', layout=l,main="Large Graph Layout")
plot(slack, edge.arrow.size = .3, vertex.label=NA,vertex.size=10,
     vertex.color='lightblue', layout=l2,main="Fruchterman-Reingold")
plot(slack, edge.arrow.size = .3, vertex.label=NA,vertex.size=10,
     vertex.color='lightblue', layout=l3,main="DrL")
plot(slack, edge.arrow.size = .3, vertex.label=NA,vertex.size=10,
     vertex.color='lightblue', layout=l4,main = "MDS")
```
<!-- Well, that didn't go quite as planned! Both DrL and Large Graph Layout provide positively terrible results on this network. Both of these methods are designed to work efficiently with very large networks, and as such their performance on easier problems is insufficient. Alas, we can never know what technique will work best for each application, so we leave this in the text to support our main thesis that: __It Depends!__  -->

To reset your plot window, you should run ``dev.off()`` or else your future plots will continue to display in a 2x2 grid.

```{r , results=F}
dev.off()
```

### Adding attribute information to your visualization

We commonly want to represent information about our nodes using color or size. This is easily done by passing a vector of colors into the plot function that maintains the order in the _users_ data frame. We can then create a legend and locate it in our plot window as desired.

```{r }

plot(slack, edge.arrow.size = .2,
     vertex.label=V(slack)$name,
     vertex.size=10,
     vertex.label.cex = 0.3,
     vertex.color=c("blue","orange")[as.factor(V(slack)$Cohort)],
     layout=l3,
     main = "Slack Network Colored by Cohort")

legend(x=-1.5,y=0,unique(V(slack)$Cohort),pch=21,
       pt.bg=c("blue","orange"),pt.cex=2,bty="n",ncol=1)
```
A (nearly) complete list of plot option parameters is given below:

- __vertex.color__: Node color
- __vertex.frame.color__: Node border color
- __vertex.shape__: Vector containing shape of vertices, like  “circle”, “square”, “csquare”, “rectangle” etc
- __vertex.size__: Size of the node (default is 15)
- __vertex.size2__: The second size of the node (e.g. for a rectangle)
- __vertex.label__: Character vector used to label the nodes
- __vertex.label.color__: Character vector specifying color the nodes
- __vertex.label.family__: Font family of the label (e.g.“Times”, “Helvetica”)
- __vertex.label.font__: Font: 1 plain, 2 bold, 3, italic, 4 bold italic, 5 symbol
- __vertex.label.cex__: Font size (multiplication factor, device-dependent)
- __vertex.label.dist__: Distance between the label and the vertex
- __vertex.label.degree__: The position of the label in relation to the vertex (use pi)
- __edge.color__: Edge color
- __edge.width__: Edge width, defaults to 1
- __edge.arrow.size__: Arrow size, defaults to 1
- __edge.arrow.width__: Arrow width, defaults to 1
- __edge.lty__: Line type, 0 =“blank”, 1 =“solid”, 2 =“dashed”, 3 =“dotted”, etc
- __edge.curved__: Edge curvature, range 0-1 (FALSE sets it to 0, TRUE to 0.5)

and if you'd like to try a dark-mode style visualization, consider the global graphical parameter to change the background color of your visual: ``par(bg="black")``.

Any one of these option parameters can be set according to a variable in your dataset, or a metric about your graph. For example, let's define __degree__ as the number of edges that are adjacent to a given vertex. We can _size_ the vertices according to their degree by including that information in the plot function as follows, using the ``degree()`` function. We just have to keep in mind that the ``vertex.size`` plot attribute is expecting the same range of sizes that you would provide for any points on a plot, and since the degree of a vertex can be very high in this case, we should put it on a scale that seems more reasonable. In this example. we divide the degree by the maximum degree to create a number between 0 and 1 and then multiply it by 10 to create ``vertex.size`` values between zero and 10.

```{r }
plot(slack, edge.arrow.size = .2,
     vertex.label=V(slack)$name,
     vertex.size=10*degree(slack, v=V(slack), mode='all')/max(degree(slack, v=V(slack), mode='all')),
     vertex.label.cex = 0.3,
     vertex.color=c("blue","orange")[as.factor(V(slack)$Cohort)],
     layout=l3,
     main = "Slack Network Colored by Cohort")

legend(x=-1.5,y=0,c("Orange","Blue"),pch=21,
       pt.bg=c("Orange","Blue"),pt.cex=2,bty="n",ncol=1)


```
## Package ``networkD3``

The network D3 package creates the same type of visualizations that you would see in the [JavaScript library D3](https://d3js.org). These visualizations are highly interactive and quite beautiful. 

```{r, results=F}
library(networkD3)
```
### Preparing the data for ``networkD3``

The one thing that you'll have to keep in mind when creating this visualization is the insistence of this package that your label names (indices) of your nodes start from zero. To use this package, you need a data frame containing the edge list and a data frame containing the node data. While we already have these data frames prepared, the following chunk of code shows you how to extract them from an igraph object and easily transform your ID or label column into a counter that starts from 0. You can see the first few rows of the resulting data frames below.

```{r }
nodes=data.frame(vertex_attr(slack))
nodes$ID=0:(vcount(slack)-1)
#data frame with edge list
edges=data.frame(get.edgelist(slack))
colnames(edges)=c("source","target")
edges=merge(edges, nodes[,c("name","ID")],by.x="source",by.y="name")
edges=merge(edges, nodes[,c("name","ID")],by.x="target",by.y="name")
edges=edges[,3:4]
colnames(edges)=c("source","target")
head(edges)
head(nodes)
```

Once we have our data in the right format it's easy to create the force-directed network a la D3 with the ``forceNetwork()`` function, and to save it as an .html file with the ``saveNetwork()``  function. 

### Creating an Interactive Visualization with ``networkD3``

The following visualization is interactive! Try it by hovering on or dragging a node.

```{r }
colors = JS('d3.scaleOrdinal().domain(["b", "o"]).range(["#0000ff", "#ffa500"])')

forceNetwork(Links=edges, Nodes=nodes, Source = "source",
             Target = "target", NodeID="name", Group="Cohort", colourScale=colors,
             charge=-100,fontSize=12, opacity = 0.8, zoom=F, legend=T)

```

### Saving your Interactive Visualization to .html

Exploration of the resulting visualization is likely to be smoother in .html, so let's export this visualization to a file with ``saveNetwork()``. 

```{r eval=F}
j=forceNetwork(Links=edges, Nodes=nodes, Source = "source",
               Target = "target", NodeID="name", Group="Cohort",
               fontSize=12, opacity = 0.8, zoom=T, legend=T)
saveNetwork(j, file = 'Slack2021.html')

```

You can find the resulting file in your working directory (or you can specify a path rather than just a file name) and open it with any web browser. 

<!--chapter:end:10-SNA-viz.Rmd-->

# (PART) Clustering {-}
# Introduction {#clusintro}

Clustering is the task of partitioning a set of objects into subsets (clusters) so that objects in the same cluster are similar in some sense. Thus, a "cluster" is a generally a subjective entity determined by how an observer defines the similarity of objects. Take for example Figure \@ref(fig:shapes) where we depict eight objects that differ by shape and color. Depending on the notion of similarity used (color, shape, or both) these objects might be clustered in one of the three ways shown. 

```{r label='shapes', fig.align='center', fig.cap = 'Three Meaningful Clusterings of One Set of Objects', echo=F, out.width="75%"}
knitr::include_graphics("figs/shapes.jpg")
```


## Mathematical Setup

In order to define this problem in a mathematical sense, it is necessary to quantify the attributes of objects using data so that we can mathematically or statistically determine notions of similarity.  Data clustering refers to the process of grouping data points _naturally_ based on information found in the data which describes its characteristics and relationships.  It is not an exact science and, as we will discuss at length, there is no _best_ method for partitioning data into clusters. 

### Data

We will use the terms "observation", "object", and "data point" interchangeably to refer to the entities we aim to partition into clusters. These data points could represent any population of interest, be it a collection of documents or a group of Iris flowers. Each data point will be considered as a column vector, containing measurements of features, attributes, or variables (again, used interchangeably) which characterize it.  For example, a column vector characterizing a document could have as many rows as there are words in the dictionary, and each entry in the vector could indicate the number of times each word occurred in the document. An Iris flower, on the other hand, may be characterized by far fewer attributes, perhaps measurements on the size of its petal and sepal.  It is assumed we have $n$ such objects, each represented by a column vector $\x_j$ containing measurements on $m$ variables. All of this information is collected in an $m \times n$ __data matrix__, $\X$, which will serve as input to the various clustering methods.
$$\X=[\x_1,\x_2,\dots,\x_n]$$

The aim of data clustering is to automatically determine clusters in these populations based upon the information contained in those vectors. In the document collection, the goal may be to identify clusters of documents which discuss similar subject matter whereas in the Iris data, the goal may be to learn about different species of Iris flowers.

<!-- %In any situation where measurements are taken or data is collected, it becomes necessary to consider the presence of \textbf{noise:} erroneous, or meaningless information obscuring meaningful patterns in data. In Chapter \ref{chap2} we will discuss techniques for reducing noise in high-dimensional data (data with a large number of attributes).   -->

In applied data mining, variables fall into the following four categories: Nominal/Categorical, Ordinal, Interval, or Ratio. 
<ol>
<li> __Nominal/Categorical:__ Variables which have no ordering, for example ethnicity, color or shape.
<li> __Ordinal:__ Variables which can be rank-ordered but for which distances have no meaning. For example, scores assigned to levels education (0=no high school, 1=some high school, 2=high school diploma). The distance between 0 and 1 is not necessarily the same as the distance between 1 and 2, but the numbers have some meaning by order.
<li> __Interval:__ Variables for which differences can be interpreted but for with ratios make no sense. For example if temperature is measured in degrees Fahrenheit the distance from 20 to 30 degrees is the same as the distance from 70 to 80 degrees, however 80 degrees is not ``twice as hot'' as 40 degrees.
<li> __Ratio:__ variables for which a meaningful ratio can be constructed. For example height or weight. An absolute zero is meaningful for a ratio variable.
</ol>

For the algorithms contained herein, it is assumed that the attributes used are ratio variables, although many of the methods can be extended to include other types of data (or one can simply use dummy variables and/or ordinal encoding as long as one explores the potential impacts of such decisions). 

## The Number of Clusters, $k$

One of the important problems in cluster analysis is the determination of the number of clusters, $k$. The number of clusters can also be a matter of subjectivity. Take for instance the 2-dimensional scatter plot of points in Figure \@ref(fig:introex).  Using the points' proximity in Euclidean space as a measure of their similarity, one could argue that there are any number of clusters in this simple illustration. However, most people would agree that there is indeed cluster-like structure in this data. 

```{r label='introex', fig.align='center', fig.cap = 'How Many Clusters do _You_ See?', echo=F, out.width="50%"}
knitr::include_graphics("figs/introex.jpg")
```

Figure \@ref(fig:introex) also motivates a discussion of _subcluster structure_. Figure \@ref(fig:introex12) shows two possible clusterings with different numbers of clusters. The clustering on the right depicts a logical subdivison of the clusters on the left.


```{r label='introex12', fig.align='center', fig.cap = 'Two Reasonable Answers with Different Numbers of Clusters', echo=F, out.width="100%"}
knitr::include_graphics("figs/introex12.png")
```


It is easy to imagine data in which the number of clusters to specify is a matter of debate only because groups of related objects can be meaningfully divided into subcategories or _subclusters._ For example a collection of webpages may clearly fall into 3 categories: sports, investment banking, and astronomy. If the webpages about sports further divide into 2 categories like baseball and basketball then we'd refer to that as subclustering. 

## Partitioning of Graphs and Networks

Another popular research problem in clustering is the partitioning of graphs. In network applications this problem has become known to many as _community detection_ [@ncdnewman; @ncdmucha, @mahoney], although the underlying problem of partitioning graphs has been studied extensively for years [@drineassvd; @fiedlerev; @chung; @ng; @poweriteration; @pothen; @minmax]. A __graph__ is a set of _vertices_ (or equivalently _nodes_) $V=\{1,2,\dots, n\}$ together with a set of edges $E=\{(i,j) : i,j \in V\}$ which connect vertices together. A __weighted graph__ is one in which the edges are assigned some weight $w_{ij}$ whereas an unweighted graph has binary weights for edges: $w_{ij}=1$ if $(i,j) \in E$, $w_{ij}=0$ otherwise. Our focus will be on _undirected graphs_ in which $w_{ij}=w_{ji}$.  All algorithms for graph partitioning (or network community detection) will rely on an __adjacency matrix__. 

:::{.definition  name='Adjacency Matrix' #adjmat} 
An __adjacency matrix__ $\A$ for an undirected graph, $\mathcal{G}=(V,E)$, is an $n \times n$ symmetric matrix defined as follows:
$$\A_{ij}=
\begin{cases}
w_{ij}, & \text{if } (i,j) \in E \\
0 & \text{otherwise}
\end{cases}$$
:::

Figure \@ref(fig:introexg) is an example of a graph exhibiting cluster or community structure. The weights of the edges are depicted by their thickness. It is expected that edges within the clusters occur more frequently and with higher weight than edges between the clusters. Thus, once the rows and columns of the matrix are reordered according to their cluster membership, we expect to see a matrix that is _block-diagonally dominant_ - that is, one in which values in square diagonal blocks are relatively large compared to those in the off-diagonal blocks. 

```{r label='introexg', fig.align='center', fig.cap = 'A Graph with Clusters and its Block Diagonally Dominant Adjacency Matrix', echo=F, out.width="100%"}
knitr::include_graphics("figs/introexg.png")
```

In much of the literature on graph partitioning, it is suggested that the data clustering problem can be transformed into a graph partitioning problem by means of a similarity matrix [@minmax,@spectraltutorial,@pothen,@poweriteration]. A __similarity matrix__ $\bo{S}$ is an $n \times n$ symmetric matrix where $\bo{S}_{ij}$ measures some notion of similarity between data points $\x_i$ and $\x_j$. There are a wealth of metrics available to gauge similarity or dissimilarity, see for example [@dcebook]. Common measures of similarity rely on Euclidean or angular distance measures. Two of the most popular similarity functions in the literature are the following:
\begin{itemize}
\item \textbf{Gaussian Similarity:} $$\bo{S}_{ij} =\mbox{exp}(-\frac{\|\x_i-\x_j\|^2}{2\alpha^2})$$ where $\alpha$ is a tuning parameter.
\item \textbf{Cosine Similarity:} $$\bo{S}_{ij}=\cos (\x_i,\x_j) = \frac{\x_i^T\x_j}{\|\x_i\|\|\x_j\|}$$
\end{itemize}

Any similarity matrix can be considered an adjacency matrix for a graph, thus transforming the original data clustering problem into a graph partitioning problem. Several algorithms for data clustering and graph partitioning are provided in Section \@ref(clusteralgos). Regardless of the method chosen for clustering, similarity matrices can be a useful tool for visualizing cluster results in light of the block diagonal structure shown in Figure \@ref(fig:introexg). This block diagonal structure can be visualized using a heat map of a similarity matrix. A _matrix heat map_ represents each value in a matrix by a colored pixel indicating the magnitude of the value. Figure \@ref(fig:heatmapex) is an example of a heat map using real data. The data are a collection of news articles (documents) from the web containing seven different topics of discussion. The rows and columns of the cosine similarity matrix for these documents have been reordered according to some cluster solution. White pixels represent negligible values of similarity.  Some of these topics are closely related, which can be seen by the heightened level of similarities between blocks.

```{r label='heatmapex', fig.align='center', fig.cap = 'Heat Map of Similarity Matrix Exhibiting Cluster Structure', echo=F, out.width="50%"}
knitr::include_graphics("figs/heatmapex.jpg")
```


While a heat-map visualization allows the user to get a sense of the quality of a specific clustering, it does not always make it easy to determine which is a better solution given two different clusterings. Since clustering is an unsupervised process, quantitative measures of cluster quality are often used to compare different clusterings.  A short survery of these metrics is given in Section \@ref(validation). First, we will take a brief historical look at the roots of cluster analysis and how it emerged into a major field of research in the late twentieth century.

## History of Data Clustering

According to Anil Jain in [@jain50], data clustering first appeared in an article written by Forrest Clements in 1954 about anthropological data [@1954anthro]. However, a Google Scholar search provides several earlier publications whose titles also contain the phrase "cluster analysis" [@1952office,@1952adcock]. In fact, the discussion of data clustering dates back to the 1930's when anthropologists Driver and Kroeber [@1932driver] and psychologists Zubin [@1938zubin] and Tryon [@1939tryon] realized the utility of such analysis in determining cultural or psychological classifications. While the usefulness of such techniques was clear to researchers in many social and biological disciplines at that time, the lack of computational tools made the analysis time consuming and practically impossible for large sets of data. 

Cluster analysis exploded into the limelight in the 1960's and `70's after Sokal and Sneath's 1963 book _Principles of Numerical Taxonomy_ [@sokal]. Although the text is primarily geared toward biologists faced with the task of classifying organisms, it  motivated researchers from many different disciplines to consider the problem of data clustering from other angles like computing, statistics, and domain specific applications. [@bock].  Sokal and Sneath's book presents a detailed discussion of the simple, intuitive, and still popular _hierarchical clustering_ (see Section \@ref(hc)) techniques for biological taxonomy.  These authors also provided perhaps the earliest mention of the matrix heat map visualizations of clustering that are still popular today. Figure \@ref(fig:sneathheatmap) shows an example of one of these heat maps, then drawn by hand, from their book. 

```{r label='sneathheatmap', fig.align='center', fig.cap = 'Hand Drawn Matrix Heat Map Visualization from 1963 Book by Sokal and Sneath', echo=F, out.width="50%"}
knitr::include_graphics("figs/sneathheatmap.jpg")
```

Prior to the development of modern computational resources, programs for numerical taxonomy were written in machine language and not easily transferred from one computer to another [@sokal]. However, by the mid 1960s, it was clear that the advancement in technology would probably keep pace with advancements in algorithm design and many researchers from various disciplines began to contribute to the clustering literature. The following sections present an in-depth view of the most popular developments in data clustering since that time. Chapter \@ref(dimred) explores a common problem associated with the massive datasets of modern day.



<!--chapter:end:11-Clustering.Rmd-->

# Algorithms for Data Clustering {#clusteralgos}

There have been countless algorithms proposed for data clustering. While a complete survey and discussion of clustering algorithms would be nearly impossible, this chapter provides an introduction to some of the most popular algorithms to date. For the purposes of organization, the algorithms are divided into 3 groups: Hierarchical, Iterative Partitional, and Density-based. 

## Hierarchical Algorithms {#hc}

As discussed in Chapter \@ref(clusintro), data clustering became popular in the biological fields of phylogeny and taxonomy. Even prior to the advancement of numerical taxonomy, it was common for scientists in this field to communicate relationships by way of a _dendrogram_ or tree diagram as illustrated in Figure \@ref(fig:dendrogram) [@sokal]. Dendrograms provide a nested hierarchy of similarity that allow the researcher to see different levels of clustering that may exist in data, particularly in phylogenic data.  _Agglomerative hierarchical clustering_ has its roots in this domain. 

### Agglomerative Hierarchical Clustering

The idea behind agglomerative heirarchical clustering is to link similar objects or similar clusters of objects together in a hierarchy where the highest levels of similarity is represented by the lowest level connections. These methods are called agglomerative because they begin with each data point in a separate cluster and at each step they merge clusters together according to some decision rule until eventually all of the points end up in a single cluster. For example, in Figure \@ref(fig:dendrogram), objects 1 and 2 exhibit the highest level of similarity as indicated by the height of the branch that connects them. Also illustrated in the dendrogram is the fact that the blue cluster and green cluster are more similar to each other than they are to the red cluster. One of the advantages to these hierarchical structures is that branches can be cut to achieve any number of clusters desired by the user. For example, in Figure \@ref(fig:dendrogram) if only the highest branch of the dendrogram is cut, the result is two clusters: {{1,2,3},{4,5,6,7,8,9}}. When the next highest branch is cut, we are left with 3 clusters: {{1,2,3},{4,5,6},{7,8,9}}.

```{r label='dendrogram', fig.align='center', fig.cap = 'A Dendrogram exhibiting linkage/similarity between 9 objects in 3 clusters.', echo=F, out.width="75%"}
knitr::include_graphics("figs/dendrogram.jpg")
```

There are a number of different systems for determining linkage in hierarchical clustering dendrograms. For a complete discussion, we suggests the classic books by Anderberg [@anderberg] or Jain and Dubes [@jainbook]. The basic scheme for hierarchical clustering algorithms is outlined in the algorithm below.\

__Agglomerative Hierarchical Clustering__
<ol>
<li> __Input__: n objects to be clustered.
<li> Begin by assigning each object to its own cluster. 
<li> Compute the pairwise similarities between each cluster.
<li> Find the most similar pair of clusters and merge them into a single cluster. There is now one less cluster. 
<li> Compute pairwise similarities between the new cluster and each of the old clusters.
<li> Repeat steps 3-4 until all objects belong to a single cluster of size n.
<li> __Output__: Dendrogram depicting each merge step. 
</ol>

What differentiates the numerous hierarchical clustering algorithms is the choice of similarity metric used and the way the chosen similarity metric is used to compare clusters in step 4. For example, suppose Euclidean distance is chosen to compute the similarity (or dissimilarity) between objects in step 2. In step 4, the same notion of similarity must be extended to compare _clusters_ of objects. Several methods of computing pairwise distances between clusters have been proposed over the years. The most common approaches are as follows:


- __Single-Linkage__: The distance between two clusters is equal to the _shortest_ distance from any member of one cluster to any member of the other cluster.
- __Complete-Linkage__: The distance between two clusters is equal to the _greatest_ distance from any member of one cluster to any member of the other cluster.
- __Average-Linkage__: The distance between two clusters is equal to the _average_ distance from any member of one cluster to any member of the other cluster.


While many people have been given credit for the methods listed above, it appears that numerical taxonomers Sneath, Sokal and Michener were the first to describe the Single- and Average-linkage protocols, while ecologist Sorenson had previously pioneered Complete-linkage in his ecological studies. These early researchers used correlation coefficients to measure similarity between objects, but they suggest in 1963 that other correlation-like or distance-like measures could also be useful [@sokal].  The paper by Stephen Johnson in 1967 [@johnson67] formalized the single- and complete-linkage algorithms in a more general data clustering setting.  Other linkage techniques for hierarchical clustering, such as centroid and median linkage, have been proposed as well. We refer interested readers to Anderberg [@anderberg] for more on these variants.

 The main drawback of agglomerative hierarchical schemes is their computational complexity. In recent years, variations like BIRCH [@birch] and CURE [@cure] have been developed in an effort to combat this problem. Another feature which causes problems in some applications is that once a connection between points or clusters is made, it cannot be undone. For this reason, hierarchical algorithms often suffer in the presence of noise and outliers.


### Principal Direction Divisive Partitioning (PDDP)

While the hierarchical algorithms discussed above were _agglomerative_, it is also possible to create a cluster hierarchy or dendrogram by iteratively {_dividing_ points into groups until a desired number of groups is reached. Principal Direction Divisive Partitioning (PDDP) is one example of a {_divisive hierarchical algorithm_. Other partitional methods which will be discussed in Section \@ref(spectral) can also be placed in this hierarchical framework.

PDDP was proposed in [@boleypddp] by Daniel Boley at the University of Minnesota. PDDP has become popular due to its computational efficiency and ability to handle large data sets. We will explain this algorithm in a different, but equivalent context than is done in the original paper. At each step of this method, data are projected onto the first principal component and split into two groups based upon which side of the mean their projections fall. The first principal component, as discussed in Chapter \@ref(pca), creates the _total least squares line_, $\mathcal{L}$, which is the line which minimizes the total sum of squares of orthogonal deviations between the data and $\mathcal{L}$ among all lines in $\Re^m$.  Let $\X=[\mathbf{x}_1,\mathbf{x}_2,\dots,\mathbf{x}_n]$ be the data points and $\mathcal{L}(\mathbf{u},\bo{p})$ be a line in $\Re^m$ where $\bo{p}$ is a point on a line and $\mathbf{u}$ is the direction of the line. The projection of $\mathbf{x}_j$ onto $\mathcal{L}(\mathbf{u},\bo{p})$ is given by
$$\widehat{\mathbf{x}_j} = \mathbf{u}\mathbf{u}^T(\mathbf{x}_j-\bo{p})+\bo{p},$$
and therefore the orthogonal distance between $\mathbf{x}_j$ and $\mathcal{L}(\mathbf{u},p)$ is
$$\mathbf{x}_j - \widehat{\mathbf{x}_j} = (\bo{I}-\mathbf{u}\mathbf{u}^T)(\mathbf{x}_j-\bo{p}).$$
Consequently, the total least squares line is the line $\mathcal{L}$ which minimizes (over directions $\mathbf{u}$ and points $\bo{p}$)
\begin{equation*}
\begin{split}
f(\mathbf{u},\bo{p}) &= \sum_{j=1}^{n} \|\mathbf{x}_j - \widehat{\mathbf{x}_j}\|_2^2\\
&=\sum_{j=1}^{n} \|(\bo{I}-\mathbf{u}\mathbf{u}^T)(\mathbf{x}_j-\bo{p})\|_2^2\\
&= \|(\bo{I}-\mathbf{u}\mathbf{u}^T)(\X-\bo{p}\e^T)\|_F^2.
\end{split}
\end{equation*}

The following definition precisely characterizes the first principal component as the total least squares line. 


:::{.definition name='First Principal Component (Total Least Squares Line)' #rowcol}
The __First Principal Component (total least squares line)__ for the column data in $\X=[\mathbf{x}_1,\mathbf{x}_2,\dots,\mathbf{x}_n]$ is given by
$$\mathcal{L} = \{\alpha \mathbf{u}_1(\X_c) + \boldsymbol\mu | \alpha \in \Re\},$$
where $\boldsymbol\mu= \X\e/n$ is the mean (centroid) of the column data, and $\mathbf{u}_1(\bo{X}_c)$ is the principal left-hand singular vector of the centered matrix 
$$\bo{X}_c=\X-\boldsymbol\mu\e^T = \X(\bo{I}-\e\e^T/n).$$
:::

The orthogonal projection of the data onto the total least squares line will capture the maximum amount of directional variance over all possible one dimensional orthogonal projections. This fact is treated in greater detail in Chapter \@ref(pca). 

Boley's PDDP algorithm partitions the data into two clusters at each step based upon whether their projections onto the total least squares line fall to the left or to the right of $\boldsymbol \mu$. This is equivalent to examining the signs of the projections of the _centered_ data, $\X_c$, onto the direction $\mathbf{u}_1(\bo{X}_c)$. Conveniently, the signs of the projections are determined by the signs of the entries in the principal _right-hand_ singular vector, $\vv_1(\X_c)$.  A simple example motivating this method is illustrated in Figure \@ref(fig:pddpgood).

```{r label='pddpgood', fig.align='center', fig.cap = 'Illustration of Principal Direction Divisive Partitioning: Two Clusters and their Corresponding Projections on the First Principal Component', echo=F, out.width="80%"}
knitr::include_graphics("figs/pddpgood.png")
```

 Once the data are divided, the two clusters are examined to find the one with the greatest variance (scatter). This subset of data is then extracted from the original data matrix, centered and projected onto the span of its own first principal component. The split at zero is made again and the algorithm proceeds iteratively until the desired number of clusters has been produced.

 It is necessary to note, however, that the example in Figure \@ref(fig:pddpgood) is truly an ideal geometric configuration of data. Figure \@ref(fig:pddpbad) illustrates two configurations in which PDDP would fail. In the configuration on the left, both clusters would be split down the middle, and in the configuration on the right, the middle cluster would be split in the first iteration. Unfortunately, once data points are separated in an iteration of PDDP, there is no chance for them to be rejoined later. Table \@ref(tab:algpddp) provides the PDDP Algorithm.

```{r label='pddpbad', fig.align='center', fig.cap = 'Failures of Principal Direction Divisive Partitioning: Two Configurations of Data that would be Poorly Clustered by PDDP', echo=F, out.width="80%"}
knitr::include_graphics("figs/pddpbad.png")
```



<table>
<tr> <td>
<ol>
<li> __Input:__ $n$ data points $\X=[\mathbf{x}_1,\mathbf{x}_2,\dots,\mathbf{x}_n]$ and number of clusters $k$
<li> Center the data to have mean zero: $\X_c = \X-\boldsymbol \mu\e^T$.
<li> Compute the first right singular vector of $\X_c$, $\vv_1$.
<li> Partition the data into two clusters based upon the signs of the entries in $\vv_1$.
<li> Compute the variance of each existing cluster and choose the cluster with largest variance to partition next.
<li> Repeat steps 1-4 using only the data in the cluster with largest variance until eventually $k$ clusters are formed.
<li> __Output:__ Resulting $k$-clusters
</table>

<caption>(\#tab:algpddp) Principal Direction Divisive Partitioning (PDDP)</caption>
 <br>

Since its initial publication, variations of the PDDP algorithm have been proposed, most notably PDDP($\ell$) [@pddpl] and KPDDP [@kpddp], both developed by Dimitrios Zeimpekis and Efstratios Gallopoulos from the University of Patras in Greece. PDDP($\ell$) uses the sign patterns in the first $\ell$ principal components to partition the data into at most $2^\ell$ clusters at each step of the algorithm, whereas KPDDP is a kernel variant which uses $k$-means to steer the cluster assignments at each step.

## Iterative Partitional Algorithms {#kmeanshistory}

 Iterative partitional algorithms begin with an initial partition of the data into $k$ clusters and iteratively update the cluster memberships according to some notion of what constitutes a ``better" partition [@jainbook; @anderberg].   The $k$-means algorithm is one example of a partitional algorithm. Before we get into the details of the modern day $k$-means algorithms, we'll take a look back at the history that fostered its development as one of the best-known and most widely used clustering algorithms in the world.

### Early Partitional Algorithms

 Although the name ``$k$-means" was first used by MacQueen in 1967 [@macqueen], the partitional method generally referred to by this name today was proposed by Forgy in 1965 [@forgy]. Forgy's algorithm involves iteratively updating $k$ _seed points_ which, at each pass of the algorithm, define a partitioning of the data by associating each data point with its nearest seed point. The seeds are then updated to represent the centroids (means) of the resulting clusters and the process is repeated. Euclidean distance is the most common metric for measuring the nearness of points in these algorithms, but other metrics, such as Mahalanobis distance and angular distance, can and have been used as well. $K$-means can also handle binary or categorical variables by using simple matching coefficients found in the data mining literature, for example [@datamining].  Forgy's method is outlined in Table \@ref(tab:algforgy).

<table>
<tr> <td>
<ol>
<li>    __Input__: Data points and an initial cluster configuration of the data, defined by $k$ seed points (start in step 1) or an initial clustering (start in step 2).
<li> Assign each data point to the cluster associated with the nearest seed point.
<li> Compute new seed points to be the centroids of the clusters.
<li> Repeat steps 1 and 2 until no data points change cluster membership in step 2.
<li>   __Output__: Final Clusters
</ol>
</table>

<caption>(\#tab:algforgy) Forgy's $k$-means Algorithm [@anderberg]</caption>
 <br>

In 1966, Jancey suggested a variation of this method where the new seeds points in step 2 were computed by reflecting the old seed point across the new centroid, as depicted in Figure \@ref(fig:jancey). Jancey argued that the data's nearness to point 1 grouped them into a cluster initially, and thus using a seed point which exaggerates this movement toward the new centroid ought to help speed up convergence, and possibly lead to a better solution by avoiding local minima [@jancey].

```{r label='jancey', fig.align='center', fig.cap = "Jancey's method of reflecting old seed point across the centroid to determine new seed point", echo=F, out.width="35%"}
knitr::include_graphics("figs/Jancey.jpg")
```


MacQueen's 1967 partitional process, which he called ``$k$-means", differs from Forgy's formulation in that it a) specifies initial seed points and b) assigns data points to clusters one-by-one, updating the seed to be the centroid of the cluster each time a new point is added. The algorithm only makes one pass through the data. MacQueen's method is presented in Table \@ref(tab:algmacqueen).


<table>
<tr><td>
 __Input__: $n$ data points
 <ol>
<li> Choose the first $k$ data points as clusters with one member each. Set i=1.
<li> Assign the $(k+i)^{th}$ data point to the cluster with the closest centroid. Recompute the cetroid of the updated cluster. Set $i=i+1$.
<li> Repeat step 2 until $i=n-k$ and all the data points have been assigned. Use final cluster centroids to determine a final clustering by re-assigning each data point to the cluster associated with its nearest centroid.
</ol>
 __Output__: Final Clusters
</table>

<caption> (\#tab:algmacqueen) MacQueens $k$-means Algorithm</caption>
 <br>

As you can see, MacQueen's algorithm, while similar in spirit, is quite different from that proposed by Forgy. The set of clusters found is likely to be dependent upon the order of the data, a property generally undesirable in cluster analysis. MacQueen stated that in his experience, these discrepancies in final solution based upon the order of the data were generally minor, and thus not unlike those caused by the choice of initialization in Forgy's method.  An advantage of MacQueen's algorithm is the reduced computation load achieved by avoiding the continual processing of the data to convergence.  It has also been suggested that MacQueen's method may be useful to initialize the seeds for Forgy's algorithm [@anderberg] and in fact this option is available in many data mining software packages like SAS's Enterprise Miner.

Discussion of some additional partitional methods, including Dubes and Jain's FORGY implementation and Ball and Hall's ISODATA algorithm, is deferred to Chapter \@ref(number) because they involve procedures aimed at determining the number of clusters in the data.


### $k$-means {#kmeans}

We will finish our discussion of $k$-means with what has become the classical presentation. We begin with a matrix of column data, $\X=[\mathbf{x}_1,\mathbf{x}_2,\dots,\mathbf{x}_n]$ where $\mathbf{x}_i \in \Re^m, 1 \leq i \leq n$. The objective of $k$-means is to determine a partitioning of the data into $k$ sets, $C=\{C_1, C_2, \dots, C_k\}$, such that an intra-cluster sum of squares cost function is minimized:
\[
\mbox{arg}\min_C \sum_{i=1}^{k} \sum_{\mathbf{x}_j \in C_i} \|\bo{x}_j-\boldsymbol \mu_i \|^2
\]
Any desired distance metric can be used, according to the applications and whims of the user.  Euclidean distance is standard, and leads to the specification _Euclidean $k$-means_.   In document clustering, it is common to use the cosine of the angle between two data vectors (documents) to measure their distance from each other. This variant is commonly referred to as _spherical $k$-means_ and will be discussed briefly in Section \@ref(skmeans).  The $k$-means algorithm, which is essentially the same as Forgy's algorithm in Section \@ref(kmeanshistory), is presented in Table \@ref(tab:algkmeans).



<table>
<tr><td>
 __Input__: Data points $\{\mathbf{x}_1,\mathbf{x}_2,\dots,\mathbf{x}_n\}$ and set of initial centroids $\{\boldsymbol \mu_1^{(0)},\boldsymbol \mu_2^{(0)},\dots, \boldsymbol \mu_k^{(0)}\}$.
<ol>
<li> Assign each data point to cluster associated with the nearest centroid. $$ C_j^{(t)} = \{\mathbf{x}_i : \|\mathbf{x}_i-\boldsymbol \mu_j^{(t)} \| \leq \|\mathbf{x}_i-\boldsymbol \mu_l^{(t)} \| \,\, \forall 1 \leq l \leq k\}$$ If two centroids are equally close, the tie is broken arbitrarily.
<li> The new centroid for each cluster is calculated by setting $$\boldsymbol \mu_j^{(t+1)}=\frac{1}{|C_j^{(t)}|} \sum_{\mathbf{x}_i \in C_j^{(t)}} \mathbf{x}_i$$
<li> Repeat steps 2 and 3 until the centroids remain stationary.
</ol>
 __Output__: $k$ clusters $C_1,C_2,\dots,C_k$
</table>

<caption> (\#tab:algkmeans) Euclidean $k$-means </caption>
 <br>
 This algorithm is guaranteed to converge because there are a finite number of partitions possible and at each pass of the algorithm the intra-cluster sum of squares cost function is decreased due to the fact that points are reassigned to a new cluster only if they are closer to the existing centroid of the new cluster than they were to the old one. The cost function is further reduced as the new centroids are calculated and the process repeats, lowering the cost function at each step. However, it is quite common for the algorithm to converge to local minima, particularly with large datasets. The output of $k$-means is sensitive to the initialization of the centroids and the choice of distance metric used in step 2. Randomly initialized centroids tend to be the most popular, but one can also seed the algorithm with centroids of clusters determined by another clustering algorithm. 
<!-- % \subsubsection*{Linear Algebraic Formulation of $k$-means Objective} -->
<!-- % Let $\bo{H}$ be a $k \times n$ matrix indicating the cluster memberships of the $m$-dimensional data $\X=[\mathbf{x}_1,\mathbf{x}_2,\dots, \mathbf{x}_n]$ into a set of clusters $C=\{C_1,\dots C_k\}$ as follows: -->
<!-- % $$\bo{H}_{ij} = \left\{ -->
<!-- %     \begin{array}{lr} -->
<!-- %       \frac{1}{\sqrt{n_i}} : &\mbox{if  } \mathbf{x}_j \in C_i \\ -->
<!-- %       0  : &\mbox{otherwise} -->
<!-- %     \end{array} -->
<!-- %   \right. -->
<!-- % $$ -->
<!-- % Then, using $\mathcal{H}$ to denote the set of all such indicator matrices $\bo{H}$, the $k$-means objective function can be written as follows: -->
<!-- % $$\min_{\bo{H} \in \mathcal{H}} \|\X-\X\bo{H}^T\bo{H}\|_F^2$$ -->
<!-- % -->

#### Spherical $k$-means {#skmeans}

In some applications, such as document clustering, similarity is often measured by the cosine of the angle $\theta$ between two objects $\mathbf{x}_i$ and $\mathbf{x}_j$ (each normalized to have unit norm),
$$\cos(\theta)=\mathbf{x}_i^T\mathbf{x}_j.$$
This similarity is often transformed into a distance by computing the quantity $d(\mathbf{x}_i,\mathbf{x}_j)=1-\cos(\theta)$ to formulate the spherical $k$-means objective function as follows:
$$\min_C \sum_{i=1}^k \sum_{\mathbf{x} \in C_i} 1- \mathbf{x}^T \bo{c}_i.$$
Where $\bo{c}_i = \frac{1}{\|\boldsymbol \mu_i\|}\boldsymbol \mu_i $ is the normalized centroid of the cluster. The spherical $k$-means algorithm is the same as the euclidean $k$-means algorithm aside from the definition of nearness in step 2.

#### $k$-mediods: Partitioning around Mediods (PAM) and Clustering Large Applications (CLARA)

In 1987, Kaufman and Rousseeuw devised another partitional method which searched through data in order to find $k$ representative points (or mediods) belonging to the dataset which would serve as cluster centers in the same way the centroids do in $k$-means. They called these points ``representative" because it was thought the points would give some interpretability to the groups by exhibiting some defining characteristics of their associated clusters and distinguishing characteristics from other clusters.  The authors' original algorithm, Partitioning around Mediods (PAM), was not suitable for large datasets because of the computation time necessary to search through the data points to build the set of $k$ representative points. The same authors developed a second algorithm, Clustering Large Applications (CLARA), to combat this problem. The central idea of CLARA was to use PAM on large datasets by sampling the data and applying the algorithm on the smaller sample. Once $k$ representative points were found in the sample, the remaining data were associated with the mediod to which they were closest. The quality of the clustering is measured by the average distance of every object to its representative point. Five such samples are drawn, and the clustering that results in the lowest average distance is retained [@kaufman].

### The Expectation-Maximization (EM) Clustering Algorithm

The Expectation-Maximization (EM) Algorithm, originally proposed by Dempster, Laird, and Rubin in 1977 [@emdempster], is one that has been used to solve many types of statistical problems over the years. It is generally used to determine parameters of a statistical model used to describe observations in a dataset. Here we will show how the algorithm is used for clustering, as in [@emcluster]. Our discussion is limited to the variant of the algorithm which uses Gaussian mixtures to model the data.

Supposing that our data points, $\mathbf{x}_1,\mathbf{x}_2,\dots,\mathbf{x}_n$, each belonging to one of $k$ clusters (or classes), $C_1,C_2,\dots, C_k$. Then there exists some latent variables $y_i, \,\, 1\leq i\leq n$, which identify the class membership of each $\mathbf{x}_i$. It is assumed that each class label $C_i$ determines the probability distribution of the data in that class. Here, we assume that this distribution is multivariate Gaussian.  The parameters of this model include the a priori probabilities of each of the $k$ classes, $P(C_i)$, and the parameters of the corresponding normal distributions $\boldsymbol \mu_i$ and $\mathbf{\Sigma_i}$, which are the mean and covariance matrix respectively. The objective of the EM algorithm is to determine the parameters which maximize the likelihood of the data:
$$\log L = \sum_i (\log P(y_i) + \log P(\mathbf{x}_i|y_i))$$

 The EM algorithm takes as input a set of $m$-dimensional data points, $\{\mathbf{x}_i\}_{i=1}^n$, the desired number of clusters $k$, and an initial set of parameters $\theta_j$ for each cluster $C_j$ $1\leq j\leq k$. For Guassian mixtures, $\theta_j$ consists of mean $\boldsymbol \mu_j$ and an $m\times m$ covariance matrix $\mathbf{\Sigma_j}$. The a priori probability of each cluster, $\alpha_j = P(C_j)$ must also be initialized and updated throughout the algorithm. If no information is known about the underlying clusters, then we suggest initialization $\alpha_j = 1/k$ for all clusters $C_j$.   EM then operates by iteratively executing an _expectation step_, where the probability that each data point belongs to each of the $k$ classes is computed, followed by a _maximization step_, where the parameters for each class are updated to maximize the likelihood of the data [@emcluster]. These steps are summarized in Table \@ref(tab:algem).

<table>
<tr><td>
__Input__: $n$ data points, $\{\mathbf{x}_i\}_{i=1}^n$, number of clusters $k$, and initial set of parameters for each cluster $C_j$: $\alpha_j$ and $\theta_j = \{\boldsymbol \mu_j, \Sigma_j\}\,\,1\leq  j\leq k$
<ol>
<li> _Expectation Step_: Compute the probability of each data point $\mathbf{x}_i$ being drawn from each class distribution, $C_j$:
$$p_{ij} = P(\mathbf{x}_i|\alpha_j,\boldsymbol \mu_j,\Sigma_j) \propto \alpha_j P(\mathbf{x}_i|\boldsymbol \mu_j,\Sigma_j)$$
<li> _Maximization Step_: Update the parameters to maximize the likelihood of the data:
$$\alpha_j = \frac{1}{n} \sum_{i=1}^{n} p_{ij}$$
$$\boldsymbol \mu_j = \frac{\sum_{i=1}^{n} p_{ij}\mathbf{x}_i}{\sum_{i=1}^{n} p_{ij}}$$
$$\Sigma_j = \frac{\sum_{i=1}^n p_{ij}(\mathbf{x}_i-\boldsymbol \mu_j)(\mathbf{x}_i-\boldsymbol \mu_j)^T}{\sum_{i=1}^{n} p_{ij}}$$
<li> Repeat steps 1-2 until convergence.
</ol>
 __Output__: Class label $j$ for each $\mathbf{x}_i$ such that $p_{ij} \geq p_{il}\,\,1\leq l \leq k$
</table>

<caption> (\#tab:algem) Expectation-Maximization Algorithm for Clustering [@emcluster]. </caption>
<br>

The EM Algorithm with Gaussian mixtures works well for clustering when the normality assumption of the underlying clusters holds true. Unfortunately, it is difficult to know if this is the case prior to the identification of the clusters. The algorithm suffers considerable computational drawbacks, particularly with regards to storage of the $k$ covariance matrices $\mathbf{\Sigma_j}\in \Re^{m\times m}$, and is not easily run in parallel. For this reason, the EM algorithm is generally limited in its ability to be used on large datasets, particularly when the number of attributes $m$ is very large, as it is in document clustering.

## Density Search Algorithms

If objects are depicted as data points in a metric space, then one may interpret the problem of clustering as an attempt to find areas of the space that are densely populated by points, separated by less populated areas. A natural approach to the problem is then to search through the space seeking these dense regions. Such algorithms have been referred to as _density search_ algorithms [@everitt]. While these algorithms tend to suffer on real data in both accuracy efficiency, their ability to identify noise and to estimate the number of clusters $k$ makes them worthy of discussion.

Many density search algorithms have their roots in the single-linkage hierarchical algorithms described in Section \@ref(hc). Individual points are joined together in clusters one-by-one based upon their similarity (or nearness in space). However in this case there exists some criteria for which objects are rejected from joining an existing cluster and instead are set out to form their own cluster. For example, suppose we had two distinct well separated dense regions of points. Beginning with a single point in the first region, we form a cluster and search through the remaining points one by one adding them to the cluster in they satisfy some specified criterion of nearness to the points already in the cluster. Once all the points in the first region are combined into a single cluster, the purpose of the criterion is to reject points from the second region from joining the first cluster, causing them to create a new cluster.

The conception of density search algorithms dates to the late `60s with the _taxmap_ method of Carmichael _et al_. in [@carmichael;@carmichaelsneath] and the _mode analysis_ method of Wishart [@wishart]. In _taxmap_ the authors suggested criterion like the drop in average similarity upon adding a new point to a cluster. In _mode analysis_ the criterion was simply containment in a specified radius of points in a cluster. The problem with this approach was that it had trouble finding both large and small clusters simultaneously [@everitt].

All density search algorithms suffer from the inability to find clusters of varying density, no matter how the term is defined in application, because the density of points is used to define the notion of a cluster. High dimensional data adds to this problem as demonstrated in Chapter \@ref(dimred) because as the size of the space grows, the points naturally become less and less dense inside of it. Another problem with density search algorithm is the necessity to search through data again and again, making their implementation difficult if not irrelevant for large data sets. Among the benefits to these methods are the inherent estimation of the number of clusters and their ability to find irregularly shaped (non-convex) clusters. Several algorithms in this category, like Density Based Spacial Clustering of Applications with Noise (DBSCAN) also make an effort to determine outliers or noise in the data.  Because of the computational workload of these methods, we will abandon them after the present discussion in favor of more efficient methods. For an in-depth analysis of other density search algorithms and their variants, see  [@density1].

### Density Based Spacial Clustering of Applications with Noise (DBSCAN)

Density Based Spacial Clustering of Applications with Noise (DBSCAN) is an algorithm proposed by Ester, Kriegel, Sander, and Xu in 1996 [@dbscan], which uses the Euclidean nearness of a group of points in $m$-space to define density. The algorithm uses the following definitions and parameters to determine what constitutes a cluster:

:::{.definition name='DBSCAN Terms' #dbscandefs}
The following definitions will aid our discussion of the DBSCAN algorithm:
<ul>
<li>   __Dense Point and__ $\rho_{min}$: 
    A point $\mathbf{x}_j$ is called _dense_ if there are at least $\rho_{min}$ other points contained in its $\epsilon$-neighborhood.
<li>    __Direct Density Reachability__:
	A point $\mathbf{x}_i$ is called _directly density reachable_ from a point $\mathbf{x}_j$ if it is in the $\epsilon$-neighborhood surrounding $\mathbf{x}_j$, i.e. if $\mathbf{x}_i \in \mathscr{N}(\mathbf{x}_j,\epsilon)$, _and_ $\mathbf{x}_j$ is a dense point.
<li>   __Density Reachability__:
	A point $\mathbf{x}_i$ is called _density reachable_ from a point $\mathbf{x}_j$ if there is a sequence of points $\mathbf{x}_{1},\mathbf{x}_{2},\dots, \mathbf{x}_{p}$ with $\mathbf{x}_{1}=\mathbf{x}_j$ and $\mathbf{x}_{p}=\mathbf{x}_i$ where each $\mathbf{x}_{{k+1}}$ is directly density reachable from $\mathbf{x}_{k}.$
<li> __Noise Point__:
	A point $\mathbf{x}_l$ is called a _noise point_ or _outlier_ if it contains 0 points in its $\epsilon$-neighborhood.
</ul>

:::

The relationship of density reachability is not symmetric. This fact is illustrated in Figure \@ref(fig:dbscan). A point in this illustration is dense if its $\epsilon$-neighborhood contains at least $\rho_{min} = 2$ other points. The green point $a$ is density reachable from the blue point $b$, however the reverse is not true because $a$ is not a dense point. Because of this, we introduce the notion of _density connectedness_.

```{r label='dbscan', fig.align='center', fig.cap = 'DBSCAN Illustration', echo=F, out.width="50%"}
knitr::include_graphics("figs/dbscan.jpg")
```


:::{definition name='Density Connectedness' #dbscandefs2}
  Two points $\mathbf{x}_i$ and $\mathbf{x}_j$ are __density-connected__ if there exists some point $\mathbf{x}_k$ such that both $\mathbf{x}_i$ and $\mathbf{x}_j$ are density reachable from $x_k$.
:::

In Figure \@ref(fig:dbscan), it is clear that we can say points $a$ and $b$ are density-connected since they are each density reachable from any of the 4 points in between them. The point $c$ in this illustration is a noise point or outlier because there are no points contained in its $\epsilon$-neighborhood.


Using these definitions, we can formalize the properties that define a cluster in DBSCAN. 

:::{.definition name='DBSCAN Cluster' #dbscancluster}
Given the parameters $\rho_{min}$ and $\epsilon$, a __DBSCAN cluster__ is a set of points that satisfy the two following conditions:
<ol>
<li> All points within the cluster are mutually density-connected.
<li> If a point is density-connected to any point in the cluster, it is part of the cluster as well.
</ol>\
:::

Table \@ref(tab:algdbscan) describes how DBSCAN finds such clusters.

<table>
<tr><td>
<ol>
<li> __Input:__ Set of points $\mathbf{X}=[\mathbf{x}_1,\mathbf{x}_2,\dots,\mathbf{x}_n]$ to be clustered and parameters $\epsilon$ and $\rho_{min}$\\
<li> For each unvisited point $p=\mathbf{x}_i$, do:
<ol style="list-style-type:upper-roman">
<li> Mark $p$ as visited.
<li> Let $\mathcal{N}$ be the set of points contained in the $\epsilon$-neighborhood around $p$.
<ol style="list-style-type:upper-alpha">
<li> If $|\mathcal{N}| < \rho_{min}$ mark $p$ as noise.
<li> Else let $C$ be the next cluster. Do:
<ol style="list-style-type:lower-alpha">
<li> Add $p$ to cluster $C$.
<li> For each point $p'$ in $\mathcal{N}$, do:
<ol style="list-style-type:lower-roman">
<li> If $p'$ is not visited, mark $p'$ as visited, let $\mathscr{N}'$ be the set of points contained in the  $\epsilon$-neighborhood around $p'$.  If $|\mathcal{N}'| \geq \rho_{min}$ let $\mathcal{N}=\mathcal{N} \cup \mathcal{N}'$

<li> If $p'$ is not yet a member of any cluster, add $p'$ to cluster $C$.
</ol>
</ol>
</ol>
</ol>
<li>  __Output:__ Clusters found $C_1,\dots,C_k$
</ol>
</table>

<caption> (\#tab:algdbscan) Density Based Spacial Clustering of Applications with Noise (DBSCAN) [@datamining] </caption>

 <br>
## Conclusion

The purpose of this chapter was to give the reader a basic understanding of hierarchical, iterative partitional, and density search approaches to data clustering. One of the main concerns addressed in this paper is that all of these algorithms have merit, but in application rarely do the algorithms completely agree on a solution. In fact, algorithms with random inputs like $k$-means are not even likely to agree with themselves over a number of different trials. It can be extremely difficult to qualitatively measure the goodness of your clustering when the data cannot be visualized in 2 or 3 dimensions. While there are a number of metrics to help the user get a sense of the compactness of the clusters (see Chapter \@ref(validation)), the effect of noise and outliers can often blur the true picture. It is also common for such metrics to take nearly equivalent values for vastly different cluster solutions, forcing the user to choose a solution using domain knowledge and utility.  First we will look at another class of clustering methods which aim to solve the graph partitioning problem described in Chapter \@ref(chap-zero).

The difference between the problems of data clustering and graph partitioning is merely the structure of the input objects to be clustered. In data clustering, the input objects are composed of measurements on $m$ variables or features. If we interpret the graph partitioning problem in such a way that input objects are vertices on a graph and the variables describing them are the weights of the edges by which they are connected to other vertices, then it becomes clear we can use any of the methods in this chapter to cluster the columns of an adjacency matrix as described in Chapter \@ref(chap-zero). Similarly if one creates a similarity matrix for objects from a data clustering problem, we can cluster that matrix using the theory and algorithms from graph partitioning. While each problem can be transformed into the other, the design of the algorithms for the two cases is generally quite different. In the next chapter, we provide a thorough overview of some popular graph clustering algorithms.

<!--chapter:end:111-ClusterAlgos.Rmd-->

# Cluster Validation {#validation}

The algorithms in Chapters \@ref(chap1) and \@ref(spectral), with the exception of DBSCAN, all suffer from the same drawback: they require the user to input the number of clusters for the algorithm to create. This is problematic because in practice it is unlikely that the user knows exactly how many clusters they are looking for. In fact, this may be precisely the information that the researcher is after. In the next chapter, we will discuss some popular approaches to determining a suitable value for $k$. Many of these approaches seek to choose the "best" clustering from a set of clusterings containing different numbers of clusters. In order to present these approaches, we must first look at ways one might quantitatively describe the "goodness" of a clustering. Such measures fall into the realm of _cluster validation_

In Chapter \@ref(chap-zero) it was made clear that the concept of a cluster (and hence the "optimal clustering" of a group of data) is a subjective one. Thus, it is impossible to truly quantify the quality or accuracy of a clustering, without first being given a set of categorical labels assumed to be the optimal clustering. Cluster validation metrics which use such labels (i.e. the "answers") are called _external_ metrics because they use additional information that was not contained in the data input to the clustering algorithm.

Clearly such labels are non-existent in practice, or we would not need to do clustering at all. Thus, it is necessary to establish some _internal_ metrics, which use only the information contained in the data, in order to get a sense for the quality or validity of a given clustering. In addition, _relative_ metrics are established with the aim of comparing two different clusterings. The goal of this chapter is to provide but a brief introduction to internal, external and relative metrics to fit our needs. For a more comprehensive survey of cluster validation see for example [@kaufman; @anderberg; @dcebook; @jainbook; @chap8; @everitt].

## Internal Validity Metrics

Most _internal_ metrics aim to describe the _cohesion_ of each cluster and the _separation_ between clusters. The cohesion of each cluster is some measure of its compactness, i.e how proximal or similar the objects in that cluster are to each other. The separation between clusters is some measure of the distance between them or how dissimilar the objects in different clusters are. Some metrics aim to quantify cohesion and separation separately, while others take both ideas into account in one measure. 
### General Cluster Cohesion and Separation: Graphs vs. Data
#### Cohesion

Generally, cluster cohesion measures the similarity or proximity of the points within a cluster. The definitions of cohesion for graph partitioning and data partitioning problems differ slightly depending on the similarity measure used. In graph partitioning, the goal is to measure how similar, or close, vertices in a cluster are to one another, whereas in data clustering cohesion is generally measured by the similarity of the points in a cluster to some _representative point_ (usually the mean or centroid) of that cluster [@chap8]. This difference is illustrated in Figure \@ref(fig:cohesion). The red lines represent the similarity/distance quantities of interest in either scenario, and the red point in Figure \@ref(fig:cohesionD) is a representative point which is not necessarily a data point. In our analysis, representative points will be defined as centroids and thus may be referred to as such.

```{r label='cohesion', fig.align='center', fig.cap = 'Cluster Cohesion in Data (left) compared to Graph-Based Cluster Cohesion (right)', echo=F, out.width="50%"}
knitr::include_graphics("figs/cohesion.png")
```

Depending on the proximity or similarity function used, the two quantities in Figure \@ref(fig:cohesion) may or may not be the same. Often times for graphs and networks, there is no simple way to define a centroid or representative point for a cluster. The particular representation for a cohesion metric will always be dependent on a choice of distance or similarity function. Thus, for graphs we merely define the general concept of cohesion as follows:

:::{.definition name='General Cluster Cohesion in Graphs' #graphcohesion}

For a graph $G(V,E)$ with edge weights $w_{ij}$, and a partition of the vertices into $k$ disjoint sets $C=\{C_1,C_2,\dots, C_k\}$, the __cohesion__ of cluster $C_p$  is 
$$\mbox{cohesion}(C_p) = \sum_{i,j \in C_p} w_{ij}.$$
:::

Given this definition, it should be clear that if $w_{ij}$ is a measure of similarity between vertices $i$ and $j$ then higher values of cohesion are desired, whereas if $w_{ij}$ measures distance or dissimilarity then lower values of cohesion are desired.

For data clustering problems, cluster cohesion is similarly defined, only now the similarities or proximities are measured between each point in the cluster and the cluster's representative point.

:::{.definition name='General Cluster Cohesion for Data' #datacohesion}
Let $\X=[\x_1,\x_2,\dots, \x_n]$ be an $m\times n$ matrix of column data, and let $C=\{C_1,C_2,\dots,C_k\}$ be a set of disjoint clusters partitioning the data with corresponding representative points $\{\cc_1,\cc_2,\dots,\cc_k\}$. Then the __cohesion__ of cluster $C_p$ is 
$$\mbox{cohesion}(C_p) = \sum_{\x_i \in C_p} d(\x_i,\cc_p)$$
Where $d$ is any distance or similarity function.
:::

Again, the given definitions are not associated with any particular distance or similarity function and thus define a broad classes of metrics for measuring cluster cohesion. 

#### Separation

The goal in clustering is not only to form groups of points which are similar or proximal, but also to assure some level of separation or dissimilarity between these groups. Thus, in addition to measuring cluster cohesion, it is also wise to consider cluster separation. Again this concept is a little different for graphs, where the separation is measured pairwise between points in different clusters, than it is for data, where separation is generally measured between the representative points of different clusters. This difference is presented with the following 2 definitions.

:::{.definition name='General Cluster Separation for Graphs' #graphseparation}

For a graph $G(V,E)$ with edge weights $w_{ij}$, and a partition of the vertices into $k$ disjoint sets $C=\{C_1,C_2,\dots, C_k\}$. The __separation__ between clusters $C_p$ and $C_q$ is 
$$\mbox{separation}(C_p,C_q) = \sum_{\substack{i \in C_p \\ j \in C_q}} w_{i,j}.$$
:::

:::{.definition name='General Cluster Separation for Data' #dataseparation}
Let $\X=[\x_1,\x_2,\dots, \x_n]$ be an $m\times n$ matrix of column data, and let $C=\{C_1,C_2,\dots,C_k\}$ be a set of disjoint clusters in the data with corresponding representative points $\{\cc_1,\cc_2,\dots,\cc_k\}$. Then the __separation__ between clusters $C_p$ and $C_q$ is
$$\mbox{separation}(C_p,C_q) = d(\cc_p,\cc_q)$$
where $d$ is any distance or similarity function.
:::

#### Averaging Measures of Cohesion and Separation for a Set of Clusters
Definitions \@ref(def:graphcohesion), \@ref(def:datacohesion), \@ref(def:graphseparation) and \@ref(def:dataseparation) provide simple, well-defined metrics (given a proximity or similarity measure) for individual clusters $C_p$ or pairs of clusters $(C_p,C_q)$ that can be combined into overall measures for a clustering $C = \{C_1,C_2,\dots,C_k\}$ by some weighted average [@chap8]. The weights for such an average vary according to applications and user-preference, but they typically reflect the size of the clusters in some way. At the end of this chapter, in Table \@ref(tab:cstable), we provide a few examples of these overall metrics. 

### Common Measures of Cohesion and Separation
As stated earlier, the previous definitions were considered "general" in that they did not specify particular functions of similarity or distance. Here we discuss some specific measures which have become established as foundations of cluster validation in the literature.

#### Sum of Squared Error (SSE)
The _sum of squared error (SSE)_ metric incorporates the squared euclidean distances from each point in a given cluster to the __centroid__ of the cluster, defined as
 $$\mean_j = \frac{1}{n_j} \sum_{\x_i \in C_j} \x_i.$$
 This is equivalent to measuring the average pairwise distance between points in a cluster, as one would do in a graph having Euclidean distance as a measure of proximity.  The SSE of a single cluster is then

\begin{eqnarray*}
\text{SSE}(C_j)&=&\sum_{\x_i \in  C_j} \| \x_i - \mean_j \|_2^2 \\
    &=&\frac{1}{2n_j}\sum_{\x_i,\x_l \in C_j} \|\x_i - \x_l\|_2^2 
\label{SSE}
\end{eqnarray*}
where $n_j = |C_j|$ . The SSE of an entire clustering $C$ is simply the sum of the SSE for each cluster $C_j \in C$
$$\mbox{SSE}(C)=\sum_{j=1}^k \sum_{\x_i \in C_j} \|\x_i - \mean_j\|_2^2.$$

Smaller values of SSE indicate more cohesive or compact clusters. One may recognize Equation \@ref(eq:SSE) as the objective function from Section \@ref(kmeans) because minimizing the SSE is the goal of the Euclidean \kmeans algorithm. We can use the same idea to measure cluster separation by computing the _Between Group Sums of Squares_ (SSB), which is a weighted average of the squared distances from the cluster centroids $\{\mean_1, \mean_2,\dots,\mean_k\}$ to the over all centroid of the dataset $\mean_* = \frac{1}{n} \sum_{i=1}^n \x_i$: 
$$
\mbox{SSB}(C) =\sum_{j=1}^k n_j\|\mean_j-\mean_*\|_2^2.
$$


It is straightforward to show that the _total sum of squares_ (TSS) of the data
$$\mbox{TSS}(\X)=\sum_{i=1}^n \|\x_i-\mean_*\|_2^2,$$
which is constant, is equal to the sum of the SSE and SSB for every clustering $C$, i.e.
$$\mbox{TSS}(\X)=\mbox{SSE}(C) + \mbox{SSB}(C),$$
thus minimizing the SSE (attaining more cohesion) is equivalent to maximizing the SSB (attaining more separation).

 Sum of Squared Error is used as a tool in the calculation of the _gap statistic_, outlined in the next chapter, a popular parameter used to determine the number of clusters in data.  

#### Ray and Turi's Validity Measure {#rayturi}
In [@rayturi] a measure of cluster validity is chosen as the ratio of intracluster distance to intercluster distance. The authors define these distances as 
$$M_{intra} = \frac{1}{n}\mbox{SSE}(C) =  \frac{1}{n}\sum_{j=1}^k \sum_{\x_i \in C_j} \|\x_i - \mean_j\|^2.$$
and
$$M_{inter} = \min_{1\leq i \leq  j \leq k} \|\mean_i - \mean_j\|^2.$$
Clearly a good clustering should have small $M_{intra}$ and large $M_{inter}$.  Ray and Turi's validity measure,
$$V=\frac{M_{intra}}{M_{inter}}$$
is expected to take on smaller values for a better clustering [@dcebook]. 

#### Silhouette Coefficients
Silhouette coefficients are popular indices that combine the concepts of cohesion and separation [@datamining]. These indices are defined for each object or observation $\x_i,\, i=1,\dots, n$ in the data set using two parameters $a_i$ and $b_i$, measuring cohesion and separation respectively. These parameters and the silhouette coefficient for an object $\x_i$ are computed as follows:
<ul>
<li> Suppose, for a given clustering $C=\{C_1,\dots, \C_k\}$ with $|C_j|=n_j$, that the point $\x_i$ belongs to cluster $C_p$ 
<li> Then $a_i$ is the average distance (or similarity) of point $\x_i$ from the other points in $C_p$,
$$ a_i = \frac{1}{n_p} \sum_{\x_j \in C_p} d(\x_i,\x_j)$$
<li> Define the distance (or similarity) between $\x_i$ and the remaining clusters $C_q, 1\leq\,q\leq\,k, q\neq p$ to be the average distance (or similarity) between $\x_i$ and the points in each cluster,
$$d(\x_i,C_q) = \frac{1}{n_q} \sum_{\x_j \in C_q} d(\x_i,\x_j).$$
Then $b_i$ is defined to be the minimum of these distances (or maximum for similarity):
$$b_i = \min_{q \neq p} d(\x_i,C_q).$$
<li> The silhouette coefficient for $\x_i$ is then
$$s_i = \frac{(b_i-a_i)}{\max(a_i,b_i)} \mbox{  (for distance metrics)}$$
$$s_i = \frac{(a_i-b_i)}{\max(a_i,b_i)} \mbox{  (for similarity metrics)}$$

</ul>

The silhouette coefficient takes on values $-1 \leq s_i \leq 1$, where negative values undesirably indicate that $\x_i$ is closer (or more similar) on the average to points in another cluster than to points in its own cluster, and values close to $1$ indicate a good clustering.\\

Silhouette coefficients are commonly averaged for all points in a cluster to get an overall sense for the validity of that cluster.

## External Validity Metrics {#external}
Many of the results presented in Chapter \@ref(results) will use data sets for which the class labels of each object are known. Using this information, one can generally create validity metrics that are easier to understand and compare across clusterings. Such metrics are known as _external metrics_ because of their dependence on the external class labels. We will show that most external metrics can be transformed into _relative metrics_ which compute the similarity between two clusterings.  

Using the information from external class labels, one can create a so-called __confusion matrix__  (also called a matching matrix).  The confusion matrix is simply a table that shows correspondence between predicted cluster labels (determined by an algorithm) and the actual or "true" cluster labels of the data. A simple example is given in Figure \@ref(fig:confusion), where the actual class labels (`science', `math', and `french') are shown across the columns of the matrix and the clusters determined by an algorithm ($C_1, C_2,$ and $C_3$) are shown along the rows.  The $(i,j)^{th}$ entry in the confusion matrix is then the number of objects from the dataset that had class label $j$ and were assigned to cluster $i$. 

```{r label='confusion', fig.align='center', fig.cap = 'Example of a Confusion Matrix', echo=F, out.width="50%"}
knitr::include_graphics("figs/confusion.jpg")
```

For this simple example, one may assume that cluster 1 ($C_1$) corresponds to the class `Science', cluster 2 corresponds to the class `Math', and likewise that cluster 3 represents the class `French', even though the clustering algorithm did not split these classes apart perfectly. Most external metrics will rely on the values in the confusion matrix.

### Accuracy
Accuracy is a measure between 0 and 1 that simply measures the proportion of objects that were labelled correctly by an algorithm. This is not always a straightforward task, given that the labels assigned by a clustering algorithm are done so arbitrarily in that it does not matter if one refers to the same group of points as "cluster 1" or  "cluster 2". In the confusion matrix in Figure \@ref(fig:confusion), it is easy to identify which cluster labels corresponds to which class. In this case it is easy to see that out of a total of 153 objects, only 13 were classified incorrectly, leading to an accuracy of 140/153 $\approx$ 91.5\%. However with a more _confusing_ confusion matrix, like that shown in Figure \@ref(fig:conconfusion), the answer is not quite as clear and thus it is left to determine exactly how to match predicted cluster labels with assigned class labels in an appropriate way.

```{r label='conconfusion', fig.align='center', fig.cap = 'A More _Confusing_ Confusion Matrix', echo=F, out.width="50%"}
knitr::include_graphics("figs/conconfusion.jpg")
```


This turns out to be a well studied matching problem from graph theory, known as a _maximum matching_ for a bipartite graph. If we transform our confusion matrix from Figure \@ref(fig:conconfusion) into an undirected bipartite graph with edge weights corresponding to edges in the confusion matrix, the result would be the graph in Figure \@ref(fig:bipartite). The task is then to find a set of 3 edges, each beginning at distinct vertices on the left and ending at distinct vertices on the right such that the sum of the edge weights is maximal. The solution to this problem is shown in Figure \@ref(fig:maximummatching) and it is clear that the matching of predicted labels to actual labels did not actually change from the simpler version of this confusion matrix in Figure \@ref(fig:confusion), it just became less obvious because of the errors made by the algorithm.

```{r label='maximummatching', fig.align='center', fig.cap = 'Bipartite Graph of Confusion Matrix (left) and Matching Predicted Class Labels to Actual Class Labels (right) ', echo=F, out.width="100%"}
knitr::include_graphics("figs/maximummatching.png")
```


Once the predicted class labels are matched to the actual labels, the accuracy of a clustering is straightforward to compute by
$$\mbox{Accuracy}(C)=\frac{\mbox{# of objects labelled correctly}}{n}.$$
The accuracy of the second clustering given in Figure \@ref(fig:conconfusion) is 118/153 $\approx$ 77\%, which is sharply lower than the 91.5\% achieved by the clustering in Figure \@ref(fig:confusion). The nice thing about accuracy as a metric is it provides a contextual interpretation and thus allows us to quantify an answer to the question "how _much_ better is this clustering?" This is not necessarily true of other external metrics, as you will see in the next sections. 

 The aspect of this metric that requires some computation is the determination of the maximum matching as shown in Figure \@ref(fig:bipartitematching}. Fortunately, this problem is one that was solved by graph theorist H.W. Kuhn in 1955 [@kuhn]. Kuhn's algorithm was adapted by James Munkres in 1957 and the resulting method was dubbed the Kuhn-Munkres Algorithm, or sometimes the Hungarian Algorithm in honor of the mathematicians who pioneered the work upon which Kuhn's method was based [@munkres]. This algorithm is fast and computationally inexpensive. The details of the process are not pertinent to the present discussion, but can be found in any handbook of graph theory algorithms.

#### Comparing Two Clusterings: Agreement 
 
 The accuracy metric, along with other external metrics, can be used to compute the similarity between two different cluster solutions. Since, in practice, class labels are not available for the data, the user may run two different clustering algorithms (or even the same algorithm with different representations of the data as input or different initializations) and get two different clusterings as a result. The natural question is then "how similar are these two clusterings?"  Treating one clustering as class labels and computing the accuracy of the second compared to the first will provide the percentage of data points for which the two clusterings _agree_ on cluster assignment. Thus, when comparing two clusterings, the accuracy metric becomes a measure of __agreement__ between the two clusterings. As such, value of 90\% agreement indicates that 90\% of the data points were clustered the same way in both clusterings.

### Entropy
The notion of entropy is associated with randomness. As a clustering metric, entropy measures the degree to which the predicted clusters consist of objects belonging to a single class, as opposed to many classes. Suppose a cluster (as predicted by an algorithm) contains objects belonging to multiple classes (as given by the class labels). Define the quantities

<ul>
<li> $n_i = $ number of objects in cluster $C_i$
<li> $n_{ij} = $ number of objects in cluster $C_i$ having class label $j$
<li> $p_{ij} = \frac{n_{ij}}{n_i} = $ probability that a member of cluster $C_i$ belongs to class $j$
</ul>

Then the __entropy__ of each cluster $C_i$ is
$$\mbox{entropy}(C_i) = -\sum_{j=1}^L p_{ij} \log_2 p_{ij}$$
where $L$ is the number of classes, and the total entropy for a set of clusters, $C$, is the sum of the entropies for each cluster weighted by the proportion of points in that cluster:
$$\mbox{entropy}(C)=\sum_{i=1}^k \frac{n_i}{n} \mbox{entropy}(C_i).$$

Smaller values of entropy indicate a less random distribution of class labels within clusters [@datamining]. One benefit of using entropy rather than accuracy is that it can be calculated for any number of clusters $k$, whereas accuracy is restricted to the case where $k=L$. \

 __Sample Calculations for Entropy__\
Comparing the two clusterings represented by the confusion matrices in Figures \@ref(fig:confusion) and \@ref(fig:conconfusion), we'd see that for the first example,

\begin{eqnarray*}
p_{11}=\frac{45}{50} & p_{12}=\frac{5}{50} & p_{13}=0 \\
p_{21}=\frac{8}{48}  & p_{22}=\frac{40}{48} & p_{23}=0 \\
p_{31}=0 & p_{32}=0 & p_{33}=1 
\end{eqnarray*}

so that
\begin{eqnarray*}
\mbox{entropy}(C_1) &=& - ( \frac{45}{50} \log_2 \frac{45}{50}  + \frac{5}{50} \log_2 \frac{5}{50}) = 0.469\\
\mbox{entropy}(C_2) &=& - (\frac{8}{48} \log_2 \frac{8}{48}  + \frac{40}{48} \log_2 \frac{40}{48}) = 0.65\\
\mbox{entropy}(C_3) &=& - (\log_2 1) = 0
\end{eqnarray*}
and thus the total entropy of the first clustering is
$$\mbox{entropy}(C) = \frac{50}{153} (0.469) + \frac{48}{153}(0.65) = \framebox{0.357}.$$

And for the second example, we have 
\begin{eqnarray*}
p_{11}=\frac{25}{30} & p_{12}=\frac{5}{30} & p_{13}=0 \\
p_{21}=\frac{30}{68}  & p_{22}=\frac{38}{68} & p_{23}=0 \\
p_{31}=0 & p_{32}=0 & p_{33}=1
\end{eqnarray*}
yielding
\begin{eqnarray*}
\mbox{entropy}(C_1) &=& - ( \frac{25}{30} \log_2 \frac{25}{30}  + \frac{5}{30}  \log_2 \frac{5}{30} ) = 0.65 \\
\mbox{entropy}(C_2) &=& - (\frac{30}{68} \log_2 \frac{30}{68}  + \frac{38}{68} \log_2 \frac{38}{68}) = 0.99 \\
\mbox{entropy}(C_3) &=& - (\log_2 1) = 0
\end{eqnarray*}
and finally the total entropy of the second clustering is
$$\mbox{entropy}(C) = \frac{30}{153} (0.469) + \frac{68}{153}(0.65) = \framebox{0.568}$$

revealing a higher-overall entropy and thus a worse partition of the data compared to the first clustering.




### Purity

Purity is a simple measure of the extent to which a predicted cluster contains objects of a single class [@datamining]. Using the quantities defined in the previous section, the __purity__ of a cluster is defined as
$$\mbox{purity}(C_i) = \max_j p_{ij}$$
and the purity of a clustering $C$ is the weighted average
$$\mbox{purity}(C) = \sum_{i=1}^k \frac{n_i}{n} \mbox{purity}(C_i).$$
The purity metric takes on positive values less than 1, where values of 1 reflect the desirable situation where each cluster only contains objects from a single class. Like entropy, purity can be computed for any number of clusters, $k$. Purity and accuracy are often confused and used interchangeably but they are _not_ the same. Purity takes no matching of class labels to cluster labels into account, and thus it is possible for the purity of two clusters to count the proportion of objects having the _same_ class label. For example, suppose we had only two class labels given, A and B, for a set of 10 objects and set our clustering algorithm to seek 2 clusters in the data and the following confusion matrix resulted:


<center>
$$\begin{array}{c | c c}
  &A & B \\
\hline
C_1 & 3 & 2\\
C_2 & 3 & 2
\end{array}$$
</center>

Then the purity of each cluster would be $\frac{3}{5}$ referring in both cases to the proportion of objects having class label A. High values of purity are easy to achieve when the number of clusters is large. For example, by assigning each object to its own cluster we'd achieve perfect purity. One metric that accounts for such a tradeoff is _Normalized Mutual Information_, presented next. \

__Sample Purity Calculations__\
Again, we'll compare the two clusterings represented by the confusion matrices in Figures \@ref(fig:confusion) and \@ref(fig:conconfusion). For the first clustering, 
\begin{eqnarray*}
\mbox{purity}(C_1) &=& \max(\frac{45}{50} ,\frac{5}{50} ,0) = \frac{45}{50}= 0.9 \\
\mbox{purity}(C_2) &=& \max(\frac{8}{48},\frac{40}{48},0) = \frac{40}{48} = 0.83 \\
\mbox{purity}(C_2) &=& \max(0,0,1) = 1
\end{eqnarray*}
so the overall purity is
$$\mbox{purity}(C) =  \frac{50}{153} (0.9) + \frac{48}{153}(0.83) + \frac{55}{153}(1) = \framebox{0.914}.$$

Similarly for the second clustering we have,
\begin{eqnarray*}
\mbox{purity}(C_1) &=& \max(\frac{25}{30},\frac{5}{30},0) = \frac{25}{30}) = 0.83 \\
\mbox{purity}(C_2) &=& \max(\frac{30}{68},\frac{38}{68} ,0) = \frac{38}{68}) = 0.56 \\
\mbox{purity}(C_2) &=& \max(0,0,1) = 1
\end{eqnarray*}

And thus the overall purity is
$$\mbox{purity}(C) =  \frac{30}{153} (0.83) + \frac{68}{153}(0.56) + \frac{55}{153}(1) = \framebox{0.771}.$$

### Mutual Information (MI) and <br> Normalized Mutual Information (NMI)
Mutual Information (MI) is a measure that has been used in various data applications [@datamining]. The objective of this metric is to measure the amount information about the class labels revealed by a clustering. Adopting the previous notation,
<ul>
<li> $n_i = $ number of objects in cluster $C_i$
<li> $n_{ij} = $ number of objects in cluster $C_i$ having class label $j$
<li> $p_{ij} = n_{ij}/n_i = $ probability that a member of cluster $C_i$ belongs to class $j$
</ul>
also let

<ul>
<li> $l_j =$ the number of objects having class label $j$
<li> $L =$ the number of classes
<li> $\mathcal{L} =\{\mathcal{L}_1,\dots,\mathcal{L}_L\}$ the "proper" clustering according to class labels
</ul>
and, as always, let
<ul>
<li> $n=$ the number of objects in the data
<li> $k=$ the number of clusters in the clustering.
</ul>
 The __Mutual Information__ of a clustering $C$ is then
$$\mbox{MI}(C)=\sum_{i=1}^k \sum_{j=1}^L p_{ij} \log \frac{n n_{ij}}{n_i l_j}$$
and the __Normalized Mutual Information__ of $C$ is
$$\mbox{NMI}(C) = \frac{\mbox{MI}(C)}{[\mbox{entropy}(C) + \mbox{entropy}(\mathcal{L})]/2}$$
Clearly, when $\mathcal{L}$ corresponds the class labels we have $\mbox{entropy}(\mathcal{L})=0$ but if user's objective is instead to compare two different clusterings, this piece of the equation is necessary. Thus, using the same treatment used for _agreement_ between two clusterings, one can compute the mutual information between two clusterings.

### Other External Measures of Validity

There are a number of other measures that can either be used to validate a clustering in the presence of class labels or to compare the similarity between two clusterings $C=\{C_1,C_2,\dots,C_k\}$ and $\hat{C}=\{\hat{C}_1,\hat{C}_2,\dots,\hat{C_k}\}$. In our presentation we will consider the second clustering to correspond to the class labels, but in the same way that the accuracy metric can be used to compute agreement, these measures are often used to compare different clusterings. To begin we define the following parameters [@dcebook]:
<ul>
<li> $a$ is the number of pairs of data points which are in the same cluster in $C$ and have the same class labels (i.e. are in the same cluster in $\hat{C}$).
<li> $b$ is the number of pairs of data points which are in the same cluster in $C$ and have different class labels.
<li> $c$ is the number of pairs of data points which are in different clusters in $C$ and have the same class labels.
<li> $d$ is the number of pairs of data points which are in different clusters in $C$ and have different class labels.
</ul>
These four parameters add up to the total number of pairs of points in the data set, $N$,
$$a+b+c+d = N = \frac{n(n-1)}{2}.$$

From these values we can compute a number of different similarity coefficients, a few of which are provided in Table \@ref(tab:comparisonmeasures) [@dcebook].

<table>
<tr><td>Name <td> Formula </tr>
<tr><td>Jaccard Coefficient <td> $\displaystyle J = \frac{a}{a+b+c}$ </tr>
<tr><td>Rand Statistic <td> $\displaystyle R = \frac{a+b}{N}$</tr>
<tr><td>Folkes and Mallows Index <td> $\displaystyle \sqrt{\frac{a}{a+b} \frac{a}{a+c}}$</tr>
<tr><td>Odds Ratio  <td> $\displaystyle \frac{ad}{bc}$</tr>
</table>
 <caption>(\#tab:comparisonmeasures) Some Common Similarity Coefficients </caption>
<br>

#### Hubert's $\Gamma$ Statistic

Another measure popular in the clustering literature is Hubert's $\Gamma$ statistic, which aims to measure the correlation between two clusterings, or between one clustering and the class label solution [@datamining;@dcebook]. Here we define an $n\times n$ __adjacency matrix__ for a clustering $C$, denoted $\bo{Y}$ such that

\begin{equation}
\label{clusteradjacency}
\bo{Y}_{ij} = 
\begin{cases}
1 & \text{object } i \text{ and object } j \text{ are in the same cluster in } C \\
0 & \text{otherwise}
\end{cases}
\end{equation}

Similarly, let $\bo{H}$ be an adjacency matrix pertaining to the class label partition (or a different clustering) as follows:
\begin{equation}
\bo{H}_{ij} = 
\begin{cases}
1 & \mbox{object } i \mbox{ and object } j \mbox{ have the same class label }  \\
0 & \mbox{otherwise}
\end{cases}
\end{equation}

Then __Hubert's __$\Gamma$ __statistic__,  defined as
$$\Gamma=\frac{1}{N} \sum_{i=1}^{n-1} \sum_{i+1}^n \bo{Y}_{ij} H_{ij},$$
is a way of measuring the correlation between the clustering and the class label partition [@dcebook].

### Summary Table

<table>
<tr>
<td>
Name <td> Overall Measure <td> Cluster Weight <td> Type </tr>

<td>Overall Cohesion (Graphs) <td> $\displaystyle \sum_{p=1}^k \alpha_p \sum_{i,j \in C_p} w_{ij} $  <td> $\displaystyle \alpha_p= \frac{1}{n_i}$<td> Graph Cohesion </tr>

<td> $\mathcal{G}_{CS}$ (Graph C-S Measure) <td> $\displaystyle\sum_{p=1}^k \alpha_p \sum_{\substack{q=1 \\ q\neq p}}^k \sum_{\substack{i \in C_p \\ j \in C_q}} w_{ij}$ <td> $\displaystyle\alpha_p = \frac{1}{\sum_{i,j \in C_p} w_{ij}}$ <td> Graph Cohesion \& Separation </tr>

<td>Sum Squared Error (SSE) (Data) <td> $\displaystyle \sum_{p=1}^k \alpha_p  \sum_{\x_i\in C_p} \|\x_i - \mean_p\|^2$ <td> $\displaystyle\alpha_p = 1$ <td> Data Cohesion </tr>

<td>Ray and Turi's $M_{intra}$ <td>  $\displaystyle \sum_{p=1}^k \alpha_p  \sum_{\x_i\in C_p} \|\x_i - \mean_p\|^2$ <td> $\displaystyle\alpha_p = \frac{1}{n}$ <td> Data Cohesion </tr>

<td>Ray and Turi's $M_{inter}$ <td> $\displaystyle\min_{1\leq i \leq  j \leq k} \|\mean_i - \mean_j\|^2 $<td> N/A <td> Data Separation </tr>

<td>Ray and Turi's Validity Measure <td> $\displaystyle\frac{M_{intra}}{M_{inter}}$ <td> N/A <td> Data Cohesion \& Separation </tr>


</table>
<caption> (\#tab:cstable) Some Common Measures of Overall Cohesion and Separation [@dcebook,@datamining]</caption> 


<!--chapter:end:1113-validation.Rmd-->

# Determining the Number of Clusters $k$ {#findk}

As previously discussed, one of the major dilemmas faced when using the clustering algorithms from Chapters \@ref(chap1) and \@ref(spectral) is that these algorithms take the number of clusters, $k$, as input. Therefore, it is necessary to somehow determine a reasonable estimate for the number of clusters present. Many methods have been proposed for this task, for a more in-depth summary we suggest the 1999 book by Gordon [@gordon] or the 1985 paper by Milligan and Cooper [@milligan]. The purpose of this chapter is to survey some established methodologies for this task, and to motivate our novel method discussed in Chapter \@ref(consensus).  

## Methods based on Cluster Validity (Stopping Rules)

The most popular methods for determining the number of clusters involve observing some internal measure of cluster validity (like those outlined in the previous chapter) as the number, $k$, of clusters increases. Cohesion scores like SSE are expected to be monotonically decreasing as $k$ increases. At some value, $k^*$, the marginal drop in SSE is expected to flatten drastically, indicating that further division of the clusters does not provide a significant improvement in terms of cohesion [@milligan, @gapstat, @jainbook]. Methods based on cluster validity can be implemented with any clustering algorithm the user desires. Unfortunately, if the algorithm used is not working well with the dataset then the resulting determination of the number of clusters will be flawed. Furthermore, it is possible to get different results with different algorithms or different cohesion metrics, which may instil the user with little confidence in a given solution.

In the hierarchical algorithms from Section \@ref(hc), a series of solutions ranging from $k=1$ to $k=n$ clusters are output, and thus the methods for determining an appropriate value for $k$ in these procedures are often referred to as stopping rules. Since hierarchical algorithms tend to be slow and computationally expensive for large datasets, the stopping rules which cannot be extended to include general partitions of the data will be omitted from the discussion.

## Sum Squared Error (SSE) Cohesion Plots

For a simple example for this stopping rule methodology, consider the so-called _Ruspini dataset_ in Figure \@ref(fig:ruspini), which has been used to demonstrate clustering algorithms in the literature. This dataset consists of 75 two dimensional points in the first Cartesian quadrant, and visually it seems clear that these points fall into $k=4$ different clusters, using Euclidean distance as a measure of proximity. (Some individuals may argue that these 4 clusters could be meaningfully broken down into smaller clusters. These arguments are certainly valid, but we base our decision to specify 4 on the following assumptions: a) If asked to choose 4 clusters, most human beings would choose the same 4 - this may not be the case with 5 or 6; b) If we consider these points as a sample from a population, then it is reasonable to suspect that the collection of more data may destroy the subcluster appearance - that is, there is more observed evidence of 4 clusters than any other number.)  We ought to be able to uncover this "true" number of clusters by observing the level of decrease in the SSE metric as the number of clusters increase, and determining an "elbow" in the curve at $k^*=4$ where the SSE flattens out for $k\geq 4$.

```{r label='ruspini', fig.align='center', fig.cap = 'The Two-Dimensional Ruspini Dataset', echo=F, out.width="50%"}
knitr::include_graphics("figs/RuspiniScatter.jpg")
```


Figure \@ref(fig:ruspiniSSEplotgood) shows some examples of clusters found in the data using $k$-means and $k=2, 3, 4, 5, 6$ clusters. The initialization of seed points was done randomly in each case. Figure \@ref(fig:ruspiniSSEplotgood) shows the SSE (as described in Section \@ref(SSE) for the 6 different clusterings. We wish to point out that these 5 clusterings are "good" or reasonable clusterings upon visual inspection. Indeed, this first SSE plot properly depicts $k^*=4$ as the "elbow" of the curve, where the marginal decrease in SSE for adding additional clusters flattens out.\

(ref:rusSSEgoodcap) 5 "Good" $k$-means Clusterings of the Ruspini Dataset and the Corresponding SSE Plot

```{r label='ruspiniSSEplotgood',fig.show="hold", fig.align='center', fig.cap = "(ref:rusSSEgoodcap)", echo=F, out.width="50%"}
knitr::include_graphics("figs/rusk2.jpg")
knitr::include_graphics("figs/rusk3.jpg")
knitr::include_graphics("figs/rusk4.jpg")
knitr::include_graphics("figs/rusk5.jpg")
knitr::include_graphics("figs/rusk6.jpg")

```



__User Beware__ <br>

As always, with $k$-means, it is of the utmost importance that the user pay close attention to the output from the algorithm. In our creation of the SSE plot in Figure \@ref(fig:ruspiniSSEplotgood), we came by the two solutions, associated with $k=4$ and $k=5$ clusters respectively, that are shown in Figure \@ref(fig:rusSSEbad). Because we are able to visualize the data in 2 dimensions (which, practically speaking, means we could have identified $k^*=4$ by visualizing the original data anyhow), we were able to throw away these two solutions upon inspection. If we did not do this, the resulting SSE plot shown in Figure \@ref(fig:rusSSEbad) would have clearly misled us to choose $k^*=3$ clusters. Without being able to visually inspect the solutions, it is wise to run several iterations of the $k$-means algorithm for each $k$ and use some criteria (like lowest SSE, or most frequent SSE [@poweriteration]) to choose an appropriate clustering for inclusion in the SSE plot. While this is not guaranteed to circumvent problematic SSE plots like that shown in Figure \@ref(fig:rusSSEbad), it can help in many situations and certainly won't hurt in others.  This dependence on good clusterings is a glaring drawback of stopping rule methodology, because not all algorithms can produce multiple results for a single value of $k$ to choose from.

```{r label='rusSSEbad',fig.show="hold", fig.align='center', fig.cap = "Example of 'Poor' Clusterings and their Effect on SSE Plot", echo=F, out.width="50%"}
knitr::include_graphics("figs/rusk4bad.jpg")
knitr::include_graphics("figs/rusk5bad.jpg")
knitr::include_graphics("figs/rusSSEbad.jpg")

```

### Cosine-Cohesion Plots for Text Data

Further complicating the method of cohesion plots is the curse of dimensionality discussed in Chapter \@ref(dimred).  For high dimensional data, it is unusual to witness such drastic "elbows" in these plots. To illustrate this effect, we consider a combination of 3 text datasets used frequently in the information retrieval literature: 'Medlars', 'Cranfield', 'CISI' [@surveytextmining,@kogan].  The Medlars-Cranfield-CISI (MCC) collection consists of nearly 4,000 scientific abstracts from 3 different disciplines. These 3 disciplines (Medlars = medicine, Cranfield = aerodynamics, CISI = information science) form 3 relatively distinct clusters in the data, which are not particularly difficult to uncover (For example, spherical $k$-means frequently achieves 98\% accuracy on the full-dimensional data). 

For this experiment, we ran 25 trials of the spherical $k$-means algorithm for each value of $k=2,3,\dots,20$ and from each set of trials chose the solution with the lowest objective value. The resulting SSE plot is shown in Figure \@ref(fig:MCCSSE). It is difficult to identify a distinct "elbow" in this curve.

(ref:MCCSSElab) Spherical $k$-means Objective Function Values for $2\leq k \leq 20$

```{r label='MCCSSE', fig.align='center', fig.cap = '(ref:MCCSSElab)', echo=F, out.width="50%"}
knitr::include_graphics("figs/MCCSSEPlotCoskmeans25Iter.jpg")
```


Because of the behavior of distance metrics in high dimensional space, it is often easier (and always faster) to find clusters after reducing the dimensions of a dataset by one of the methods discussed in Chapter \@ref(dimred). Because the singular value decomposition generally works well for text data, we conduct this same experiment on the Medlars-Cranfield-CISI dataset using projections onto the first $r=8,12, \mbox{ and } 20$ singular vectors. Using the correct number of clusters $k^*=3$, the $k$-means algorithm is able to achieve the same accuracy of 98\% on each of these dimension reductions, indicating that the clustering information is by no means lost in the lower dimensional representations.  However, the SSE plots for these lower dimensional representations, shown in Figure \@ref(fig:MCCsvdSSE), do no better at clearly indicating an appropriate number of clusters. In fact, these graphs seem to flatten out at $k=r$.  Again, 25 trials of the $k$-means algorithm were run for each value of $k$ and the solution with the lowest SSE was chosen to represent that value of $k$ in the plots. 

(ref:sseplotlab) SSE Plots for Medlars-Cranfield-CISI Clusterings using SVD Reduction to $r=\{8,12,20\}$ dimensions

```{r label='MCCsvdSSE',fig.show="hold", fig.align='center', fig.cap = '(ref:sseplotlab)', echo=F, out.width="50%"}
knitr::include_graphics( "figs/MCCsvd8SSEPlotCoskmeans25Iter.jpg")
knitr::include_graphics("figs/MCCsvd12SSEPlotCoskmeans25Iter.jpg")
knitr::include_graphics("figs/MCCsvd20SSEPlotCoskmeans25Iter.jpg")


```

### Ray and Turi's Method

In [@rayturi], Ray and Turi suggested the use of their validity metric for determining the number of clusters. Unlike the SSE plots investigated previously, this method does not rely on the subjectivity of the user. Instead, the goal is simply to find the minimum value of their validity metric over the clusterings produced for various values of $k$. Recalling the definition from Chapter \@ref(validation) Section \@ref(rayturi), we have the validity of a clustering defined as
\begin{equation}
v=\frac{M_{intra}}{M_{inter}}
(\#rayturivalidity)
\end{equation}
where
\begin{eqnarray}
M_{intra} &=& \frac{1}{n} \sum_{i=1}^{k} \sum_{\x in C_i} \| \x - \mean_i\|_2^2 \\
M_{inter} &=&  \min_{1\leq i <j \leq k} \|\mean_i - \mean_j\|^2
\end{eqnarray}
and
$\mean_i$ is the centroid of cluster $i$. In their original work in [@rayturi], the authors' goal was to cluster images. They noticed for these datasets that the minimum value for the validity metric frequently occurred for small numbers of clusters in the range of 2, 3, or 4 because of the large inter-cluster distances occurring when the number of clusters is small. This was undesirable in their application to image processing because the number of clusters was not expected to be small. To account for this fact, they proposed the following procedural adjustment for determining the number of clusters: 

<ol>
<li> Specify the maximum number of clusters to be considered, $k_{max}$.
<li> For $k=2,\dots,k_{max}$ use $k$-means to cluster the data into $k$ clusters.
<li> For each clustering $C(k)$ compute the validity metric, $v(k)$ from Equation \@ref(eq:rayturivalidity).
<li> Locate the _first_ local maximum in the validity measure, $\tilde{k}$ such that
$$v(\tilde{k}-1) < v(\tilde{k}) > v(\tilde{k}+1)$$
<li> Choose the optimal number of clusters, $k^*$, to be the __modified minimum__ such that $\tilde{k} < k^* \leq k_{max}$ is the number which minimizes the validity measure _after_ the first local maximum.
</ol>

__Ray and Turi Plots for the Ruspini Data __ \

We applied the above method to the 2-dimensional Ruspini data which was depicted in Figure \@ref(fig:ruspini). To avoid the type of poor clusterings that were displayed in Figure \@ref(fig:ruspinibadclusters), for each value of $k$, the $k$-means algorithm was run 25 times and the best solution (that is, the solution with the lowest objective function) was chosen to represent that value of $k$. Figure \@ref(fig:ruspinirayturi) shows the plot of Ray and Turi's validity metric computed on each solution. If one were to pick the global minimum from this set of clusterings, the optimal number of clusters would be $k^*=2$. However, according to the modified minimum favored in the original paper [@rayturi], the optimal number of clusters for the Ruspini data is $k^*=5$.  Neither of these solutions impose quite as obvious a clustering as the true number, 4.

```{r label='ruspinirayturi', fig.align='center', fig.cap = 'Ray and Turi Validity Plot for Ruspini Data', echo=F, out.width="50%"}
knitr::include_graphics("figs/ruspinirayturi.jpg")
```

__Ray and Turi Plots for Medlars-Cranfield-CISI__ \

We can generate similar plots using the same clusterings found by spherical $k$-means that were used to generate the SSE plots in Figures \@ref(fig:MCCSSE) and \@ref(fig:MCCsvdSSE). Obviously, the plots of Ray and Turi's validity metric are far more definitive in their determination of $k^*$, although it is left to the user to determine whether to pick the _global minimum_ or _modified minimum_ [@rayturi]. 

(ref:rayturicap) Ray \& Turi's Validity Plots for Medlars-Cranfield-CISI Clusterings on Raw Data and SVD Reductions to $r=\{8,12,20\}$ Dimensions Respectively.

```{r label='MCCrayturi',fig.show="hold", fig.align='center', fig.cap = '(ref:rayturicap) ', echo=F, out.width="50%"}
knitr::include_graphics("figs/MCCrayturi.jpg")
knitr::include_graphics("figs/MCCrayturiSVD8.jpg")
knitr::include_graphics("figs/MCCrayturiSVD12.jpg")
knitr::include_graphics("figs/MCCrayturiSVD20.jpg")

```

The results from Figures \@ref(fig:ruspinirayturi) and \@ref(fig:MCCrayturi) are summarized in the following table, which shows the number of clusters that would be chosen if one were to pick the global minimum validity or the modified minimum validity along with the actual number of clusters.\

<table>
<tr>
<td> Data Input <td>  Global Min <td>  Modified Min.<td>  Actual $k$ 
<tr>
<td>Medlars-Cranfield-CISI <td>  4 <td>  6 <td>  3 or 5  
<tr>
<td>Ruspini <td>  2<td>  5 <td>  4 
</table>
 <caption>(\#tab:mccrayturitable) Approximated Number of Clusters via Ray and Turi's Method  </caption>

### The Gap Statistic

The _gap statistic_ is an index devised by Tibshirani, Walther, and Hastie in 2000 that has received massive amounts of attention in the literature. This method is quite similar to the stopping methods previously discussed, only now the objective is to compare the cluster cohesion values with what is expected under some null reference distribution [@gapstat]. Supposing the $n$ data points $\x_1,\dots,\x_n$ are clustered in to $k$ clusters, $C_1,C_2,\dots,C_k$ and $|C_j|=n_j$ some measure of cohesion is defined as
$$W_k = \sum_{j=1}^k \frac{1}{2n_j} \sum_{\x_p,\x_q \in \C_j} d(\x_p,\x_q)$$
where $d(x,y)$ is a distance function. The idea is then to compare the graph of $\log(W_k), \,\,k=1,\dots,K$ to its expectation under some null reference distribution and to choose the value of $k, 1\leq k \leq K$ for which $\log(W_k)$ falls the farthest below its expectation. This distance between $\log(W_k)$ and its expectation under the reference distribution, denoted by $E^*$, is called the _gap_:
$$\mbox{Gap}(k) = E^*(\log(W_k)) - \log(W_k).$$

This expectation is estimated by drawing a Monte Carlo sample, $\X^*_1,\X^*_2,\dots,\X^*_B$ from the reference distribution. Each dataset in the sample is clustered, and the values of $\log(W^*_k), \,\,k=1,\dots,K$ are averaged over the samples. The sampling distribution of the gap statistic is controlled using the standard deviation, $sd_k$, of the B Monte Carlo replicates of $\log(W^*_k)$. Accounting for the simulation error in $E^*(\log(W_k))$ yields the standard error
$$s_k = sd_k\sqrt{1+\frac{1}{B}} .$$

Using the common "one standard error" rule, the number of clusters $k^*$ is chosen to be the smallest $k$ such that $$Gap(k)\geq Gap(k+1)-s_{k+1}.$$

The authors in [@gapstat] suggest, both for simplicity and performance, using a uniform distribution as the null reference distribution. This process is summarized in Table \@ref(tab:gapstat).

<table><tr><td>
<ol>
<li> Cluster the observed data in $X$ (which contains $n$ objects and $m$ features), varying the total number of clusters from $k=1,2,\dots K$, recording within dispersion measures (SSE function values) $W_k, \,\,k=1,2,\dots,K$.
<li> Generate $B$ reference datasets, each with $n$ objects having $m$ reference features generated uniformly over the range of the observed values for the original features in $X$. Cluster each of the $B$ reference datasets, recording within dispersion measures (SSE function values) $W^*_{kb},\,\, b=1,2,\dots,B,\,\,k=1,2,\dots,K.$ Compute the estimated Gap statistic:
$$Gap(k)=(1/B)\sum_b \log(W^*_{kb})-\log(W_k)$$
<li> Let $\bar{\mathit{l}} = (1/B)\sum_b \log(W^*_{kb})$ and compute the standard deviation:
$$sd_k = [(1/B)\sum_b \left(\log(W^*_{kb})-\bar{\mathit{l}} \right)^2]^{1/2}.$$
Define $s_k = sd_k \sqrt{1+\frac{1}{B}}.$ Finally choose the number of clusters to be the smallest value of $k$ such that 
$$Gap(k) \geq Gap(k+1) - s_{k+1}$$.
</ol>
</table>
 <caption>(\#tab:gapstat) Computation of the Gap Statistic [@gapstat] </caption>

In Figure \@ref(fig:ruspinigapstat) we provide the results from the gap statistic procedure on the ruspini data. Our Monte Carlo simulation involved $B=10$ generated datasets. The gap statistic indicates the presence of $k^*=4$ clusters.

(ref:gapstatlab) Results for gap statistic procedure on Ruspini data. Observed vs. Expected values of $\log(W_k)$ (left) and Width of Gap (right).  The maximum gap occurs at $k^*=4$ clusters.

```{r label='ruspinigapstat',fig.show="hold", fig.align='center', fig.cap = '(ref:gapstatlab) ', echo=F, out.width="50%"}
knitr::include_graphics("figs/ruspiniobsexp.jpg")
knitr::include_graphics("figs/ruspinigap.jpg")

```


## Graph Methods Based on Eigenvalues <br> (Perron Cluster Analysis) {#perroncluster}

Another commonly used methodology for determining the number of clusters relies upon the examination of eigenvalues of a graph Laplacian. Keeping with our focus in Chapter \@ref(spectral) we consider only undirected graphs. The methodology contained herein is motivated by the following observation: suppose we had an undirected graph consisting of $k$ connected components (i.e. $k$ distinct components, none of which are connected to any other). The adjacency matrix of such a graph would be block diagonal with $k$ diagonal blocks $\A_1, \dots, \A_k$, and each diagonal block would itself be an adjacency matrix for one connected component.

\begin{equation}
\A = 
\left[ 
\begin{array}{ccccc}
\A_1 & 0 & 0& \dots  & 0 \\
0   & \A_2 & 0 & \dots & 0 \\
0   & 0 & \A_3 & \ddots & 0 \\
0 & 0& 0 & \ddots & \vdots  \\
0 & 0 & 0 & \dots & \A_k 
\end{array}
\right] 
(\#eq:componentA)
\end{equation}

Thus, the Laplacian matrix $\mathbf{L}=\mathbf{D}-\A$ would also be block diagonal and each diagonal block would be the Laplacian matrix for one component of the graph.

\begin{equation}
\mathbf{L} = 
\left[ 
\begin{array}{ccccc}
\mathbf{L}_1 & 0 & 0& \dots  & 0 \\
0   & \mathbf{L}_2 & 0 & \dots & 0 \\
0   & 0 & \mathbf{L}_3 & \ddots & 0 \\
0 & 0& 0 & \ddots & \vdots  \\
0 & 0 & 0 & \dots & \mathbf{L}_k 
\end{array}
\right]
\hspace{.5cm}
\mbox{ with } \mathbf{L}_i \e = \mathbf{0} \mbox{ for } i=1,\dots, k
(\#eq:componentlaplacian)
\end{equation}

Thus, if each component is connected, the multiplicity of the smallest eigenvalue, $\lambda_1 = 0$, will count the number of diagonal blocks and thus the number of components. Of course the situation depicted in Equation \@ref(eq:componentlaplacian) is ideal and unlikely to be encountered in practice. However when the graph is _nearly_ decomposable into disconnected components, continuity of the eigenvalues suggests that one may be able to count the number of tightly connected components by counting the number of eigenvalues _near_ $\lambda_1 =0$. In order to be able to characterize eigenvalues as being _near_ $\lambda_1 =0$, it is necessary to transform (normalize) the Laplacian matrix so that its spectrum is contained in the interval $[0,1]$. This type of analysis is usually done using one of the two _normalized Laplacian matrices_ discussed in Chapter \@ref(spectral) and defined again here.
<ol>
<li> __The random-walk Laplacian__ $$\mathbf{L}_{rw} = \mathbf{D}^{-1}\mathbf{L} = \mathbf{I}-\mathbf{D}^{-1}\mathbf{A} = \mathbf{I}-\mathbf{P}$$
<li> __The symmetric Laplacian__ $$\mathbf{L}_{sym} = \mathbf{D}^{-1/2}\mathbf{L}\mathbf{D}^{-1/2}=\mathbf{I}-\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}.$$
</ol>

The normalized Laplacians, like the Laplacian matrix itself, are both positive definite. Furthermore, $\mathbf{L}_{rw}$ and $\mathbf{L}_{sym}$ have the same spectrum.   The following well-known and easily verified fact characterizes the relationship between the eigenvalues and eigenvectors of these two matrices [@chung].

(ref:thmcap1) Eigenvalues of $\mathbf{L}_{sym}$ and $\mathbf{L}_{rw}$

:::{.theorem name='(ref:thmcap1)'}
$\lambda$ is an eigenvalue of $\mathbf{L}_{rw}$ with eigenvector $\mathbf{v}$ if and only if $\lambda$ is an eigenvalue of $\mathbf{L}_{sym}$ with eigenvector $\mathbf{w}=\mathbf{D}^{1/2}\mathbf{v}$.
:::

In light of this fact, we will limit our discussion to the properties of the transition probability matrix of a random walk on the graph associated with the adjacency matrix $\A$, denoted
 $$\mathbf{P}=\mathbf{D}^{-1} \A = \mathbf{I}-\mathbf{L}_{rw},$$  
 since $$\lambda \in \sigma(\mathbf{P}) \quad \Rightarrow \quad (1-\lambda) \in \sigma(\mathbf{L}_{rw}).$$
 Random walks on undirected graphs are _reversible Markov chains_, which satisfy the so-called _detailed balance equations_  [@kemenysnell;@stewart]:
$$\mathbf{Q}\mathbf{P}=\mathbf{P}^T \mathbf{Q} \hspace{.2cm} \mbox{ where  } \mathbf{Q}=diag(\mathbf{\pi}).$$
The stationary distribution for $\mathbf{P}$ given by  $\mathbf{\pi}^T= \frac{\e^T\mathbf{D}}{\e^T\mathbf{D}\e}$.

We assume the underlying graph (which we aim to partition) is connected so that the matrix $\mathbf{P}$ is irreducible. If the graph is composed of connected components, like the one associated with Equation \@ref(eq:componentA), the resulting random walk is equivalently referred to as _completely reducible, uncoupled, or completely decomposable_ and there simple efficient algorithms available to identify the connected components [@concomp]. 

 In our connected graph, we assume that there exists some cluster or community structure, i.e. that there are $k$ groups of vertices, $C_1,C_2, \dots, C_k$ with $|C_k|=n_k$, for which edges exist more frequently and with higher weight within each group than between each group. With this assumption, we can reorder the rows and columns of the transition probability matrix $\mathbf{P}$ according to group membership so that the result is _block-diagonally dominant_. By this we essentially mean that $\mathbf{P}$ is a perturbation of a block-diagonal matrix $\mathbf{B}$, such that

\begin{equation}
\mathbf{P}=\mathbf{B}+\mathbf{E} = \left[ 
\begin{array}{ccccc}
\mathbf{B}_{11} & \mathbf{E}_{12} & \mathbf{E}_{13}& \dots  & \mathbf{E}_{1k} \\
\mathbf{E}_{21}   & \mathbf{B}_{22} & \mathbf{E}_{23} & \dots & \mathbf{E}_{2k} \\
\mathbf{E}_{31}   & \mathbf{E}_{32} & \mathbf{B}_{33} & \ddots & \mathbf{E}_{3k} \\
\vdots& \vdots& \vdots & \ddots & \vdots  \\
\mathbf{E}_{k1} & \mathbf{E}_{k2}& \mathbf{E}_{k3} & \dots & \mathbf{B}_{kk}
\end{array}
\right]
 (\#eq:bdd)
 \end{equation}
 where the off-diagonal blocks, $\mathbf{E}_{ij}$, are much smaller in magnitude than the the diagonal blocks. In fact, the entries in the off-diagonal blocks are small enough that the diagonal blocks are _nearly stochastic_, i.e. $\mathbf{B}_{ii} \e \approx 1$ for $i=1,2,\dots,k$.  A transition probability matrix taking this form describes a __nearly uncoupled__ or __nearly completely reducible__ Markov Chain.
 
The degree to which a matrix is considered nearly uncoupled is dependent on one's criteria for measuring the level of _coupling_ (interconnection) between the _aggregates_ (clusters of states) of the Markov chain [@fischer;@meyernumc;@chuckthesis]. In [@meyernumc], the _deviation from complete reducibility_ is defined as follows:
<!-- %  -->
<!-- % \begin{definition}[Uncoupling Measure] -->
<!-- % Let $n_1$ and $n_2$ be fixed positive integers such that $n_1+n_2=n$, and let $\mathbf{P}$ be an $n\times n$ irreducible stochastic matrix, whose respective rows and columns have been rearranged to the form -->
<!-- % $$\mathbf{P}=\left[ \begin{array}{cc} -->
<!-- % \mathbf{P}_{11} & \mathbf{P}_{12} \\ -->
<!-- % \mathbf{P}{21} & \mathbf{P}_{22} \end{array} \right]$$ -->
<!-- % where $\mathbf{P}_{11}$ is $n_1 \times n_1$ and $\mathbf{P}_{22}$ is $n_2 \times n_2$ so that the ratio -->
<!-- % $$\sigma(\mathbf{P},n_1) = \frac{ \e^T \mathbf{P}_{12} \e + \e^T \mathbf{P}_{21} \e}{\e^T\mathbf{P}\e}$$ is minimized over all symmetric permutations of $\mathbf{P}$. The quantity $\sigma(\mathbf{P},n_1)$ is called the \textbf{uncoupling measure} of $\mathbf{P}$ with respect to the parameter $n_1$, defined as the ratio of the sum of the entries in the off-diagonal blocks to the sum  of all the entries in the matrix. -->
<!-- % \end{definition} -->
 
:::{.definition name='Deviation from Complete Reducibility'}
For an $m\times n$ irreducible stochastic matrix with a $k$-level partition
 $$\mathbf{P} = \left[ 
\begin{array}{ccccc}
\mathbf{P}_{11} & \mathbf{P}_{12} & \mathbf{P}_{13}& \dots  & \mathbf{P}_{1k} \\
\mathbf{P}_{21}   & \mathbf{P}_{22} & \mathbf{P}_{23} & \dots & \mathbf{P}_{2k} \\
\mathbf{P}_{31}   & \mathbf{P}_{32} & \mathbf{P}_{33} & \ddots & \mathbf{P}_{3k} \\
\vdots& \vdots& \vdots & \ddots & \vdots  \\
\mathbf{P}_{k1} & \mathbf{P}_{k2}& \mathbf{P}_{k3} & \dots & \mathbf{P}_{kk}
\end{array}
\right]$$
 the number $$\delta=2\max_{i} \|\mathbf{P}_{i*}\|_{\infty}$$ is called the __deviation from complete reducibility.__ 
:::
  
It is important to point out that the parameter $\delta$, or any other parameter that measures the level of coupling between clusters in a graph (like those suggested in [@fischer;@chuckthesis;@meyerharfield]) cannot be computed without knowing a priori the clusters in the graph. Such parameters are merely tools for the perturbation analysis, used to prove the next theorem regarding the spectrum of block-diagonally dominant stochastic matrices [@fischer; @kato; @chuck; @meyernumc; @meyerharfield;@perroncluster;@stewartnumc].

:::{.theorem name='The Spectrum of a Block-Diagonally Dominant Stochastic Matrix' #bddsm} 
For sufficiently small $\delta \neq 0$, the eigenvalues of $\mathbf{P}(\delta)$ are continuous in $\delta$, and can be divided into 3 parts [@fischer;@meyernumc;@perroncluster;@chuck]:
<ol>
<li> The Perron root, $\lambda_1(\delta)=1$,
<li> a cluster of $k-1$ eigenvalues $\lambda_2(\delta),\lambda_3(\delta),\dots,\lambda_k(\delta)$ that approach 1 as $\delta \to 0$, and
<li> the remaining eigenvalues, which are bounded away from 1 as $\delta \to 0$.
</ol>\
:::


<!-- %Before we discuss the notion of coupling and the parameter $\epsilon$,  -->
<!-- %  -->
<!-- % Let $\mathbf{P}(\epsilon)$ be a family of matrices having the form in Equation \@ref(eq:bdd} and define $\epsilon^*$ such that $\mathbf{P}(\epsilon^*)=\mathbf{P}$. In order to continue such perturbation analysis, the following assumptions are adopted from [@fischer} as needed to implement Theorem 6.1 from [@kato}.  -->
<!-- %  -->
<!-- % \begin{itemize} -->
<!-- % \item[] \textbf{Assumptions} -->
<!-- % \item[1.] Let $\mathbf{P}(\epsilon) = \mathbf{P}(0) + \epsilon \mathbf{P}^{(1)} + \epsilon^2\mathbf{P}^{(2)} + \dots$ be a family of matrices that are analytic in a domain containing the origin. -->
<!-- % \item[2.] Let $\mathbf{P}(\epsilon)$ be stochastic and reversible for all $\epsilon$, and primitive for all $\epsilon \neq 0$. -->
<!-- % \item[3.] For $\epsilon = 0$, let $\mathbf{P}(0)$ be block-diagonal with primitive diagonal blocks. -->
<!-- % \end{itemize}  -->
<!-- %  -->

 
The cluster of $k$ eigenvalues surrounding and including the Perron root $\lambda_1=1$ is known as the __Perron cluster__ [@chuck,@fischer,@perroncluster]. The analysis in [@chuck] explains that if there is no further decomposition (or meaningful sub-clustering) of the diagonal blocks, a relatively large gap between the eigenvalues $\lambda_k$ and $\lambda_{k+1}$ is expected. Thus, we can determine the number of clusters in the state space of a nearly uncoupled Markov chain (i.e. the number of clusters in a graph) by counting the number of eigenvalues in this Perron Cluster. 

This method is extremely effective when the graph to be partitioned is sufficiently close to being uncoupled. Problems arise when either high levels of coupling (intercluster linkage) are in play or when some vertices within a cluster are weakly connected to that cluster (for example, _dangling nodes_ - vertices with degree 1).

The examples in Figure \@ref(fig:perronex) illustrate this point. Firts we show a synthetic example of a graph exhibiting cluster structure and the eigenvalues of the associated transition probability matrix respectively.  The thickness of the edges in the graph correspond to their respective weights. Because there is a limited amount of coupling (intercluster connection) in this first example, the Perron cluster of eigenvalues is easy to identify. Because there are 3 eigenvalues near 1, the user would conclude that the graph has 3 clusters. 

 Occasionally a user can get a sense of the cluster structure in a graph with an appropriate layout of the nodes and edges. Force-directed graph drawing algorithms are common in this practice. The basic idea behind these algorithms is to model the edges as springs connecting the nodes and then to somehow minimize the total amount of tension in the system. Thus, densely connected groups of nodes are placed proximal to each other and the edges which loosely connect these groups are stretched. The graph drawings in Figure \@ref(fig:perronex) are all examples of force-directed layouts. Graph drawing algorithms are beyond the scope of this paper, but for information the interested reader should see, for example, [@graphdrawing1,@graphdrawing2]. 

The second two rows of Figure \@ref(fig:perronex) display a real-world example using the hyperlink graph between a sample of 1222 American political blogs. Based upon the force-directed drawing of the graph, it is clear that there are 2 large communities or clusters in this graph. These clusters correspond to the liberal and conservative division of American politics. The Perron cluster is not easily identified on the eigenvalue plot in Figure \@ref(fig:perronex), and thus no conclusion should be drawn regarding the number of clusters in this data. However, after removing a large number of dangling nodes from the graph, or blogs which link only to a single neighboring page in the sampled population, a different picture comes to light. In the final row of Figure \@ref(fig:perronex) we illustrate the effect of removing these dangling nodes (about 200 in total) on the eigenvalues of the transition probability matrix. Luckily, for this particular graph, removing the dangling nodes did not create more, a situation that is not guaranteed in general. The third eigenvalue in the Perron cluster likely identifies the small group of 3 blogs that is now visible in the force directed drawing of the graph. Such small clusters are generally undesirable in graph partitioning, and since the eigenvalues tell the user nothing about the size or composition of the graph communities counted by the eigenvalues in the Perron cluster, this method must be used with caution! 

```{r label='perronex',fig.show="hold", fig.align='center', fig.cap = 'Some Examples of Perron Cluster Identification on various Network Datasets', echo=F, out.width="50%"}
knitr::include_graphics("figs/numcGraphex.jpg")
knitr::include_graphics("figs/numcEigsex1.jpg")
knitr::include_graphics("figs/agblog.jpg")
knitr::include_graphics("figs/agblogEigs.jpg")
knitr::include_graphics("figs/AGnoDangle.jpg")
knitr::include_graphics("figs/AGnoDangleEigs.jpg")

```

 In the next Chapter, we will introduce a similarity matrix that is well suited for this Perron-cluster analysis. Our method has the ability of estimating the number of clusters in very noisy and high-dimensional data when other methods fail.

<!--chapter:end:1114-findk.Rmd-->

# Algorithms for Graph Partitioning {#chap1.5}

## Spectral Clustering {#spectral}
Spectral clustering is a term that data-miners have given to the partitioning problem as it arose in graph theory. The theoretical framework for spectral clustering was laid in 1973 by Miroslav Fiedler [@fiedlerac;@fiedlerev]. We will begin with a discussion of this early work, and then take a look at how others have adapted the framework to meet the needs of data clustering. In this setting, we have a graph  $G$ on a set of vertices $N=\{1,2,\dots,n\}$ with edge set $E=\{(i,j) : i,j \in N \mbox{and} i \leftrightarrow j\}$. Edges between the vertices are recorded in an  _adjacency matrix_ $\A = (a_{ij})$, where $a_{ij}$ is equal to the weight of the edge connecting vertex (object) $i$ and vertex $j$ and $a_{ij}=0$ if $(i,j) \notin E$. For the immediate discussion, we will assume the graph has no "self-loops", i.e. $a_{ii}=0 \forall i$. 
Spectral clustering algorithms typically involve the  _Laplacian matrix_ associated with a graph. A Laplacian matrix is defined as follows:

:::{.definition name='The Laplacian Matrix' #laplaciandef}
The  _Laplacian Matrix_, $\mathbf{L}$, of an undirected, weighted graph with adjacency matrix $\A=(a_{ij})$ and diagonal degree matrix $\mathbf{D}=\mbox{diag}(\A\e)$ is:
$$\mathbf{L}=\mathbf{D}-\A$$
:::

The Laplacian matrix is symmetric, singular, and positive semi-definite. To see this third property, construct an $n \times |E|$ "vertex-edge incidence" matrix $\U$ with rows corresponding to vertices and columns corresponding to edges. Allow the edges of the original graph to be directed arbitrarily, and set 
\[
\U_{v,e} = \left\{
     \begin{array}{lr}
       +\sqrt{a_{ij}} : &\mbox{if  } v \mbox{  is the head of  } e\\
       -\sqrt{a_{ij}} : &\mbox{if  } v \mbox{  is the tail of  } e\\
       0  : &\mbox{otherwise}
     \end{array}
   \right.
 \]
 
 Then $\mathbf{L}=\U\U^T$ is positive semi-definite [@fiedlerac]. $\mathbf{L}$ gives rise to a nice quadratic form:
 
 \begin{equation}
(\#eq:quadlaplacian)
 \mathbf{y}^T \mathbf{L} \mathbf{y} = \sum_{(i,j) \in E}a_{ij} (y_i - y_j)^2.
 \end{equation}
  
  Let $\sigma(\mathbf{L})=\{\lambda_1 \leq \lambda_2 \leq \dots \leq \lambda_n\}$ be the spectrum of $\mathbf{L}$. Since $\mathbf{L}$ is positive semi-definite, $\lambda_i \geq 0 \forall i$. Also, since the row sums of $\mathbf{L}$ are zero, $\lambda_1=0$. Furthermore if the graph, $G$, is composed of $k$ connected components, each disconnected from each other, then $\lambda_1=\lambda_2=\dots=\lambda_k = 0$ and $\lambda_j \geq 0 \mbox{ for } j\geq k+1$.  In [@fiedlerac] Fiedler defined the  _algebraic connectivity_ of the graph as the second smallest eigenvalue, $\lambda_2$, because its magnitude provides information about how easily the graph is to be disconnected into two components. Later, in [@fiedlerev], he alluded to the utility of the eigenvector associated with $\lambda_2$ in determining this two-component decomposition of a graph. 
## Fiedler Partitioning
  Suppose we wish to decompose our graph into two components (or clusters of vertices) $C_1$ and $C_2$ where the edges exist more frequently and with higher weight inside the clusters than between the two clusters. In other words, we intend to make an  _edge-cut_ disconnecting the graph into two clusters. It is desired that the resulting partition satisfies the following objectives:
  \begin{itemize}
  \item[1.] minimize the total weight of edges cut (edges in between components)
  \item[2.] maximize the total weight of edges inside the two components.
  \end{itemize}
 To begin with, lets take the quadratic form in \eref{quadlaplacian} and let $\y$ be a vector that determines the cluster membership of each vertex as follows:
 $$
\y_i=\left\{
     \begin{array}{lr}
       +1 & : \mbox{if vertex } i \mbox{  belongs in } C_1\\
       -1 & : \mbox{if vertex } i \mbox{  belongs in  } C_2\\
     \end{array}
   \right.
 $$
Our first goal is then to minimize \eref{quadlaplacian} over all such vectors $\y$:
\begin{equation}
(\#eq:mincut)
\min_{\y} \y^T \mathbf{L} \y = \sum_{(i,j) \in E} a_{ij} (\y_i-\y_j)^2 = 2 \sum_{\substack{(i,j) \in E \\i \in C_1, j \in C_2}} 4 a_{ij}
\end{equation}

Note that the final sum is doubled to reflect the fact that each edge connecting $C_1$ and $C_2$ will be counted twice. However, the above formulation is incomplete because it does not take into account the second objective, which is to maximize the total weight of edges inside the two components. Indeed it seems the minimum solution to Equation \@ref(mincut) would often involve cutting all of the edges adjacent to a single vertex of minimal degree, disconnecting the graph into components of size $1$ and $n-1$, which is generally undesirable. In addition, the above optimization problem is NP-hard. To solve the latter problem, the objective function is relaxed from discrete to continuous. By the Rayleigh theorem,
$$\min_{\|\y\|_2=1} \y^T\mathbf{L} \y = \lambda_1$$ with $\y^*$ being the eigenvector corresponding to the smallest eigenvalue. However, for the Laplacian matrix, $y^*=\e$. In context, this makes sense - in order to minimize the weight of edges cut, we should simply assign all vertices to one cluster, leaving the second empty. In order to divide the vertices into two clusters we need an additional constraint on $\y$. Since clusters of relatively balanced size are desirable, a natural constraint is $\y^T\e=0$. By the Courant-Fischer theorem,
\begin{equation}
(\#eq:fiedlercut)
\min_{\substack{\| \y \|_2=1 \\ \y^T \e=0}} \y^T \mathbf{L} \y = \lambda_2
\end{equation}
with $\y^*=\textbf{v}_2$ being the eigenvector corresponding to the second smallest eigenvalue, $\lambda_2$. This vector is often referred to as the  _Fiedler vector_ after the man who identified its usefulness in graph partitioning. We define the Fiedler graph partition as follows:

:::{.definition name='Fiedler Graph Partition' #fiedlerpart}
Let $G=(N,E)$ be a connected graph on vertex set $N=\{1,2,\dots,n\}$ with adjacency matrix $\A$. Let $\mathbf{L}=\mathbf{D}-\A$ be the Laplacian matrix of $G$. Let $\textbf{v}_2$ be an eigenvector corresponding to the second smallest eigenvalue of $\mathbf{L}$. The __Fiedler partition__ is:
\begin{eqnarray*}
C_1 &=& \{i \in N : \textbf{v}_2(i) <0\}\\
C_2 &=& \{i \in N : \textbf{v}_2(i) >0\}
\end{eqnarray*}
Vertices $j$, for which $\textbf{v}_2(j)=0$, can be arbitrarily placed into either cluster.
:::


There is no uniform agreement on how to determine the cluster membership of vertices for which $\textbf{v}_2(j)=0$. The decision to make the assignment arbitrarily comes from experimental results that indicate in  _some scenarios_ these zero valuated vertices are equally drawn to either cluster. Situations where there are a large proportion of zero valuated vertices may be indicative of a graph which does not conform well to Fiedler's partition, and we suggest the user tread lightly in these cases. Figure \@ref(fig:ptsart) shows the experimental motivation for our arbitrary assignment of zero valuated vertices. The vertices in these graphs are labelled according to the sign of the corresponding entry in $\textbf{v}_2$. We highlight the red vertex and watch how its sign in $\textbf{v}_2$ changes as nodes and edges are added to the graph.


```{r label='ptsart', fig.align='center', fig.cap = 'Fiedler Partitions and Zero Valuated Vertices', echo=F, out.width="75%"}
knitr::include_graphics("figs/ptsart.jpg")
```

 In order to create more than two clusters, the Fiedler graph partition can be performed iteratively, by examining the subgraphs induced by the vertices in $C_1$ and $C_2$ and partitioning each based upon their own Fiedler vector, or in an extended fashion using multiple eigenvectors. This iterative method requires a cluster to be chosen for further division, perhaps based upon the algebraic connectivity of the cluster. It is also possible to use the sign patterns in subsequent eigenvectors to further partition the graph. This approach is called Extended Fiedler Clustering and is discussed in Section \@ref(extendedfiedler). First, let's take a more rigorous look at why the sign patterns of the Fiedler vector provide us with a partition.
 
### Linear Algebraic Motivation for the Fiedler vector

The relaxation proposed in Equation \@ref(eq:fiedlercut) does not give us a general understanding of why the sign patterns of the Fiedler vector are useful in determining cluster membership information. We will use the following facts:

:::{.lemma name='Fiedler Lemma 1' #flem1}
Let $\mathbf{L}=\mathbf{D}-\A$ be a Laplacian matrix for a graph $G=(V,E)$ with $|V|=n$. Let $\sigma(L)=\lambda_1 \leq \lambda_2 \leq \dots \leq \lambda_n$ Then $\mathbf{L}$ is symmetric and positive semi-definite with $\lambda_1=0$. 
:::

:::{.lemma name='Fiedler Lemma 2' #flem2}

$\lambda_2(L)=0$ if and only if the graph $G$ has 2 components, $C_1$ and $C_2$ which are completely disconnected from each other (i.e. there are no edges connecting the vertices in $C_1$ to the vertices in $C_2$.
:::
:::{.lemma name='Fiedler Lemma 3' #flem3}
Let $\mathbf{M}$ be a symmetric matrix of rank $r$. Let 
$$\mathbf{M} = \mathbf{V} \mathbf{D}  \mathbf{V}^T = (\textbf{v}_1, \textbf{v}_2, \dots, \textbf{v}_r)
\left(
\begin{matrix}
\sigma_1 &  0  & \ldots & 0\\
0  &  \sigma_2 &  \ldots &  0\\
\vdots & \vdots & \ddots&  \vdots\\
0  &   0       &\ldots & \sigma_r\\
\end{matrix}
\right)
\left(
\begin{matrix}
\textbf{v}_1^T\\
\textbf{v}_2^T\\
\vdots \\
\textbf{v}_r^T\\
\end{matrix} 
\right)
=\sum_{i=1}^r \sigma_i \textbf{v}_i \textbf{v}_i^T$$
 be the singular value decomposition of $\mathbf{M}$, with $\sigma_1 \geq \sigma_2 \geq \dots \sigma_r$.  Let $\widetilde{\mathbf{M}}$ be the closest (in the euclidean sense) rank $k$ approximation to $\mathbf{M}$:
 $$\widetilde{\mathbf{M}}=\mbox{arg}\min_{\mathbf{B} , rank(\mathbf{B})=k}\|\mathbf{M} - \bf B\|.$$
Then $\widetilde{\mathbf{M}}$  is given by the  _truncated singular value decomposition_:
 $$B=\sum_{i=1}^k \sigma_i \textbf{v}_i \textbf{v}_i^T$$
:::

From these simple tools, we can get a sense for why the signs of the Fiedler vector will determine a natural partition of a graph into two components. In the simplest case, $\lambda_1=\lambda_2=0$, the graph contains two disjoint components, $C_1$ and $C_2$. Thus, there exists some permutation matrix $\mathbf{P}$ such that $\mathbf{P} \mathbf{L} \mathbf{P}$ is block diagonal:

$$
\mathbf{P} \mathbf{L} \mathbf{P} =\left( \begin{array}{cc}
\mathbf{L}_1 & 0\\
0 & \mathbf{L}_2\\
\end{array}
\right)
$$

and $\mathbf{L}_1$ is the Laplacian matrix for the graph of $C_1$ and $\mathbf{L}_2$ is the Laplacian for $C_2$. Let $n_1$ and $n_2$ be the number of of vertices in each component. Clearly the eigenvectors $\textbf{v}_2$ and $\textbf{v}_2$ associated with $\lambda_1=0$ and $\lambda_2=0$ are contained in the span of $\mathbf{u}_1$ and $\mathbf{u}_2$ where:

\begin{equation}
\mathbf{u}_1 =
\begin{array}{cc}\begin{array}{c} 1\\ \vdots \\ n_1 \\ n_1+1 \\ \vdots \\ n \end{array} &\left( \begin{array}{c} 1 \\ \vdots \\ 1 \\ 0 \\ \vdots \\ 0 \end{array} \right) \end{array}

\qquad\mbox{and}\qquad

\mathbf{u}_2 =\begin{array}{cc}\begin{array}{c} 1\\ \vdots \\ n_1 \\ n_1+1 \\ \vdots \\ n \end{array} &\left( \begin{array}{c}0 \\ \vdots \\ 0 \\ 1 \\ \vdots \\ 1 \end{array} \right)\end{array}
\end{equation}

For all Laplacian matrices, it is convention to consider $\textbf{v}_1=\e$, the vector of all ones. Under this convention, the two conditions $\textbf{v}_2 \perp \textbf{v}_1$ and $\textbf{v}_2 \in span \{ \mathbf{u}_1,\mathbf{u}_2\}$ necessarily force $\textbf{v}_2$ to have a form such that the component membership of each vertex is discernible from the sign of the corresponding entry in the vector:
$$\textbf{v}_2 = \alpha(\mathbf{u}_1-\frac{n_1}{n_2} \mathbf{u}_2).$$

The case when $\mathbf{L}$ is  _not_ disconnected into two components is far more interesting, as this is generally the problem encountered in practice. We will start with a connected graph, so that our Laplacian matrix has rank $n-1$. Let the spectral decomposition of our Laplacian matrix $\mathbf{L}$ (which is equivalent to its singular value decomposition because $\mathbf{L}$ is symmetric) be:
$$\mathbf{L}=\sum_{i=2}^n \lambda_i \textbf{v}_i \textbf{v}_i^T.$$
Let $\widetilde{\mathbf{L}}$ be the closest rank $n-2$ approximation to $\mathbf{L}$. Then, by Lemmas \@ref(lem:flem2) and \@ref(lem:flem3),
$$\widetilde{\mathbf{L}} = \sum_{i=3}^n \lambda_i  \textbf{v}_i \textbf{v}_i^T$$
and there exists a permutation matrix $\mathbf{P}$ such that

$$
\mathbf{P} \widetilde{\mathbf{L}} \mathbf{P} =\left( \begin{matrix}
\widetilde{\mathbf{L}}_1 & 0\\
0 & \widetilde{\mathbf{L}}_2
\end{matrix}
\right).
$$

Suppose we permute the rows of $\mathbf{L}$ accordingly so that

$$
\mathbf{P} \mathbf{L} \mathbf{P} =\left( \begin{matrix}
\mathbf{L}_1 & -\mathbf{E}\\
-\mathbf{E}^T & \mathbf{L}_2\\
\end{matrix}
\right)
$$
where $\mathbf{E}_{ij} \geq 0 \forall i,j$ because $\mathbf{L}$ is a Laplacian matrix. Consider the difference between $\mathbf{L}$ and $\widetilde{\mathbf{L}}$:

$$\mathbf{L}-\widetilde{\mathbf{L}} = \lambda_2 \textbf{v}_2 \textbf{v}_2^T$$

which entails $\mathbf{L} - \lambda_2 \textbf{v}_2 \textbf{v}_2^T = \widetilde{\mathbf{L}}$. If we permute the vector $\textbf{v}_2$ in the same manner as the matrices $\mathbf{L}$ and $\widetilde{\mathbf{L}}$, then one thing is clear:

\begin{equation*}
\lambda_2\mathbf{P}\textbf{v}\textbf{v}_2^T\mathbf{P} = \left(\begin{matrix}
\A & -\mathbf{E} \\
-\mathbf{E}^T & B 
\end{matrix}\right)
\end{equation*}
Thus, if 
\begin{equation*}\mathbf{P}\textbf{v} =\left(\begin{matrix}
\mathbf{a} \\
 \mathbf{b}\\
\end{matrix} \right)
\end{equation*}

where $\mathbf{a} \in \Re^{n_1}$ and $\mathbf{b} \in \Re^{n_2}$


#### Extended Fiedler Clustering {#extendedfiedler}
 In the extended Fiedler algorithm, we use the sign patterns of entries in the first $l$ eigenvectors of $\mathbf{L}$ to create up to $k=2^l$ clusters. For instance, suppose we had 10 vertices, and used the $l=3$ eigenvectors $\textbf{v}_2,\textbf{v}_3,\mbox{ and  }\textbf{v}_4$. Suppose the sign of the entries in these eigenvectors are recorded as follows:
\[
       \begin{array}{cc} & \begin{array}{ccc} \mathbf{v}_2 & \mathbf{v}_3&\mathbf{v}_4 \end{array}\cr
       \begin{array}{c}
        1 \\
        2 \\
        3 \\
        4 \\
        5 \\
        6 \\
        7 \\
        8 \\
        9 \\
        10 \end{array} & \left(
        \begin{array}{ccc} 
              +&+&-\cr
              -&+&+\cr
              +&+&+\cr
              -&-&-\cr
              -&-&-\cr
              +&+&-\cr
              -&-&-\cr
              -&+&+\cr
              +&-&+\cr
              +&+&+ \end{array}
              \right) \end{array}
\]
     Then the 10 vertices are clustered as follows:
  $$
       \{1,6\},\quad
       \{2,8\},\quad 
       \{3,10\},\quad 
       \{4,5,7\},\quad
       \{9\}.
     $$
 Extended Fiedler makes clustering the data into a specified number of clusters $k$ difficult, but may be able determine a natural choice for $k$ as it partitions the data along several eigenvectors.
 
 In a 1990 paper by Pothen, Simon and Liou, an alternative formulation of the Fiedler partition is proposed [@pothen]. Rather than partition the vertices based upon the sign of their corresponding entries in $\mathbf{v}_2$, the vector $\mathbf{v}_2$ is instead divided at its median value. The main motivation for this approach was to split the vertices into sets of equal size. In 2003, Ding et al. derived an objective function for determining an ideal split point for similar partitions using the second eigenvector of the _normalized_ Laplacian, defined in Section \@ref(ncut) [@minmax]. The basic idea outlined above has been adapted and altered hundreds if not thousands of times in the past 20 years. The present discussion is meant merely as an introduction to the literature. 
 
 
### Graph Cuts

 The majority of spectral algorithms are derived as alterations of the objective function in Equation \@ref(eq:fiedlercut).The idea is the same: partition the graph into two components by means of a minimized edge cut, while requiring that the two components remain somewhat balanced in size (i.e. do not simply isolate a small number of vertices). Two common objective functions which embody this idea are the ratio cut (RatioCut) [@ratiocut], the normalized cut (Ncut) [@shi]. 
 
#### Ratio Cut

The ratio cut objective function was first introduced by Hagen and Kahng in 1992 [@ratiocut]. Given a graph $G(V,E)$ with vertex set $V$ partitioned into $k$ disjoint clusters, $V_1,V_2,\dots V_k$, the __ratio cut__ of the given partition is defined as
 $$\mbox{RatioCut}(V_1,V_2,\dots,V_k) = \sum_{i=1}^k \frac{w(V_i,\bar{V_i})}{|V_i|}$$
 where $|V_i|$ is the number of vertices in $V_i$, $\bar{V_i}$ is the complement of the set $V_i$ and, given two vertex sets $A$ and $B$, $w(A,B)$ is the sum of the weights of the edges between vertices in $A$ and vertices in $B$.  Let $\mathbf{H}$ be an $n\times k$ matrix indicating cluster membership of vertices by its entries:
\begin{equation}
(\#eq:ratioH)
 \mathbf{H}_{ij}=
 \begin{cases}
\frac{1}{\sqrt{|V_j|}}, & \mbox{if the } i^{th} \mbox{ vertex is in cluster } V_j \\
0 & \text{otherwise}
\end{cases}
\end{equation}

 Then $\mathbf{H}^T\mathbf{H}=\mathbf{I}$ and minimizing the ratio cut over all possible partitionings is equivalent to minimizing
 $$f(\mathbf{H}) = \mbox{Trace}(\mathbf{H}^T\mathbf{L}\mathbf{H})$$
 over all matrices $\mathbf{H}$ described by Equation \@ref(eq:ratioH), where $\mathbf{L}$ is the Laplacian matrix from Definition \@ref(def:laplaciandef). The exact minimization of this objective function is again NP-hard, but relaxing the conditions on $\mathbf{H}$ to $\mathbf{H}^T\mathbf{H}=\mathbf{I}$ yields a solution $\mathbf{H}^*$ with columns containing the eigenvectors of $\mathbf{L}$ corresponding to the $k$ smallest eigenvalues.
 
 Unfortunately, after this relaxation it is not necessarily possible to automatically determine from $\mathbf{H}^*$ which vertices belong to each cluster. Instead, it is necessary to look for clustering patterns in the rows of $\mathbf{H}^*$. This is a common conceptual drawback of the relaxation of objective functions in spectral clustering. The best way to procede after the relaxation is to cluster the rows of $\mathbf{H}^*$ with an algorithm like $k$-means to determine a final clustering. The ratio cut minimization method is generally referred to as _unnormalized spectral clustering_ [@spectraltutorial]. The algorithm is as follows:
 
 <table><tr><td>
__Input__: $n \times n$ adjacency (or similarity) matrix $\mathbf{A}$ for a graph on vertices (or objects) $\{1,\dots,n\}$ and desired number of clusters $k$
<ol>
<li> Compute the Laplacian $\mathbf{L}=\mathbf{D}-\mathbf{A}$.
<li> Compute the first $k$ eigenvectors $\mathbf{V}=\mathbf{v}_1,\mathbf{v}_2,\dots,\mathbf{v}_k$ of $\mathbf{L}$ corresponding to the $k$ smallest eigenvalues.
<li> Let $\mathbf{y}_{i}$ be the $i^{th}$ row of $\mathbf{V}$
<li> Cluster the points $\mathbf{y}_i \in \Re^k$ with the $k$-means algorithm into clusters $\bar{C}_1,\dots \bar{C}_k$.
</ol>
__Output__: Clusters $C_1,\dots,C_k$ such that $C_j = \{i : \mathbf{y}_i \in \bar{C}_j\|$
</table>
 <caption>(\#tab:algratiocut) Unnormalized Spectral Clustering (RatioCut) [@spectraltutorial] </caption>
<br>
#### Normalized Cut (Ncut) {#ncut}

 The normalized cut objective function was introduced by Shi and Malik in 2000 [@shi]. Given a graph $G(V,E)$ with vertex set $V$ partitioned into $k$ disjoint clusters, $V_1,V_2,\dots V_k$, the __normalized cut__ of the given partition is defined as
$$\mbox{Ncut}(V_1,V_2,\dots,V_k)= \sum_{i=1}^k \frac{w(V_i,\bar{V_i})}{\mbox{vol}(V_i)},$$
where $\mbox{vol}(V_i)$ is the sum of the weights of the edges connecting the vertices in $V_i$. Whereas the size of a subgraph $V_i$ in the ratio cut formulation is measured by the number of vertices $|V_i|$, in the normalized cut formulation it is measured by the total weight of the edges in the subgraph.
Thus, minimizing the normalized cut is equivalent to minimizing
$$f(\mathbf{H}) = \mbox{Trace}(\mathbf{H}^T\mathbf{L}\mathbf{H})$$ over all matrices $\mathbf{H}$ with the following form:
 
\begin{equation}
\mathbf{H}_{ij}=
\begin{cases}
\frac{1}{\sqrt{\mbox{vol}(V_j)}}, & \mbox{if the } i^{th} \mbox{ vertex is in cluster } V_j \\
0 & \text{otherwise.}
\end{cases}
(\#eq:ncutH)
\end{equation}

With $\mathbf{H}^T \mathbf{D} \mathbf{H} = \mathbf{I}$ where $\mathbf{D}$ is the diagonal degree matrix from Definition \@ref(def:laplaciandef). Thus, to relax the problem, we substitute $\mathbf{G}=\mathbf{D}^{1/2}\mathbf{H}$ and minimize
 $$f(\mathbf{G})=\mathbf{G}^T \mathscr{L} \mathbf{G}$$ subject to $\mathbf{G}^T\mathbf{G}=\mathbf{I}$, where $\mathscr{L}=\mathbf{D}^{-1/2}\mathbf{L} \mathbf{D}^{-1/2}$ is called the __normalized Laplacian__. Similarly, the solution to the relaxed problem is the matrix $\mathbf{G}^*$ with columns containing eigenvectors associated with the $k$ smallest eigenvalues of $\mathscr{L}$. Again, the immediate interpretation of the entries in $\mathbf{G}^*$ is lost in the relaxation and so a clustering algorithm like $k$-means is used to determine the patterns.
 <table><tr><td>
__Input__: $n \times n$ adjacency (or similarity) matrix $\mathbf{A}$ for a graph on vertices (or objects) $\{1,\dots,n\}$ and desired number of clusters $k$
<ol>
<li> Compute the _normalized_ Laplacian $\mathscr{L}=\mathbf{D}^{-1/2}\mathbf{L} \mathbf{D}^{-1/2}$.
<li> Compute the first $k$ eigenvectors $\mathbf{V}=[\mathbf{v}_1,\mathbf{v}_2,\dots,\mathbf{v}_k]$ of $\mathscr{L}$ corresponding to the $k$ smallest eigenvalues.
<li> Let $\mathbf{y}_{i}$ be the $i^{th}$ row of $\mathbf{V}$
<li> Cluster the points $\mathbf{y}_i \in \Re^k$ with the $k$-means algorithm into clusters $\bar{C}_1,\dots \bar{C}_k$.
</ol>
__Output__: Clusters $C_1,\dots,C_k$ such that $C_j = \{i : \mathbf{y}_i \in \bar{C}_j\|$
 </table>
 <caption>(\#tab:algncut) Normalized Spectral Clustering (Ncut) [@spectraltutorial] </caption>
 <br>
#### Other Normalized Cuts
While the algorithm in Table \@ref(tab:algncut) carries "normalized cut" in its title, other researchers have suggested alternative ways to consider normalized cuts in a graph. In a popular 2001 paper, Ng, Jordan, and Weiss made a slight alteration of  the previous algorithm which simply normalized the rows of the eigenvector matrix computed in step 2 to have unit length before proceeding to step 3 [@ng]. This algorithm is presented in Table \@ref(tab:algnjw).

 <table><tr><td>

__Input__: $n \times n$ adjacency (or similarity) matrix $\mathbf{A}$ for a graph on vertices (or objects) $\{1,\dots,n\}$ and desired number of clusters $k$
 <ol>
<li> Compute the _normalized_ Laplacian $\mathscr{L}=\mathbf{D}^{-1/2}\mathbf{L} \mathbf{D}^{-1/2}$.
<li> Compute the first $k$ eigenvectors $\mathbf{V}=[\mathbf{v}_1,\mathbf{v}_2,\dots,\mathbf{v}_k]$ of $\mathscr{L}$ corresponding to the $k$ smallest eigenvalues.
<li> Normalize the rows of $\mathbf{V}$ to have unit 2-norm.
<li> Let $\mathbf{y}_{i}$ be the $i^{th}$ row of $\mathbf{V}$
<li> Cluster the points $\mathbf{y}_i \in \Re^k$ with the $k$-means algorithm into clusters $\bar{C}_1,\dots \bar{C}_k$.
</ol>
__Output__: Clusters $C_1,\dots,C_k$ such that $C_j = \{i : \mathbf{y}_i \in \bar{C}_j\|$
 </table>
<caption> (\#tab:algnjw) Normalized Spectral Clustering according to Ng, Jordan and Weiss (NJW) [@spectraltutorial]</caption>
<br>
 
 In 2001, Meila and Shi altered the objective function once again, and derived yet another spectral algorithm using the _normalized random walk_ Laplacian, $\mathscr{L}_{rw} = \mathbf{D}^{-1}\mathbf{L} = \mathbf{I} - \mathbf{D}^{-1} \A$ [@meila]. As shown in [@tutorial], if $\lambda$ is an eigenvalue for $\mathscr{L}$ with corresponding eigenvector $\mathbf{v}$ then $\lambda$ is also an eigenvalue for  $\mathscr{L}_{rw}$ with corresponding eigenvector $\mathbf{D}^{1/2}\mathbf{v}$. This formulation amounts to a different scaling of the eigenvectors in step 3 of Table \@ref(tab:algnjw). This normalized random walk Laplacian will present itself again in Section \@ref(pic).  Meila and Shi's spectral clustering method is outlined in Table \@ref(tab:algmeila).

 
  <table><tr><td>
__Input__: $n \times n$ adjacency (or similarity) matrix $\mathbf{A}$ for a graph on vertices (or objects) $\{1,\dots,n\}$ and desired number of clusters $k$
<ol>
 <li> Compute the _normalized random walk_ Laplacian $\mathscr{L}_{rw}=\mathbf{D}^{-1}\mathbf{L} $.
 <li> Compute the first $k$ eigenvectors $\mathbf{V}=[\mathbf{v}_1,\mathbf{v}_2,\dots,\mathbf{v}_k]$ of $\mathscr{L}_{rw}$ corresponding to the $k$ smallest eigenvalues.
 <li> Normalize the rows of $\mathbf{V}$ to have unit 2-norm.
 <li> Let $\mathbf{y}_{i}$ be the $i^{th}$ row of $\mathbf{V}$
 <li> Cluster the points $\mathbf{y}_i \in \Re^k$ with the $k$-means algorithm into clusters $\bar{C}_1,\dots \bar{C}_k$.
 </ol>
__Output__: Clusters $C_1,\dots,C_k$ such that $C_j = \{i : \mathbf{y}_i \in \bar{C}_j\|$

 </table>
 <caption>(\#tab:algmeila) Normalized Spectral Clustering according to Meila and Shi  </caption>
 <br>
All of the spectral algorithms outlined thus far seem very similar in their formulation, yet in practice they tend to produce quite different results. This presents a problem because while each method has merit in its own right, it is impossible to predict which one will work best on any particular graph. We will discuss this problem further in \cref{consensus}.

### Power Iteration Clustering {#pic}
  In a 2010 paper, Frank Lin and William Cohen propose a fast, scalable algorithm for clustering graphs using the power method (or power iteration) [@poweriteration]. Let $\mathbf{W}=\mathbf{D}^{-1}\A$ be the $n\times n$ row-normalized  (row stochastic) adjacency matrix for a graph, and let $\mathbf{v}_0 \neq 0$ be a vector in $\Re^n$. A simple method for computing the eigenvector corresponding to the largest eigenvalue of $\mathbf{W}$ is the power method, which repeatedly computes the power iteration
  $$\mathbf{v}_{t+1}=c\mathbf{W}\mathbf{v}_t$$
  where $c=1/\|\mathbf{W}\mathbf{v}_t\|_1$ is a normalizing constant to prevent $\mathbf{v}_t$ from growing too large.
  
Applying the power method to convergence on $\mathbf{W}$ would result in the uniform vector $\alpha \e$ where 
$\alpha = 1/n.$  However, stepping through a small number of power iterations, will result in a vector that contains combined information from the eigenvectors associated with the largest eigenvalues. The formulation of Meila and Shi's spectral algorithm in [@meila] warranted the use of the eigenvectors corresponding to the $k$ smallest eigenvalues of the normalized random walk Laplacian $\mathscr{L}_{rw} = \mathbf{I}-\mathbf{W}$ which is equivalent to the consideration of the eigenvectors of the largest eigenvalues of $\mathbf{W}$. Thus, the idea behind Power Iteration Clustering (PIC) is to detect and stop the power method at some number of iterations $t$ such that $\mathbf{v}_t$ is a useful linear combination of the first $k$ eigenvectors.  The analysis in [@poweriteration] motivates the idea that the power method should pass through some initial stage of local convergence at the cluster level before going on to the stage of global convergence toward the uniform vector. At this stopping point, it is expected that $\mathbf{v}_t$ will be an approximately piecewise constant vector, nearly uniform on each of the clusters. Thus, the clusters at this stage will be revealed by the closeness of their corresponding entries in $\mathbf{v}_t$. See [@poweriteration] for the complete analysis. The PIC procedure is given in Table \@ref(tab:algpic). 

Applying the power method to $\mathbf{P}^T$ would equate to watching the probability distribution of a random walker evolve through time steps of the Markov Chain, where $\mathbf{v}_t$ is the distribution at time $t$, and eventually would converge to the stationary distribution in Equation \@ref(eq:pi).  
 However, according to Lin and Cohen, stepping through a limited number of power iterations on $\mathbf{P}$ is equivalent to observing the same chain backwards, so that $\mathbf{v}_t(i)$ gives the observer a sense of the most likely distribution of the chain $t$ steps in the past, given that the walk ended with distribution $\mathbf{v}_0$. On a graph with cluster structure as described above, a random walker that ends up on a particular vertex $j \in C_1$ is more or less equally likely to have come from any other node in $C_1$ (but relatively unlikely to have come from $C_i, i\neq1$), making the distribution close to uniform on the vertices in $C_1$. The same argument is true for any cluster $C_j$ and thus, by stepping backwards through time we expect to find these distribution vectors which are nearly uniform on each cluster $C_j$. For a complete discussion of the algorithm, including a more detailed mathematical analysis, consult [@poweriteration].

<table><tr><td>
__Input:__ A row-stochastic matrix $\mathbf{P}=\mathbf{D}^{-1}\A$ where $\A$ is an adjacency or similarity matrix and the number of clusters $k$.
<ol>
<li> Pick an initial vector $\mathbf{v}_0$. [@poweriteration] suggests the degree vector $\mathbf{v}_0 = \A\e.$
<li> Set $\mathbf{v}_{t+1} = \frac{\mathbf{P}\mathbf{v}_t}{\|\mathbf{P}\mathbf{v}_t\|_1}$ and $\delta_{t+1} = |\mathbf{v}_{t+1}-\mathbf{v}_t|.$
<li> Increment $t$ and repeat step 2 until $|\delta_t-\delta_{t+1}| \simeq \mathbf{0}.$
<li> Use $k$-means to cluster points on $\mathbf{v}_t$ and return clusters $C_1, C_2,\dots,C_k.$
</ol>
__Output:__ Clusters $C_1, C_2,\dots,C_k$.

 </table>
<caption> (\#tab:algpic) Power Iteration Clustering (PIC) [@poweriteration] </caption>
<br>
<!-- %  -->
<!-- % \subsubsection{Spectral Dimension Reduction} -->
<!-- % In light of the discussion in \cref{dimred}, the author prefers to view Algorithms \ref{algratiocut}, \ref{algncut}, and \ref{algnjw} as graph-specialized forms of dimension reduction. Let $\mathbf{L}= \mathbf{D}-\A$ be the Laplacian matrix and let $\sigma(\mathbf{L}') = \{|\lambda_1| \geq |\lambda_2| \geq \dots \geq |\lambda_n|\}$ be the spectrum of $\mathbf{L}$ with corresponding orthonormal eigenvectors $\U=[\uu_1, \uu_2,\dots,\uu_n]$ such that -->
<!-- % $$\mathbf{L}' = \sum_{i=1}^n \lambda_i \uu_i\uu_i^T.$$ -->
<!-- % Then the spectral clustering in Algorithms \ref{algratiocut}, \ref{algncut}, and \ref{algnjw} amount to an unusual truncation of the above sum, containing the terms with the most trivial contribution to the over-all signal: -->
<!-- % $$\mathbf{L}' \approx \sum_{i=K+1}^n \lambda_i \uu_i\uu_i^T.$$ -->
<!-- %  -->
<!-- % This interpretation of spectral clustering not intuitive and I'm hoping that by the time I finish this paper I will have some way to explain it. -->
<!-- %  -->
<!-- %%%%%%%%%%%%%%%%%%%%POINTS OF ARTICULATION%%%%%%%%%%%%%%%% -->
<!-- %Fiedler gave the name \textit{points of articulation} to those vertices $j$ satisfying $\textbf{v}_2(j)=0$ because their removal from the graph (along with all adjacent edges) disconnects the graph into multiple components. In \fref{ptsofart} we try to illustrate this concept with multiple graphs. The vertices in these graphs are labelled according to the sign of the corresponding entry in $\textbf{v}_2$. -->


<!-- %Two of his important results are combined and recast as follows: -->
<!-- %\begin{thm}[Fiedler] -->
<!-- %Let $G$ be a connected graph with Laplacian matrix $\mathbf{L}$. Let $\mathbf{v}_2$ be an eigenvector corresponding to the second smallest eigenvalue of $\mathbf{L}$. If $\mathbf{v}_2$ is nonzero, that is if $\mathbf{v}_2(i) \neq 0 \,\,\, \forall i$, then the subgraphs induced by cutting the edges between vertices in $C_1$ and $C_2$, where  -->
<!-- %$C_1= \{ i \in N : \mathbf{v}_2(i) < 0 \}$ and $C_2=\{ i \in N : \mathbf{v}_2(i) >0 \}$ are both connected. Furthermore if $\mathbf{v}_2(i) = 0$ for some vertices $i$, the subgraph induced by $C_1= \{ i \in N : \mathbf{v}_2(i) \leq 0 \}$ is connected and the subgraph induced by $C_1= \{ i \in N : \mathbf{v}_2(i) \geq 0 \}$ is connected. -->
<!-- %\end{thm} -->
<!-- %This result ensures that we can divide our graph into two (and no more than two) separate components by the following partition rule: -->

### Clustering via Modularity Maximization {#modularity}
 
 Another technique proposed in the network community detection literature compares the structure of a given graph to what one may expect from a random graph on the same vertices [@ncdnewman;@ncdmucha]. The motivation for this method was that simply counting edges between clusters as was done in previous spectral methods may not be the best way to define clusters in graph. A better approach may be to somehow measure whether they are fewer edges than _expected_ between communities. Let $\A$ be the adjacency matrix of the graph (or network) and let $\mathbf{P}$ be the adjacency matrix of a random graph on the same vertices containing the expected value of weights on that graph. Then the matrix $\mathbf{B}=\A-\mathbf{P}$ would contain information about how the structure of $\A$ deviates from what is expected. Obviously this formulation relies on some underlying probability distribution of the weights in the random graph, known as the _null model_. The most common null model uses the degree sequence of the vertices in the given graph, $\{d_1,d_2,\dots,d_n\}$, where $d_i$ is the degree of vertex $i$ (i.e. $d_i$ is the sum of the weights of the edges connected to vertex $i$), to create the probabilities [@ncdmucha,@ncdnewman]
 \begin{equation}
(\#eq:null) 
 p(\mbox{edge}(i,j)) = \frac{d_j}{\sum_{k=1}^n d_k}.
 \end{equation}
 
 Thus, the expected value of the weight of the edge from $i$ to $j$ is
 $$\mathbf{P}_{ij} = E(w(i,j)) = d_i \left(\frac{d_j}{\sum_{k=1}^n d_k}\right).$$
 
 One may recognize that the probabilities in Equation \@ref(eq:null) are precisely the stationary probabilities of the random walk on the graph defined by $\A$, and thus seem a reasonable choice for a null model. This formulation gives us $E(w(i,j)) = E(w(j,i))$ as desired for an undirected graph. Using this null model, a _modularity matrix_ $\mathbf{B}$ is formed as
$$\mathbf{B} = \A - \mathbf{P}.$$

 For a division of the data into two clusters, let $\mathbf{s}$ be an $n\times 1$ vector indicating cluster membership by
$$\mathbf{s}_{i} = 
\begin{cases}
-1 &: \mbox{vertex }  i \mbox{ belongs in cluster 1}\\
\,\,\,\,1 &: \mbox{vertex } i \mbox{ belongs in cluster 2}
\end{cases}.
$$
Let $d=\sum_{k=1}^n d_k$. The __modularity__ of a given partition is defined by
\begin{equation}
(\#eq:modeqn)
Q= \frac{1}{2d} \sum_{i,j} \mathbf{B}_{ij} \mathbf{s}_i\mathbf{s}_j = \frac{1}{2d}\mathbf{s}^T\mathbf{B}\mathbf{s}.
\end{equation}
The goal of the algorithm proposed in [@ncdnewman] is to maximize this quantity, thus we can drop the constant $1/2d$ and write the objective as
\begin{equation}
(\#eq:modobj)
\max_{\substack{\mathbf{s} \\ \mathbf{s}_i = \pm 1}} Q = \mathbf{s}^T \mathbf{B} \mathbf{s}
 \end{equation}

#### Illustrative Example {-}
  To get an idea of why this is true, consider the case where we have two relatively obvious clusters $C_1$ and $C_2$ in a graph and reorder the rows and columns of the adjacency matrix to reflect this structure,
$$
\A=\left[ \begin{array}{cc}
\A_{C_1} & \mathbf{E} \\
\mathbf{E}^T & \A_{C_2} \end{array}\right]
$$
Where $\A_{C_1}$ and $\A_{C_2}$ are relatively dense matrices with larger entries representing the weight of edges within the clusters $C_1$ and $C_2$ respectively and $\mathbf{E}$ is a sparse matrix with smaller entries representing the weight of edges which connect the two clusters. In a random graph with no community or cluster structure, we'd be likely to find just as many edges between the clusters as within clusters. Thus, after subtracting $\mathbf{P}$ our modularity matrix may look something like
$$
\mathbf{B}=\left[ \begin{array}{cc}
\mathbf{B}_{11} & \mathbf{B}_{12} \\
\mathbf{B}_{21} & \mathbf{B}_{22} \end{array}\right]
\approx\left[ \begin{array}{cc}
+ & - \\
- & + \end{array}\right]
$$
Where the indicated signs reflect the sign _tendancy_ of values in $\mathbf{B}$. In other words, the entries in the diagonal blocks $\mathbf{B}_{11}$ and $\mathbf{B}_{22}$ _tend_ to be positive because the edges within clusters had larger weights than one would expect at random and the entries in the off diagonal blocks $\mathbf{B}_{12}$ and $\mathbf{B}_{21}$ _tend_ to be negative because the edges between clusters had smaller weights than one would expect at random. Thus, the modularity of this graph, $\mathbf{Q} = \mathbf{s}^T \mathbf{B} \mathbf{s}$, will be maximized by the appropriate partition $\mathbf{s}^T=[\mathbf{s}^T_1,\mathbf{s}^T_2]=[\e^T_{C_1}, -\e^T_{C_2}]$.


In order to maximize the modularity objective function given in Equation \@ref(eq:modobj), let $\uu_1,\uu_2,\dots,\uu_n$ be an orthonormal set of eigenvectors for $\mathbf{B}$ corresponding respectively to the eigenvalues $\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_n$. Write the vector $\mathbf{s}$ as a linear combination of eigenvectors,
$$\mathbf{s} = \sum_{i=1}^n \alpha_i \uu_i$$ where $$\alpha_i =\uu_i^T \mathbf{s}.$$
Then, the objective function from Equation \@ref(eq:modobj) becomes
$$\max_{\substack{\mathbf{s} \\ \mathbf{s}_i = \pm 1}}  \left(\sum_{i=1}^n \alpha_i \uu_i^T \mathbf{B}\right)\left(\sum_{i=1}^n \alpha_i \uu_i\right) = \sum_{i=1}^n \lambda_i (\uu_i^T \mathbf{S})^2.$$

This optimization is NP-hard due to the constraint that $\mathbf{s}_i =\pm 1$. It is clear that without this constraint one would choose $\mathbf{s}$ proportional to $\uu_1$, maximizing the first term in the summation (associated with the largest eigenvalue) and terminating the others. A reasonable way to proceed in light of this information is to maximize the leading term and ignore the remaining terms. To accomplish this, it is quite clear that we should choose $\mathbf{s}$ so that its entries match the signs of the entries in $\uu_1$. The placement of vertices corresponding to zero entries in $\uu_1$ will be decided arbitrarily. However, if the leading eigenvalue of the modularity matrix is _negative_ then the corresponding eigenvector is $\e$, leading us to no partition. According to [@ncdnewman] the ``no partition'' solution in this scenario is in fact the correct result, i.e. a negative leading eigenvalue indicates there is no community or cluster structure in the graph. This gives a clear stopping point for the procedure, which allows it to automatically determine an appropriate number of clusters or communities to create. Unfortunately, the arbitrary placement of vertices corresponding to zero entries in $\uu_1$ may in some cases affect the determined number of clusters.

To create more than 2 clusters, the above procedure can be repeated on each of the subgraphs induced by the vertices in each cluster found. This leads us to an iterative divisive (hierarchical) algorithm like the iterative Fiedler method in Section \@ref(extendedfiedler) and PDDP in Section \@ref(pddp). The modularity clustering procedure is formalized in Table \ref{algmod}.

<table><tr><td>
__Input:__ $n \times n$ adjacency matrix $\A$ for an undirected graph to be partitioned
<ol>
<li> Let $d_i$ be the $i^{th}$ row sum of $\A$. Let $d=\sum_{i=1}^n d_i$
<li> Form the matrix $\mathbf{P}$ with $\mathbf{P}_{ij}=d_i d_j / d$.
<li> Form the modularity matrix $\mathbf{B}=\A-\mathbf{P}$.
<li> Compute the largest eigenvalue $\lambda_1$ and corresponding eigenvector $\uu_1$ of $\mathbf{B}$.
<li> If $\lambda_1 < 0$, stop. There is no partition of this graph.
<li> Otherwise partition the vertices of the graph into 2 clusters as follows
\begin{equation}
(\#eq:modsplit)
\begin{split}
C_1 &= \{i : \uu_1(i) <0\} \cr
C_2 &= \{i : \uu_1(i) \geq 0\}
\end{split}
\end{equation}
<li> Determine further partitions by extracting the rows and columns of the original adjacency matrix corresponding to the vertices in each cluster to form $\A'$ and repeat the algorithm with $\A'$ until each created cluster fails to partition in step 5.
</ol>
 __Output:__ Final clusters.
</table>
<caption>(\#tab:algmod) Modularity Procedure for Network Community Detection (Newman) [@ncdnewman]</caption>
<br>
 

## Stochastic Clustering
  
  An alternative way to interpret a graph is by considering a random walk along the edges. For an undirected graph with adjacency matrix $\A$, we can create a transition probability matrix $\mathbf{P}$ by dividing each row by the corresponding row sum. Using the degree matrix from Definition \@ref(def:laplaciandef) we have $$\mathbf{P}=\mathbf{D}^{-1}\A.$$ If our graph does indeed have some cluster structure, i.e. sets of vertices $C_1,C_2, \dots,C_k$ for which the total weight of edges within each set are substantially higher than the total weight of edges between the different sets, then a random walker in a given cluster $C_i$ is more likely to stay in $C_i$ for several steps than he is to transition to another cluster $C_j$. It is well known that for a connected and undirected graph, the long term probability distribution is given by
\begin{equation}
(\#eq:pi)
\pi^T = \frac{\e^T\mathbf{D}}{\e^T\mathbf{D}\e^T}
\end{equation}
  Which is not likely to give any cluster information. However, the short-term evolution of this walk can tell us something about the cluster structure because a random walker is far more likely, in the short-run, to remain inside a cluster than he is to transition between clusters. The Stochastic Clustering Algorithm (SCA) of Wessell and Meyer [@chuck] takes advantage of this fact.

### Stochastic Clustering Algorithm (SCA)
  
  In a 2012 paper, Chuck Wessell and Carl Meyer formulated a clustering model by creating a symmetric (doubly stochastic) transition matrix $\mathbf{P}$ [@chuck;@chuckthesis] from the adjacency matrix of a graph. The method in this paper is quite similar to that in PIC except that here the mathematics of the ``backward'' Markov Chain intuition given in [@poweriteration] works out in this context because the probability transition matrix is symmetric. One added feature in this algorithm is the automatic determination of the number of clusters in the data, using eigenvalues of the transition matrix $\mathbf{P}$.  Wessell and Meyer's formulation is based on theory that was developed by Nobel Laureate economist Herbert Simon and his student Albert Ando.  This theory surrounds the mixing rates of resources or wealth in local economies (composed of states in a Markov chain) as part of a global economy (which links together some states from each local economy). It is assumed that the adjacency matrix for the graph is irreducible, or equivalently that the graph is connected. 
  
  The basic idea is that resources will be exchanged more frequently at a local level than they will at the global level. Suppose individual companies from a global economy are represented as nodes in a graph with edges between them signifying the amount of trade between each pair of companies. Natural clusters would form in this graph at a local level, represented by the strong and frequent trade relationships of proximal companies. Let $k$ be the number of local economies (clusters), each containing $n_i$ states $i=1,\dots,k$, and define the distribution of resources at time $t$ as $\mathbf{\pi}_t$, given a starting distribution $\mathbf{\pi}_0$. Then
  $$\mathbf{\pi}_t^T = \mathbf{\pi}_0^T\mathbf{P}^t$$
  
   The heavily localized trade in this global economy leads to a so-called _short-term stabilization_ of the system characterized by a distribution vector at some time $t$ which is nearly constant across each local economy:
$$\mathbf{\pi}_t^T \approx \left( \frac{\alpha_1}{n_1}  \frac{\alpha_1}{n_1}   \dots \frac{\alpha_1}{n_1}   | \frac{\alpha_2}{n_2}   \frac{\alpha_2}{n_2}   \dots \frac{\alpha_2}{n_2}   | \dots | \frac{\alpha_k}{n_k}   \frac{\alpha_k}{n_k}   \dots \frac{\alpha_k}{n_k} \right)$$
   After this short-term stabilization, the distribution of goods in the Markov Chain is eventually expected to converge to a constant level across every state. However, in the period following the short-run stabilization, the distribution vector retains its approximately piecewise constant structure for a some time before settling down into its final uniform equilibrium.

Wessell and Meyer's derivation requires the creation of a symmetric probability transition matrix $\mathbf{P}$ from the adjacency matrix $\A$ by means of a simultaneous row and column scaling. In other words, a diagonal matrix $\mathbf{S}$ is determined for which
$$\mathbf{S}\A\mathbf{S}= \mathbf{P}$$
is a doubly stochastic transition probability matrix.  This task turns out to be quite simple, $\mathbf{S}$ is found by iterating a single step until convergence. Letting $\mathbf{S}_{ii}=\mathbf{s}(i)$, the diagonal scaling procedure put forth by Ruiz [@ruiz} is simply:
\begin{equation}
(\#eq:diagscaling)
\begin{split}
\mathbf{s}_0 &= \e \\
\mathbf{s}_{t+1}(i) &=\sqrt{\frac{\mathbf{s}_t(i)}{\A_{i*}^T\mathbf{s}_t}}
\end{split}
\end{equation}
 
 In [@chuckthesis], it is convincingly argued that the diagonal scaling procedure does not change the underlying cluster structure of the data in $\A$, and thus that the desired information is not damaged by this transformation.  The clusters in this method are found in a similar manner to PIC, where $k$-means is employed to find the nearly piecewise constant segments of the distribution vector $\mathbf{\pi}_t$ after a short number of steps. The Stochastic Clustering Algorithm automatically determines the number of clusters in the data by counting the number of eigenvalues whose value is close to $\lambda_1=1$. This group of eigenvalues near 1 is referred to as the _Perron cluster_. We postpone discussion of this matter to \cref{findk} where it will be analyzed in detail. For now, we present the Stochastic Clustering method in Table \@ref(tab:algsc). The eigenvector iterations in SCA are quite similar to those put forth in PIC, and users commonly create visualizations of the iterations that look quite similar to those in Figure \@ref(fig:picex).
 
 <table><tr><td>
__Input:__ Adjacency matrix $\A$ for some graph to be partitioned
<ol>
<li> Convert $\A$ to a symmetric probability transition matrix $\mathbf{P}$ using the diagonal scaling procedure given in \@ref(eq:diagscaling).
<li> Calculate the eigenvalues of $\mathbf{P}$ and determine $k$ to be the number of eigenvalues in the Perron cluster.
<li> Create a random initial probability distribution $\mathbf{\pi}_0^T$.
<li> Track the evolution of $\mathbf{\pi}_{t+1}^T = \mathbf{\pi}_t^T \mathbf{P}$. After each multiplication, cluster the entries of $\mathbf{\pi}_t$ using $k$-means. When this clustering has remained the same for a user-preferred number of iterations stop.
</ol>
__ Output:__ $k$ clusters found $C_1, C_2,\dots, C_k$.
 </table>
<caption> (\#tab:algsc) Stochastic Clustering Algorithm (SCA) [@chuck]</caption>
<br>
 

<!--chapter:end:112-NetworkAlgos.Rmd-->

# References

<div id="refs"></div>

<!--chapter:end:99-References.Rmd-->

